{
    "version": "https://jsonfeed.org/version/1",
    "title": "QWERTY • All posts by \"cfr\" tag",
    "description": "Programming | Computer Science | Thought",
    "home_page_url": "http://gitqwerty777.github.io",
    "items": [
        {
            "id": "http://gitqwerty777.github.io/Counterfactual-Regret-Minimization/",
            "url": "http://gitqwerty777.github.io/Counterfactual-Regret-Minimization/",
            "title": "CFR(Counterfactual Regret Minimization) 演算法簡介",
            "date_published": "2015-06-15T03:49:49.000Z",
            "content_html": "<!-- RENEW: -->\n\n<p>參考作者在Quora上的解釋<br>  <a id=\"more\"></a></p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>self-learning algorithm  </p>\n<ul>\n<li>learns strategy by repeatedly playing against itself</li>\n<li>initialized with uniformly random<ul>\n<li>playing every action at every decision point with <strong>equal</strong> probability</li>\n</ul>\n</li>\n<li>play the action with maximum regret</li>\n<li>it will converge to optimal strategy that can do no worse than tie against any opponent</li>\n</ul>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><ul>\n<li><p>Summing total regret for each action at each decision point</p>\n<ul>\n<li>regret: how much better if just <strong>always</strong> played this one action at this decision, instead of previous choices?<ul>\n<li>Positive regret means that we would have done better if we had taken that action more often</li>\n<li>Negative regret means that we would have done better by not taking that action at all</li>\n<li>愈多regret，代表此選項要多選</li>\n</ul>\n</li>\n<li>do actions with probabilities proportional to their positive regret</li>\n<li>after each game, update regret values </li>\n</ul>\n</li>\n<li><p>Counter-intuitively, sequence of strategies <strong>does not necessarily converge to anything useful</strong></p>\n<ul>\n<li>But it now does so in practice</li>\n<li><strong>in a two-player zero-sum game</strong>, if you compute the average strategy over those billions of strategies in the sequence, then that average strategy will converge towards <strong>Nash equilibrium</strong> of the game</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Nash-equilibrium-納許均衡\"><a href=\"#Nash-equilibrium-納許均衡\" class=\"headerlink\" title=\"Nash equilibrium(納許均衡)\"></a>Nash equilibrium(納許均衡)</h2><ul>\n<li>Do no worse than tie against any other strategy</li>\n<li>Plays perfect defence<ul>\n<li>Just wins when the opponent makes mistakes<ul>\n<li>since attempting to find and exploit an opponent’s mistakes usually makes it possible for an even smarter opponent to exploit your new strategy</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>exploitability(利用度)<ul>\n<li>maximum expectation that a perfect counter-strategy could win </li>\n<li>exploitability = 0 when Nash equilibrium</li>\n<li>CFR can make average strategy’s exploitability converges towards zero</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><ul>\n<li>best poker programs started beating the world’s best human players in heads-up limit hold’em in 2008, even though there were still massively exploitable by this worst-case measure</li>\n<li>In January 2015, we’ve essentially <strong>weakly solved</strong> the game<ul>\n<li>a strategy with such a low exploitability (0.000986 big blinds per game)<ul>\n<li>have 95% statistical confidence that they were actually winning against everyone</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Algorithm-Implementation\"><a href=\"#Algorithm-Implementation\" class=\"headerlink\" title=\"Algorithm Implementation\"></a>Algorithm Implementation</h2><p>待補充</p>\n<h2 id=\"Example-Code\"><a href=\"#Example-Code\" class=\"headerlink\" title=\"Example Code\"></a>Example Code</h2><p>待補充</p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>待補充</p>\n<h2 id=\"參考資料\"><a href=\"#參考資料\" class=\"headerlink\" title=\"參考資料\"></a>參考資料</h2><ul>\n<li><span class=\"exturl\" data-url=\"aHR0cDovL3Bva2VyLmNzLnVhbGJlcnRhLmNhL3B1YmxpY2F0aW9ucy9OSVBTMDctY2ZyLnBkZg==\">(CFR)Regret Minimization in Games with Incomplete Information<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cDovL2FyeGl2Lm9yZy9wZGYvMTQwNy41MDQyLnBkZg==\">(CFR+)Solving Large Imperfect Information Games Using CFR+<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5xdW9yYS5jb20vV2hhdC1pcy1hbi1pbnR1aXRpdmUtZXhwbGFuYXRpb24tb2YtY291bnRlcmZhY3R1YWwtcmVncmV0LW1pbmltaXphdGlvbg==\">(CFR)Explanation of CFR by inventor himself<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cDovL3NwZWN0cnVtLmllZWUub3JnL3JvYm90aWNzL2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL3Bva2VycGxheWluZy1haXMtdG9kYXktc2t5bmV0LXRvbW9ycm93\">poker AI news<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cDovL3NwZWN0cnVtLmllZWUub3JnL3RlY2gtdGFsay9jb21wdXRpbmcvc29mdHdhcmUvY29tcHV0ZXJzLWNvbnF1ZXItdGV4YXMtaG9sZGVtLXBva2VyLWZvci1maXJzdC10aW1l\">poker AI news2<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cDovL3Bva2VyLmNzLnVhbGJlcnRhLmNhL29wZW5fY2ZyLmh0bWw=\">(Implementation)openCFR<i class=\"fa fa-external-link-alt\"></i></span></li>\n</ul>\n",
            "tags": [
                "CFR",
                "電腦對局理論"
            ]
        }
    ]
}