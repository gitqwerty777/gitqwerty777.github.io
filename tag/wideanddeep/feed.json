{
    "version": "https://jsonfeed.org/version/1",
    "title": "QWERTY • All posts by \"wideanddeep\" tag",
    "description": "Programming | Computer Science | Thought",
    "home_page_url": "http://gitqwerty777.github.io",
    "items": [
        {
            "id": "http://gitqwerty777.github.io/recommender-wide-and-deep/",
            "url": "http://gitqwerty777.github.io/recommender-wide-and-deep/",
            "title": "Wide And Deep 論文簡介：快思慢想的神經網路版",
            "date_published": "2021-12-20T07:18:44.000Z",
            "content_html": "<h2 id=\"簡介\"><a href=\"#簡介\" class=\"headerlink\" title=\"簡介\"></a>簡介</h2><p>Wide And Deep 模型由簡單的Wide模型和複雜的Deep模型組成</p>\n<a id=\"more\"></a>\n\n<ul>\n<li>Wide<ul>\n<li>Memorization(記憶)<ul>\n<li><strong>Generalized linear model</strong>(e.g., Linear Regression Model)</li>\n<li>適合學習稀疏、簡單的規則<ul>\n<li>看了 A 電影的使用者經常喜歡看電影 B，這種「因為 A 所以 B」式的規則</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>從歷史資料學習規則(exploit)</li>\n<li>讓模型記住大量的直接且重要的規則，這正是單層的線性模型所擅長的</li>\n</ul>\n</li>\n<li>Deep<ul>\n<li>Generalization(泛化)<ul>\n<li><strong>Embedding-based models</strong>(e.g., Deep Neural Network)</li>\n<li>適合學習通用、深層的規則</li>\n</ul>\n</li>\n<li>學習新的特徵組合(explore)</li>\n</ul>\n</li>\n<li>合併 Wide and Deep(Jointly Training) <img data-src=\"/img/recommend/wide-and-deep.png\" alt=\"\"><ul>\n<li>既能快速處理和記憶大量歷史行為特徵，又具有強大的表達能力</li>\n<li>和 Deep-only 比: 準確率高</li>\n<li>和 Wide-only 比: 更好的泛化規則</li>\n</ul>\n</li>\n<li>當user-item matrix非常稀疏時，例如有獨特愛好的users以及很小眾的items，NN很難為users和items學習到有效的embedding。導致over-generalize，並推薦不怎麼相關的物品。此時Memorization就展示了優勢，它可以「記住」這些特殊的特徵組合</li>\n</ul>\n<h2 id=\"實作\"><a href=\"#實作\" class=\"headerlink\" title=\"實作\"></a>實作</h2><h3 id=\"Wide\"><a href=\"#Wide\" class=\"headerlink\" title=\"Wide\"></a>Wide</h3><ul>\n<li>$y = w^Tx+b$</li>\n<li>Cross product transformation<ul>\n<li><img data-src=\"/img/recommend/wide-and-deep-cross-product.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Optimizer: Follow-the-regularized-leader (FTRL) + L1 regularization<ul>\n<li>FTRL with L1非常注重模型的稀疏性。採用L1 FTRL是想讓Wide部分變得更加稀疏<ul>\n<li>但是兩個id類特徵向量進行組合，在維度爆炸的同時，會讓原本已經非常稀疏的multihot特徵向量，變得更加稀疏。正因如此，wide部分的權重數量其實是海量的。為了不把數量如此之巨的權重都搬到線上進行model serving，採用FTRL過濾掉哪些稀疏特徵無疑是非常好的工程經驗</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Wide的輸入特徵較少<ul>\n<li>只有已安裝app和瀏覽過的app</li>\n<li>希望能充份發揮Wide記憶能力強的優勢</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Deep\"><a href=\"#Deep\" class=\"headerlink\" title=\"Deep\"></a>Deep</h3><ul>\n<li>特徵(節錄)<ul>\n<li>用戶特徵<ul>\n<li>年齡、國家、語言</li>\n<li>行為特徵<ul>\n<li>已安裝App個數</li>\n<li>已安裝的App</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>情境特徵<ul>\n<li>使用裝置</li>\n<li>目前時間(星期，小時)</li>\n</ul>\n</li>\n<li>App特徵<ul>\n<li>發佈時間</li>\n<li>下載數</li>\n</ul>\n</li>\n<li>候選App</li>\n<li>部份特徵有做embedding(Wide完全沒有)<ul>\n<li>32 dimension</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Optimizer: AdaGrad</li>\n</ul>\n<h3 id=\"Jointly-Training\"><a href=\"#Jointly-Training\" class=\"headerlink\" title=\"Jointly Training\"></a>Jointly Training</h3><ul>\n<li>同時更新Wide和Deep的權重<ul>\n<li><img data-src=\"/img/recommend/wide-and-deep-joint-train.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>結構圖<ul>\n<li><img data-src=\"/img/recommend/wide-and-deep-features.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"結果\"><a href=\"#結果\" class=\"headerlink\" title=\"結果\"></a>結果</h2><p>實際用在 Google Play Store App 推薦</p>\n<p><img data-src=\"/img/recommend/wide-and-deep-exp.png\" alt=\"\"></p>\n<ul>\n<li>Deep雖然離線結果較差，但實際結果仍比Wide好<ul>\n<li>深層模型有學習到使用者的隱含喜好，而非直接記憶規則</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"心得\"><a href=\"#心得\" class=\"headerlink\" title=\"心得\"></a>心得</h2><p>這就是<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cubWFuYWdlcnRvZGF5LmNvbS50dy9hcnRpY2xlcy92aWV3LzUwOTA1Pw==\">快思慢想<i class=\"fa fa-external-link-alt\"></i></span>的神經網路版</p>\n<p>Wide處理簡單的規則且省力，Deep處理複雜的規則但費力</p>\n<p>和純粹的deep learning相比，適合需要記憶大量簡易規則的情境。如App推薦中，有安裝A就推薦B</p>\n<p>Wide and Deep是一個架構，Wide模型和Deep模型可以為任意實作，所以衍生出許多變形，如DeepFM, Deep and Cross等</p>\n<!--\ndeep的效率跟不上，可以固定住deep，對wide進行online learning來增強記憶性。\n非常贊 跟我們的討論結果基本一致，deep部分做batch update保證准確性和充足表達能力，wide部分做online learning保證實效性。\n\n用戶-物品互動太少 → over-generalize\nwide部分的引入是為瞭解決 niche items的問題，對於很長尾的物品，dense features是沒法學到什麼東西的\n\nHowever,deep neural networks with embeddings can over-generalize\nand recommend less relevant items when the user-item inter-\nactions are sparse and high-rank.\n當user-item matrix非常稀疏時，例如有和獨特愛好的users以及很小眾的items，NN很難為users和items學習到有效的embedding。這種情況下，大部分user-item應該是沒有關聯的，但dense embedding 的方法還是可以得到對所有 user-item pair 的非零預測，因此導致 over-generalize並推薦不怎麼相關的物品。此時Memorization就展示了優勢，它可以“記住”這些特殊的特徵組合。\nhttps://en.wikipedia.org/wiki/Rank_(linear_algebra)\n-->\n\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE2MDYuMDc3OTI=\">Cheng, Heng-Tze, et al. “Wide &amp; deep learning for recommender systems.” Proceedings of the 1st workshop on deep learning for recommender systems. 2016.<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tZWRpdW0uY29tL2RhdGEtc2NpZW50aXN0cy1wbGF5Z3JvdW5kL3dpZGUtZGVlcCVFNiVBOCVBMSVFNSU5RSU4Qi0lRTYlOEUlQTglRTglOTYlQTYlRTclQjMlQkIlRTclQjUlQjEtJUU1JThFJTlGJUU3JTkwJTg2LThiYWRhY2Y3NzdmMw==\">https://medium.com/data-scientists-playground/wide-deep%E6%A8%A1%E5%9E%8B-%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1-%E5%8E%9F%E7%90%86-8badacf777f3<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xNDI5NTg4MzQ=\">https://zhuanlan.zhihu.com/p/142958834<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81MzM2MTUxOQ==\">https://zhuanlan.zhihu.com/p/53361519<i class=\"fa fa-external-link-alt\"></i></span></li>\n</ul>\n",
            "tags": [
                "推薦系統",
                "WideAndDeep",
                "Google"
            ]
        }
    ]
}