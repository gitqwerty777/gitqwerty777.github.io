{
    "version": "https://jsonfeed.org/version/1",
    "title": "QWERTY • All posts by \"自然語言處理\" tag",
    "description": "Programming | Computer Science | Thought",
    "home_page_url": "http://gitqwerty777.github.io",
    "items": [
        {
            "id": "http://gitqwerty777.github.io/foreign-language-police/",
            "url": "http://gitqwerty777.github.io/foreign-language-police/",
            "title": "如何當稱職的支語警察？",
            "date_published": "2020-09-13T09:25:34.000Z",
            "content_html": "<p><img data-src=\"/img/Other/foreign-terms-police.png\" alt=\"\"></p>\n<h2 id=\"簡介\"><a href=\"#簡介\" class=\"headerlink\" title=\"簡介\"></a>簡介</h2><p>PTT 上常有支語警察</p>\n<p>也常發現 google 翻譯的中文常常會<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuc2V0bi5jb20vTmV3cy5hc3B4P05ld3NJRD03NDY3OTM=\">翻譯成中國慣用語<i class=\"fa fa-external-link-alt\"></i></span></p>\n<p>所以就寫了一個支語警察的網站：<a href=\"/foreign-terms-police\">支語警察</a></p>\n<a id=\"more\"></a>\n\n<h2 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a>流程</h2><ol>\n<li>文章分詞</li>\n<li>標出非台灣習慣用語</li>\n<li>從資料庫尋找對應的用語</li>\n<li>若無對應用語則尋找其解釋</li>\n</ol>\n<h2 id=\"資料庫\"><a href=\"#資料庫\" class=\"headerlink\" title=\"資料庫\"></a>資料庫</h2><p>基本上都是使用現有的詞典，其資料都是人工更新的，<strong>目前還沒有自動辨識習慣用語的方法</strong>。</p>\n<h3 id=\"萌典\"><a href=\"#萌典\" class=\"headerlink\" title=\"萌典\"></a>萌典</h3><p>從萌典的兩岸字典獲取用語中台灣和中國的意思不同者(同字不同義，同義不同字)</p>\n<h3 id=\"開放中文轉換-OpenCC\"><a href=\"#開放中文轉換-OpenCC\" class=\"headerlink\" title=\"開放中文轉換 OpenCC\"></a>開放中文轉換 OpenCC</h3><p>支持詞彙級別的轉換、異體字轉換和地區習慣用語及字體轉換（中國大陸、臺灣、香港、日本）。</p>\n<p>使用詞典對應的方式實作，像是簡體轉繁體就使用<span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL0JZVm9pZC9PcGVuQ0MvYmxvYi9tYXN0ZXIvZGF0YS9kaWN0aW9uYXJ5L1NUUGhyYXNlcy50eHQ=\">此詞典<i class=\"fa fa-external-link-alt\"></i></span></p>\n<h2 id=\"實作\"><a href=\"#實作\" class=\"headerlink\" title=\"實作\"></a>實作</h2><ul>\n<li>支援單字或文章檢測</li>\n<li>Google Trends<ul>\n<li>使用台灣的搜尋趨勢比較原本用語及對應的外來用語</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"困難\"><a href=\"#困難\" class=\"headerlink\" title=\"困難\"></a>困難</h3><ol>\n<li>資料庫通常沒有收錄網路流行語</li>\n<li>同詞不同義<ol>\n<li>質量：中國常用於表示「品質」，但兩岸都有「重量」的意思 <span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucHR0LmNjL2Jicy9DX0NoYXQvTS4xNTk1NTU0NDM0LkEuNjc2Lmh0bWw=\">範例<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li>土豆：中國為「馬鈴薯」，台灣為「花生」</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"參考資料\"><a href=\"#參考資料\" class=\"headerlink\" title=\"參考資料\"></a>參考資料</h2><ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cubW9lZGljdC50dy8=\">萌典<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL0JZVm9pZC9PcGVuQ0M=\">OpenCC<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cucHR0LmNjL2Jicy9DX0NoYXQvTS4xNTk1NTEyNDgwLkEuNTY5Lmh0bWw=\">[閒聊] 支語警察是誰畫的？<i class=\"fa fa-external-link-alt\"></i></span></li>\n</ul>\n",
            "tags": [
                "自然語言處理",
                "外國用語",
                "萌典",
                "opencc",
                "PTT",
                "vuejs"
            ]
        },
        {
            "id": "http://gitqwerty777.github.io/natural-language-processing2/",
            "url": "http://gitqwerty777.github.io/natural-language-processing2/",
            "title": "自然語言處理(下)",
            "date_published": "2015-05-01T04:37:47.000Z",
            "content_html": "<h2 id=\"Chap08-Syntax-and-Grammars\"><a href=\"#Chap08-Syntax-and-Grammars\" class=\"headerlink\" title=\"Chap08 Syntax and Grammars\"></a>Chap08 Syntax and Grammars</h2><p>Grammar    </p>\n<ul>\n<li>represent certain knowledges of what we know about a language</li>\n<li>General criteria<ul>\n<li>linguistic naturalness</li>\n<li>mathematical power</li>\n<li>computational effectiveness</li>\n</ul>\n</li>\n</ul>\n<p>Context-free grammars(CFG)</p>\n<ul>\n<li>Alias<ul>\n<li>Phrase structure grammars</li>\n<li>Backus-Naur form</li>\n</ul>\n</li>\n<li>More powerful than finite state machine</li>\n<li>Rules <ul>\n<li>Terminals <ul>\n<li>words</li>\n</ul>\n</li>\n<li>Non-terminals <ul>\n<li>Noun phrase, Verb phrase …</li>\n</ul>\n</li>\n<li>Generate strings in the language</li>\n<li>Reject strings not in the language  </li>\n<li>LHS → RHS<ul>\n<li>LHS: Non-terminals </li>\n<li>RHS: Non-terminals or Terminals</li>\n</ul>\n</li>\n<li>Context Free<ul>\n<li>probability of a subtree does not depend on words not dominated by the subtree(subtree出現的機率和上下文無關)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<a id=\"more\"></a>\n\n<p>Evaluation  </p>\n<ol>\n<li>Does it undergenerate?<ul>\n<li>rules cannot completely explain language</li>\n<li>not a problem if the goal is to produce a language</li>\n</ul>\n</li>\n<li>Does it overgenerate?<ul>\n<li>rules overly explain the language</li>\n<li>not a problem if the goal is to recognize or understand well-formed(correct) language</li>\n</ul>\n</li>\n<li>Does it assign appropriate structures to the strings that it generates?</li>\n</ol>\n<p>Parsing  </p>\n<ul>\n<li>take a string and a grammar</li>\n<li>assigning trees that covers all and only words in input strings</li>\n<li>return parse tree for that string</li>\n</ul>\n<p>English Grammar Fragment  </p>\n<ul>\n<li>Sentences</li>\n<li>Noun Phrases<ul>\n<li>Ex. NP → det Nominal</li>\n<li><strong>head: central criticial noun in NP</strong><ul>\n<li>important in statistical parsing</li>\n<li>after det(冠詞), before pp(介系詞片語) <img data-src=\"/img/NLP/np-parse.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Agreement<ul>\n<li>a part of overgenerate</li>\n<li>This flight(○)</li>\n<li>This flights(×)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Verb Phrases<ul>\n<li>head verb with arguments</li>\n<li>Subcategorization(分類)<ul>\n<li>categorize according to VP rules</li>\n<li>a part of overgenerate</li>\n<li>Prefer<ul>\n<li>I prefer [to leave earlier]TO-VP</li>\n</ul>\n</li>\n<li>Told<ul>\n<li>I was told [United has a flight]S</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Overgenerate Solution<br>    - transform into multiple rules<br>        - NP → Single_Det Single_Nominal<br>        - NP → 複數_Det 複數_Nominal<br>        - Will generate a lot of rules!<br>    - out of CFG framework<br>        - Chomsky hierarchy of languages <img data-src=\"/img/NLP/modelclass.png\" alt=\"\"></p>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL3poLndpa2lwZWRpYS5vcmcvd2lraS8lRTklOTklODQlRTYlQTAlODclRTglQUYlQUQlRTglQTglODA=\">Indexed Grammar<i class=\"fa fa-external-link-alt\"></i></span>  </p>\n<ul>\n<li>Indexed grammars and languages problem <img data-src=\"/img/NLP/index-example.png\" alt=\"\"> </li>\n<li>recognized by nested stack automata <img data-src=\"/img/NLP/index-grammar.png\" alt=\"\"></li>\n</ul>\n<h3 id=\"Treebanks-and-headfinding\"><a href=\"#Treebanks-and-headfinding\" class=\"headerlink\" title=\"Treebanks and headfinding\"></a>Treebanks and headfinding</h3><p>critical to the development of statistical parsers</p>\n<p>Treebanks  </p>\n<ul>\n<li>corpora with parse trees<ul>\n<li>created by automatic parser and human annotators</li>\n</ul>\n</li>\n<li>Ex. Penn Treebank</li>\n<li>Grammar<ul>\n<li>Treebanks implicitly define a grammar<ul>\n<li>Simply make all subtrees fit the rules</li>\n</ul>\n</li>\n<li>parse tree tend to be very flat to avoid recursion<ul>\n<li>Penn Treebank has ~4500 different rules for VPs</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Head Finding  </p>\n<ul>\n<li>use tree traversal rules specific to each non-terminal in the grammar</li>\n<li>先向右再向左 <img data-src=\"/img/NLP/head-np.png\" alt=\"\"><!--重要--></li>\n</ul>\n<h3 id=\"Dependency-Grammars\"><a href=\"#Dependency-Grammars\" class=\"headerlink\" title=\"Dependency Grammars\"></a>Dependency Grammars</h3><ul>\n<li>every possible parse is a tree <img data-src=\"/img/NLP/dependency-parse.png\" alt=\"\"><ul>\n<li>every node is a word </li>\n<li>every link is dependency relations between words </li>\n</ul>\n</li>\n<li>Advantage<ul>\n<li>Deals well with long-distance word order languages <ul>\n<li>structure is flexible</li>\n</ul>\n</li>\n<li>Parsing is much faster than CFG</li>\n<li>Tree can be used by later applications</li>\n</ul>\n</li>\n<li>Approaches<!--重要--><ul>\n<li>Optimization-based approaches <ul>\n<li>search for the tree that matches some criteria the best</li>\n<li>spanning tree algorithms</li>\n</ul>\n</li>\n<li>Shift-reduce approaches<ul>\n<li>greedily take actions based on the current word and state</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Summary  </p>\n<ul>\n<li>Constituency(顧客, words that behave as a single unit) is a key phenomena easily captured with CFG rules<ul>\n<li>But agreement and subcategorization make problems</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap09-Syntactic-Parsing\"><a href=\"#Chap09-Syntactic-Parsing\" class=\"headerlink\" title=\"Chap09 Syntactic Parsing\"></a>Chap09 Syntactic Parsing</h2><ul>\n<li>Top-Down Search <img data-src=\"/img/NLP/top-down.png\" alt=\"\"><ul>\n<li>Search trees among possible answers  <!--- But suggests trees that are not consistent with words--></li>\n</ul>\n</li>\n<li>Bottom-Up Parsing<ul>\n<li>Only forms trees that can fit the words <!-- global tree may not form answer(S) --></li>\n</ul>\n</li>\n<li>Mixed parsing strategy<ul>\n<li>looks like Binomial Search</li>\n<li>The number of rules tried at each deicision of the analysis (branching factor)<ul>\n<li>top-down parsing: categories of LHS(Left Hand Side) word</li>\n<li>bottom-up parsing: categories of left most RHS(Right Hand Side) word<ul>\n<li>倒推：從最左邊可以倒推的字開始</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>backtracking methods are doomed because of two inter-related problems  </p>\n<ul>\n<li>(1)Structural and lexical ambiguity<ul>\n<li>PP(介系詞片語) attachment<ul>\n<li>PP can attach to [sentences, verb phrases, noun phrases, and adjectival phrases]</li>\n</ul>\n</li>\n<li>coordination<ul>\n<li>P and Q or R <ul>\n<li>P and (Q or R)</li>\n<li>(P and Q) or R</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>noun-noun compounding<ul>\n<li>town widget hammer<ul>\n<li>((town widget) hammer)</li>\n<li>(town (widget hammer))</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Solution<ul>\n<li>how to determine the intended structure?</li>\n<li>how to store the partial structures?</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>(2)Shared subproblems<ul>\n<li>naïve backtracking will lead to duplicated work(不一定會對，所以會一直backtrack…)</li>\n</ul>\n</li>\n</ul>\n<p>Dynamic Programming  </p>\n<ul>\n<li>Avoid doing repeated work</li>\n<li>Solve in polynomial time</li>\n<li>approaches<ul>\n<li>CKY</li>\n<li>Earley</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"CKY-bottom-up\"><a href=\"#CKY-bottom-up\" class=\"headerlink\" title=\"CKY(bottom-up)\"></a>CKY(bottom-up)</h3><ul>\n<li>transform rules into Chomsky-Normal Form <img data-src=\"/img/NLP/cnf-transform.png\" alt=\"\"></li>\n<li>build a table <ul>\n<li>A spanning from i to j in the input is in [i,j]</li>\n<li>A → BC == [i,j] → [i,k] [k,j]</li>\n<li>entire string = [0, n] <ul>\n<li>expected to be S</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>iterate all possible k <img data-src=\"/img/NLP/CKY-table2.png\" alt=\"\"><ul>\n<li>[i,j] = [i,i+1]x[i+1, j], [i,i+2]x[i+2,j] ……</li>\n</ul>\n</li>\n<li>fill the table a column at a time, from left to right, bottom to top <img data-src=\"/img/NLP/CKY-table3.png\" alt=\"\"><ul>\n<li>Ex. [1,3] = [1,2]Det + [2,3] Nomimal, Noun = NP</li>\n<li>Ex. <img data-src=\"/img/NLP/CKY-ex.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Algorithm <img data-src=\"/img/NLP/CKY-algo.png\" alt=\"\"></li>\n<li>Performance<ul>\n<li>a lot of elements unrelated to the answer</li>\n<li>can use online search to fill table (from left to right)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Earley\"><a href=\"#Earley\" class=\"headerlink\" title=\"Earley\"></a>Earley</h3><ul>\n<li>parser that exploits chart as data structure</li>\n<li>node = vertex</li>\n<li>arc = edge<ul>\n<li>active edge: (a) and (b)</li>\n<li>inactive edge: (c)</li>\n</ul>\n</li>\n</ul>\n<p>decorated grammar  </p>\n<ul>\n<li>(a) “•” in the first <img data-src=\"/img/NLP/dot-a.png\" alt=\"\"><ul>\n<li>• NP VP</li>\n<li>A hypothesis has been made, but has not been verified yet</li>\n</ul>\n</li>\n<li>(b) “•” in the middle <img data-src=\"/img/NLP/dot-b.png\" alt=\"\"><ul>\n<li>NP • VP</li>\n<li>A hypothesis has been partially confirmed</li>\n</ul>\n</li>\n<li>(c) “•” in the last<ul>\n<li>NP VP •</li>\n<li>A hypothesis has been wholly confirmed</li>\n</ul>\n</li>\n<li>representation of edge <img data-src=\"/img/NLP/chart-struct.png\" alt=\"\"></li>\n<li>initialization <img data-src=\"/img/NLP/chart-initialize.png\" alt=\"\"><ul>\n<li>for each rule A → W, if A is a category that can span a chart (typically S), add &lt;0, 0, A → •W&gt; <img data-src=\"/img/NLP/chartchart-init.png\" alt=\"\"><ul>\n<li>A implies •W from position 0 to 0</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Housekeeping<ul>\n<li>prevent duplicate rules</li>\n</ul>\n</li>\n</ul>\n<p>Fundamental rule  </p>\n<ul>\n<li>If the chart contains &lt;i, j, A → W1 •B W2&gt; and &lt;j, k, B → W3 •&gt;, then add edge &lt;i, k, A → W1 B •W2&gt; to the chart <img data-src=\"/img/NLP/chart-fund.png\" alt=\"\"></li>\n<li>Notes<ol>\n<li>New edge may be either active or inactive</li>\n<li>does not remove the active edge that has succeeded</li>\n</ol>\n</li>\n</ul>\n<p>Bottom-up rule  </p>\n<ul>\n<li>if adding edge &lt;i, j, C → W1 •&gt; to the chart, then for every rule that has the form B → C W2, add &lt;i, i, B → • C W2&gt; <img data-src=\"/img/NLP/chart-bottom.png\" alt=\"\"></li>\n</ul>\n<p>Top-down rule   </p>\n<ul>\n<li>If adding edge &lt;i, j, C → W1 •B W2&gt; to the chart, then for each rule B → W, add &lt; j, j, B →•W&gt;</li>\n</ul>\n<h3 id=\"Full-Syntactic-Parsing\"><a href=\"#Full-Syntactic-Parsing\" class=\"headerlink\" title=\"Full Syntactic Parsing\"></a>Full Syntactic Parsing</h3><ul>\n<li>necessary for deep semantic analysis of texts</li>\n<li>not practical for many applications (given typical resources)<ul>\n<li>O(n^3) for straight parsing</li>\n<li>O(n^5) for probabilistic versions</li>\n<li>Too slow for real time applications (search engines)</li>\n</ul>\n</li>\n<li>Two Alternatives<ul>\n<li>Dependency parsing<ul>\n<li>Change the underlying grammar formalism</li>\n<li>can get a lot done with just binary relations among the words</li>\n<li>詳見Chap08 dependency grammar</li>\n</ul>\n</li>\n<li>Partial parsing<ul>\n<li>Approximate phrase-structure parsing with finite-state and statistical approaches</li>\n</ul>\n</li>\n<li>Both of these approaches give up something (syntactic, structure) in return for more robust and efficient parsing</li>\n</ul>\n</li>\n</ul>\n<p>Partial parsing</p>\n<ul>\n<li>For many applications you don’t really need full parse</li>\n<li>For example, if you’re interested in locating all the people, places and organizations  <ul>\n<li>base-NP chunking <ul>\n<li>[NP The morning flight] from [NP Denvar] has arrived </li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Two approaches<ul>\n<li>Rule-based (hierarchical) transduction(轉導) <!--???--><ul>\n<li>Restrict recursive rules (make the rules flat)<ul>\n<li>like NP → NP VP</li>\n</ul>\n</li>\n<li>Group the rules so that RHS of the rules can refer to non-terminals introduced in earlier transducers, but not later ones</li>\n<li>Combine the rules in a group in the same way we did with the rules for spelling changes</li>\n<li>Combine the groups into a cascade<ul>\n<li>can be used to find the sequence of flat chunks you’re interested in</li>\n<li>or approximate hierarchical trees you get from full parsing with a CFG</li>\n</ul>\n</li>\n<li>Typical Architecture ![](/img/NLP/Cascaded Transducers.png)<ul>\n<li>Phase 1: Part of speech tags</li>\n<li>Phase 2: Base syntactic phrases</li>\n<li>Phase 3: Larger verb and noun groups</li>\n<li>Phase 4: Sentential level rules</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Statistical sequence labeling<ul>\n<li>HMMs</li>\n<li>MEMMs</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap10-Statistical-Parsing\"><a href=\"#Chap10-Statistical-Parsing\" class=\"headerlink\" title=\"Chap10 Statistical Parsing\"></a>Chap10 Statistical Parsing</h2><p>Motivation  </p>\n<ul>\n<li>N-gram models and HMM Tagging only allowed us to process sentences linearly</li>\n<li><strong>Probabilistic Context Free Grammars</strong>(PCFG)<ul>\n<li>alias: Stochastic context-free grammar(SCFG)</li>\n<li>simplest and most natural probabilistic model for tree structures</li>\n<li>closely related to those for HMMs</li>\n<li>為每一個CFG的規則標示其發生的可能性</li>\n</ul>\n</li>\n</ul>\n<p>Idea  </p>\n<ul>\n<li>reduce “right” parse to “most probable parse”<ul>\n<li>Argmax P(Parse|Sentence)</li>\n</ul>\n</li>\n</ul>\n<p>A PCFG consists of  </p>\n<ul>\n<li>set of terminals, {wk}</li>\n<li>set of nonterminals, {Ni}</li>\n<li>start symbol N1</li>\n<li>set of rules<ul>\n<li>{Ni –&gt; ξj}(ξj is a sequence of terminals and nonterminals)</li>\n</ul>\n</li>\n<li>probabilities of rules<ul>\n<li>total probability of imply Ni to other sequence ξj is 1 </li>\n<li>∀i Σj P(Ni → ξj) = 1</li>\n</ul>\n</li>\n<li>Probability of sentence according to grammar G <ul>\n<li>P($w_{1m}$) = sum of P($w_{1m}$, t) for every possible tree t</li>\n</ul>\n</li>\n<li>Nj dominates the words wa … wb<ul>\n<li>Nj → wa … wb</li>\n</ul>\n</li>\n</ul>\n<p>Assumptions of the Model  </p>\n<ul>\n<li>Place Invariance<ul>\n<li>probability of a subtree does not depend on its position in the string</li>\n<li>similar to time invariance in HMMs</li>\n</ul>\n</li>\n<li>Ancestor Free<ul>\n<li>probability of a subtree does not depend on nodes in the derivation outside the subtree(subtree的機率只和subtree內的node有關)</li>\n<li>can simplify probability calculation <img data-src=\"/img/NLP/after-assump.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>Questions of PCFGs(similar to three questions of HMM)    </p>\n<ul>\n<li>Assign probabilities to parse trees<ul>\n<li>What is the probability of a sentence $w_{1m}$ according to a grammar G<ul>\n<li>P(w1m|G)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Parsing with probabilities(Decoding)<ul>\n<li>What is the most likely parse for a sentence<ul>\n<li>argmax_t P(t|w1m,G) </li>\n<li>How to efficiently find the best (or N best) trees </li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Training the model (Learning) <ul>\n<li>How to set rule probabilities(parameter of grammar model) that maximize the probability of a sentence<ul>\n<li>argmax_G P(w1m|G)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Simple Probability Model  </p>\n<ul>\n<li>probability of a tree is the product of the probabilities of rules in derivation</li>\n<li>Rule Probabilities<ul>\n<li>S → NP </li>\n<li>P(NP | S)</li>\n</ul>\n</li>\n<li>Training the Model<ul>\n<li>estimate probability from data</li>\n<li>P(α → β | α) = Count(α→β) / Count(α) = Count(α→β) / Σγ Count(α→γ)</li>\n</ul>\n</li>\n<li>Parsing (Decoding)<ul>\n<li>trees with highest probability in the model</li>\n</ul>\n</li>\n<li>Example: Book the dinner flight<ul>\n<li><img data-src=\"/img/NLP/pm-ex.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/pm-ex2.png\" alt=\"\"></li>\n<li>too slow!</li>\n</ul>\n</li>\n</ul>\n<p>Dynamic Programming again  </p>\n<ul>\n<li>use CKY and Earley to <strong>parse</strong></li>\n<li>Viterbi and HMMs to <strong>get the best parse</strong></li>\n<li>Parameters of a PCFG in Chomsky Normal Form<ul>\n<li>P(Nj→NrNs | G) , $n^3$ matrix of parameters</li>\n<li>P(Nj→wk | G), $nV$ parameters</li>\n<li>n is the number of nonterminals </li>\n<li>V is the number of terminals</li>\n</ul>\n</li>\n<li>Σr,s P(Nj→NrNs) + ΣkP(Nj→wk) = 1<ul>\n<li>所有由Nj導出的rule，機率總和必為1</li>\n</ul>\n</li>\n</ul>\n<p>Probabilistic Regular Grammars (PRG)    </p>\n<ul>\n<li>start state N1 </li>\n<li>rules<ul>\n<li>Ni → wjNk</li>\n<li>Ni → wj</li>\n</ul>\n</li>\n<li>PRG is a HMM with [start state] and [finish(sink) state] <img data-src=\"/img/NLP/prg-sink.png\" alt=\"\"></li>\n</ul>\n<p>Inside and Outside probability <img data-src=\"/img/NLP/prg-graph.png\" alt=\"\"> <img data-src=\"/img/NLP/prg-bf.png\" alt=\"\"> <img data-src=\"/img/NLP/prg-bf2.png\" alt=\"\">  </p>\n<ul>\n<li>Forward(Outside) probability<ul>\n<li>$ α<em>i(t) = P(w</em>{1(t-1)}, X_t = i)$</li>\n<li>everything above a certain node(include the node)</li>\n</ul>\n</li>\n<li>Backward(Inside) probability<ul>\n<li>$ β<em>i(t, T) = P(w</em>{tT} | X_t = i)$</li>\n<li>everything below a certain node</li>\n<li>total probability of generating words $w_t \\cdots w_T$, given the root nonterminal $N^i$ and a grammar G</li>\n</ul>\n</li>\n</ul>\n<p>Inside Algorithm (bottom-up)      </p>\n<ul>\n<li>$P(w_{1m} | G) = P(N_1 → w_{1m} | G) = P(w_{1m} | N^1_{1m}, G) = B_1(1,m)$<ul>\n<li>$B_1(1,m)$ is Inside probability<ul>\n<li>P(w1~wm are below N1(start symbol))</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>base rule<ul>\n<li>$ B_j(k, k) = P(w_k | N^j_{kk}, G) = P(N^j → w_k | G)$</li>\n</ul>\n</li>\n<li>$ B_j(p, q) = P(w_{pq} | N^j_{pq}, G) = $ <img data-src=\"/img/NLP/inside-induction.png\" alt=\"\"><ul>\n<li>try every possible rules to split Nj, product of *<em>rule probabilty and segments’ inside probabilities *</em> </li>\n</ul>\n</li>\n<li>use grid to solve again<ul>\n<li><img data-src=\"/img/NLP/inside-grid.png\" alt=\"\"><ul>\n<li>X軸代表起始座標，Y軸代表長度<ul>\n<li>(2,3) → flies like ants</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Outside Algorithm (top-down)     </p>\n<ul>\n<li>$ P(w_{1m} | G) = Σ_j α_j(k, k)P(N^j → w_k$ <img data-src=\"/img/NLP/outside-graph.png\" alt=\"\"> <!--為何是sum...--><ul>\n<li>outside probability of wk x (inside) probability of wk  of every Nj</li>\n</ul>\n</li>\n<li>basecase <ul>\n<li>$ α_1(1, m) = 1, α_j(1,m) = $</li>\n<li>P(N1) = 1, P(Nj outside w1 to wm) = 0</li>\n</ul>\n</li>\n<li>自己的outside probability 等於 <ul>\n<li>爸爸的outside probability 乘以 爸爸的inside probability 除以 自己的inside probability<ul>\n<li>inside x outside 是固定值？</li>\n</ul>\n</li>\n<li>爸爸的inside probabiliity 除以 自己的inside probability 就是其兄弟的inside probability</li>\n<li>使用此公式計算 <img data-src=\"/img/NLP/inout.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>$ α<em>j(p, q)β_j(p, q) = P(w</em>{1m}, N^j_{pq} | G) $<ul>\n<li>某個點的inside 乘 outside = 在某grammar中，出現此句子，且包含此點的機率 </li>\n<li>所有點的總和：在某grammar下，某parse tree(包含所有node)的機率 <img data-src=\"/img/NLP/parse-probability.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Outside example: 這些數字理論上算起來會一樣… <img data-src=\"/img/NLP/outside-forward.png\" alt=\"\"></li>\n</ul>\n<p>Finding the Most Likely Parse for a Sentence     </p>\n<ul>\n<li>δi(p,q)= the highest inside probability parse of a subtree $N_{pq}^i$</li>\n<li>Initialization <ul>\n<li>δi(p,p)= P(Ni → wp)</li>\n</ul>\n</li>\n<li>Induction and Store backtrace<ul>\n<li>δi(p,q)= $argmax(j,k,r)P(Ni→NjNk)δj(p,r)δk(r+1,q)$</li>\n<li>找所有可能的切法</li>\n</ul>\n</li>\n<li>Termination<ul>\n<li>answer = δ1(1,m)</li>\n</ul>\n</li>\n</ul>\n<p>Training a PCFG</p>\n<ul>\n<li>find the optimal probabilities among grammar rules</li>\n<li>use EM Training Algorithm to seek the grammar that maximizes the likelihood of the training data<ul>\n<li>Inside-Outside Algorithm </li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/inoutagain.png\" alt=\"\"></li>\n<li>將產生句子的機率視為π，為Nj產生pq的機率 <img data-src=\"/img/NLP/pi.png\" alt=\"\"></li>\n<li>Nj被使用的機率 <img data-src=\"/img/NLP/pi2.png\" alt=\"\"></li>\n<li>Nj被使用，且Nj→NrNs的機率 <img data-src=\"/img/NLP/pi3.png\" alt=\"\"></li>\n<li>Nj→NrNs這條rule被使用的機率=前兩式相除 <img data-src=\"/img/NLP/pi4.png\" alt=\"\"></li>\n<li>Nj→wk <img data-src=\"/img/NLP/pi5.png\" alt=\"\"><ul>\n<li>僅分子差異 <img data-src=\"/img/NLP/pi6.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>Problems with the Inside-Outside Algorithm    </p>\n<ul>\n<li>Extremely Slow<ul>\n<li>For each sentence, each iteration of training is $O(m^3n^3)$</li>\n</ul>\n</li>\n<li>Local Maxima</li>\n<li>Satisfactory learning requires many more nonterminals than are theoretically needed to describe the language</li>\n<li>There is no guarantee that the learned nonterminals will be linguistically motivated</li>\n</ul>\n<h2 id=\"Chap11-Dependency-Parsing\"><a href=\"#Chap11-Dependency-Parsing\" class=\"headerlink\" title=\"Chap11 Dependency Parsing\"></a>Chap11 Dependency Parsing</h2><p><span class=\"exturl\" data-url=\"aHR0cDovL3N0cC5saW5nZmlsLnV1LnNlL35uaXZyZS9kb2NzL0FDTHNsaWRlcy5wZGY=\">COLING-ACL 2006, Dependency Parsing, by Joachim Nivre and Sandra Kuebler<i class=\"fa fa-external-link-alt\"></i></span><br><span class=\"exturl\" data-url=\"aHR0cDovL25hYWNsaGx0MjAxMC5pc2kuZWR1L3R1dG9yaWFscy90Ny1zbGlkZXMucGRm\">NAACL 2010, Recent Advances in Dependency Parsing, by Qin Iris. Wang and YueZhang<i class=\"fa fa-external-link-alt\"></i></span><br><span class=\"exturl\" data-url=\"aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3NpdGUvemhlbmdodWFubHAvcHVibGljYXRpb25zL0lKQ05MUDIwMTMtdHV0b3JpYWwtRFAucGRmP2F0dHJlZGlyZWN0cz0wJmQ9MQ==\">IJCNLP 2013, Dependency Parsing: Past, Present, and Future, by Zhenghua Li, Wenliang Chen, Min Zhang<i class=\"fa fa-external-link-alt\"></i></span></p>\n<p>Dependency Structure vs. Constituency Structure <img data-src=\"/img/NLP/parse.png\" alt=\"\"><br>Parsing is one way to deal with the ambiguity problem in<br>natural language<br>dependency syntax is syntactic relations (dependencies) </p>\n<p>Constraint: between word pairs  <img data-src=\"/img/NLP/depend.png\" alt=\"\"><br>    Projective: No crossing links(a word and its dependents form a contiguous substring of the sentence)<br>    An arc (wi , r ,wj ) ∈ A is projective iff wi →∗ wk for all:<br>    i &lt; k &lt; j when i &lt; j<br>    j &lt; k &lt; i when j &lt; i<br>    射出去的那一方也可以射到兩個字中間的任何一字<br><img data-src=\"/img/NLP/depend-ex.png\" alt=\"\"></p>\n<p>Non-projective Dependency Trees  </p>\n<ul>\n<li>Long-distance dependencies  </li>\n<li>With crossing links</li>\n<li>Not so frequent in English<ul>\n<li>All the dependency trees from Penn Treebank are projective</li>\n</ul>\n</li>\n<li>Common in other languages with free word order<ul>\n<li>Prague(23%) and Czech, German and Dutch</li>\n</ul>\n</li>\n</ul>\n<p>Data Driven Dependency Parsing  </p>\n<ul>\n<li>Data-driven parsing<ul>\n<li>No grammar / rules needed</li>\n<li>Parsing decisions are made based on learned models</li>\n<li>deal with ambiguities well</li>\n<li><img data-src=\"/img/NLP/data-driven.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Three approaches<ul>\n<li>Graph-based models</li>\n<li>Transition-based models(good in practice)<ul>\n<li>Define a transition system for <strong>mapping a sentence to its dependency tree</strong></li>\n<li>Predefine some transition actions</li>\n<li>Learning: predicting the next state transition, by transition history</li>\n<li>Parsing: construct the optimal transition sequence</li>\n<li>Greedy search / beam search</li>\n<li>Features are defined over a richer parsing history</li>\n</ul>\n</li>\n<li>Hybrid models</li>\n</ul>\n</li>\n</ul>\n<p>Comparison   </p>\n<ul>\n<li>Graph-based models<ul>\n<li>Find the optimal tree from all the possible ones</li>\n<li>Global, exhaustive</li>\n</ul>\n</li>\n<li>Transition-based models<ul>\n<li>Predefine some actions (shift and reduce)</li>\n<li>use stack to hold partially built parses</li>\n<li><strong>Find the optimal action sequence</strong></li>\n<li>Local, Greedy or beam search</li>\n</ul>\n</li>\n<li>The two models produce different types of errors</li>\n</ul>\n<p>Hybrid Models  </p>\n<ul>\n<li>Three integration methods<ul>\n<li>Ensemble approach: parsing time integration (Sagae &amp; Lavie 2006)</li>\n<li>Feature-based integration (Nivre &amp; Mcdonald 2008)</li>\n<li>Single model combination (Zhang &amp; Clark 2008)</li>\n</ul>\n</li>\n<li>Gain benefits from both models</li>\n</ul>\n<p><img data-src=\"/img/NLP/parse-algo.png\" alt=\"\"></p>\n<h3 id=\"Graph-based-dependency-parsing-models\"><a href=\"#Graph-based-dependency-parsing-models\" class=\"headerlink\" title=\"Graph-based dependency parsing models\"></a>Graph-based dependency parsing models</h3><ul>\n<li>Search for a tree with the highest score</li>\n<li>Define search space<ul>\n<li>Exhaustive search</li>\n<li>Features are defined over a limited parsing history</li>\n</ul>\n</li>\n<li>The score is linear combination of features <ul>\n<li>What features we can use? (later)</li>\n<li>What learning approaches can lead us to find the best tree with the highest score (later)</li>\n</ul>\n</li>\n<li>Applicable to both probabilistic and nonprobabilistic models </li>\n</ul>\n<p>Features  </p>\n<ul>\n<li>dynamic features<ul>\n<li>Take into account the link labels of the surrounding word-pairs when predicting the label of current pair</li>\n<li>Commonly used in sequential labeling</li>\n<li>A word’s children are generated first(先生child, 再找parent), before it modifies another word</li>\n</ul>\n</li>\n</ul>\n<p>Learning Approaches   </p>\n<ul>\n<li>Local learning approaches<ul>\n<li>Learn a local link classifier given of features defined on training data</li>\n<li>example <img data-src=\"/img/NLP/local-feature-example.png\" alt=\"\"><ul>\n<li>3-class classification: No link, left link or right link</li>\n<li>Efficient O(n) local training</li>\n</ul>\n</li>\n<li>local training and parsing <img data-src=\"/img/NLP/local-train-with-parse.png\" alt=\"\"></li>\n<li>Learn the weights of features<ul>\n<li>Maximum entropy models (Ratnaparkhi 99, Charniak 00)</li>\n<li>Support vector machines (Yamada &amp; Matsumoto 03)</li>\n<li>Use a richer feature set!</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Global learning approaches</li>\n<li>Unsupervised/Semi-supervised learning approaches<ul>\n<li>Use both annotated training data and un-annotated raw text</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Transition-based-model\"><a href=\"#Transition-based-model\" class=\"headerlink\" title=\"Transition-based model\"></a>Transition-based model</h3><ul>\n<li>Stack holds partially built parses</li>\n<li>Queue holds unprocessed words</li>\n<li>Actions<ul>\n<li>use input words to build output parse</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"parsing-processes\"><a href=\"#parsing-processes\" class=\"headerlink\" title=\"parsing processes\"></a>parsing processes</h4><p>Arc-eager parser  </p>\n<ul>\n<li>4 tranition actions<ul>\n<li>SHIFT: push stack</li>\n<li>REDUCE: pop stack</li>\n<li>ARC-LEFT: pop stack and add link</li>\n<li>ARC-RIGHT: push stack and add link</li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/arc-eager-example.png\" alt=\"\"></li>\n<li>Time complexity: linear<ul>\n<li>every word will be pushed once and popped once(except root)</li>\n</ul>\n</li>\n<li>parse<ul>\n<li>by actions: arcleft → arclect subject, noun, …</li>\n</ul>\n</li>\n</ul>\n<p>Arc-standard parser  </p>\n<ul>\n<li>3 actions<ul>\n<li>SHIFT: push</li>\n<li>LEFT: pop leftmost stack element and add</li>\n<li>RIGHT: pop rightmost stack element and add</li>\n</ul>\n</li>\n<li>Also linear time</li>\n</ul>\n<p>Non-projectivity  </p>\n<ul>\n<li>neither of parser can solve it<ul>\n<li>online reorder<ul>\n<li>add extra action: swap</li>\n<li>not linear: $N^2$, but expect to belinear</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Decoding-algorithms\"><a href=\"#Decoding-algorithms\" class=\"headerlink\" title=\"Decoding algorithms\"></a>Decoding algorithms</h4><p>search action sequence to build the parse<br>scoring action given context<br>Candidate item &lt;S, G, Q&gt;</p>\n<ul>\n<li>greedy local search<ul>\n<li>initialize: Q = input</li>\n<li>goal: S=[root], G=tree, Q=[]</li>\n</ul>\n</li>\n<li>problem: one error leads to incorrect parse<ul>\n<li>Beam search: keep N highest partial states<ul>\n<li>use total score of all actions to rank a parse</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Score-Models\"><a href=\"#Score-Models\" class=\"headerlink\" title=\"Score Models\"></a>Score Models</h4><ul>\n<li>linear model</li>\n<li>SVM</li>\n</ul>\n<h2 id=\"Chap12-Semantic-Representation-and-Computational-Semantics\"><a href=\"#Chap12-Semantic-Representation-and-Computational-Semantics\" class=\"headerlink\" title=\"Chap12 Semantic Representation and Computational Semantics\"></a>Chap12 Semantic Representation and Computational Semantics</h2><p>Semantic aren’t primarily descriptions of inputs</p>\n<p>Semantic Processing  </p>\n<ul>\n<li>reason about the truth</li>\n<li>answer questions based on content<ul>\n<li>Touchstone application is often question answering</li>\n</ul>\n</li>\n<li>inference to determine the truth that isn’t actually know</li>\n</ul>\n<p>Method    </p>\n<ul>\n<li>principled, theoretically motivated approach<ul>\n<li>Computational/Compositional Semantics</li>\n</ul>\n</li>\n<li>limited, practical approaches that have some hope of being useful<ul>\n<li>Information extraction</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Information-Extraction\"><a href=\"#Information-Extraction\" class=\"headerlink\" title=\"Information Extraction\"></a>Information Extraction</h3><p>Information Extraction = segmentation + classification +  association + clustering <img data-src=\"/img/NLP/IE.png\" alt=\"\"></p>\n<ul>\n<li>superficial analysis <ul>\n<li>pulls out only the entities, relations and roles related to consuming application</li>\n</ul>\n</li>\n<li>Similar to chunking</li>\n</ul>\n<h3 id=\"Compositional-Semantics\"><a href=\"#Compositional-Semantics\" class=\"headerlink\" title=\"Compositional Semantics\"></a>Compositional Semantics</h3><ul>\n<li>Use First-Order Logic(FOL) representation that accounts for all the entities, roles and relations present in a sentence</li>\n<li>Similar to our approach to full parsing</li>\n<li>Compositional: The meaning of a whole is derived from the meanings of the parts(syntatic) <img data-src=\"/img/NLP/syntax-semantic.png\" alt=\"\"></li>\n<li>Syntax-Driven Semantic Analysis<ul>\n<li>The composition of meaning representations is guided by the <strong>syntactic</strong> components and relations provided by the  grammars</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"FOL\"><a href=\"#FOL\" class=\"headerlink\" title=\"FOL\"></a>FOL</h4><ul>\n<li>allow to answer yes/no questions</li>\n<li>allow variable</li>\n<li>allow inference</li>\n</ul>\n<p>Events, actions and relationships can be captured with representations that consist of predicates with arguments  </p>\n<ul>\n<li>Predicates<ul>\n<li>Primarily Verbs, VPs, Sentences</li>\n<li>Verbs introduce/refer to events and processes</li>\n</ul>\n</li>\n<li>Arguments <ul>\n<li>Primarily Nouns, Nominals, NPs, PPs</li>\n<li>Nouns introduce the things that play roles in those events</li>\n</ul>\n</li>\n<li>Example: Mary gave a list to John <ul>\n<li>Giving(Mary, John, List)</li>\n<li>Gave: Predicate</li>\n<li>Mary, John, List: Argument</li>\n<li>better representation <img data-src=\"/img/NLP/FOL-better.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Lambda Forms<ul>\n<li>Allow variables to be bound</li>\n<li>λxP(x)(Sally) = P(Sally)</li>\n</ul>\n</li>\n</ul>\n<p>Ambiguation  </p>\n<ul>\n<li>mismatch between syntax and semantics<ul>\n<li>displaced arguments</li>\n<li>complex NPs with quantifiers<ul>\n<li>A menu</li>\n<li>Every restaurant <img data-src=\"/img/NLP/complicate-NP.png\" alt=\"\"></li>\n<li>Not every waiter</li>\n<li>Most restaurants</li>\n<li><img data-src=\"/img/NLP/complicate-NP-induction.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>still preserving strict compositionality</li>\n</ul>\n</li>\n<li>Two (syntax) rules to revise<ul>\n<li>The S rule<ul>\n<li>S → NP VP, NP.Sem(VP.Sem)</li>\n<li>NP and VP swapped, because S is NP</li>\n</ul>\n</li>\n<li>Simple NP’s like proper nouns<ul>\n<li>λx.Franco(x)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Store and Retrieve  <ul>\n<li><img data-src=\"/img/NLP/ambiguity-of-same-POS.png\" alt=\"\"></li>\n<li>Retrieving the quantifiers one at a time and placing them in front</li>\n<li>The order determines the meaning <img data-src=\"/img/NLP/store.png\" alt=\"\"></li>\n<li>retrieve <img data-src=\"/img/NLP/retrieve.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Set-Based-Models\"><a href=\"#Set-Based-Models\" class=\"headerlink\" title=\"Set-Based Models\"></a>Set-Based Models</h3><ul>\n<li>domain: the set of elements</li>\n<li>entity: elements of domain</li>\n<li>Properties of the elements: sets of elements from the domain</li>\n<li>Relations: sets of tuples of elements from the domain</li>\n<li>FOL<ul>\n<li>FOL Terms → elements of the domain<ul>\n<li>Med -&gt; “f”</li>\n</ul>\n</li>\n<li>FOL atomic formula → sets, or sets of tuples<ul>\n<li>Noisy(Med) is true if “f is in the set of elements that corresponds to the noisy relation</li>\n<li>Near(Med, Rio) is true if “the tuple &lt;f,g&gt; is in the set of tuples that corresponds to “Near” in the interpretation</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Example: Everyone likes a noisy restaurant <img data-src=\"/img/NLP/set-based-model.png\" alt=\"\"><ul>\n<li>There is a particular restaurant out there; it’s a noisy place; everybody likes it 有一家吵雜的餐廳大家都喜歡</li>\n<li>Everybody has at least one noisy restaurant that they like 大家都喜歡一家吵雜的餐廳</li>\n<li>Everybody likes noisy restaurants (i.e., there is no noisy restaurant out there that is disliked by anyone) 大家都喜歡吵雜的餐廳</li>\n<li>Using predicates to create <strong>categories</strong> of concepts <ul>\n<li>people and restaurants</li>\n<li>basis for OWL (Web Ontology Language)網絡本體語言</li>\n</ul>\n</li>\n<li>before <img data-src=\"/img/NLP/uncategories.png\" alt=\"\"></li>\n<li>after <img data-src=\"/img/NLP/categories.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap13-Lexical-Semantics\"><a href=\"#Chap13-Lexical-Semantics\" class=\"headerlink\" title=\"Chap13 Lexical Semantics\"></a>Chap13 Lexical Semantics</h2><p>we didn’t do word meaning in compositional semantics</p>\n<p>WordNet  </p>\n<ul>\n<li>meaning and relationship about words<ul>\n<li>hypernym(上位詞)<ul>\n<li>breakfast → meal</li>\n</ul>\n</li>\n<li>hierarchies <img data-src=\"/img/NLP/wordnet-hierarchy.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>In our semantics examples, we used various FOL predicates to capture various aspects of events, including the notion of roles<br>Havers, takers, givers, servers, etc.</p>\n<p>Thematic roles(語義關係) <img data-src=\"/img/NLP/thematic-roles.png\" alt=\"\"></p>\n<ul>\n<li>semantic generalizations over the specific roles that occur with specific verbs<ul>\n<li>provide a shallow level of semantic analysis</li>\n<li>tied to syntactic analysis</li>\n</ul>\n</li>\n<li>i.e. Takers, givers, eaters, makers, doers, killers<ul>\n<li>They’re all the agents of the actions</li>\n</ul>\n</li>\n<li>AGENTS are often subjects</li>\n<li>In a VP-&gt;V NP rule, the NP is often a THEME</li>\n</ul>\n<p>2 major English resources using thematic data</p>\n<ul>\n<li>PropBank<ul>\n<li>Layered on the Penn TreeBank</li>\n<li>Small number (25ish) labels</li>\n</ul>\n</li>\n<li>FrameNet<ul>\n<li>Based on frame semantics</li>\n<li>Large number of frame-specific labels</li>\n</ul>\n</li>\n</ul>\n<p>Example  </p>\n<ul>\n<li>[McAdams and crew] covered [the floors] with [checked linoleum].格子花紋油毯<ul>\n<li>Arg0 (agent: the causer of the smearing)</li>\n<li>Arg1 (theme: “thing covered”)</li>\n<li>Arg2 (covering: “stuff being smeared”)</li>\n</ul>\n</li>\n<li>including agent and theme, remaining args are verb specific</li>\n</ul>\n<p>Logical Statements  </p>\n<ul>\n<li>Example: EAT – Eating(e) ^Agent(e,x)^ Theme(e,y)^Food(y)<ul>\n<li>(adding in all the right quantifiers and lambdas)</li>\n</ul>\n</li>\n<li>Use WordNet to encode the selection restrictions</li>\n<li>Unfortunately, language is creative<ul>\n<li>… ate glass on an empty stomach accompanied only by water and tea</li>\n<li>you <strong>can’t eat gold</strong> for lunch if you’re hungry</li>\n</ul>\n</li>\n</ul>\n<p>can we discover a verb’s restrictions by using a corpus and WordNet?    </p>\n<ol>\n<li>Parse sentences and find heads</li>\n<li>Label the thematic roles</li>\n<li>Collect statistics on the co-occurrence of particular headwords with particular thematic roles</li>\n<li>Use the WordNet hypernym structure to <strong>find the most meaningful level to use as a restriction</strong></li>\n</ol>\n<h3 id=\"WSD\"><a href=\"#WSD\" class=\"headerlink\" title=\"WSD\"></a>WSD</h3><p>Word sense disambiguation  </p>\n<ul>\n<li>select right sense for a word </li>\n<li>Semantic selection restrictions can be used to disambiguate<ul>\n<li>Ambiguous arguments to unambiguous predicates</li>\n<li>Ambiguous predicates with unambiguous arguments</li>\n</ul>\n</li>\n<li>Ambiguous arguments<ul>\n<li>Prepare a dish(菜餚)</li>\n<li>Wash a dish(盤子)</li>\n</ul>\n</li>\n<li>Ambiguous predicates<ul>\n<li>Serve (任職/服務) Denver</li>\n<li>Serve (供應) breakfast</li>\n</ul>\n</li>\n</ul>\n<p>Methodology   </p>\n<ul>\n<li>Supervised Disambiguation<ul>\n<li>based on a labeled training set</li>\n</ul>\n</li>\n<li>Dictionary-Based Disambiguation<ul>\n<li>based on lexical resource like dictionaries</li>\n</ul>\n</li>\n<li>Unsupervised Disambiguation<ul>\n<li>label training data is expensive </li>\n<li>based on unlabeled corpora</li>\n</ul>\n</li>\n<li>Upper(human) and Lower(simple model) Bounds</li>\n<li>Pseudoword<ul>\n<li>Generate artificial evaluation data for comparison and improvement of text processing algorithms</li>\n</ul>\n</li>\n</ul>\n<p>Supervised ML Approaches  </p>\n<ul>\n<li>What’s a tag?<ul>\n<li>In WordNet, “bass” in a text has 8 possible tags or labels (bass1 through bass8)</li>\n</ul>\n</li>\n<li>require very simple representation for training data<ul>\n<li>Vectors of sets of feature/value pairs</li>\n<li>need to extract training data by characterization of text surrounding the target</li>\n</ul>\n</li>\n<li>If you decide to use features that require more analysis (say parse trees) then the ML part may be doing less work (relatively) if these features are truly informative</li>\n<li>Classification<ul>\n<li>Naïve Bayes (the right thing to try first)</li>\n<li>Decision lists</li>\n<li>Decision trees</li>\n<li>MaxEnt</li>\n<li>Support vector machines</li>\n<li>Nearest neighbor methods…</li>\n<li>choice of technique depends on features that have been used</li>\n</ul>\n</li>\n<li>Bootstrapping<ul>\n<li>Use when don’t have enough data to train a system…</li>\n<li>集中有放回的均勻抽樣</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Naive-Bayes\"><a href=\"#Naive-Bayes\" class=\"headerlink\" title=\"Naive Bayes\"></a>Naive Bayes</h4><ul>\n<li>Argmax P(sense|feature vector) <img data-src=\"/img/NLP/bayesian-decision.png\" alt=\"\"> </li>\n<li>find maximum probabilty of words given possible sk <img data-src=\"/img/NLP/bayesian-decision2.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/bayesian-classifier.png\" alt=\"\"></li>\n<li>assumption<ul>\n<li>bag of words model<ul>\n<li>structure and order of words is ignored</li>\n<li>each pair of words in the bag is independent</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>73% correct</li>\n</ul>\n<h4 id=\"Dictionary-Based-Disambiguation\"><a href=\"#Dictionary-Based-Disambiguation\" class=\"headerlink\" title=\"Dictionary-Based Disambiguation\"></a>Dictionary-Based Disambiguation</h4><ol>\n<li>Disambiguation based on sense definitions</li>\n<li>Thesaurus-Based Disambiguation</li>\n<li>Disambiguation based on translations in a second-language corpus</li>\n</ol>\n<p>sense definition</p>\n<ul>\n<li>find keywords in definition of a word<ul>\n<li>cone<ul>\n<li>… pollen-bearing scales or bracts in <strong>trees</strong></li>\n<li>shape for holding <strong>ice cream</strong></li>\n</ul>\n</li>\n<li>50%~70% accuracies</li>\n<li>Alternatives<ul>\n<li>Several iterations to determine correct sense</li>\n<li>Combine the dictionary-based and thesaurus-based disambiguation</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU3JUI0JUEyJUU1JUJDJTk1JUU1JTg1JUI4\">Thesaurus-Based(索引典)<i class=\"fa fa-external-link-alt\"></i></span> Disambiguation    </p>\n<ul>\n<li>Category can determine which word senses are used</li>\n<li>Each word is assigned one or more subject codes which correspond to its different meanings<ul>\n<li>select the most often subject code</li>\n<li>考慮w的context，有多少words的senses與w相同</li>\n</ul>\n</li>\n<li>Walker’s Algorithm<ul>\n<li>50% accuracy for “interest, point, power, state, and terms”</li>\n</ul>\n</li>\n<li>Problems<ul>\n<li>general topic categorization, e.g., mouse in computer</li>\n<li>coverage, e.g., Navratilova</li>\n</ul>\n</li>\n<li>Yarowsky’s Algorithm <img data-src=\"/img/NLP/yarowsky-algo.png\" alt=\"\"> <img data-src=\"/img/NLP/yarowsky-algo2.png\" alt=\"\"> <img data-src=\"/img/NLP/yarowsky-algo3.png\" alt=\"\"><ul>\n<li><ol>\n<li>categorize sentences</li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li>categorize words</li>\n</ol>\n</li>\n<li><ol start=\"3\">\n<li>disambiguate by decision rule for Naïve Bayes</li>\n</ol>\n</li>\n<li>result <img data-src=\"/img/NLP/yarowsky-result.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>Disambiguation based on translations in a second-language corpus  </p>\n<ul>\n<li>the word “interest” has two translations in German<ul>\n<li>“Beteiligung” (legal share–50% a interest in the company)</li>\n<li>“Interesse” (attention, concern–her interest in Mathematics)</li>\n</ul>\n</li>\n<li>Example: … showed interest …<ul>\n<li>Look up English-German dictionary, show → zeigen</li>\n<li>Compute R(Interesse, zeigen) and R(Beteiligung, zeigen)</li>\n<li>R(Interesse, zeigen) &gt; R(Beteiligung, zeigen)</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Unsupervised-Disambiguation\"><a href=\"#Unsupervised-Disambiguation\" class=\"headerlink\" title=\"Unsupervised Disambiguation\"></a>Unsupervised Disambiguation</h4><p>P(vj|sk) are estimated using the EM algorithm  </p>\n<ol>\n<li>Random initialization of P(vj|sk)(word)</li>\n<li>For each context ci of w, compute P(ci|sk)(sentence)</li>\n<li>Use P(ci|sk) as training data</li>\n<li>Reestimate P(vj|sk)(word)</li>\n</ol>\n<p>Surface Representations(features)   </p>\n<ul>\n<li>Collocational<ul>\n<li>words that appear in specific positions to the right and left of the target word</li>\n<li>limited to the words themselves as well as part of speech</li>\n<li>Example: guitar and bassplayer stand<ul>\n<li>[guitar, NN, and, CJC, player, NN, stand, VVB]</li>\n<li>In other words, a vector consisting of [position n word, position n part-of-speech…]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Co-occurrence<ul>\n<li>words that occur regardless of position</li>\n<li>Typically limited to frequency counts</li>\n<li>Assume we’ve settled on a possible vocabulary of 12 words that includes guitarand playerbut not andand stand</li>\n<li>Example: guitar and bassplayer stand<ul>\n<li>Assume a 12-word sentence includes guitar and player but not “and” and stand</li>\n<li>[0,0,0,1,0,0,0,0,0,1,0,0]</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Applications  </p>\n<ul>\n<li>tagging<ul>\n<li>translation</li>\n<li>information retrieval</li>\n</ul>\n</li>\n</ul>\n<p>different label  </p>\n<ul>\n<li>Generic thematic roles (aka case roles)<ul>\n<li>Agent, instrument, source, goal, etc.</li>\n</ul>\n</li>\n<li>Propbank labels<ul>\n<li>Common set of labels ARG0-ARG4, ARGM</li>\n<li>specific to verb semantics</li>\n</ul>\n</li>\n<li>FrameNet frame elements<ul>\n<li>Conceptual and frame-specific </li>\n</ul>\n</li>\n<li>Example: [Ochocinco] bought [Burke] [a diamond ring]<ul>\n<li>generic: Agent, Goal, Theme</li>\n<li>propbank: ARG0, ARG2, ARG1</li>\n<li>framenet: Customer, Recipe, Goods</li>\n</ul>\n</li>\n</ul>\n<p>Semantic Role Labeling  </p>\n<ul>\n<li>automatically identify and label thematic roles<ul>\n<li>For each verb in a sentence<ul>\n<li>For each constituent<ul>\n<li>Decide if it is an argument to that verb</li>\n<li>if it is an argument, determine what kind</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>feature<ul>\n<li>from parse and lexical item</li>\n<li>“path” </li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Lexical-Acquisition\"><a href=\"#Lexical-Acquisition\" class=\"headerlink\" title=\"Lexical Acquisition\"></a>Lexical Acquisition</h3><ul>\n<li>Verb Subcategorization<ul>\n<li>the syntactic means by which verbs express their arguments</li>\n</ul>\n</li>\n<li>Attachment Ambiguity<ul>\n<li>The children ate the cake with their hands</li>\n<li>The children ate the cake with blue icing</li>\n</ul>\n</li>\n<li>SelectionalPreferences<ul>\n<li>The semantic categorization of a verb’s arguments</li>\n</ul>\n</li>\n<li>Semantic Similarity (refer to IR course)<ul>\n<li>Semantic similarity between words</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Verb-Subcategorization\"><a href=\"#Verb-Subcategorization\" class=\"headerlink\" title=\"Verb Subcategorization\"></a>Verb Subcategorization</h4><p>a particular set of syntactic categories that a verb can appear with is called a <strong>subcategorization frame</strong> <img data-src=\"/img/NLP/subcategorization.png\" alt=\"\"></p>\n<p>Brent’s subcategorization frame learner  </p>\n<ol>\n<li>Cues: Define a regular pattern of words and syntactic categories<ol>\n<li>ε: error rate of assigning frame f to verb v based on cue cj</li>\n</ol>\n</li>\n<li>Hypothesis Testing: Define null hypothesis H0: “the frame is not appropriate for the verb” <ol>\n<li>Reject this hypothesis if the cue cj indicates with high probability that our H0 is wrong</li>\n</ol>\n</li>\n</ol>\n<p>Example<br>Cues  </p>\n<ul>\n<li><p>regular pattern for subcategorization frame “NP NP”</p>\n<ul>\n<li>(OBJ | SUBJ_OBJ | CAP) (PUNC |CC)<br>Null hypothesis testing</li>\n</ul>\n</li>\n<li><p>Verb vi occurs a total of n times in the corpus and there are m &lt; n occurrences with a cue for frame fj</p>\n</li>\n<li><p>Reject the null hypothesis H0 that vi does not accept fj with the following probability of error <img data-src=\"/img/NLP/brent-null-hypothesis.png\" alt=\"\"></p>\n</li>\n<li><p>Brent’s system does well at precision, but not well at recall</p>\n</li>\n<li><p>Manning’s system</p>\n<ul>\n<li>solve this problem by using a tagger and running the cue detection on the output of the tagger</li>\n<li>learn a lot of subcategorization frames, even those it is low-reliability</li>\n<li>still low performance </li>\n<li>improve : use prior knowledge</li>\n</ul>\n</li>\n</ul>\n<p>PCFG prefers to parse common construction  </p>\n<ul>\n<li>P(A|prep, verb, np1, np2, w) ~= P(A|prep, verb, np1, np2)<ul>\n<li>Do not count the word outside of frame</li>\n<li>w: words outside of “verb np1(prep np2)”</li>\n<li>A: random variable representing attachment decision</li>\n<li>V(A): verb or np1</li>\n<li>Counter example<ul>\n<li>Fred saw a movie with Arnold Schwarzenegger</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>P(A|prep, verb, np1, np2, noun1, noun2) ~= P(A|prep, verb, noun1, noun2)<ul>\n<li>noun1 = head of np1, noun2 = head of np2</li>\n<li>total parameters: $10^{13}$ = #(prep) x #(verb) x #(noun) x #(noun) </li>\n</ul>\n</li>\n<li>P(A= noun | prep, verb, noun1) vs. P(A= verb | prep, verb, noun1)<ul>\n<li>compare probability to be verb and probability to be noun</li>\n</ul>\n</li>\n</ul>\n<p>Technique: Alternative to reduce parameters   </p>\n<ul>\n<li>Condition probabilities on fewer things</li>\n<li>Condition probabilities on more general things</li>\n</ul>\n<p>The model asks the following questions  </p>\n<ul>\n<li>VAp: Is there a PP headed by p and following the verb v which attaches to v(VAp=1) or not (VAp=0)?</li>\n<li>NAp: Is there a PP headed by p and following the noun n which attaches to n (NAp=1) or not (NAp=0)?</li>\n<li>(1) Determine the attachment of a PP that is immediately following an object noun, i.e. compute the probability of NAp=1</li>\n<li>In order for the first PP headed by the preposition p to attach to the verb, both VAp=1 and NAp=0<ul>\n<li>calculate likelihood ratio between V and N <img data-src=\"/img/NLP/likelihood-ratio-vn.png\" alt=\"\"></li>\n<li>maximum estimation<ul>\n<li>P(VA = 1 | v) = C(v, p) / C(v)</li>\n<li>P(NA = 1 | n) = C(n, p) / C(n)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Estimation of PP attachment counts<ul>\n<li>Sure Noun Attach<ul>\n<li>If a noun is followed by a PP but no preceding verb, increment C(prep attached to noun) </li>\n</ul>\n</li>\n<li>Sure Verb Attach<ul>\n<li>if a passive verb is followed by a PP other than a “by” phrase, increment C(prep attached to verb) </li>\n<li>if a PP follows both a noun phrase and a verb but the noun phrase is a pronoun, increment C(prep attached to verb)</li>\n</ul>\n</li>\n<li>Ambiguous Attach<ul>\n<li>if a PP follows both a noun and a verb, see if the probabilities based on the attachment decided by previous way</li>\n<li>otherwise increment both attachment counters by 0.5</li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/attach-example.png\" alt=\"\"></li>\n<li>Sparse data is a major cause of the difference between the human and program performance(attachment indeterminacy不確定性)</li>\n</ul>\n</li>\n</ul>\n<p>Using Semantic Information  </p>\n<ul>\n<li>condition on semantic tags of verb &amp; noun<ul>\n<li>Sue bought a plant with Jane(human)</li>\n<li>Sue bought a plant with yellow leaves(object)</li>\n</ul>\n</li>\n</ul>\n<p>Assumption<br>The noun phrase serves as the subject of the relative clause</p>\n<ul>\n<li>collect “ subject-verb” and “verb-object” pairs.(training part)  </li>\n<li>compute t-score (testing part) <ul>\n<li>t-score &gt; 0.10 (significant)</li>\n</ul>\n</li>\n</ul>\n<p>P (relative clause attaches to x | main verb of clause =v) &gt; P (relative clause attaches to y | main verb of clause=v)<br>↔ P (x= subject/object | v) &gt; P (y= subject/ object|v)</p>\n<p>Selectional Preferences  </p>\n<ul>\n<li>Most verbs prefer particular type of arguments<ul>\n<li>eat → object (food item)</li>\n<li>think → subject (people)</li>\n<li>bark → subject (dog)</li>\n</ul>\n</li>\n<li>Aspects of meaning of a word can be inferred<ul>\n<li>Susan had never eaten a fresh <strong>durian</strong> before (food item)</li>\n</ul>\n</li>\n<li>Selectional preferences can be used to rank different parses of a sentence</li>\n<li>Selectional preference strength<ul>\n<li>how strongly the verb constrains its direct object</li>\n<li><img data-src=\"/img/NLP/selection-strength.png\" alt=\"\"></li>\n<li>KL divergence between the prior distribution of direct objects of general verb and the distribution of direct objects of specific verb</li>\n<li>2 assumptions<ul>\n<li>only the head noun of the object is considered</li>\n<li>rather than dealing with individual nouns, we look at classes of nouns</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Selectional association<ul>\n<li>Selectional Association between a verb and a class is this class’s contribution to S(v) / the overall preference strength S(v) <img data-src=\"/img/NLP/selectional-association.png\" alt=\"\"></li>\n<li>There is also a rule for assigning association strengths to nouns instead of noun classes<ul>\n<li>If noun belongs to several classes, then its choose the highest association strength among all classes </li>\n</ul>\n</li>\n<li>estimating the probability that a direct object in noun class c occurs given a verb v<ul>\n<li>A(interrupt, chair) = max(A(interrupt, people), A(interrupt, furniture)) = A(interrupt, people)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Example <img data-src=\"/img/NLP/selectional-example.png\" alt=\"\"><ul>\n<li>eat prefers fooditem <ul>\n<li>A(eat, food)=1.08 → very specific</li>\n</ul>\n</li>\n<li>seehas a uniform distribution<ul>\n<li>A(see, people)=A(see, furniture)=A(see, food)=A(see, action)=0 → no selectional preference</li>\n</ul>\n</li>\n<li>find disprefers action item<ul>\n<li>A(find, action)=-0.13 → less specific</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Semantic Similarity  </p>\n<ul>\n<li>assessing semantic similarity between a new word and other already known words</li>\n<li>Vector Space vs Probabilistic</li>\n<li>Vector Space<ul>\n<li>Words can be expressed in different spaces: document space, word spaceand modifier space</li>\n<li>Similarity measures for binary vectors: matching coefficient, Dice coefficient, Jaccard(or Tanimoto) coefficient, Overlap coefficientand cosine</li>\n<li>Similarity measures for the real-valued vector space: cosine, Euclidean Distance, normalized correlation coefficient<ul>\n<li>cosine assumes a Euclidean space which is not well-motivated when dealing with word counts</li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/similarity-measure.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Probabilistic Measures<ul>\n<li>viewing word counts by representing them as probability distributions</li>\n<li>compare two probability distributions using<ul>\n<li>KL Divergence</li>\n<li>Information Radius(Irad)</li>\n<li>L1Norm</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap14-Computational-Discourse\"><a href=\"#Chap14-Computational-Discourse\" class=\"headerlink\" title=\"Chap14 Computational Discourse\"></a>Chap14 Computational Discourse</h2><table>\n<thead>\n<tr>\n<th>Level</th>\n<th>Well-formedness constraints</th>\n<th>Types of ambiguity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Lexical</td>\n<td>Rules of inflection and derivation</td>\n<td></td>\n</tr>\n<tr>\n<td>structural, morpheme boundaries, morpheme identity</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Syntactic</td>\n<td>Grammar rules</td>\n<td>structural, POS</td>\n</tr>\n<tr>\n<td>Semantic</td>\n<td>Selection restrictions</td>\n<td>word sense, quantifier scope</td>\n</tr>\n<tr>\n<td><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU4JUFGJUFEJUU3JTk0JUE4JUU1JUFEJUE2\">Pragmatic<i class=\"fa fa-external-link-alt\"></i></span></td>\n<td>conversation principles</td>\n<td>pragmatic function</td>\n</tr>\n</tbody></table>\n<p>Computational Discourse  </p>\n<ul>\n<li>Discourse(語篇)<ul>\n<li>A group of sentences with the same coherence relation</li>\n</ul>\n</li>\n<li>Coherence relation<ul>\n<li>the 2nd sentence offers the reader an explaination or cause for the 1st sentence</li>\n</ul>\n</li>\n<li>Entity-based Coherence<ul>\n<li>relationships with the entities, introducing them and following them in a focused way</li>\n<li>Discourse Segmentation<ul>\n<li>Divide a document into a linear sequence of multiparagraph passages</li>\n<li>Academic article<ul>\n<li>Abstract</li>\n<li>Introduction</li>\n<li>Methodology</li>\n<li>Results</li>\n<li>Conclusion</li>\n</ul>\n</li>\n<li><img data-src=\"http://www.wannabehacks.co.uk/images/Inverted_pyramid_in_comprehensive_form.jpg\" alt=\"Inverted Pyramid\"></li>\n<li>Applications<ul>\n<li>News</li>\n<li>Summarize different segments of a document</li>\n<li>Extract information from inside a single discourse segment</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>TextTiling (Hearst,1997)  </p>\n<ul>\n<li>Tokenization<ul>\n<li>Each space-delimited word in the input is converted to lower-case</li>\n<li>Words in a stop list of function words are thrown out</li>\n<li>The remaining words are morphologically stemmed</li>\n<li>The stemmed words are grouped into pseudo-sentencesof length w = 20</li>\n</ul>\n</li>\n<li>Lexical score determination<ul>\n<li>compute a lexical cohesion(結合) score between pseudo-sentences<ul>\n<li>score: average similarity of words in the pseudo-sentences before gap to pseudo-sentences after the gap(??)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Boundary identification    <ul>\n<li>Compute a depth score for each gap</li>\n<li>Boundaries are assigned at any valley which is deeper than a cutoff</li>\n</ul>\n</li>\n</ul>\n<p>Coherence Relations  </p>\n<ul>\n<li>Result<ul>\n<li>The Tin Woodman was caught in the rain. His joints rusted</li>\n</ul>\n</li>\n<li>Explanation<ul>\n<li>John hid Bill’s car keys. He was drunk</li>\n</ul>\n</li>\n<li>Parallel<ul>\n<li>The Scarecrow wanted some brains</li>\n<li>The Tin Woodman wanted a heart</li>\n</ul>\n</li>\n<li>Elaboration(詳細論述)<ul>\n<li>Dorothy was from Kansas</li>\n<li>She lived in the midst of the great Kansas prairies</li>\n</ul>\n</li>\n<li>Occasion(起因)<ul>\n<li>Dorothy picked up the oil-can</li>\n<li>She oiled the Tin Woodman’s joints</li>\n</ul>\n</li>\n</ul>\n<p>Coherence Relation Assignment  </p>\n<ul>\n<li>Discourse parsing</li>\n<li>Open problems</li>\n</ul>\n<p>Cue-Phrase-Based Algorithm  </p>\n<ul>\n<li><p>Using cue phrases</p>\n<ul>\n<li>Segment the text into discourse segments</li>\n<li>Classify the relationship between each consecutive discourse</li>\n</ul>\n</li>\n<li><p>Cue phrase</p>\n<ul>\n<li>connectives, which are often conjunctions or adverbs <ul>\n<li>because, although, but, for example, yet, with, and</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>discourse uses vs. sentential uses</p>\n<ul>\n<li><strong>With</strong> its distant orbit, Mars exhibits frigid weather conditions. (因為長距離的運行軌道，火星天氣酷寒)</li>\n<li>We can see Mars <strong>with</strong> an ordinary telescope</li>\n</ul>\n</li>\n<li><p><img data-src=\"/img/NLP/discourse-relation.png\" alt=\"\"></p>\n</li>\n<li><p>Temporal Relation  </p>\n<ul>\n<li>ordered in time (Asynchronous)<ul>\n<li>before, after …</li>\n</ul>\n</li>\n<li>overlapped (Synchronous)<ul>\n<li>at the same time</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Contingency Relation</p>\n<ul>\n<li>因果關係，附帶條件</li>\n</ul>\n</li>\n<li><p>Comparison Relation</p>\n<ul>\n<li>difference between two arguments</li>\n</ul>\n</li>\n<li><p>Expansion Relation</p>\n<ul>\n<li>expands the information for one argument in the other one or continues the narrative flow</li>\n</ul>\n</li>\n<li><p>Implicit Relation</p>\n<ul>\n<li>Discourse marker is absent</li>\n<li>颱風來襲，學校停止上課</li>\n</ul>\n</li>\n<li><p>Chinese Relation Words <img data-src=\"/img/NLP/chinese-coherence-relation.png\" alt=\"\"> </p>\n<ul>\n<li>Ambiguous Discourse Markers <ul>\n<li>而：而且, 然而, 因而</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Reference-Resolution\"><a href=\"#Reference-Resolution\" class=\"headerlink\" title=\"Reference Resolution\"></a>Reference Resolution</h3><p><img data-src=\"/img/NLP/reference-resolution.png\" alt=\"\">  </p>\n<ul>\n<li>Evoke<ul>\n<li>When a referent is first mentioned in a discourse, we say that a representation for it is <strong>evoked into</strong> the model</li>\n</ul>\n</li>\n<li>Access<ul>\n<li>Upon subsequent mention, this representation is <strong>accessed from</strong> the model</li>\n</ul>\n</li>\n</ul>\n<p>Five Types of Referring Expressions  </p>\n<ul>\n<li>Indefinite Noun Phrases(不定名詞)<ul>\n<li>marked with the determiner a, some, this …</li>\n<li>Create a new internal symbol and add to the current world model<ul>\n<li>Mayumi has bought a new automobile</li>\n<li>automobile(g123)</li>\n<li>new(g123)</li>\n<li>owns(mayumi, g123)</li>\n</ul>\n</li>\n<li>non-specific sense to describe an object<ul>\n<li>Mayumi wantsto buy a new XJE</li>\n</ul>\n</li>\n<li>whole classes of objects<ul>\n<li>A new automobiletypically requires repair twice in the first 12 months</li>\n</ul>\n</li>\n<li>collect one or more properties<ul>\n<li>The Macho GTE XL is a new automobile</li>\n</ul>\n</li>\n<li>Question and commands<ul>\n<li>Is her automobile in a parking placenear the exit?</li>\n<li>Put her automobile into a parking placenear the exit!</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Definite Noun Phrases(定名詞)<ul>\n<li>simple referential and generic uses(the same as indefinite)</li>\n<li>indicate an individual by description that they satisfy<ul>\n<li>The manufacturer <strong>of this automobile</strong> should be indicted</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Pronouns(代名詞)<ul>\n<li>reference backs to entities that have been introduced by previous nounphrases in a discourse</li>\n<li>non-referential noun phrase<ul>\n<li>non-exist object</li>\n</ul>\n</li>\n<li>logical variable<ul>\n<li>No male driveradmits that heis incompetent </li>\n</ul>\n</li>\n<li>something that is available from the context of utterance, but has not been explicitly mentioned before<ul>\n<li>Here they come, late again!</li>\n<li>Can’t easily know who are “they”</li>\n</ul>\n</li>\n<li>Anaphora<ul>\n<li>Number Agreements<ul>\n<li>John has a Ford Falcon. It is red</li>\n<li>John has three Ford Falcons. They are red</li>\n</ul>\n</li>\n<li>Person Agreement(人稱)</li>\n<li>Gender Agreement</li>\n<li>Selection Restrictions<ul>\n<li>verb and its arguments</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Demonstratives (指示詞)<ul>\n<li>this, that</li>\n</ul>\n</li>\n<li>Names<ul>\n<li>Full name &gt; long definite description &gt; short definite description &gt; last name&gt; first name &gt; distal demonstrative &gt; proximate demonstrative &gt; NP &gt; stressed pronoun &gt; unstressed pronoun</li>\n</ul>\n</li>\n</ul>\n<p>Information Status  </p>\n<ul>\n<li>Referential forms used to provide new or old information</li>\n<li>givenness hierarchy <img data-src=\"/img/NLP/givenness-hierarchy.png\" alt=\"\"></li>\n<li>Definite-indefinite is a clue to given-new status<ul>\n<li>The sales managere(given) employed a foreign distributor(new)</li>\n</ul>\n</li>\n<li>If there are ambiguous noun phrases in a sentence, then it extracts the presuppositions to provide extra constraints</li>\n<li>When some new information is added to knowledge base, check if it is consistent with what we already know</li>\n</ul>\n<p>Active model of understanding  </p>\n<ul>\n<li>Given a text, build up predictions or expectations about new information and actively compare these with successive input to resolve ambiguities</li>\n<li>Construct a proof of the information provided in a sentence from the existing world knowledge and plausible inference rules illustrated</li>\n<li>the inference are not sensitive to the order<ul>\n<li>if the proposition that the disc is heavy is inferred, then it is not changed after the discourse has finished</li>\n<li>Solution: describe the propositions in temporal order</li>\n</ul>\n</li>\n<li>Script: encapsulate a sequence of actions that belong together into a script<figure class=\"highlight dns\"><table><tr><td class=\"code\"><pre><span class=\"line\">automobile_buying:</span><br><span class=\"line\">&lt;&#123;customer(C), automobile(<span class=\"keyword\">A</span>), dealer(D), garage(G)&#125;,</span><br><span class=\"line\">\t&lt;</span><br><span class=\"line\">\t\tgoes(C, G),</span><br><span class=\"line\">\t\ttest_drives(C, <span class=\"keyword\">A</span>),</span><br><span class=\"line\">\t\torders(C, <span class=\"keyword\">A</span>, D),</span><br><span class=\"line\">\t\tdelivers(D, <span class=\"keyword\">A</span>, C),</span><br><span class=\"line\">\t\tdrives(C, <span class=\"keyword\">A</span>)</span><br><span class=\"line\">\t&gt;</span><br><span class=\"line\">&gt;</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<h2 id=\"參考資料\"><a href=\"#參考資料\" class=\"headerlink\" title=\"參考資料\"></a>參考資料</h2><ul>\n<li>HHChen 課堂講義</li>\n</ul>\n",
            "tags": [
                "機器學習",
                "自然語言處理",
                "統計"
            ]
        },
        {
            "id": "http://gitqwerty777.github.io/natural-language-processing/",
            "url": "http://gitqwerty777.github.io/natural-language-processing/",
            "title": "自然語言處理(上)",
            "date_published": "2015-03-07T03:00:47.000Z",
            "content_html": "<h2 id=\"Chap01-Introduction\"><a href=\"#Chap01-Introduction\" class=\"headerlink\" title=\"Chap01 Introduction\"></a>Chap01 Introduction</h2><h3 id=\"Applications-of-NLP\"><a href=\"#Applications-of-NLP\" class=\"headerlink\" title=\"Applications of NLP\"></a>Applications of NLP</h3><ul>\n<li>Machine translation<ul>\n<li>google translate</li>\n</ul>\n</li>\n<li>Speech recognition<ul>\n<li>Siri</li>\n</ul>\n</li>\n<li>Smart input method<ul>\n<li>ㄐㄅㄈㄏ → 加倍奉還</li>\n</ul>\n</li>\n<li>Sentiment(情感) analysis</li>\n<li>Information retrieval</li>\n<li>Question Anwering<ul>\n<li>Turing Test</li>\n</ul>\n</li>\n<li>Optical character recognition (OCR)<a id=\"more\"></a>\n\n</li>\n</ul>\n<h3 id=\"Critical-Problems-in-NLP\"><a href=\"#Critical-Problems-in-NLP\" class=\"headerlink\" title=\"Critical Problems in NLP\"></a>Critical Problems in NLP</h3><ul>\n<li>Ambiguity(不明確性)<ul>\n<li><strong>The most important thing in NLP</strong></li>\n<li>Lexical(字辭)<ul>\n<li><code>current</code>: noun or adjective</li>\n<li><code>bank</code> (noun): money or river</li>\n</ul>\n</li>\n<li>Syntactic(語法)<ul>\n<li><code>[saw [the boy] [in the park]]</code></li>\n<li><code>[saw [the boy in the park]]</code></li>\n</ul>\n</li>\n<li>Semantic(語義)<ul>\n<li>“John kissed his wife, and so did Sam”. (Sam kissed John’s wife or his own?)</li>\n<li>agent(施事) vs. patient(受事)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>ill-form(bad form)<ul>\n<li>typo</li>\n<li>grammatical errors</li>\n</ul>\n</li>\n<li>Robustness<ul>\n<li>various domain</li>\n<li>網路語言：取材於方言俗語、各門外語、縮略語、諧音、甚至以符號合併以達至象形效果等等<ul>\n<li>emoticon(表情符號)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Main-Topics-in-Large-Scale-NLP-Design\"><a href=\"#Main-Topics-in-Large-Scale-NLP-Design\" class=\"headerlink\" title=\"Main Topics in Large-Scale NLP Design\"></a>Main Topics in Large-Scale NLP Design</h3><ul>\n<li>Knowledge representation<ul>\n<li>organize and describe linguistic knowledge</li>\n</ul>\n</li>\n<li>Knowledge strategies<ul>\n<li>use knowledge for efficient parsing, ambiguity resolution, ill-formed recovery</li>\n</ul>\n</li>\n<li>Knowledge acquisition<ul>\n<li>setup and maintain knowledge base systematically and cost-effectively</li>\n</ul>\n</li>\n<li>Knowledge integration<ul>\n<li>consider various knowledge sources effectively</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Models\"><a href=\"#Models\" class=\"headerlink\" title=\"Models\"></a>Models</h3><p>用演算法來轉換文字結構，以產生最後結果   </p>\n<ul>\n<li>State machines</li>\n<li>Rule-based approaches</li>\n<li>Logical formalisms</li>\n<li>Probabilistic models</li>\n</ul>\n<h3 id=\"Approaches\"><a href=\"#Approaches\" class=\"headerlink\" title=\"Approaches\"></a>Approaches</h3><p>NLP start from 1960, <strong>statictics method</strong> wins after 1995</p>\n<p>Rule-based approach</p>\n<ul>\n<li>Advantages<ul>\n<li>No need database</li>\n<li>Easy to incorporate with knowledge</li>\n<li>Better generalization to a unseen domain</li>\n<li>Explainable and traceable<ul>\n<li>easy to understand</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Disadvantages<ul>\n<li>Hard to maintain consistency (at different situation)</li>\n<li>Hard to handle uncertain knowledge (define uncertainty factor)<ul>\n<li>irregular information</li>\n</ul>\n</li>\n<li>Not easy to avoid redundancy</li>\n<li>Knowledge acquisition is time consuming</li>\n</ul>\n</li>\n</ul>\n<p>Corpus(語料庫)-based approach  </p>\n<ul>\n<li>Advantages<ul>\n<li>Knowledge acquisition can be automatically achieved by the computer</li>\n<li>Uncertain knowledge can be objectively quantified(知識可被量化)</li>\n<li><strong>Consistency and completeness</strong> are easy to obtain</li>\n<li>Well established statistical theories and technique are available</li>\n</ul>\n</li>\n<li>Disadvantages<ul>\n<li>Generalization is poor for small-size database</li>\n<li>Unable to reasoning</li>\n<li>Hard to identify the effect of each parameter</li>\n<li>Build database is time consuming</li>\n</ul>\n</li>\n<li>Corpus<ul>\n<li>Brown Corpus (1M words),Birmingham Corpus (7.5M words), LOB Corpus (1M words), etc</li>\n<li>Corpora(語料庫(複數)) of special domains or style<ul>\n<li>Newspaper, Bible, etc</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Information in Corpora<ul>\n<li>pure-text corpus<ul>\n<li>language usage of real world, word distribution, co-occurrence</li>\n</ul>\n</li>\n<li>tagged corpus<ul>\n<li>parts of speech, structures, features</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Hybrid approach   </p>\n<ul>\n<li>Use rule-based approach when <ul>\n<li>there are rules that have good coverage<ul>\n<li>it can be governed by a small number of rules</li>\n</ul>\n</li>\n<li>extensional knowledge is important to the system</li>\n</ul>\n</li>\n<li>Use corpus-based approach when<ul>\n<li>Knowledge needed to solve the problem is huge and intricate</li>\n<li>A good model or formulation exists</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h3><p><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5ubHRrLm9yZy8=\">Natural Language Toolkit(NLTK)<i class=\"fa fa-external-link-alt\"></i></span>: Open source Python modules, linguistic data and documentation for research and development in natural language processing</p>\n<p>features  </p>\n<ul>\n<li>Corpus readers</li>\n<li>Tokenizers<ul>\n<li>whitespace, newline, blankline, word, treebank, sexpr, regexp, Punkt sentence segmenter</li>\n</ul>\n</li>\n<li>Stemmers<ul>\n<li>Porter, Lancaster, regexp</li>\n</ul>\n</li>\n<li>Taggers<ul>\n<li>regexp, n-gram, backoff, Brill, HMM, TnT</li>\n</ul>\n</li>\n<li>Chunkers<ul>\n<li>regexp, n-gram, named-entity</li>\n</ul>\n</li>\n<li>Metrics<ul>\n<li>accuracy, precision, recall, windowdiff, distance metrics, inter-annotator agreement coefficients, word association measures, rank correlation</li>\n</ul>\n</li>\n<li>Estimation<ul>\n<li>uniform, maximum likelihood, Lidstone, Laplace, expected likelihood, heldout, cross-validation, Good-Turing, Witten-Bell</li>\n</ul>\n</li>\n<li>Miscellaneous<ul>\n<li>unification, chatbots, many utilities</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap02-Overall-Pictures\"><a href=\"#Chap02-Overall-Pictures\" class=\"headerlink\" title=\"Chap02 Overall Pictures\"></a>Chap02 Overall Pictures</h2><p><img data-src=\"/img/NLP/overview.png\" alt=\"overview\"></p>\n<p>Knowledge Categories     </p>\n<ul>\n<li>Phonology(聲音，資料來源)</li>\n<li>Morphology(詞性)</li>\n<li>Syntax(句構)</li>\n<li>Semantics(語義)</li>\n<li>Pragmatics(句子關聯，語用學)</li>\n<li>Discourse(篇章分析，話語)</li>\n</ul>\n<h3 id=\"Morphology-Structure-of-words\"><a href=\"#Morphology-Structure-of-words\" class=\"headerlink\" title=\"Morphology(Structure of words)\"></a>Morphology(Structure of words)</h3><ul>\n<li>part-of-speech(POS) tagging(詞性標註, lexical category)</li>\n<li>find the roots of words<ul>\n<li>e.g., going → go, cats → cat</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Syntax-structure-of-sentences\"><a href=\"#Syntax-structure-of-sentences\" class=\"headerlink\" title=\"Syntax(structure of sentences)\"></a>Syntax(structure of sentences)</h3><ul>\n<li>Context-Free Grammars(CFG) <img data-src=\"/img/NLP/cfg.png\" alt=\"parse tree\"></li>\n<li>Chomsky Normal Form(CNF)<ul>\n<li>can only use following two rules <ol>\n<li><code>non-terminal → terminal</code></li>\n<li><code>non-terminal → non-terminal non-terminal</code></li>\n</ol>\n</li>\n</ul>\n</li>\n<li>dependency<ul>\n<li>local dependency<ul>\n<li>words near together would probably have the same syntax rule</li>\n</ul>\n</li>\n<li>long-distance dependency <ul>\n<li>wh-movement(疑問詞移位)<ul>\n<li>What did Jennifer buy? → 什麼 (助動詞) 珍妮佛 買了</li>\n</ul>\n</li>\n<li>分裂句 Right-node raising<ul>\n<li>[[she would have bought] and [he might sell]] shares</li>\n</ul>\n</li>\n<li>Argument-cluster coordination<ul>\n<li>I give [[you an apple] and [him a pear]]</li>\n</ul>\n</li>\n<li><strong>challenge for some statistical NLP approaches (like n-grams)</strong></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Semantics-meaning-of-individual-sentences\"><a href=\"#Semantics-meaning-of-individual-sentences\" class=\"headerlink\" title=\"Semantics(meaning of individual sentences)\"></a>Semantics(meaning of individual sentences)</h3><ul>\n<li>semantic roles  <ul>\n<li>agent(主詞)</li>\n<li>patient(受詞)</li>\n<li>instrument(工具)</li>\n<li>goal(目標)</li>\n<li>Beneficiary(受益)</li>\n<li>He threw the book(patient) at me(goal)</li>\n<li>John sold the car for a friend(beneficiary)</li>\n</ul>\n</li>\n<li>Subcategorizations(次分類)<ul>\n<li>及物、不及物動詞</li>\n</ul>\n</li>\n<li>Semantics can be divided into two parts<ul>\n<li>Lexical Semantics<ul>\n<li>上下位，同義(反義)，部分-整體</li>\n</ul>\n</li>\n<li>Composition Semantics<ul>\n<li>合起來的意義與單一字意義不同</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Implementation<ul>\n<li>WordNet®(large lexical database of English)</li>\n<li>Thesaurus(索引典)</li>\n<li>同義詞詞林</li>\n<li>廣義知網中文詞知識庫(E-HowNet)</li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9mcmFtZW5ldC5pY3NpLmJlcmtlbGV5LmVkdS9mbmRydXBhbC9hYm91dA==\">FrameNet<i class=\"fa fa-external-link-alt\"></i></span></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"FrameNet\"><a href=\"#FrameNet\" class=\"headerlink\" title=\"FrameNet\"></a>FrameNet</h4><p>A dictionary of more than 10,000 word senses, 170,000 manually annotated sentences</p>\n<p>Frame Semantics(Charles J. Fillmore)  </p>\n<ul>\n<li>the meanings of most words can be more understood by semantic frame</li>\n<li>Including description of a type of event, relation, or entity and the participants in it</li>\n<li>Example: <code>apply_heat</code> frame<ul>\n<li>When one of these words appear, this frame will be applied<ul>\n<li><code>Fry(炸)</code>, <code>bake(烘)</code>, <code>boil(煮)</code>, a`nd broil(烤)</li>\n</ul>\n</li>\n<li>Frame elements: Cook, Food, Heating_instrument and Container<ul>\n<li>a person doing the cooking (Cook)</li>\n<li>the food that is to be cooked (Food)</li>\n<li>something to hold the food while cooking (Container)</li>\n<li>a source of heat (Heating_instrument)</li>\n</ul>\n</li>\n<li>[<code>Cook</code> the boys] … GRILL [<code>Food</code> fish] [<code>Heating_instrument</code> on an open fire]</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Pragmatics-how-sentences-relate-to-each-other\"><a href=\"#Pragmatics-how-sentences-relate-to-each-other\" class=\"headerlink\" title=\"Pragmatics(how sentences relate to each other)\"></a>Pragmatics(how sentences relate to each other)</h3><ul>\n<li>explain what the speaker really expressed</li>\n<li>Understand the scope of <ul>\n<li>quantifiers</li>\n<li>speech acts</li>\n<li>discourse analysis</li>\n<li>anaphoric relations(首語重複)</li>\n</ul>\n</li>\n<li>Anaphora(首語重複) and Coreference(指代)<ul>\n<li>張三是老師,他教學很認真,同時,他也是一個好爸爸。</li>\n<li>Type/Instance: “老師”/“張三”, “一個好爸爸”/“張三”</li>\n</ul>\n</li>\n<li>crucial to <strong>information extraction</strong></li>\n<li>Dialogue Tagging <img data-src=\"/img/NLP/dialogue_tag.png\" alt=\"\"></li>\n</ul>\n<h3 id=\"Discourse-Analysis\"><a href=\"#Discourse-Analysis\" class=\"headerlink\" title=\"Discourse Analysis\"></a>Discourse Analysis</h3><p>Example  </p>\n<ul>\n<li>1a: 佛羅倫斯哪個博物館在1993年的爆炸事件中受到破壞？</li>\n<li>1b: 這個事件哪一天發生？<ul>\n<li>問句1b「這個事件」，指的是問句1a「1993年的爆炸事件」</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>From <span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5paXMuc2luaWNhLmVkdS50dy9wYWdlL2V2ZW50cy9GSUxFLzEyMDMxMzEwMTA3U2xpZGVzLnBkZg==\">The Three (and a Half) Futures of NLP<i class=\"fa fa-external-link-alt\"></i></span></p>\n<ul>\n<li>NLP is <strong>Notation Transformation</strong>(e.g. English → Chinese), with some information(POS, syntatic, senmatic…) added</li>\n<li>Much NLP is engineering<ul>\n<li>select and tuning learning performance</li>\n</ul>\n</li>\n<li>Knowledge is crucial in language-related research areas, but providing a large scaleknowledge base is difficult and costly<ul>\n<li>Knowledge Base<ul>\n<li>WordNet</li>\n<li>FrameNet</li>\n<li>Wikipedia</li>\n<li>Dbpedia</li>\n<li>Freebase</li>\n<li>Siri</li>\n<li>Google Knowledge Graph</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Hierarchy of transformations(由深至淺)<ul>\n<li>pragmatics, writing style<ul>\n<li>deeper semantics, discourse<ul>\n<li>shallow semantics, co-reference<ul>\n<li>syntax, POS(part-of-speech)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>分析時由淺至深</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Analysis\"><a href=\"#Analysis\" class=\"headerlink\" title=\"Analysis\"></a>Analysis</h4><p><img data-src=\"/img/NLP/layer.png\" alt=\"Layer\"><br><img data-src=\"/img/NLP/l1.png\" alt=\"L1\"><br><img data-src=\"/img/NLP/l2.png\" alt=\"L2\"><br><img data-src=\"/img/NLP/l3.png\" alt=\"L3\"><br><img data-src=\"/img/NLP/l4.png\" alt=\"L4\"></p>\n<h4 id=\"NLP-progress-by-now\"><a href=\"#NLP-progress-by-now\" class=\"headerlink\" title=\"NLP progress by now\"></a>NLP progress by now</h4><p><img data-src=\"/img/NLP/sub.png\" alt=\"NLP subclass\"><br><img data-src=\"/img/NLP/dowell.png\" alt=\"NLP do today\"><br><img data-src=\"/img/NLP/cantdo.png\" alt=\"NLP can&#39;t do today\">  </p>\n<h2 id=\"Chap03-Collocations-搭配詞\"><a href=\"#Chap03-Collocations-搭配詞\" class=\"headerlink\" title=\"Chap03 Collocations(搭配詞)\"></a>Chap03 Collocations(搭配詞)</h2><ul>\n<li>多個單字組合成一個有意義的語詞，其意義無法從各個單字中推得<ul>\n<li>e.g. black market</li>\n</ul>\n</li>\n<li>Subclasses of Collocations<ul>\n<li>compound nouns<ul>\n<li>telephone box and post office</li>\n</ul>\n</li>\n<li>idioms<ul>\n<li>kick the bucket(氣絕)  </li>\n</ul>\n</li>\n<li>Light verbs(輕動詞)<ul>\n<li>動詞失去其意義，需要和其他有實質意義的詞作搭配</li>\n<li>e.g. The man took a walk(walk, not take) vs The man took a radio(take)</li>\n</ul>\n</li>\n<li>Verb particle constructions(語助詞) or Phrasal Verbs(詞組動詞, 短語動詞, V + 介系詞)<ul>\n<li>take in = deceive, look sth. up</li>\n</ul>\n</li>\n<li>proper names<ul>\n<li>San Francisco</li>\n</ul>\n</li>\n<li>Terminology(專有名詞)</li>\n</ul>\n</li>\n<li>Classification<ul>\n<li>Fixed expressions<ul>\n<li>in short (O)</li>\n<li>in shorter or in very short(X)</li>\n</ul>\n</li>\n<li>Semi-fixed expressions(可用變化形)<ul>\n<li>non-decomposable idioms<ul>\n<li>kick the bucket (O)</li>\n<li>he kicks the bucket(O)</li>\n<li>the bucket was kicked (X)</li>\n</ul>\n</li>\n<li>compound nominals<ul>\n<li>car park, car parks</li>\n</ul>\n</li>\n<li>Proper names</li>\n</ul>\n</li>\n<li>Syntactically-Flexible Expressions<ul>\n<li>decomposable idioms<ul>\n<li>let the cat out of the bag</li>\n</ul>\n</li>\n<li>verb-particle constructions</li>\n<li>light verbs</li>\n</ul>\n</li>\n<li>Institutionalized Phrases (習慣用法)<ul>\n<li>salt and pepper(○) pepper and salt(×)</li>\n<li>traffic light</li>\n<li>kindle excitement(點燃激情)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Collocation-detection\"><a href=\"#Collocation-detection\" class=\"headerlink\" title=\"Collocation detection\"></a>Collocation detection</h3><ul>\n<li>by Frequency</li>\n<li>by Mean and Variance of the distance between focal word (焦點詞) and collocating word(搭配詞)</li>\n<li>Hypothesis Testing</li>\n<li>Mutual Information</li>\n</ul>\n<h4 id=\"By-Frequency\"><a href=\"#By-Frequency\" class=\"headerlink\" title=\"By Frequency\"></a>By Frequency</h4><ul>\n<li>找出現機率大的bigrams<ul>\n<li>not always significant</li>\n<li>篩選可能為組合詞的詞性組合(Ex. adj+N) </li>\n</ul>\n</li>\n<li>The collocations found <img data-src=\"/img/NLP/freandtag.png\" alt=\"\"></li>\n<li>What if a word have two possible collocations?(strong force, powerful force) <img data-src=\"/img/NLP/frecomp.png\" alt=\"\"></li>\n</ul>\n<h4 id=\"By-Mean-and-Variance-of-the-distance-between-focal-word-焦點詞-and-collocating-word-搭配詞\"><a href=\"#By-Mean-and-Variance-of-the-distance-between-focal-word-焦點詞-and-collocating-word-搭配詞\" class=\"headerlink\" title=\"By Mean and Variance of the distance between focal word(焦點詞) and collocating word(搭配詞)\"></a>By Mean and Variance of the distance between focal word(焦點詞) and collocating word(搭配詞)</h4><ul>\n<li>many collocations consist of more flexible relationships<ul>\n<li>frequency is not suitable</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li>Define a collocational window<ol>\n<li>e.g., 3-4 words before/after</li>\n</ol>\n</li>\n<li>assemble every word pair as a bigram<ol>\n<li>e.g., A B C D → AB, AC, AD, BC, BD, CD</li>\n</ol>\n</li>\n<li>computes the mean and variance of the offset between the two words<ol>\n<li>變異數愈低，代表兩個字之間的位置關聯愈固定 <img data-src=\"/img/NLP/meanvar.png\" alt=\"\"></li>\n</ol>\n</li>\n</ol>\n<ul>\n<li>z-score $z = {freq - \\mu \\over \\sigma}$: the strength of a word pair</li>\n</ul>\n<h4 id=\"Hypothesis-Testing-假設檢定\"><a href=\"#Hypothesis-Testing-假設檢定\" class=\"headerlink\" title=\"Hypothesis Testing(假設檢定)\"></a>Hypothesis Testing(假設檢定)</h4><ul>\n<li>Even high frequency and low variance can be accidental</li>\n<li>null hypothesis(虛無假設, H0) <ul>\n<li>設 w1 and w2 is completely independent → w1 and w2 不是搭配詞<ul>\n<li>P(w1w2) = P(w1)P(w2) </li>\n</ul>\n</li>\n</ul>\n</li>\n<li>假設H0為真，計算這兩個字符合H0的機率P<ul>\n<li>若P太低則否決H0(→ 是搭配詞)</li>\n</ul>\n</li>\n<li>Two issues<ul>\n<li>Look for particular patterns in the data</li>\n<li>How much data we have seen</li>\n</ul>\n</li>\n<li>種類包括：t檢驗，Z檢驗，卡方檢驗，F檢驗      </li>\n</ul>\n<h5 id=\"t-test\"><a href=\"#t-test\" class=\"headerlink\" title=\"t-test\"></a>t-test</h5><ul>\n<li>Test whether <strong>distributions of two groups</strong> are <strong>statistically different</strong> or not<ul>\n<li>H0 → (w1, w2) has no differnece with normal distribution</li>\n<li>considering <strong>variance</strong> of the data <img data-src=\"/img/NLP/ttest.png\" alt=\"\"></li>\n<li>formula <img data-src=\"/img/NLP/ttest2.png\" alt=\"\"> <img data-src=\"/img/NLP/ttest3.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Calculate t by alpha level and degree of freedom<ul>\n<li>alpha level <code>α</code>: confidence<ul>\n<li>in normal distribution，α = 95%落在mean±1.96std之間, α = 99%落在mean±2.576std之間</li>\n<li>If t-value is larger than 2.576, we say the two groups <strong>are different</strong> with 99% confidence</li>\n</ul>\n</li>\n<li>degree of freedom: number of sample-1<ul>\n<li>total = number of two groups-2</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>t↑ → more difference → more possible to reject null hypothesis → more possible to be collocation</strong></li>\n<li>Example: “new” occurs 15,828 times, “companies” 4,675 times, “new companies” 8 times, total 14,307,668 tokens<ul>\n<li><strong>Null hypothesis: the occurrences of new and companies are independent(not collocation)</strong></li>\n<li>H0 mean = P(new, companies) = P(new) x P(companies) = $\\frac{15828 \\times 4678}{14307668^2} = 3.615 \\times 10^{-7}$</li>\n<li>H0 var = p(1-p) ~= p when p is small</li>\n<li>tvalue = $\\frac{5.591 \\times 10^7 - 3.615\\times 10^7}{\\sqrt{\\frac{5.591 \\times 10^7}{14307668}}} = 0.999932$</li>\n<li>0.999932 &lt; 2.576, we cannot reject the null hypothesis<ul>\n<li>new company are not collocation</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>the above words are possible collocations <img data-src=\"/img/NLP/ttest4.png\" alt=\"\"></li>\n</ul>\n<!--???????-->\n<p>Hypothesis testing of differences    </p>\n<ul>\n<li>useful for lexicography <ul>\n<li>which word(strong, powerful) is suitable to modify “computer”?</li>\n</ul>\n</li>\n<li>T-test can be used for <strong>comparison of the means of two normal populations</strong> <ul>\n<li>H0 is that the average difference is 0 (u = 0)</li>\n<li>v1 and v2 are the words we are comparing (e.g., powerful and strong), and w is the collocate of interest(e.g., computers)</li>\n<li><img data-src=\"/img/NLP/ttest5.png\" alt=\"\">)<img data-src=\"/img/NLP/ttest6.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<h5 id=\"Chi-Square-test\"><a href=\"#Chi-Square-test\" class=\"headerlink\" title=\"Chi-Square test\"></a>Chi-Square test</h5><ul>\n<li>T-test assumes that probabilities are normally distributed<ul>\n<li>not really</li>\n</ul>\n</li>\n<li>Chi-Square: compare <strong>observed frequencies</strong> with <strong>expected frequencies</strong><ul>\n<li>If <strong>difference between observed and expected frequencies</strong> is large, we can reject H0</li>\n</ul>\n</li>\n<li>Example<ul>\n<li>expected frequency of “new companies”: $\\frac{8+4667}{14307668} \\times \\frac{8+15820}{14307668} \\times 14307668$ = 5.2 <img data-src=\"/img/NLP/chi1.png\" alt=\"\"></li>\n<li>chi-square value = χ^2 <img data-src=\"/img/NLP/chi3.png\" alt=\"\"> <img data-src=\"/img/NLP/chi2.png\" alt=\"\"></li>\n<li>When α=0.05, χ^2=3.841</li>\n<li>Because 1.55&lt;3.841, we cannot reject the null hypothesis. new companies is not a good candidate for a collocation</li>\n</ul>\n</li>\n<li>Comparison with T-test<ul>\n<li>The 20 bigrams with the highest t scores in the test corpus are also the 20 bigrams with the highest χ^2 scores</li>\n<li>χ^2 is appropriate for large probabilities(t-test is not because of normality assumption)</li>\n</ul>\n</li>\n<li>Application: Translation<ul>\n<li>find similarity of word pairs</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Likelihood-Ratios\"><a href=\"#Likelihood-Ratios\" class=\"headerlink\" title=\"Likelihood Ratios\"></a>Likelihood Ratios</h3><ul>\n<li>Advantage compared with Chi-Square test  <ul>\n<li>more appropriate for sparse data</li>\n<li>easier to interpret</li>\n</ul>\n</li>\n</ul>\n<p>Likelihood Ratios within single corpus  </p>\n<ul>\n<li>examine two hypothesis<ul>\n<li>H1: occurrence of w2 is independent of the previous occurrence of w1(null hypothesis)</li>\n<li>H2: occurrence of w2 is dependent of the previous occurrence of w1 </li>\n</ul>\n</li>\n<li>maximum likelihood estimate <img data-src=\"/img/NLP/like1.png\" alt=\"\"></li>\n<li>using binomial distribution<ul>\n<li>$b(k;n, x) = \\binom nk x^k \\times (1-x)^{n-k}$</li>\n<li>only different at probability<ul>\n<li>$L(H_1) = b(c_{12};c_1, p)b(c_2-c_{12}; N-c_1, p)$</li>\n<li>$L(H_2) = b(c_{12};c_1, p_1)b(c_2-c_{12}; N-c_1, p_2)$</li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/likew.png\" alt=\"likely probability\"></li>\n<li>log of likelihood ratio λ <img data-src=\"/img/NLP/like2.png\" alt=\"log likelihood ratio\"></li>\n<li>use D = -2logλ to examine the significance of two words, which can asymptotically(漸近) chi-square distributed</li>\n</ul>\n</li>\n</ul>\n<p>Likelihood Ratios between two or more corpora   </p>\n<ul>\n<li>useful for the discovery of <strong>subject-specific collocations</strong></li>\n</ul>\n<h3 id=\"Mutual-Information\"><a href=\"#Mutual-Information\" class=\"headerlink\" title=\"Mutual Information\"></a>Mutual Information</h3><ul>\n<li>measure of <strong>how much one word tells us about the other</strong>(information theory)   </li>\n<li>pointwise mutual information(PMI) <img data-src=\"/img/NLP/mutual.png\" alt=\"PMI formula\"><ul>\n<li>MI是在獲得一個隨機變數的資訊之後，觀察另一個隨機變數所獲得的「資訊量」（單位通常為位元）</li>\n</ul>\n</li>\n<li>mutual information = Expection(PMI) <img data-src=\"/img/NLP/newMI.png\" alt=\"\"></li>\n<li>works bad in sparse environments<ul>\n<li>As the perfectly dependent bigrams get rarer, their mutual information increases → <strong>bad measure of dependence</strong> <img data-src=\"/img/NLP/pmi-depend.png\" alt=\"\"></li>\n</ul>\n</li>\n<li><strong>good measure of independence</strong><ul>\n<li>when perfect independence, I(x, y) = 0</li>\n<li><img data-src=\"/img/NLP/pmi-independ.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>New formula: $C(w1w2)I(w1w2)$<ul>\n<li>With MI, bigrams composed of low-frequency words will receive a higher score than bigrams composed of high-frequency words</li>\n</ul>\n</li>\n</ul>\n<p>Chain rule for entropy   </p>\n<ul>\n<li>$H(X,Y) = H(Y|X) + H(X) = H(X|Y) + H(Y)$</li>\n<li>Conditional entropy $H(Y|X)$ expresses how much <strong>extra information</strong> you still need to supply on average to communicate Y when X is known <img data-src=\"/img/NLP/conditional.png\" alt=\"\"></li>\n<li>$H(X)-H(X|Y) = H(Y)-H(Y|X)$<ul>\n<li>This difference is called the <strong>mutual information between X and Y</strong>(X, Y共同擁有的information)</li>\n<li>MI is not similar to chi-square <img data-src=\"/img/NLP/wrongMI.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>Entropy  </p>\n<ul>\n<li>Entropy: uncertainty of a variable <img data-src=\"/img/NLP/entropy1.png\" alt=\"\">  </li>\n<li>Incorrect model’s cross entropy is larger than correct model’s <img data-src=\"/img/NLP/entropy2.png\" alt=\"\"><ul>\n<li>正確model和猜測model的差別：P(X)logP(X) ↔ P(X)logPM(X)</li>\n</ul>\n</li>\n<li>Entropy Rate: Per-word entropy(= sentence entropy / N) <img data-src=\"/img/NLP/entropy_rate.png\" alt=\"\"></li>\n<li>Cross Entropy: <strong>average informaton</strong> needed to <strong>identify an event drawn from the set</strong> between two probability distributions<ul>\n<li>交叉熵的意義是用該模型對文本識別的難度，或者從壓縮的角度來看，每個詞平均要用幾個位來編碼</li>\n<li><img data-src=\"/img/NLP/entropy_cross.png\" alt=\"\">  </li>\n</ul>\n</li>\n<li>Joint entropy H(X, Y): average information needed to <strong>specify both values</strong> <img data-src=\"/img/NLP/joint.png\" alt=\"\"></li>\n</ul>\n<h3 id=\"Case-Study\"><a href=\"#Case-Study\" class=\"headerlink\" title=\"Case Study\"></a>Case Study</h3><p>Emotion Analysis  </p>\n<ul>\n<li>Non-verbal Emotional Expressions</li>\n<li>text (raw) and emoticons(表情符號) (tag) form collection</li>\n<li>appearance of an emoticon is a good emotion indicator to sentences</li>\n<li>check the dependency of each word in sentences</li>\n<li>Evaluation<ul>\n<li>Use top 200 lexiconentries as features</li>\n<li>Tag={Positive, Negative}</li>\n<li>LIBSVM</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap04-N-gram-Model\"><a href=\"#Chap04-N-gram-Model\" class=\"headerlink\" title=\"Chap04 N-gram Model\"></a>Chap04 N-gram Model</h2><p>N-grams are token sequences of length N</p>\n<p>applications   </p>\n<ul>\n<li>Automatic speech recognition</li>\n<li>Author Identification</li>\n<li>Spelling correction</li>\n<li>Grammatical Error Diagnosis</li>\n<li>Machine translation</li>\n</ul>\n<h3 id=\"Counting\"><a href=\"#Counting\" class=\"headerlink\" title=\"Counting\"></a>Counting</h3><ul>\n<li>Example: <em>I do uh main-mainly business data processing</em><ul>\n<li>Should we count “uh”(pause) as tokens?</li>\n<li>What about the repetition of “mainly”? Should such do-overs count twice or just once?(重複)</li>\n<li>The answers depend on the application<ul>\n<li>“uh” is not needed for query </li>\n<li>“uh” is very useful in dialog management</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Corpora: Google Web Crawl<ul>\n<li>1,024,908,267,229 English tokens</li>\n<li>13,588,391 wordform types</li>\n<li>even large dictionaries of English have only around 500k types. Why so many here?<ul>\n<li>Numbers</li>\n<li>Misspellings</li>\n<li>Names</li>\n<li>Acronyms(縮寫)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Language-model\"><a href=\"#Language-model\" class=\"headerlink\" title=\"Language model\"></a>Language model</h3><p>Language models assign a probability to a word sequence<br>Ex. <code>P(the mythical unicorn) = P(the) * P(mythical | the) * P(unicorn | the mythical)</code></p>\n<ul>\n<li>Markov assumption: the probability of a word depends only on <strong>limited previous words</strong>     <ul>\n<li>Generalization: n previous words, like bigram, trigrams, 4-grams……</li>\n<li>As we increase the value of N, the accuracy of model increases</li>\n</ul>\n</li>\n</ul>\n<p>N-Gram probabilities come from a training corpus<br>overly narrow corpus: probabilities don’t generalize<br>overly general corpus: probabilities don’t reflect task or domain  </p>\n<p>maximum likelihood estimate  </p>\n<ul>\n<li>maximizes the probability of the training set T given the model M</li>\n<li>Suppose the word “Chinese” occurs 400 times in a corpus<ul>\n<li>MLE estimate is 400/1000000 = .004</li>\n<li>makes it most likely that “Chinese” will occur 400 times in a million word corpus</li>\n</ul>\n</li>\n<li>P([s] I want englishfood [s]) = P(I|[s]) x P(want|I) x P(english|want) x P(food|english) x P([s]|food) = 0.000031$</li>\n</ul>\n<p>Usage</p>\n<ul>\n<li><p>capture some knowledge about language</p>\n<ul>\n<li>World Knowledge<ul>\n<li>P(english food|want) = .0011</li>\n<li>P(chinese food|want) = .0065</li>\n</ul>\n</li>\n<li>syntax<ul>\n<li>P(to|want) = .66</li>\n<li>P(eat| to) = .28</li>\n<li>P(food| to) = 0</li>\n</ul>\n</li>\n<li>discourse<ul>\n<li><code>P(i|&lt;s&gt;)</code> = .25</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Shannon’s Method: use language model to generate random sentences</p>\n<ul>\n<li>Shakespeare as a Corpus  <ul>\n<li>99.96% of the possible bigrams were never seen (have zero entries in the table)</li>\n</ul>\n</li>\n<li>This is the biggest problem in language modeling</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Evaluating-N-Gram-Models\"><a href=\"#Evaluating-N-Gram-Models\" class=\"headerlink\" title=\"Evaluating N-Gram Models\"></a>Evaluating N-Gram Models</h3><ul>\n<li>Extrinsic(外在的) evaluation<ul>\n<li>Compare performance of the application within two models</li>\n<li>time-consuming</li>\n</ul>\n</li>\n<li>Intrinsic evaluation<ul>\n<li>perplexity<ul>\n<li>But get poor approximation unless the test data looks just like the training data</li>\n</ul>\n</li>\n<li>not sufficient to publish</li>\n</ul>\n</li>\n</ul>\n<p>Standard Method</p>\n<ul>\n<li>Train → Test </li>\n<li>A dataset which is different from our training set, but both drawn from the same source</li>\n<li>use evaluation metric(Ex. perplexity)</li>\n<li>Example <ul>\n<li>Create a fixed lexicon L, of size V<ul>\n<li>At text normalization phase, <strong>any training word not in L changed to UNK</strong>(unknown word token)</li>\n<li><strong>count UNK like a normal word</strong></li>\n</ul>\n</li>\n<li>When testing, also use UNK counts for any word not in training</li>\n<li>The best language model is one that best predicts an unseen test set</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"perplexity-複雜度\"><a href=\"#perplexity-複雜度\" class=\"headerlink\" title=\"perplexity(複雜度)\"></a>perplexity(複雜度)</h3><ul>\n<li>Definition  <ul>\n<li>notion of surprise<ul>\n<li>The more surprised the model is, the lower probability it assigned to the test set</li>\n<li><strong>Minimizing perplexity is the same as maximizing probability</strong></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>probability of a test set, as normalized by the number of words <img data-src=\"/img/NLP/perplexity.png\" alt=\"\"></li>\n<li>物理意義是單詞的編碼大小<ul>\n<li>如果在某個測試語句上，語言模型的perplexity值為2^190，說明該句子的編碼需要190bits</li>\n</ul>\n</li>\n<li>relate to entropy<ul>\n<li>Perplexity(p, q) = $2^{H(p,q)}$   </li>\n<li>p is the test sample distribution, and q is the distribution of language model</li>\n<li>do everything in log space to avoid underflow and calculate faster</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"word-entropy\"><a href=\"#word-entropy\" class=\"headerlink\" title=\"word entropy\"></a>word entropy</h3><ul>\n<li>word entropy for English<ul>\n<li>11.82 bits per word [Shannon, 1951]</li>\n<li>9.8 bits per word [Grignetti, 1964]</li>\n</ul>\n</li>\n<li>word entropy in medical language<ul>\n<li>11.15 bits per word</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap05-Statistical-Inference\"><a href=\"#Chap05-Statistical-Inference\" class=\"headerlink\" title=\"Chap05 Statistical Inference\"></a>Chap05 Statistical Inference</h2><ul>\n<li>Statistical Inference：<strong>taking some data</strong> (generated by unknown distribution) and then <strong>making some inferences(推理，推測)</strong> about this distribution</li>\n<li>three issues<ul>\n<li><strong>Dividing the training data into equivalence classes</strong></li>\n<li><strong>Finding a good statistical estimator for each equivalence class</strong></li>\n<li><strong>Combining multiple estimators</strong></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Form-Equivalence-Class\"><a href=\"#Form-Equivalence-Class\" class=\"headerlink\" title=\"Form Equivalence Class\"></a>Form Equivalence Class</h3><ul>\n<li>Classification Problem<ul>\n<li><strong>predict target feature</strong> based on various <strong>classificatory features</strong></li>\n<li>reliability v.s. discrimination<ul>\n<li>The more classes, the more discrimination, but estimation feature is not reliable</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Independent assumption<ul>\n<li>assume data is nearly independent</li>\n</ul>\n</li>\n<li>Statistical Language Modeling<ul>\n<li><img data-src=\"/img/NLP/smodel.png\" alt=\"\"></li>\n<li>Language Model: P(W)</li>\n<li>LM does not depend on acoustics<ul>\n<li>the acoutstics probability is constant(calculated by data)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>n-gram model<ul>\n<li>assume equivalence classes are previous n-1 words</li>\n<li>Markov Assumption: Only the prior n-1 local context affects the next entry<ul>\n<li>(n-1)th Markov Model or n-gram</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Building n-grams</strong><ol>\n<li>Remove punctuation(標點) and normalize text</li>\n<li>Map out-of-vocabulary words to unknown symbol(UNK)</li>\n<li>Estimate conditional probabilities by joint probabilities<ul>\n<li>P(n | n-2, n-1) = P(n-2, n-1, n) / P(n-2, n-1)</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"Finding-statistical-estimator\"><a href=\"#Finding-statistical-estimator\" class=\"headerlink\" title=\"Finding statistical estimator\"></a>Finding statistical estimator</h3><ul>\n<li>Goal: derive <strong>probability estimate of target feature</strong> based on observed data</li>\n<li>Running Example<ul>\n<li>From n-gram data P(w1,..,wn), predict P(wn|w1,..,wn-1)</li>\n</ul>\n</li>\n<li>Solutions<ul>\n<li>Maximum Likelihood Estimation</li>\n<li>Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws</li>\n<li>Held Out Estimation</li>\n<li>Cross-Validation</li>\n<li>Good-Turing Estimation</li>\n</ul>\n</li>\n<li>Model combination<ul>\n<li>Combine models (unigram, bigram, trigram, …) to use the most precise model available</li>\n<li>interpolation(內插) and back-off(後退)</li>\n<li>use higher order models when model has enough data</li>\n<li>back off to lower order models when there isn’t enough data</li>\n</ul>\n</li>\n</ul>\n<p>Terminology  </p>\n<ul>\n<li>Ex. <code>[s] a b a b a</code><ul>\n<li>N = 5 (<code>[s]a,ab,ba,ab,ba</code>)</li>\n<li>B = 3 (<code>[s]a,ab,ba</code>)</li>\n<li>C(w1, w2…) = 某N-gram(Ex. ab)出現次數</li>\n<li>r =  某N-gram出現頻率</li>\n<li>Nr = 有幾個「出現r次的N-gram」</li>\n<li>Tr = 出現r次的N-gram，在test data出現的總次數</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"1-Maximum-Likelihood-Estimation\"><a href=\"#1-Maximum-Likelihood-Estimation\" class=\"headerlink\" title=\"(1) Maximum Likelihood Estimation\"></a>(1) Maximum Likelihood Estimation</h4><ul>\n<li>usually unsuitable for NLP <ul>\n<li>sparseness of the data(a lot of word sequences with zero probabilities)</li>\n</ul>\n</li>\n<li>Use Discounting or Smoothing technique to improve<ul>\n<li>Smoothing<ul>\n<li>Smoothing is like Robin Hood: Steal from the rich and give to the poor</li>\n<li>no word sequences has 0 probability <img data-src=\"/img/NLP/fBBrh6P.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Discounting<ul>\n<li>assign some probability to unseen events</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"2-Laplace’s-Lidstone’s-and-Jeffreys-Perks’-Laws\"><a href=\"#2-Laplace’s-Lidstone’s-and-Jeffreys-Perks’-Laws\" class=\"headerlink\" title=\"(2) Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws\"></a>(2) Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws</h4><ul>\n<li>Laplace: add 1 to every count <ul>\n<li>gives far too much probabilities to unseen events</li>\n<li>Usage: In domains where the number of zeros isn’t so huge<ul>\n<li>pilot studies</li>\n<li>document classification</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Lidstone and Jeffreys-Perks: add a smaller value λ &lt; 1<ul>\n<li>B:number of bins <img data-src=\"/img/NLP/lidstone.png\" alt=\"lid\"></li>\n<li>Expected Likelihood Estimation (ELE)(Jeffreys-Perks Law)<ul>\n<li>λ=1/2</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"3-Held-Out-Estimation\"><a href=\"#3-Held-Out-Estimation\" class=\"headerlink\" title=\"(3) Held Out Estimation\"></a>(3) Held Out Estimation</h4><ul>\n<li>compute frequencies in training data and held out data</li>\n<li><img data-src=\"/img/NLP/heldout.png\" alt=\"\"><ul>\n<li>Tr / Nr = Average frequency of training frequency r N-grams<ul>\n<li>estimate frequency(value for validation)</li>\n<li>計算出現在training corpus r次的bigrams，在held-out corpus出現的次數稱為Tr。 因為這種bigrams有Nr個，因此平均為Tr / Nr</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Validation<ul>\n<li>if the probabilities estimated on training data are close to those on held-out data, it’s a good language model</li>\n<li><a href=\"http://gitqwerty777.github.io/MLfoundation2/#chap15-validation\">參考資料–validation in machine learning</a></li>\n</ul>\n</li>\n<li>Prevent Overtraining(overfit)<ul>\n<li>test on different data</li>\n</ul>\n</li>\n</ul>\n<p>Training portion and testing portion (5-10% of total data)  </p>\n<ul>\n<li>Held out data (validation data)<ul>\n<li>available training data: real training data(90%) + held out data(10%)</li>\n</ul>\n</li>\n<li>Instead of presenting a single performance figure, testing result on each smaller sample<ul>\n<li>Using t-test to reject the possibility of an accidental difference</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"4-Cross-Validation\"><a href=\"#4-Cross-Validation\" class=\"headerlink\" title=\"(4) Cross-Validation\"></a>(4) Cross-Validation</h4><p>If data is not enough, use each part of the data both as training data and held out data  </p>\n<ul>\n<li>Deleted Estimation<ul>\n<li>$N_r^a$ = number of n-grams occurring r times in the <strong>a th part</strong> of the training data</li>\n<li>$T_r^{ab}$ = number of occurs in part b of 「bigrams occurs r times in part a」</li>\n<li><img data-src=\"/img/NLP/deleted_estimate.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<ol>\n<li>Split the training data into K sections</li>\n<li>For each section k: hold-out section k and compute counts from remaining K-1 sections; compute Tr(k) </li>\n<li>Estimate probabilities by averaging over all sections</li>\n</ol>\n<p>estimate frequency of deleted estimation <img data-src=\"/img/NLP/del-estimate.png\" alt=\"\"></p>\n<h4 id=\"5-Good-Turing-Estimation\"><a href=\"#5-Good-Turing-Estimation\" class=\"headerlink\" title=\"(5) Good-Turing Estimation\"></a>(5) Good-Turing Estimation</h4><ul>\n<li>用出現一次的來預測沒出現過的</li>\n<li>若出現次數&gt;k，不變，否則套用公式</li>\n<li><img data-src=\"/img/NLP/goodturing.png\" alt=\"\"><ul>\n<li>renormalize to sum = 1</li>\n</ul>\n</li>\n<li>Simple Good-Turing<ul>\n<li>replace any zeros in the sequence by linear regression: <code>log(Nc) = a+blog(c)</code></li>\n</ul>\n</li>\n<li>after good-turing <img data-src=\"/img/NLP/gttable.png\" alt=\"\"></li>\n</ul>\n<p>explaination from stanford NLP course   </p>\n<ul>\n<li>when use leave-one-out validation, the possibilities of unseen validation data is $\\frac{N_1}{N}$(when thing-saw-once is the validation data), the possibilities of validation data have been seen K times is $\\frac{(k+1)N_{k+1}}{N}$ </li>\n<li>Josh Goodman’s intuition: assume You are fishing, and caught 10 carp,3 perch,2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish<ul>\n<li>P(unseen) = N1/N0 = N1/N = 3/18</li>\n<li>C(trout) = $2 \\times N_2/N_1$ = $2 \\times (1/3)$ = 2/3<ul>\n<li>P(trout) = 2/3 / 18 = 1/27</li>\n</ul>\n</li>\n<li>for large k, often get zero estimate, so do not change the count<ul>\n<li>C(the) = 200000, C(a) = 190000, $C*(the) = (200001)N_{200001} / N_{200000} = 0 (because N_{200001} = 0)$</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"6-Absolute-Discounting\"><a href=\"#6-Absolute-Discounting\" class=\"headerlink\" title=\"(6) Absolute Discounting\"></a>(6) Absolute Discounting</h4><p>從所有非零N-gram中拿出λ，平均分配給所有未出現過的N-gram  </p>\n<h3 id=\"Combining-Estimator\"><a href=\"#Combining-Estimator\" class=\"headerlink\" title=\"Combining Estimator\"></a>Combining Estimator</h3><p>Combination Methods   </p>\n<ul>\n<li>Simple Linear Interpolation(內插)(finite mixture models)<ul>\n<li>Ex. trigram, bigram and unigram <img data-src=\"/img/NLP/linearde.png\" alt=\"\"><ul>\n<li>More generally, λ can be a function of (wn-2, wn-1, wn)</li>\n</ul>\n</li>\n<li>use <a href=\"#backward-forward\">Expectation-Maximization (EM) algorithm</a> to get weights</li>\n</ul>\n</li>\n<li>General Linear Interpolation<ul>\n<li>general form for a linear interpolation model</li>\n<li>weights are a function of the history <img data-src=\"/img/NLP/gli.png\" alt=\"\"> </li>\n</ul>\n</li>\n<li>Katz’s Backing-Off<ul>\n<li>choose proper order to train model (base on training data)<ul>\n<li>If the n-gram appeared more than k times<ul>\n<li>use MLE estimate and discount it</li>\n</ul>\n</li>\n<li>If the n-gram appeared k times or less<ul>\n<li>use an estimate from <strong>lower-order n-gram</strong></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>back-off probability <img data-src=\"/img/NLP/pbo.png\" alt=\"\"><ul>\n<li>$P_{Dis}(w_n|w_{n-2},w_{n-1})$ is specific discounted estimate. e.g., Good-Turing or Absolute Discounting </li>\n<li>unseen trigram is estimated by bigram and β <img data-src=\"/img/NLP/bosmooth.png\" alt=\"\"></li>\n<li><strong>β(wn-2, wn-1)</strong> and <strong>α</strong> are chosen so that sum of probabilities = 1</li>\n</ul>\n</li>\n<li>more genereal form <img data-src=\"/img/NLP/botable.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>Most usual approach in large speech recognition: trigram language model, Good-Turing discounting, back-off combination</p>\n</blockquote>\n<h2 id=\"Chap06-Hidden-Markov-Models-HMM\"><a href=\"#Chap06-Hidden-Markov-Models-HMM\" class=\"headerlink\" title=\"Chap06 Hidden Markov Models(HMM)\"></a>Chap06 Hidden Markov Models(HMM)</h2><ul>\n<li>statistical tools that are useful for NLP<ul>\n<li><strong>part-of-speech-tagging</strong> </li>\n<li>We construct “Visible” Markov Models in training, but treat them as Hidden Markov Models when tagging new corpora  </li>\n</ul>\n</li>\n<li>model a <strong>state sequence</strong> (perhaps through time) <strong>of random variables</strong> that have dependencies<ul>\n<li>狀態(state)並不是直接可見的，但受狀態影響的某些變量(output symbol)則是可見的</li>\n<li>known value<ul>\n<li><strong>output symbols(words)</strong> 字詞</li>\n<li>probabilistic function of state relation 和state相關的機率函式</li>\n</ul>\n</li>\n<li>unknown value<ul>\n<li><strong>state(part-of-speech tags)</strong> 目前的state，即POS tag</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>rely on 2 assumptions<ul>\n<li>Let X=(X1, …, XT) be a sequence of random variables, X is markov chain if</li>\n</ul>\n<ol>\n<li>Limited Horizon<ul>\n<li>a word’s tag only depends on <strong>previous</strong> tag(state只受前一個state影響)</li>\n</ul>\n</li>\n<li>Time Invariant<ul>\n<li>the dependency does not change over time(轉移矩陣不變)</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<p>Description   </p>\n<ul>\n<li>initial state π, state = Q, Observations = O, transition matrix = A, output(observation) matrix = B  </li>\n<li><img data-src=\"/img/NLP/hmm1.png\" alt=\"\"><ul>\n<li>$a_{ij}$ = probability of state $q_i$ transition to state $q_j$ </li>\n<li>$b_i(k)$ = probability of observe output symbol $O_k$ when state = $q_i$  </li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-problems-of-HMM\"><a href=\"#3-problems-of-HMM\" class=\"headerlink\" title=\"3 problems of HMM\"></a>3 problems of HMM</h3><p><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy41Mm5scC5jbi9obW0tbGVhcm4tYmVzdC1wcmFjdGljZXMtZm91ci1oaWRkZW4tbWFya292LW1vZGVscw==\">中文解說：隱馬可夫鏈<i class=\"fa fa-external-link-alt\"></i></span></p>\n<ol>\n<li>評估（Evaluation）：what is probability of the observation sequence given a model? (P(Observes|Model))<ul>\n<li>Used in model improvement</li>\n<li>Used in classification<ul>\n<li>Word spotting in speech recognition, language identification, speaker identification, author identification……</li>\n<li>Given an observation, compute P(O|model) for all models</li>\n</ul>\n</li>\n<li>Use <strong>Forward algorithm</strong> to solve it</li>\n</ul>\n</li>\n<li>解碼（Decoding）：Given an observation sequence and model, what is the <strong>most likely state sequence</strong>? (P(States|Observes, Model)) 下一個state是什麼<ul>\n<li>Used in tagging (tags=hidden states)</li>\n<li>Use <strong>Viterbi algorithm</strong> to solve it</li>\n</ul>\n</li>\n<li>學習（Learning）：Given an observation sequence, infer the best model parameters (argmax(Model) P(Model|Observes))<ul>\n<li>「fill in model parameters that make the observation sequence most likely」</li>\n<li>Used for building HMM Model from data</li>\n<li>Use <strong>EM(Baum-Welch, backward-forward algorithm)</strong> to solve it</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"Solutions-of-HMM-problem\"><a href=\"#Solutions-of-HMM-problem\" class=\"headerlink\" title=\"Solutions of HMM problem\"></a>Solutions of HMM problem</h3><h4 id=\"Forward\"><a href=\"#Forward\" class=\"headerlink\" title=\"Forward\"></a>Forward</h4><p><a href=\"http://www.52nlp.cn/hmm-learn-best-practices-five-forward-algorithm-1\" target=\"_blank\" rel=\"noopener external nofollow noreferrer\"></a></p>\n<ul>\n<li><img data-src=\"/img/NLP/fwformula.png\" alt=\"\"></li>\n<li>simply sum of the probability of each possible state sequence </li>\n<li>Direct evaluation<ul>\n<li>time complexity = $(2T+1) \\times N^{T+1}$ -&gt; too big <img data-src=\"/img/NLP/fw.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Use dynamic programming<ul>\n<li>record the probability of subpaths of the HMM</li>\n<li>The probability of longer subpaths can be calculated from shorter subpaths</li>\n<li>similar to Viterbi: viterbi use MAX() instead of SUM()</li>\n</ul>\n</li>\n</ul>\n<!-- Description:DP  \n- ![dp](/img/NLP/dp.png)\n- ![dp](/img/NLP/dptable.png)\n    - 選最高機率的路徑(將其他路徑的機率加入最高機率) \n    - 例：p(qqqq) = 0.01, p(qrrq) = 0.007 → P(qqqq) = 0.017\n-->\n\n<p>Forward Algorithm  </p>\n<ul>\n<li>$α_t(i)$ = probability of state = qi at time = t <img data-src=\"/img/NLP/forwardalgo.png\" alt=\"dp\"></li>\n<li>α的求法：將time = t-1 的 α 值，乘上在time = t時會在qi state的機率，並加總 <img data-src=\"/img/NLP/forwardfex.png\" alt=\"dp\"></li>\n<li>順向推出所有可能的state sequence會產生此observation的機率和, 即為此model會產生此observation的機率 <img data-src=\"/img/NLP/forwardexample.png\" alt=\"dp\"><ul>\n<li>Σ P($O_1, O_2, O_3$ | possible state sequence) = P($O_1, O_2, O_3$ | Model)</li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/forwardpseudo.png\" alt=\"dp\"></li>\n</ul>\n<p>Example:Urn(甕)  </p>\n<ul>\n<li>genie has two urns filled with red and blue balls</li>\n<li>genie selects an urn and then draws a ball from it<ul>\n<li>The urns are hidden</li>\n<li>The balls are observed</li>\n</ul>\n</li>\n<li>After a lot of draws<ul>\n<li>know the distribution of colors of balls in each urn(B matrix) </li>\n<li>know the genie’s preferences in draw from one urn or the next(A matrix)</li>\n</ul>\n</li>\n<li>assume output (observation) is Blue Blue Red (BBR)<ul>\n<li>Forward: P(BBR|model) = 0.0792 (SUM of all possible states’ probability) <img data-src=\"/img/NLP/forward-urn.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>Viterbi</p>\n<ul>\n<li>compute <strong>the most possible path</strong></li>\n<li>$v_t(i)$ = <strong>most possible path probability</strong> from time = 0 to time = t, and state = qi at time = t <img data-src=\"/img/NLP/viterbi.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/viterbi-graph.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/viterbi-algo.png\" alt=\"\"></li>\n<li>Viterbi in Urn example <img data-src=\"/img/NLP/urn-cal.png\" alt=\"\"></li>\n</ul>\n<pre><code>def viterbi(obs, states, start_p, trans_p, emit_p):\n    V = [{}]\n    path = {}\n\n    # Initialize base cases (t == 0)\n    for y in states:\n        V[0][y] = start_p[y] * emit_p[y][obs[0]]\n        path[y] = [y]\n\n    # Run Viterbi for t &gt; 0\n    for t in range(1,len(obs)):\n        V.append({})\n        newpath = {}\n\n        for y in states:\n            (prob, state) = max([(V[t-1][y0] * trans_p[y0][y] * emit_p[y][obs[t]], y0) for y0 in states]) \n            # ↑ find the most possible state transitting to given state y at time=t\n            V[t][y] = prob\n            newpath[y] = path[state] + [y] \n\n        # newpath(at time t) can overwrite path(at time t-1) \n        path = newpath\n\n    (prob, state) = max([(V[len(obs) - 1][y], y) for y in states])\n    return (prob, path[state])</code></pre><h4 id=\"Backward\"><a href=\"#Backward\" class=\"headerlink\" title=\"Backward\"></a>Backward</h4><ul>\n<li>Useful for parameter estimation</li>\n</ul>\n<p>Description  </p>\n<ul>\n<li>Backward variables β, which are the total probability of seeing the rest of the observation sequence($O_t to O_T$) given state qi at time t <img data-src=\"/img/NLP/bw-procedure.png\" alt=\"\"><ul>\n<li><img data-src=\"/img/NLP/bw-f.png\" alt=\"\"></li>\n<li>初始化β：令t=T時刻所有狀態的β為1</li>\n</ul>\n</li>\n<li>由後往前計算 <img data-src=\"/img/NLP/bw-graph.png\" alt=\"\"></li>\n<li>如果要計算某observation的概率，只需將t=1的後向變量相加</li>\n</ul>\n<h4 id=\"Backward-Forward\"><a href=\"#Backward-Forward\" class=\"headerlink\" title=\"Backward-Forward\"></a>Backward-Forward</h4><ul>\n<li>We can locally maximize model parameter λ, by an iterative hill-climbing known as Baum-Welch algorithm(=Forward-Backward) (by EM Algorithm structure)</li>\n</ul>\n<p>Forward-Backward Algorithm    </p>\n<ul>\n<li>find which** state transitions(A matrix)** and <strong>symbol observaions(B matrix)</strong> were <strong>probably used the most</strong></li>\n<li>By <strong>increasing the probability of those</strong>, we can get a better model which gives a higher probability to the observation sequence</li>\n<li>transition probabilities and path probabilities are both require each other to calculate<ul>\n<li>use A matrix to calculate path probabilities</li>\n<li>need path probabilities to update A matrix</li>\n<li>use EM algorithm</li>\n</ul>\n</li>\n</ul>\n<p>EM algorithm (Expectation-Maximization)    </p>\n<ul>\n<li>迭代算法，它的最大優點是簡單和穩定，但容易陷入局部最優</li>\n<li>(隨機)選擇參數λ0，找出在λ0下最可能的狀態，計算每個訓練樣本的可能結果的概率，再<strong>重新估計新的參數λ</strong>。經過多次的迭代，直至某個收斂條件滿足為止</li>\n<li>Urn Example<ul>\n<li>update transition matrix A ($a_{12}, a_{11}$ … ) <img data-src=\"/img/NLP/newtrans.png\" alt=\"\"><ul>\n<li>P(1→2) = 0.0414 <img data-src=\"/img/NLP/1-2.png\" alt=\"1→2\"></li>\n<li>P(1→1) = 0.0537 <img data-src=\"/img/NLP/1-1.png\" alt=\"1→1\"></li>\n<li>normalize: P(1→2)+P(1→1) = 1, P(1→2) = 0.435 …</li>\n</ul>\n</li>\n<li>若state數目多的時候，計算量過大…<ul>\n<li>用backward, forward</li>\n<li>前面用forward, 後面用backward <img data-src=\"/img/NLP/bf-graph.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Combine forward and backward  </p>\n<ul>\n<li>Let ξt be the probability of being in state i at time t and state j at time t+1, <strong>given observation and model λ</strong><img data-src=\"/img/NLP/kesin.png\" alt=\"\"></li>\n<li>use not-quite-ξ to get ξ <img data-src=\"/img/NLP/nqkesin.png\" alt=\"\"> because <img data-src=\"/img/NLP/kesin-formula.png\" alt=\"\"><ul>\n<li>P(O|λ) → problem1 of HMM 的答案 → 用forward解</li>\n</ul>\n</li>\n<li>見上方backward, forward同時使用之圖 <img data-src=\"/img/NLP/nqkesin-formula.png\" alt=\"\"></li>\n<li>ξ可用來計算transition matrix <img data-src=\"/img/NLP/newtrans-final.png\" alt=\"\"></li>\n</ul>\n<p>Summary of Forward-Backward <img data-src=\"/img/NLP/fb-algo.png\" alt=\"\"> </p>\n<ol>\n<li>Initialize λ=(A,B)</li>\n<li>Compute α, β, ξ using observations</li>\n<li>Estimate new λ’=(A,B)</li>\n<li>Replace λ with λ’</li>\n<li>If not converged go to 2</li>\n</ol>\n<h2 id=\"Chap07-Part-of-Speech-Tagging\"><a href=\"#Chap07-Part-of-Speech-Tagging\" class=\"headerlink\" title=\"Chap07 Part-of-Speech Tagging\"></a>Chap07 Part-of-Speech Tagging</h2><p>alias: <strong>parts-of-speech</strong>, <strong>lexical categories</strong>, <strong>word classes</strong>, <strong>morphological classes</strong>, <strong>lexical tags</strong></p>\n<ul>\n<li>Noun, verb, adjective, preposition, adverb, article, interjection, pronoun, conjunction</li>\n<li>preposition(P)<ul>\n<li>of, by, to</li>\n</ul>\n</li>\n<li>pronoun(PRO)<ul>\n<li>I, me, mine</li>\n</ul>\n</li>\n<li>determiner(DET)<ul>\n<li>the, a, that, those</li>\n</ul>\n</li>\n</ul>\n<p>Usage  </p>\n<ul>\n<li><p>Speech synthesis</p>\n</li>\n<li><p>Tag before parsing</p>\n</li>\n<li><p>Information extraction</p>\n</li>\n<li><p>Finding names, relations, etc.</p>\n</li>\n<li><p>Machine Translation</p>\n</li>\n<li><p>Closed class</p>\n<ul>\n<li>the class that is hard to add new words</li>\n<li>Usually function words (short common words which play a role in grammar)<ul>\n<li>prepositions: on, under, over,…</li>\n<li>particles: up, down, on, off, …</li>\n<li>determiners: a, an, the, …</li>\n<li>pronouns: she, who, I, …</li>\n<li>conjunctions: and, but, or, …</li>\n<li>auxiliary verbs: can, may should, …</li>\n<li>numerals: one, two, three, third, …</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Open class</p>\n<ul>\n<li>new ones can be created all the time</li>\n<li>For English: Nouns, Verbs, Adjectives, Adverbs</li>\n</ul>\n</li>\n</ul>\n<p>Choosing Tagset: Ex. “Penn TreeBank tagset”, 45 tag<br><img data-src=\"/img/NLP/tagset.png\" alt=\"\"></p>\n<p>Methods for POS Tagging  </p>\n<ol>\n<li>Rule-based tagging<ul>\n<li>ENGTWOL: ENGlish TWO Level analysis</li>\n</ul>\n</li>\n<li>Stochastic: Probabilistic sequence models<ul>\n<li>HMM (Hidden Markov Model)</li>\n<li>MEMMs (Maximum Entropy Markov Models)</li>\n</ul>\n</li>\n<li>Transformation-Based Tagger (Brill)</li>\n</ol>\n<h3 id=\"Rule-Based-Tagging\"><a href=\"#Rule-Based-Tagging\" class=\"headerlink\" title=\"Rule-Based Tagging\"></a>Rule-Based Tagging</h3><ol>\n<li>Assign all possible tags to each word</li>\n<li>Remove tags according to set of rules<ol>\n<li>Typically more than 1000 hand-written rules</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"Hidden-Markov-Model-tagging\"><a href=\"#Hidden-Markov-Model-tagging\" class=\"headerlink\" title=\"Hidden Markov Model tagging\"></a>Hidden Markov Model tagging</h3><ul>\n<li>special case of Bayesian inference<ul>\n<li>Foundational work in computational linguistics</li>\n</ul>\n</li>\n<li>related to the “noisy channel” model that’s the basis for ASR, OCR and MT</li>\n<li>Decoding view  <ul>\n<li>Consider all possible sequences of tags</li>\n<li>choose the tag sequence which is most possible given the observation sequence of n words w1…wn</li>\n</ul>\n</li>\n<li>Generative view<ul>\n<li>This sequence of words must have resulted from some hidden process</li>\n<li>A sequence of tags (states), each of which emitted a word</li>\n</ul>\n</li>\n<li>$t^n_1$(t hat), which is the most possible tag <img data-src=\"/img/NLP/best-t.png\" alt=\"\"></li>\n<li>use viterbi to get tag <img data-src=\"/img/NLP/viterbi-ex.png\" alt=\"\"></li>\n</ul>\n<p>Evaluation  </p>\n<ul>\n<li>Overall error rate with respect to a gold-standard test set</li>\n<li>Error rates on particular tags/words</li>\n<li>Tag confusions, Unknown words…</li>\n<li>Typically accuracy reaches 96~97%</li>\n</ul>\n<p>Unknown Words</p>\n<ul>\n<li>Simplest model<ul>\n<li>Unknown words can be of any part of speech, or only in any open class</li>\n</ul>\n</li>\n<li>Morphological and other cues<ul>\n<li>~ed: past tense forms or past participles</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Maximum-entropy-Markov-model-MEMM\"><a href=\"#Maximum-entropy-Markov-model-MEMM\" class=\"headerlink\" title=\"Maximum entropy Markov model (MEMM)\"></a>Maximum entropy Markov model (MEMM)</h3><p>Maximum Entropy Model  </p>\n<ul>\n<li>MaxEnt: multinomial(多項式) logistic regression</li>\n<li>Used for sequence classification/sequence labeling</li>\n<li>Maximum entropy Markov model (MEMM)<ul>\n<li>a common MaxEnt classifier</li>\n</ul>\n</li>\n</ul>\n<!-- Classification\n- Task\n    - observation, Extract useful features, Classify the observation based on these features\n- Probabilistic classifier\n    - Given an observation, it gives a probability distribution over all classes\n- Non-sequential(連續的) Applications\n    - Text classification\n    - Sentiment analysis\n    - Sentence boundary detection\n-->\n\n\n<p>Exponential(log-linear) classifiers </p>\n<ul>\n<li>Combine features linearly</li>\n<li>Use the sum as an exponent <img data-src=\"/img/NLP/maxent.png\" alt=\"\"></li>\n<li>Example <img data-src=\"/img/NLP/maxent-ex.png\" alt=\"\"></li>\n</ul>\n<p>Maximum Entropy Markov Model       </p>\n<ul>\n<li>MaxEnt model<ul>\n<li>classifies <strong>a</strong> observation into <strong>one</strong> of discrete classes</li>\n</ul>\n</li>\n<li>MEMM<ul>\n<li>augmentation(增加) of the basic MaxEnt classifier</li>\n<li><strong>assign a class to each element in a sequence</strong></li>\n</ul>\n</li>\n</ul>\n<p>POS tagging from MaxExt to MEMM   </p>\n<ul>\n<li>include some source of knowledge into the tagging process</li>\n<li>The simplest approach<ul>\n<li>run the local classifier and <strong>feature is classifier from the previous word</strong></li>\n<li>Flaw<ul>\n<li>It makes a hard decision on each word before moving on the next word</li>\n<li>cannot use information from the later words</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>discriminative model</strong>(判別模型)   </p>\n<ul>\n<li>Compute the posterior P(Tag|Word) directly to decide tag <img data-src=\"/img/NLP/memm.png\" alt=\"\"></li>\n<li>求解條件機率分佈 P(y|x) 預測 y → 求P(tag|word)來取得tag </li>\n<li>不考慮聯合機率分佈 P(x, y)</li>\n<li>對於諸如分類和回歸問題，由於不考慮聯合機率分佈，採用判別模型可以取得更好的效果</li>\n</ul>\n<p>HMM and MEMM(順推和逆推的差別) <img data-src=\"/img/NLP/hmmandmemm.png\" alt=\"\">  </p>\n<ul>\n<li>Unlike HMM, MEMM can condition on any <strong>useful feature of observation</strong><ul>\n<li>HMM: state is the fiven value</li>\n<li>MEMM: observation is the given value</li>\n</ul>\n</li>\n<li>viterbi function for MEMM <img data-src=\"/img/NLP/viterbi-new.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/memm-ex.png\" alt=\"\"></li>\n</ul>\n<h3 id=\"Transformation-Based-Learning-of-Tags\"><a href=\"#Transformation-Based-Learning-of-Tags\" class=\"headerlink\" title=\"Transformation-Based Learning of Tags\"></a>Transformation-Based Learning of Tags</h3><ul>\n<li>Tag each word with its most frequent tag</li>\n<li>Construct a list of transformations that <strong>improve the initial tag</strong></li>\n<li>trigger environment: at the limited number of words before/after <img data-src=\"/img/NLP/transformed-learn.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/transformed-algo.png\" alt=\"\"></li>\n</ul>\n<ol>\n<li>Trigger by tags </li>\n<li>Trigger by word</li>\n<li>Trigger by morphology(詞法學)</li>\n</ol>\n<p>&lt;! – ====================分水嶺：尚未分類========================== –&gt;</p>\n<h3 id=\"Zipf’s-Law-long-tail-phenomenon\"><a href=\"#Zipf’s-Law-long-tail-phenomenon\" class=\"headerlink\" title=\"Zipf’s Law (long tail phenomenon)\"></a>Zipf’s Law (long tail phenomenon)</h3><p>a word’s frequency is approximately inversely proportional to its rank in the word distribution list<br>單詞出現的頻率與它在頻率表裡的排名成反比:<br>頻率最高的單詞出現的頻率大約是出現頻率第二位的單詞的2倍</p>\n<h4 id=\"Jelinek-Mercer-Smoothing\"><a href=\"#Jelinek-Mercer-Smoothing\" class=\"headerlink\" title=\"Jelinek-Mercer Smoothing\"></a>Jelinek-Mercer Smoothing</h4><p>interpolate(插值) between bigram and unigram<br>because if p(eat the) = 0 and p(eat thou) = 0<br>it still must consider that  p(eat the) &gt; p(eat thou)<br>because p(the) &gt; p(thou)<br>so p(eat the) = N * p(the | eat) + (1-N) * p(the | thou) </p>\n<h2 id=\"Language-Model-Applications\"><a href=\"#Language-Model-Applications\" class=\"headerlink\" title=\"Language Model: Applications\"></a>Language Model: Applications</h2><h3 id=\"Query-Likelihood-Model\"><a href=\"#Query-Likelihood-Model\" class=\"headerlink\" title=\"Query Likelihood Model\"></a>Query Likelihood Model</h3><p>given a query 𝑞, rank the probability 𝑝(𝑑|q)<br><img data-src=\"/img/NLP/cfKf6I3.png\" alt=\"\"><br>So the following arguments are equivalent:<br>1.𝑝𝑑𝑞: find the document 𝑑 that is most likely to be relevant to 𝑞<br>2.𝑝𝑞𝑑: find the document 𝑑 that is most likely to generate the query 𝑞</p>\n<p>Typically, unigram LMs are used in IR(information retrieval)<br>Retrieval does not depend that much on sentence structure</p>\n<h3 id=\"Dependence-Language-Model\"><a href=\"#Dependence-Language-Model\" class=\"headerlink\" title=\"Dependence Language Model\"></a>Dependence Language Model</h3><p>Relax the independence assumption of unigram LMs<br>Do not assume that the dependency only exist between <strong>adjacent</strong> words<br>Introduce a hidden variable: “linkage” 𝐿<br>Ex.<br><img data-src=\"/img/NLP/Z8ftSRP.png\" alt=\"\"></p>\n<p>skipped….</p>\n<h3 id=\"Proximity-Language-Model\"><a href=\"#Proximity-Language-Model\" class=\"headerlink\" title=\"Proximity Language Model\"></a>Proximity Language Model</h3><p>Proximity: how close the query terms appear in a document<br>the closer they are, the more likely they are describing the same topic or concept</p>\n<h3 id=\"Positional-Language-Model\"><a href=\"#Positional-Language-Model\" class=\"headerlink\" title=\"Positional Language Model\"></a>Positional Language Model</h3><p>Position: define a LM for each position of a document, instead of the entire document<br>Words closer to a position will contribute more to the language model of this position</p>\n<h3 id=\"Speech-Recognition\"><a href=\"#Speech-Recognition\" class=\"headerlink\" title=\"Speech Recognition\"></a>Speech Recognition</h3><ul>\n<li>The “origin” of language models</li>\n<li>used to restrict the search space of possible word sequences</li>\n<li>requires higher order models: knowing previous acoustic is important!</li>\n<li>Speed is important!</li>\n<li>N-gram LM with modified Kneser-Ney smoothing is extensively used</li>\n</ul>\n<h3 id=\"Machine-Translation-MT\"><a href=\"#Machine-Translation-MT\" class=\"headerlink\" title=\"Machine Translation (MT)\"></a>Machine Translation (MT)</h3><ul>\n<li>Decoding: given the probability model(s), find the best translation</li>\n<li>Similar role as in speech recognition: <strong>eliminate unlikely word sequences</strong></li>\n<li>Higher order Kneser-Ney smoothed n-gram LM is widely used</li>\n<li>NNLM-style models tend to outperform standard back-off LMs</li>\n<li>Also significantly speeded up in (Delvin et al, 2014)</li>\n</ul>\n<h2 id=\"參考資料\"><a href=\"#參考資料\" class=\"headerlink\" title=\"參考資料\"></a>參考資料</h2><ul>\n<li>HHChen 課堂講義</li>\n<li>SDLin 講義</li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9jbGFzcy5jb3Vyc2VyYS5vcmcvbmxwLw==\">Stanford NLP course<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><a href=\"www.52nlp.cn\">52nlp</a></li>\n</ul>\n",
            "tags": [
                "機器學習",
                "自然語言處理",
                "統計"
            ]
        }
    ]
}