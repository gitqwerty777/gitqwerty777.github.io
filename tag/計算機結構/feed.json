{
    "version": "https://jsonfeed.org/version/1",
    "title": "QWERTY • All posts by \"計算機結構\" tag",
    "description": "Programming | Computer Science | Thought",
    "home_page_url": "http://gitqwerty777.github.io",
    "items": [
        {
            "id": "http://gitqwerty777.github.io/computer-architecture2/",
            "url": "http://gitqwerty777.github.io/computer-architecture2/",
            "title": "計算機結構(下)",
            "date_published": "2014-11-25T12:50:00.000Z",
            "content_html": "<!-- RENEW: -->\n\n<h2 id=\"Chap07-Pipelining-II\"><a href=\"#Chap07-Pipelining-II\" class=\"headerlink\" title=\"Chap07 Pipelining (II)\"></a>Chap07 Pipelining (II)</h2><p><img data-src=\"/img/CA/detect-and-forward.png\" alt=\"detect and forward\"><br>Data Dependence Detection<br>Hazard conditions:</p>\n<ul>\n<li>EX hazard<ul>\n<li>EX/MEM.RegisterRd = ID/EX.Register Rs or Rt </li>\n</ul>\n</li>\n<li>MEM hazard<ul>\n<li>MEM/WB.RegisterRd = ID/EX.RegisterRs or Rt</li>\n<li>Ex/MEM.RegisterRd != ID/Ex.RegisterRs(優先選EX)</li>\n</ul>\n</li>\n<li>RegWrite == true</li>\n<li>RegisterRd != $0<a id=\"more\"></a>\nResolving Hazards by Forwarding<br><img data-src=\"/img/CA/forward.png\" alt=\"forward\"></li>\n</ul>\n<p>Add MUX to ALU inputs<br>Forwarding Control in EX<br><img data-src=\"/img/CA/forward-logic-mux.png\" alt=\"logic with forward\"></p>\n<p>load-use data hazard(instruction after LW) -&gt; stall it  </p>\n<pre><code>If (ID/EX.MemRead and\n((ID/EX.RegisterRt = IF/ID.RegisterRs) or\n(ID/EX.RegisterRt = IF/ID.RegisterRt)))\n    stall the pipeline</code></pre><p>Stall: ID/EX裡的control value改成0，使EX MEM WB都不做事(nop) 並防止PC和IF/ID register更新，也就是確保他們下個cycle做跟這個cycle一樣的事<br>Insert a bubble:空白的instruction, 不做事<br><img data-src=\"/img/CA/bubble.png\" alt=\"bubble\"></p>\n<p>Control Hazard Solutions</p>\n<p>Branch 有沒有taken可以在MEM確認。如果發現prediction錯誤就重新設定PC，並把control都設為0，flush掉跑錯的instruction。<br>一個簡單的改進方式就是，在ID stage把資料從register讀出來後加上比較兩個值是否相等的元件，compare完才進到id/ex，這時候提早了一個cycle知道branch是否taken。</p>\n<p>當branch發生使用的值在ALU做完運算時，透過forwarding就可以解決了<br>但如果branch發生在使用的值正在load的話，就必須stall。而如果是一load出來馬上就要做branching的判斷的話，就必須stall 2個cycle<br><img data-src=\"/img/CA/2stall.png\" alt=\"\"></p>\n<p>Dynamic prediction的branch history table，以branch instruction的address(取最後n個bit)做索引，並儲存branch的結果。如果猜錯的話就做之前一樣的flush並修改表。<br>跳出loop時會猜繼續，第一次進入loop會猜跳出 -&gt; 導致錯誤率大幅提高<br>-&gt; 2bit的predictor, 連續兩個taken/not taken才會改變狀態</p>\n<p>但就算猜對，還是要算出target address，所以在branch taken時會有一個cycle的penalty。解決的方法是新增buffer存放branch target address。</p>\n<h3 id=\"Exception\"><a href=\"#Exception\" class=\"headerlink\" title=\"Exception\"></a>Exception</h3><p>syscall，未定義的opcode，或overflow處理等等(以上為CPU內產生的指令)或是受外部的I/O控制器的干涉。導致performance的降低。</p>\n<ol start=\"2\">\n<li>完成先前的指令</li>\n<li>Flush the instruction in the IF, ID and EX stages</li>\n<li>把這些違例或是被干擾的instruction的PC(實際上是PC+4)存在Exception program counter(EPC)</li>\n<li>問題的跡像(indicator)也存起來，在MIPS中是使用Cause register</li>\n<li>然後再跳到handler(PC = 0x40000040)</li>\n<li>另外一種解決的機制是，以硬體等級去告訴I/O handler，根據不同的cause跳去不同的handler(不同的address)，instructions要不就去執行interrupt的部份，要不就跳到handler去處理。</li>\n<li>Handler先讀看原因(indicator)然後再轉至專門解決此類問題的handler，然後決定該採取什麼行動，如果是restartable(可以重跑)，就用EPC回到原本執行的地方(EPC-4)，並採取判斷正確的行動。否則就終止程序並根據EPC cause來回報錯誤。</li>\n</ol>\n<p><img data-src=\"/img/CA/exception-handle.png\" alt=\"step to handle exception\"></p>\n<p>ILP(instruction level parallelism)，指令層級的平行處理。<br>增進ILP的方法</p>\n<ol>\n<li>Deeper pipeline，把pipeline分成更多stage，而每個stage因為workload相對的比較少，所以可以讓cpu clock cycle變短，進而增進效能</li>\n<li>Multiple issue，有多個pipeline同時進行，所以每個clock cycle都同時跑好幾個instruction。但是互相依賴性(比如說不同pipeline之間的hazard或共用到哪些資源)會使得實際上不是變幾倍的pipeline IPC就變幾倍。</li>\n</ol>\n<h3 id=\"Multiple-issue\"><a href=\"#Multiple-issue\" class=\"headerlink\" title=\"Multiple issue\"></a>Multiple issue</h3><p>可以分為static和dynamic。<br>Static的是由compiler把要同時執行的instruction包成一包一包的instruction packets。<br>可以把instruction packet想成一個非常長的instruction裡面有好幾個同時運作的operations。這樣的概念叫做VLIW(very long instruction word)<br>偵測避免hazard</p>\n<ol>\n<li>把instruction重新排列並包成issue packets時避免會造成hazard的順序。</li>\n<li>同時在跑的instruction要互相independent不然就會搶資源或造成data hazard。</li>\n<li>在不同的packets之間，可以有dependency，但這部份根據不同的ISA要有不同的設計。</li>\n<li>有時候要放入nop(不做任何動作)</li>\n</ol>\n<p>Dynamic是由CPU選擇每個cycle要issue哪些instructions，而compiler可以藉由把instruction串流做較好的排列來幫助。CPU會用比較進階的技術在運行時解決hazard。</p>\n<p>Speculation：先去猜測要做什麼，如果做錯了再從頭來過。比如說branch的時候就先猜taken或not taken，在load的時候先拿原有的位址去load如果發現其他該在前面的instruction更新了這個位址，就roll-back。</p>\n<p>Compiler可以重新排列一些code比如說把branch之前的load移到更早，避免stall，或是寫一些instruction來修正做出錯誤的speculation的狀況。增加用來延遲exception的ISA。</p>\n<p>而硬體可以做look-ahead，將instruction的結果和exception都先存在buffer裡，直到他們要被用到或是判斷speculation正確。如果判斷speculation錯誤就把buffer flush掉。</p>\n<h3 id=\"dual-issue\"><a href=\"#dual-issue\" class=\"headerlink\" title=\"dual issue\"></a>dual issue</h3><p>一個packet有兩組instruction。一個只做load/store一個只做ALU/branch，所以只要加一個ALU和一個sign extender就可以實做。</p>\n<p>dual issue’s Data hazard:</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"builtin-name\">add</span> <span class=\"variable\">$t0</span>, <span class=\"variable\">$s0</span>, <span class=\"variable\">$s1</span></span><br><span class=\"line\">load <span class=\"variable\">$s2</span>, 0(<span class=\"variable\">$t0</span>)</span><br></pre></td></tr></table></figure>\n\n<p>把這兩個指令拆開放在兩個不同的packets，就像stall一樣</p>\n<p>Load-use hazard，一樣會造成一個cycle的延遲，但是一個cycle變成影響兩個instructions。<br>-&gt; 需要把指令做更好的排程(aggressive scheduling)</p>\n<p>loop unrolling: 把一次完成多個loop內的iteration來減少loop-control overhead(bne)，並用不同的register來存放(register renaming)(每一份replicate就是原本的loop跑一次)<br>避免loop裡面有 anti–dependencies(name dependency): write-after-read<br>ex: B=7; A=B+1; B=3 在a=b+1和b=3之間就有anti dependent的關係</p>\n<p><img data-src=\"/img/CA/unroll-before.png\" alt=\"before\"><br><img data-src=\"/img/CA/unroll-after.png\" alt=\"after\"><br>原本：for(load -&gt; 計算 -&gt; save)<br>之後：for(load) + for(add) -&gt; for(save)<br>IPC從1.25提升到1.75(更接近極限，2)不過code和register也變得更大。  </p>\n<p>Dynamic multiple issue通常在超大型處理器中使用。CPU每個cycle會決定issue的對象。以幫助cpu對code的語義有更好的掌握(compiler做的事變少CPU更直接掌握code在做什麼)。<br>dynamic pipeline scheduling: 讓cpu可以不照順序執行instruction以避免stall，但是會把資料<strong>照順序存回</strong>register(比如說在stall的時候就先處理無關的instruction)</p>\n<p><img data-src=\"/img/CA/dynamic-schedule-CPU.png\" alt=\"dynamic schedule CPU\"><br>Dynamically scheduled CPU的運作跟一般的pipeline有些出入，可以分為4個stage</p>\n<ol>\n<li>IF/ID<br>照順序做完instruction fetch和decode(這邊的動作很快)</li>\n<li>reservation stations<br>控制哪些instruction要先pending</li>\n<li>functional units<br>做不同的功能 – 浮點數運算, load-store…<br>完成後把資料給commit unit及相對應在pending等這個結果的reservation station</li>\n<li>commit unit<br>重新排列register write要用的buffer<br>並提供operands給某些在reservation pending的function(類似之前單issue裡要flush重做的function)</li>\n</ol>\n<p>?? Reservation station 和 commit unit在reorder buffer時，自動達到了register renaming。</p>\n<p>?? 當一個instruction被issue到reservation station的時候，如果instruction的operands在register或reorder buffer裡可以被找到也可以被存取的話，把它複制到reservation station，並且標明那個register已經無用可以被複寫。如果operands無法存取(unavailable)的話，有一個function unit會把該給的值給reservation unit，而register裡面的值需不需要更新就要看指令。</p>\n<p>dynamically scheduled的speculation: 在branching的結果確認之前不要commit。而speculation一樣可以用在減少load 和cache miss delay。根據預測的address先取出值然後等store有沒有更改到這個load的address，store會把那個address bypass到load unit。沒問題就把結果送到commit unit，有問題就重做。</p>\n<p>Dynamically scheduling的原因:<br>不是所有的stall都是可以從code裡看出來的 比如說:cache miss。<br>branch的結果也不能靠scheduling來解決。<br>不同的ISA有不同的延遲和不同的hazard，都要交給compiler來處理實在非常麻煩。</p>\n<p>Multiple issue的效能：程式內有dependency會限制ILP(instruction level parallelism)而且有些dependency很難去除，如pointer aliasing(不同的名字的pointer指到同一個地方)<br>有一些平行也很難做到比如說IF/ID的部份<br>memory還有delay而且也有他的頻寬，也導致pipeline常常有nop<br>Speculation如果做的好的話可以改善以上原因引起的performance下降</p>\n<p>多顆簡單的核心(沒speculation, issue width低, pipeline, stage少)可以達到省電的作用</p>\n<h3 id=\"結論\"><a href=\"#結論\" class=\"headerlink\" title=\"結論\"></a>結論</h3><p>pipeline的概念很簡單，但是細節很複雜。Ex: hazard detection<br>pipeline跟cpu的其他科技無關，其他科技進步的同時還是可以做pipelining  </p>\n<p>不好的ISA設計可能在某些狀況下會讓pipelining變得很困難<br>ex:<br>太複雜的instruction set 需要巨大的overhead來讓pipeline可行(ex: IA-32,VAX)<br>太複雜的addressing mode<br>間接讀取memory(指標)，及register update<br>複雜的pipeline，會有比較長的branching delay slots  </p>\n<p><strong>ISA會影響data path和control的設計</strong><br>Pipelining利用平行處理的技術可以提高總輸出，並不影響單個指令的latency<br>Dependency限制了平行處理的程度，太過複雜又會導致電耗過高</p>\n<h2 id=\"Chap08-Memory-Hierarchy\"><a href=\"#Chap08-Memory-Hierarchy\" class=\"headerlink\" title=\"Chap08 Memory Hierarchy\"></a>Chap08 Memory Hierarchy</h2><p><img data-src=\"/img/CA/memory-hierarchy.png\" alt=\"hieraracy\"><br>記憶體的層級化: 愈常用的放在愈快拿到的地方</p>\n<ul>\n<li>Static Ram: 0.5ns-2.5ns， 每gb要2000 – 5000元</li>\n<li>Dynamic Ram: 50ns -70ns 每gb要 20- 75元</li>\n<li>硬碟:5-20ms，每gb只要0.2-2元</li>\n</ul>\n<p>Temporal locality: 最近被存取過的data容易再被存取<br>Ex:在loop裡面的程序，或是在loop裡面一直被重複操作的數字</p>\n<p>Spatial locality: 位址接近最近被存取過的data較有可能被存取，如array</p>\n<p>資料的copy以block為最小單位。<br>Hit Time: memory access time + time to determine hit/miss<br>Miss Penalty: Time to replace a block in the upper level + Time to deliver the block to processor  </p>\n<p>direct mapped cache: mod餘1, mod餘2 … 各放在同一個cache block<br>memory address前面的bit也存過去做為tag，可知道是從memory的哪個位置將資料load到這個cache<br>Valid bit可以很快的判斷一個block裡面是否有資料<br><img data-src=\"/img/CA/direct-map.png\" alt=\"direct map\"></p>\n<p>Q:How many total bits are required for a direct-mapped cache with 16 KB of data and 4-word(16byte) blocks, assuming 32bit address?</p>\n<ul>\n<li>num of sets = 16KB(cache size)/16B(block size) = 2^10</li>\n<li>num of data bits for each set = 16byte data = 4</li>\n<li>num of tag bits for each set = 32-10-4 = 18</li>\n</ul>\n<p>Cache Controller FSM<br><img data-src=\"/img/CA/cache-controller-fsm.png\" alt=\"FSM\"></p>\n<h3 id=\"考慮因素\"><a href=\"#考慮因素\" class=\"headerlink\" title=\"考慮因素\"></a>考慮因素</h3><p>若增大block size，block數變少，有利於spatial locality，但miss penalty上升</p>\n<p>解決miss penalty上升  </p>\n<ul>\n<li>Early restart：正常運作，只要一fetch到需要的word，就馬上把這個word送到CPU</li>\n<li>Critical word first：先fetch需要的word，再把剩下的word填進cache bloc</li>\n</ul>\n<p>各層級的資料不同步</p>\n<ul>\n<li>Write through：寫入cache時同時寫入memory</li>\n<li>Write back：cache被代替時，再寫入memory，設dirty bit</li>\n<li>使用write buffer：只有在write buffer滿的時候才stall</li>\n</ul>\n<p>Write Miss Policy</p>\n<ul>\n<li>Write allocate(fetch on write): 先load到cache再修改</li>\n<li>No Write allocate(write around): 直接write底層的資料，不load到cache</li>\n</ul>\n<p>在做初始化時，寫入的資料(全都是0)不會在近期內就被讀取，採取write around就是一個比較好的選擇</p>\n<p>是否合併instruction cache 和 data cache?  </p>\n<ul>\n<li>Combined cache – higher cache hit rate &amp; lower cache bandwidth   </li>\n<li>Split cache – lower cache hit rate &amp; higher cache bandwidth</li>\n</ul>\n<p>memory interleave: 讓BUS可以同步讀取不同BANK<br><img data-src=\"/img/CA/memory-interleave.png\" alt=\"memory design\"></p>\n<ul>\n<li>DDR: RAM不只在clock 0變1時動作，在1變0的時候也做一次動作，使data rate變為兩倍故名 ddr</li>\n<li>QDR: DDR再加上將input 和output分開，在同一個clock變更時可以同時做input 和output的技術</li>\n</ul>\n<p>當CPU的效能增進時，miss penalty的影響就越來越大</p>\n<p>How to Improve Cache Performance?</p>\n<ol>\n<li>Reduce miss rate -&gt; Increasing associativity<br> direct mapped -&gt; set associative -&gt; fully associative<br> 效能提升呈邊際遞減<br> 缺點：mux delay, data comes after hit/miss, tag bit increase</li>\n<li>Reduce miss penalty -&gt; multi-level cache<br> high performance improvement<br> Ex. radix sort: cache miss rate high, so performance worse than quick sort</li>\n<li>Reduce hit time -&gt; small cache(…)</li>\n</ol>\n<p>記憶體的層級化(memory hierarchy)：每一層裡面都有4個重點，block要怎麼放置，要怎麼找到需要的block，當miss的時候怎麼替換，寫入時的規矩(write policy)。</p>\n<ol>\n<li>block要怎麼放置:由associativity決定。可分為direct mapped, n-way,和fully associative。越高的associatvie就越少miss但是cost，access time 和複雜度也越高。(三種associative參考前面)</li>\n<li>要怎麼找到需要的block: direct mapping需要做1次的comparison，n-way需要做n(看多少way)次，而fully associative如果建表就不用沒的話就要做entry的次數次(每個entry都要比對)。這邊我們的目標是降低comparison以降低硬體cost。VM由於full look-up table的查表方式使得fully associative可行，可以大大降低miss rate。</li>\n<li>當miss的時候怎麼替換(algo): 替換的方法有LRU和random兩者比較在前面講過了。在VM裡面的話我們藉由硬體的幫忙實作LRU。</li>\n<li>Write policy:write-through和write-back，在VM裡只有write-back可行(28點)。</li>\n</ol>\n<h2 id=\"Chap-08-2-Virutual-Memory\"><a href=\"#Chap-08-2-Virutual-Memory\" class=\"headerlink\" title=\"Chap 08-2 Virutual Memory\"></a>Chap 08-2 Virutual Memory</h2><p>Idea: use memory as cache for disk  </p>\n<p>block叫做page，miss叫做page fault</p>\n<p>Disk讀取的速度非常慢，要花上百萬個cycle。必須使用Fully associative和較佳的replacement algorithm，及軟體為主的exception handler</p>\n<p>page fault 發生時，os會把相對應的page抓進來並update page table然後再重新執行導致page fault的instruction</p>\n<p>LRU replacement: 每個PTE(entry)加個bit叫reference bit，每次當這個page 被access就把這個reference bit設為1，然後系統會自動定期將所有 reference bit 清為0，這樣我們可以判斷reference bit是0的page最近沒有被access。</p>\n<ol start=\"3\">\n<li><p>Page table:由virtual page number作index，值為physical index。</p>\n</li>\n<li><p>Page table can be very large!<br>–Solution: inverted page table &amp;  multi-level paging</p>\n</li>\n<li><p>inverted page table: use hash to search(非常耗時、無法支援Memory sharing)</p>\n</li>\n<li><p>multi-level: 分層, decrease total page table size<br><img data-src=\"/img/CA/multilevel-pagetable.png\" alt=\"two-level\"></p>\n</li>\n<li><p>TLB(translation look-aside buffer)可以很快的cache在cpu內存放PTE。通常可存放16<del>512個PTE，hit時只要花0.5</del>1個cycle，miss的話也只要10<del>100個cycle。並且有0.01%</del>1%的低的miss rate</p>\n</li>\n</ol>\n<p>不同的任務(task)有時候可以共用他們的虛擬位址，但是需要OS的協調指派，並防止不相干的程式的access。<br>需要硬體的支援: kernel mode, 包含特有的instruction. page table和他的state資訊只有在kernel mode下可以access。並且還要有system call exception</p>\n<p><img data-src=\"/img/CA/memory-retrieve-events.png\" alt=\"Possible Combinations of Events\"></p>\n<p>Virtually Addressed Cache only Translated on miss</p>\n<p>distinguish data of different processes<br>-&gt; Virtually indexed &amp; physically tagged cache<br>-&gt; read data by tag and translate index in the same time<br><img data-src=\"/img/CA/vipt-flow.png\" alt=\"Virtually indexed &amp; physically tagged cache\"></p>\n<h3 id=\"Performance-issue-in-Virtual-Memory\"><a href=\"#Performance-issue-in-Virtual-Memory\" class=\"headerlink\" title=\"Performance issue in Virtual Memory\"></a>Performance issue in Virtual Memory</h3><p>Thrashing Solutions: Buy more memory<br>High TLB misses Solutions:Variable page size</p>\n<ol>\n<li>compulsory misses，也叫做cold-start misses。資料第一次被存取。</li>\n<li>capacity misses，cache的大小有限，一個剛被replace掉的block馬上又需要被access。</li>\n<li>conflict misses(collision misses)。多個block要競爭同一個index的entry，如果是fully-associative就不會發生<br><img data-src=\"/img/CA/collision-miss-rate.png\" alt=\"3C absoluate miss rate\"></li>\n</ol>\n<p>若想要減低miss rate, 就會造成總體效能的負面效應。<br>Trends:<br>–Redesign DRAM chips to provide higher bandwidth or processing<br>–Use prefetching &amp; non-blocking cache (make cache visible to ISA)<br>–Restructure code to increase locality</p>\n<p>Reduce miss penalty</p>\n<ul>\n<li>Non-blocking caches<ul>\n<li>Non-blocking cache or lockup-free cache allowing the data cache to continue to supply cache hits during a miss</li>\n</ul>\n</li>\n<li>Prefetching<ul>\n<li>Requesting data early, so it’s in cache when needed.</li>\n<li>預測技術(complier or hardware)</li>\n<li>Problem: May replace data in cache that is still needed.</li>\n</ul>\n</li>\n</ul>\n<p>VMM(virtual machine monitor)將虛擬的資源map到實體資源上，ex: memory I/O, CPU。guest的code在我們的本機端跑時是使用user mode，VMM可以控管一些要有權限才能用的instructions和一些資源是否可以access。Guest的os可能跟我們使用不同套，於是vmm就要產生一個虛擬的I/O給guest使用，來處理真正的I/O。<br>如果VM request一個timer-interrupt，這時候vmm就會根據本機的timer虛擬出一個虛擬的timer。利用這個timer來判斷interrupt的發生。<br>在vm上所有必須access實體資源的動作都要透過由VMM監控的privileged instructions才使用。比如說page tables, I/O , interrupt controls, registers等。<br>做某一些動作比如說要建立多重的web service時，所有東西都要經過VMM，VMM就回成為一個很大的 threshold。</p>\n<h2 id=\"Chap-10-Storage-Network-and-Other-Peripherals\"><a href=\"#Chap-10-Storage-Network-and-Other-Peripherals\" class=\"headerlink\" title=\"Chap 10 Storage, Network and Other Peripherals\"></a>Chap 10 Storage, Network and Other Peripherals</h2><p><img data-src=\"/img/CA/iosystem.png\" alt=\"IO System\"></p>\n<p>I/O Device Characteristics</p>\n<ul>\n<li>behavior<ul>\n<li>input, output or storage</li>\n</ul>\n</li>\n<li>partner</li>\n<li>data transmit rate</li>\n</ul>\n<p>performance metrics: Throughput, Response time<br><img data-src=\"/img/CA/iodevicechart.png\" alt=\"I/O device characterstics\"></p>\n<dl><dt>I/O System performance: find limited by weakest link in the chain</dt><dd>CPU, memory, bus, IO controller, IO device, OS, software<br>Two common </dd></dl><p>reliability</p>\n<ul>\n<li>MTTF:平均要多久會出現一次failure</li>\n<li>MTTR:平均遇到failure以後多久會修好</li>\n<li>availability是MTTF/(MTTF+MTTR)</li>\n<li>改進availability<ul>\n<li>增進MTTF，有避免fault的發生，減少fault發生時造成的損失，還有fault的預測</li>\n<li>減少MTTR，加強repair，增強fault的原因的分析功能，還有repair的機制</li>\n</ul>\n</li>\n</ul>\n<p>Disk Performance<br>seek time: 上下移動<br>Rotational latency: 轉到讀取的資料所需時間(RPM)，平均計算：轉一半(0.5round)<br>transfer rate: 傳送資料速度<br>Controller time: I/O controller花的時間</p>\n<p>快閃記憶體(flash)比硬碟快上100~1000倍。<br>比較小，比較不耗電。但是比較貴(介於disk和dram之間。)<br>Flash可以分為NOR或NAND flash。<br>NOR flash是random access通常用在嵌入式系統的instruction memory。<br>NAND 同時只能access某個block，而且同樣的大小有比較大的容量。成本也比較低。通常用來當我們常用的usb drive或記憶卡，SSD等等。<br>Flash的bit約在access 千次以後會壞掉。所以不適合拿來做ram或硬碟。解決方法是把data平均放在每個block上。</p>\n<p>flash’s block: 包含多個page</p>\n<ol>\n<li>Write Once: 無法直接覆蓋檔案，需先清除，一次清除一個block</li>\n<li>When # of free pages &lt;= Garbage Collection Threshold<br>: move live page to other block , and erase this block</li>\n</ol>\n<p>SSD (Solid Storage Disk)<br>no actual “disk”, use integrated circuit assemblies as memory to store data persistently.<br>SSD uses electronic interfaces compatible with traditional block drives</p>\n<ul>\n<li>no mechanical failure</li>\n<li>Green<ul>\n<li>SSDs consume over 50% less power compared to HDD</li>\n</ul>\n</li>\n<li>Higher initial cost</li>\n<li>Ex. Facebook data center</li>\n<li>Active SSD: 在I/O端作(簡單的)計算，減少L/W時間</li>\n</ul>\n<p>將PCI-e flash作為I/O cache(比SSD快！)放在General IO bus 上以加速I/O</p>\n<p>Bus: Connection between Processors, Memory, and I/O Device<br>有很好的同步性和低維護費，但造成效能瓶頸(受限於長度，BUS數目…)<br>有Control line 和 Data line</p>\n<p>Bus可以分為</p>\n<ul>\n<li>Processor-memory bus: 較短較快，要照著memory的規劃做設計</li>\n<li>I/O bus: 較長，可以有多重的互相連結。要照著互通性的基準設計</li>\n<li>Backplane bus: 所有device都可連接，花費較少，用來連接前兩者</li>\n</ul>\n<p><img data-src=\"/img/CA/three-type-bus.png\" alt=\"three bus system\"></p>\n<p>Synchronous Bus: </p>\n<ul>\n<li>includes a clock in the control lines</li>\n<li>advantage: involves very little logic and can run very fast</li>\n<li>disadvantages: <ul>\n<li>every device on the bus must run at the same clock rate</li>\n</ul>\n</li>\n</ul>\n<p>Asynchronous Bus:</p>\n<ul>\n<li>No clock, can accommodate a wide range of devices</li>\n<li>can be lengthened without worrying about clock</li>\n<li>requires a handshaking protocol<br><img data-src=\"/img/CA/asynchronous-handshaking.png\" alt=\"Asynchronous handshaking: Read Transaction\"></li>\n</ul>\n<p>Multiple Potential Bus Masters: use Arbiter to control<br>Arbiter: select who can use bus by priority and fairness</p>\n<p>Daisy Chain Bus Arbitrations<br><img data-src=\"/img/CA/daisy-chain.png\" alt=\"Daisy Chain Bus Arbitrations\"><br>Advantage: simple<br>Disadvantages:<br>–Cannot assure fairness: A low-priority device may be locked out indefinitely<br>–The use of the daisy chain grant signal also limits the bus<br>speed</p>\n<p>Centralized Parallel Arbitration<br><img data-src=\"/img/CA/Centralized-Parallel-Arbitration.png\" alt=\"Centralized Parallel Arbitration\"><br>所有bus由arbiter控管<br>適合速度較快的device組成的bus</p>\n<p>I/O的設備是由I/O controller來管理並同步。<br>command register來存放不同的command 使用不同的command來讓I/O device執行不同的動作<br>status register來指出I/O設備現在正在執行什麼task還有是否遇到什麼error<br>data register，可以把data “write”到device或從device ”read”出data。</p>\n<p>Memory mapped I/O<br>I/O的register的位址設為跟memory中的位址一樣，只有在kernel mode時可以access這些address</p>\n<p>Communicating with the Processor<br>‧Polling<br>定期檢查I/O status register，如果是ready就執行I/O，如果是error就想辦法解決。叫polling(問卷調查)。會浪費太多cpu time(busy loop)。<br>有反應時間需求時使用<br>‧Interrupt<br><img data-src=\"/img/CA/interrupt-driven-IO.png\" alt=\"Interrupt Driven Data Transfer\"><br>當ready或error時，controller就會interrupt CPU。<br>Interrupt跟exception很像，但可以在兩個instruction之間觸發handler。通常可以由cause的資訊來分辨是哪個device發生interrupt。Interrupt也有不同的priority，越緊急的interrupt priority就會越高。也可以用高priority的interrupt來呼叫低priority的interrupt的handler。<br>high-speed devices are associated with higher priority<br>‧DMA(direct memory access)<br>I/O controller主動跟memory連結傳輸資料，直到傳輸完成或是error才interrupt。比較節省cpu-time。<br>DMA寫到memory後，可能造成memory和cache不一致<br>如果write-back cache有dirty block而DMA去讀到相對應的memory的話也會讀到錯誤的資料。解法：cache的內容如果在memory中被dma寫入就把那個cache flush掉，不然就要設定noncacheable(dma不能動在cache的資料)。</p>\n<p>Parallelisms and I/O</p>\n<p>RAID: Redundant Array of (inexpensive)Independent Disks。<br>使用很多個小的disk來取代一個大的disk，好處有資料較不易受損和平行處理速度加快<br>‧Improve availability with redundancy</p>\n<p>Raid 0，是最早的RAID，沒有redundancy，只是把資料分散在不同的小disk可以平行讀取<br>RAID 1(Disk Mirroring):是兩個一模一樣的disk，一個是當作備份用，如果主disk的資料受損就從mirror copy過去<br>RAID 2: 把資料拆到以bit為單位分散的存在disk內。並用E-bit來做Error correction。拆到以bit為單位的話假設有n個disk則要讀任何資料理論上可以有n倍快。但是太複雜的設計導致實際上raid2並沒有在使用。只用於memory<br>Raid3(Bit-Interleaved Parity):<br>使用N+1個disk，資料拆成bit level分散在n個disk上<br>用剩下來的disk存parity(前面n個disk裡相對應的位址的每個資料做XOR)<br>在read時就讀取所有的disk，在write時寫入每個disk並產生新的parity。遇到failure時根據parity可以判斷failure的bit。<br>RAID4:<br>跟raid3很像只是是拆成block level，每次要讀資料時只要讀存放所需資料的block就好，寫資料也只需要動到要寫的block和parity。<br>RAID5:<br>跟RAID4接近，但是把parity分散存至每個disk以避免parity disk成為在寫入時的速度的瓶頸(Raid4每個寫入都要寫parity disk，所以parity disk寫入的速度就會限制資料寫入的速度)<br>RAID6(P + Q Redundancy):<br>跟RAID5一樣但是增加兩個parity(不同演算法)，使系統容錯率更高。</p>\n<p>RAID summary:<br>raid可以提升performance 並增加可靠性(hot swapping，在不影響系統operate的情形下修復fault)<br>可靠性是raid最重要的功能。</p>\n<p>Disk I/O Benchmarks: I/O rate vs. Data rate vs. latency</p>\n<h2 id=\"Chap12-Multicores-Multiprocessor\"><a href=\"#Chap12-Multicores-Multiprocessor\" class=\"headerlink\" title=\"Chap12 Multicores, Multiprocessor\"></a>Chap12 Multicores, Multiprocessor</h2><p>Challenges</p>\n<ul>\n<li>Partitioning</li>\n<li>Coordination</li>\n<li>Communication overheads</li>\n<li>Amdahl’s Law<br>平行化是有極限的<br>  FracX: 能被speedup的比例<br>  Speedup = 1 / [(FracX/SpeedupX + (1-FracX)]</li>\n</ul>\n<p>資料傳遞<br>Shared Memory: connect by memory<br>use lock to synchronize<br>same address space<br>Message Passing: connect by network<br>different address spaces</p>\n<p>Total network bandwidth = 所有的頻寬。bandwidth-per-link x link_no<br>Bisection bandwidth = 兩個部分之間的頻寬。the bandwidth between two parts of a multiprocessor</p>\n<p><img data-src=\"/img/CA/network-topology.png\" alt=\"netword topology\"></p>\n<p>Cache Coherency Problem: 在cache中的共享資料須保持一致<br>Protocol:</p>\n<ol>\n<li>Snoopy Bus: use for small scale machines<br>在拿資料前，先boardcast給所有processor知道<br>allow multiple readers, single writer<br>Broadcast: BW (increased) vs. latency (decreased) tradeoff<br>Write Invalidate Protocol:<br>若寫資料，也boardcast，其他有同資料的processor設invalid bit<br>Write Update Protocol:<br>若寫資料，也boardcast，其他有同資料的processor作相同的instruction</li>\n</ol>\n<p>Each block of memory is in one state:<br>    –Clean in all caches and up-to-date in memory<br>    –OR Dirty in exactly one cache<br>    –OR Not in any caches<br>Each cache block is in one state:<br>    if read miss, place readmiss on bus, goto shared<br>    if write miss, place writemiss on bus, goto exclusive<br>    if get read miss at bus(same block), if at exclusive, do write back and goto shared<br>    if get write miss at bus, goto(set) invalid<br>    –Shared: block can be read<br>    –OR Exclusive: cache has only copy, its writeable, and dirty<br>    –OR Invalid: block contains no data<br><img data-src=\"/img/CA/IO-BUS-fsm.png\" alt=\"State machine for bus requests for each cache block\"><br><img data-src=\"/img/CA/IO-CPU-fsm.png\" alt=\"State machine for CPU requests for each cache block\"></p>\n<ul>\n<li>Basic CMP Architecture Shared last level cache</li>\n<li>Scalable CMP Architecture Tiled CMP<ul>\n<li>Each tile includes processor, L1, L2, and router</li>\n<li>Physically distributed last level cache</li>\n</ul>\n</li>\n</ul>\n<p>Multithreading<br><img data-src=\"/img/CA/multithreads.png\" alt=\"Multithreaded Categories p53\"></p>\n<ul>\n<li>實作多執行緒<ul>\n<li>有多個 registers, PC</li>\n<li>Fast switching between threads</li>\n<li>減少stall的時間浪費</li>\n</ul>\n</li>\n<li>Fine-grain multithreading(一個cycle做一個thread的多個cycles)</li>\n<li>Coarse-grain multithreading(只有大的stall(L2 cache miss)才切換thread)</li>\n<li>Simultaneous Multithreading<ul>\n<li>used in dynamically scheduled processor</li>\n<li>同一個cycle可做多個thread</li>\n<li>dependencies handled by scheduling and register renaming</li>\n</ul>\n</li>\n<li>和Multiprocessing的不同：multiprocessing需多個processor</li>\n</ul>\n<p>費林分類法（Flynn’s Taxonomy），是一種高效能計算機的分類方式</p>\n<ul>\n<li>單一指令流單一資料流計算機（SISD）  </li>\n<li>單一指令流多資料流計算機（SIMD）<ul>\n<li>processors execute the same instruction at the same time.Each with different data address</li>\n<li>Works best for highly data-parallel applications</li>\n<li>Vector architecture</li>\n<li>Explicit statement of absence of loop-carried dependences(Reduced checking in hardware)</li>\n<li>Avoid control hazards by avoiding loops  </li>\n</ul>\n</li>\n<li>多指令流單一資料流計算機（MISD）</li>\n<li>多指令流多資料流計算機（MIMD）  </li>\n<li>SPMD: Single Program Multiple Data<ul>\n<li>A parallel program on a MIMD computer</li>\n<li>Conditional code for different processors</li>\n</ul>\n</li>\n</ul>\n<p>GPU(Graphics Processing Units)</p>\n<ul>\n<li>compute massive vertices, pixels, and general purpose data</li>\n<li>High availability</li>\n<li>High computing performance</li>\n<li>Low price of computing capability</li>\n</ul>\n<p>General-Purpose computing on GPU (GPGPU)<br>用處理圖形任務的圖形處理器來計算原本由中央處理器處理的通用計算任務，這些通用計算常常與圖形處理沒有任何關係。由於現代圖形處理器強大的並行處理能力和可程式流水線，令流處理器可以處理非圖形數據。特別在面對單指令流多數據流（SIMD），且數據處理的運算量遠大於數據調度和傳輸的需要時，通用圖形處理器在性能上大大超越了傳統的中央處理器應用程式。</p>\n<p>GPGPU programming models</p>\n<ul>\n<li>NVIDIA’s CUDA</li>\n<li>AMD’s StreamSDK</li>\n<li>OpenCL</li>\n</ul>\n<p>Multi-core CPU</p>\n<ul>\n<li>Coarse-grain, heavyweight threads</li>\n<li>Memory latency is resolved though large on-chip caches &amp; out-of-order execution<br>Modern GPU</li>\n<li>Fine-grain, lightweight threads</li>\n<li>Exploit thread-level parallelism for hiding latency</li>\n<li>SIMT (Single Instruction Multiple Threads)<ul>\n<li>multiple independent threads(pixel, vertex, compute…) execute concurrently using a single instruction</li>\n<li>common PC value</li>\n<li>Latency Hiding</li>\n</ul>\n</li>\n</ul>\n<p>Serial/Task-parallel workloads → CPU<br>Graphics/Data-parallel workloads → GPU<br>Behaviors of the applications are different<br>-&gt; CPU is latency sensitive, GPU is throughput oriented</p>\n<h3 id=\"參考資料\"><a href=\"#參考資料\" class=\"headerlink\" title=\"參考資料\"></a>參考資料</h3><ul>\n<li>CA_by_b95015.doc</li>\n</ul>\n",
            "tags": [
                "計算機結構"
            ]
        },
        {
            "id": "http://gitqwerty777.github.io/computer-architecture1/",
            "url": "http://gitqwerty777.github.io/computer-architecture1/",
            "title": "計算機結構(上)",
            "date_published": "2014-09-24T09:25:34.000Z",
            "content_html": "<!-- RENEW: -->\n\n<h2 id=\"Chap01-Introduction\"><a href=\"#Chap01-Introduction\" class=\"headerlink\" title=\"Chap01 Introduction\"></a>Chap01 Introduction</h2><h3 id=\"What-can-we-learn\"><a href=\"#What-can-we-learn\" class=\"headerlink\" title=\"What can we learn\"></a>What can we learn</h3><ul>\n<li>How programs finally executed on hardware</li>\n<li>How to write an efficient program</li>\n<li>How hardware and software work well together</li>\n<li>How to improve hardware performance<br><img data-src=\"/img/CA/2N0mDJJ.png\" alt=\"\"><a id=\"more\"></a>\n<blockquote>\n<p>What really matters is the functioning of the complete system, hardware, runtime system, compiler, operating system, and application<br>“In networking, this is called the “End to End argument” — H&amp;P</p>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"Why-need-to-take-this-course\"><a href=\"#Why-need-to-take-this-course\" class=\"headerlink\" title=\"Why need to take this course\"></a>Why need to take this course</h3><p>You will be able to answer the following questions:<br><strong>Q1: How can I write a program with good performance?code B perform better than code A?</strong><br>(I don’t know why)</p>\n<pre><code>Code A\nfor (i = 0; i &lt; N; i = i+1)\n    for (j = 0; j &lt; N; j = j+1)\n        {r = 0;\n            for (k = 0; k &lt; N; k = k+1)\n            {\n                r = r + y[i][k]*z[k][j];\n            };\n            x[i][j] = r;\n        };\n========\nCode B\nfor (jj = 0; jj &lt; N; jj = jj+B)\n    for (kk = 0; kk &lt; N; kk = kk+B)\n        for (i = 0; i &lt; N; i = i+1)\n            for (j = jj; j &lt; min(jj+B-1,N); j = j+1)\n            {\n                r = 0;\n                for (k = kk; k &lt; min(kk+B-1,N); k = k+1)\n                {\n                    r = r + y[i][k]*z[k][j];\n                }\n                x[i][j] = x[i][j] + r;\n            };</code></pre><p><strong>Q2: CPU frequency ？ Performance</strong><br>Ans at Chap02<br><strong>Q3: Why do Nvidia GPUs get so much attention today?</strong><br>Heterogeneous(異質) Computing : Integrated CPU/GPU<br><img data-src=\"/img/CA/MZa4WQV.png\" alt=\"\"></p>\n<h3 id=\"Post-PC-Era\"><a href=\"#Post-PC-Era\" class=\"headerlink\" title=\"Post PC Era\"></a>Post PC Era</h3><p>資訊界：小魚吃大魚<br><img data-src=\"/img/CA/XGJt3Qu.png\" alt=\"\"><br><img data-src=\"/img/CA/XXVfz5O.png\" alt=\"\"></p>\n<p>Ex. Cloud computing  </p>\n<ul>\n<li>Software as a Service (SaaS)</li>\n<li>Amazon and Google</li>\n</ul>\n<h3 id=\"Chip-War-Big-core-or-Small-core\"><a href=\"#Chip-War-Big-core-or-Small-core\" class=\"headerlink\" title=\"Chip War: Big core or Small core\"></a>Chip War: Big core or Small core</h3><p>Workload changes in data centers<br>Past: databases, financial services<br>Now: web service, cloud computing (accessing mails, photos, facebooks, etc)<br>=&gt; Simple tasks &amp; large amount of parallel tasks<br>=&gt; Small Core is better</p>\n<p>Ex. ARM</p>\n<ul>\n<li>Founded at the year of 1990, in Cambridge, England</li>\n<li>Co-founded by Apple &amp; Acron</li>\n<li>Emphasize low-power since day one</li>\n<li>First low-power, 32 bit mobile processor</li>\n<li>sells designs(IP license) instead of chips<br><img data-src=\"/img/CA/573HpxE.png\" alt=\"\"><br><img data-src=\"/img/CA/3Teh9R3.png\" alt=\"\"></li>\n</ul>\n<h3 id=\"Inside-the-Processor-CPU\"><a href=\"#Inside-the-Processor-CPU\" class=\"headerlink\" title=\"Inside the Processor (CPU)\"></a>Inside the Processor (CPU)</h3><p>-Datapath: performs operations on data<br>-Control: sequences datapath, memory, …<br>-Cache memory<br>    -Small fast SRAM memory for immediate access to data</p>\n<h3 id=\"Moore’s-Law-1965\"><a href=\"#Moore’s-Law-1965\" class=\"headerlink\" title=\"Moore’s Law (1965)\"></a>Moore’s Law (1965)</h3><ul>\n<li>Gordon Moore, Intel founder</li>\n<li>“The density of transistors in an integrated circuit will double <strong>every year</strong>.”</li>\n<li>Reality<ul>\n<li>The density of silicon chips doubles <strong>every 18 months</strong></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Uniprocessor-Performance\"><a href=\"#Uniprocessor-Performance\" class=\"headerlink\" title=\"Uniprocessor Performance\"></a>Uniprocessor Performance</h3><p><img data-src=\"\" alt=\"uni p\"><br>VAX : 25%/year 1978 to 1986<br>RISC + x86: 52%/year 1986 to 2002<br>RISC + x86: 20%/year 2002 to present</p>\n<h3 id=\"Concept-Change\"><a href=\"#Concept-Change\" class=\"headerlink\" title=\"Concept Change\"></a>Concept Change</h3><ul>\n<li><strong>Power expensive</strong>,** Transistors free** (Can put more on chip than can afford to turn on)<ul>\n<li>so hot that make low efficiency</li>\n</ul>\n</li>\n<li><strong>law of diminishing returns</strong>(收益遞減) on more HW for ILP( Instruction Level Parallelism, 指令層級平行)<ul>\n<li>愈平行，效率提升的投資投酬愈少(control overhead)</li>\n</ul>\n</li>\n<li><strong>Memory slow</strong>,** CPU fast** (200 clock cycles to DRAM memory, 4 clocks for multiply)</li>\n<li>Power Wall + ILP Wall + Memory Wall = CPU Wall</li>\n<li>Uniprocessor performance 2X / 1.5 yrs -&gt; <strong>multiple cores</strong> (2X processors per chip / 2 years)</li>\n</ul>\n<p>CPU constrained by power, instruction-level parallelism, memory latency<br><img data-src=\"/img/CA/V9Q8UOl.png\" alt=\"\"></p>\n<h3 id=\"CPU-history\"><a href=\"#CPU-history\" class=\"headerlink\" title=\"CPU history\"></a>CPU history</h3><p>8086 Introduced x86 ISA 1978<br>80286 Virtual memory 1982<br>80386 32-bit processor (1985)<br>80486 Pipelining Floating point unit 8 KB cache (1989)<br>Pentium Superscalar (1993)<br>Pentium Pro / II / III  Dynamic execution Multimedia instructions 1995-9<br>Pentium 4 HyperThreading Deep pipeline (2001)<br>Pentium D (2005) Dual core 2 Pentium 4 cores<br>AMD big.LITTLE 大核+小核<br><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy50ZWNoYmFuZy5jb20vcG9zdHMvMTUwMDMtYXJtLW91ci10ZWNobm9sb2d5LXRoYW4tdGhlaXItcml2YWxzLWFoZWFkLW9mLXR3by1nZW5lcmF0aW9ucw==\">AMD big.LITTLE<i class=\"fa fa-external-link-alt\"></i></span></p>\n<h3 id=\"8-Design-Principle-for-Computer-Architecture-System\"><a href=\"#8-Design-Principle-for-Computer-Architecture-System\" class=\"headerlink\" title=\"8 Design Principle for Computer Architecture/System\"></a>8 Design Principle for Computer Architecture/System</h3><ol>\n<li>Design for Moore’s Law</li>\n<li>Use abstraction to simplify design</li>\n<li>Make the common case fast</li>\n<li>Performance via parallelism</li>\n<li>Performance via pipelining</li>\n<li>Performance via prediction</li>\n<li>Hierarchy of memories</li>\n<li>Dependability via redundancy(多餘使其安全)</li>\n</ol>\n<h2 id=\"Chap02-Performance-Power-Cost\"><a href=\"#Chap02-Performance-Power-Cost\" class=\"headerlink\" title=\"Chap02 Performance/Power/Cost\"></a>Chap02 Performance/Power/Cost</h2><p>How to measure, report, and summarize performance/power/cost?  </p>\n<ul>\n<li>Metric(度量)</li>\n<li>Benchmark(基準)</li>\n</ul>\n<h3 id=\"Two-Notions-of-“Performance”\"><a href=\"#Two-Notions-of-“Performance”\" class=\"headerlink\" title=\"Two Notions of “Performance”\"></a>Two Notions of “Performance”</h3><ul>\n<li>Response Time (latency)</li>\n<li>Throughput</li>\n</ul>\n<p>-&gt; We focus on response time</p>\n<h3 id=\"Metrics-for-Performance-Evaluation\"><a href=\"#Metrics-for-Performance-Evaluation\" class=\"headerlink\" title=\"Metrics for Performance Evaluation\"></a>Metrics for Performance Evaluation</h3><ul>\n<li>Program execution time(Elapsed time經過時間)<ul>\n<li>Total time to complete a task, including disk access, I/O, etc</li>\n</ul>\n</li>\n<li>CPU execution time(使用CPU的時間)<ul>\n<li>doesn’t count I/O or time spent running other programs</li>\n</ul>\n</li>\n<li>Our Focus: user CPU time(without system CPU time)<ul>\n<li><strong>time spent executing the lines of code that are “in” program</strong></li>\n</ul>\n</li>\n</ul>\n<p>performance(x) = 1 / execution_time(x)</p>\n<p>For Embedded System  </p>\n<ul>\n<li>Hard real time<ul>\n<li>A fixed bound on the time to respond to or process an event</li>\n</ul>\n</li>\n<li>Soft real time(寬鬆)<ul>\n<li>An average response or a response within a limited time </li>\n</ul>\n</li>\n</ul>\n<h3 id=\"CPU-performance\"><a href=\"#CPU-performance\" class=\"headerlink\" title=\"CPU performance\"></a>CPU performance</h3><p>CPU time<br>= CPU clockcycles x Clockcycle time<br>= CPU clockcycles / clockrate<br>(Because Clock Cycles = Instruction Count x Cycles per Instruction )<br>= Instruction Count x Cycles per Instruction x Clockcycle time  </p>\n<p>Instruction Count of program is determined by program, ISA and compiler<br>cycles per instruction is determined by CPU hardware</p>\n<p>different instruction may have different CPI(Clocks Per Instruction)<br>-&gt; use weighted average CPI(加權平均)</p>\n<p>Aspects of CPU Performance<br><img data-src=\"/img/CA/CJkqLMX.png\" alt=\"\"></p>\n<p>Chap01 Q2: CPU frequency(時脈) is not fully related to performance</p>\n<h3 id=\"MIPS\"><a href=\"#MIPS\" class=\"headerlink\" title=\"MIPS\"></a>MIPS</h3><ul>\n<li>Millions of Instructions Per Second</li>\n<li>= Clock Rate / (CPI * 10^6)  </li>\n<li>Can’t be performance metric(CPI varies between programs)</li>\n</ul>\n<h3 id=\"CMOS-IC-power-performance\"><a href=\"#CMOS-IC-power-performance\" class=\"headerlink\" title=\"CMOS IC power performance\"></a>CMOS IC power performance</h3><p><img data-src=\"/img/CA/BeoKq0T.png\" alt=\"\"><br>$V ∝ Frequency$ =&gt; $power ∝ V^3$<br>high ascending rate </p>\n<p>dynamic energy: energy that is consumed when transistors switch states from 0 to 1 and vice versa. It depends on the capacitive loading of each transistor and the voltage applied:</p>\n<pre><code>Energy ∝ Capacitive load  x Voltage^2</code></pre><p><strong>Why is Multi-Core Good for Energy-Efficiency?</strong><br><img data-src=\"/img/CA/4lLg4I9.png\" alt=\"\"><br>only use 1/4 power</p>\n<p>Requires explicitly parallel programming. Hard to do because  </p>\n<ul>\n<li>Programming for performance</li>\n<li>Load balancing</li>\n<li>Optimizing communication and synchronization</li>\n</ul>\n<h3 id=\"Manufacturing-ICs\"><a href=\"#Manufacturing-ICs\" class=\"headerlink\" title=\"Manufacturing ICs\"></a>Manufacturing ICs</h3><p>Yield(良率): proportion of working dies(晶片) per wafer<br>Nonlinear relation to area and defect rate<br><img data-src=\"/img/CA/hkO62eV.png\" alt=\"Yield\">  </p>\n<h3 id=\"Benchmark\"><a href=\"#Benchmark\" class=\"headerlink\" title=\"Benchmark\"></a>Benchmark</h3><h4 id=\"SPEC-benchmark\"><a href=\"#SPEC-benchmark\" class=\"headerlink\" title=\"SPEC benchmark\"></a>SPEC benchmark</h4><ul>\n<li>Standard Performance Evaluation Corp. (SPEC)<ul>\n<li>Develops benchmarks for CPU, I/O, Web, …</li>\n<li>CPU Benchmark  <ul>\n<li>Programs to simulate actual workload</li>\n<li>focuses on CPU performance(a little I/O)</li>\n<li>Test both integer and floating point applications<br>CINT (integer) and CFP (floating-point)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>SPECRatio</strong><br><strong>Normalize</strong> execution times to reference computer<br>performance = time on standard computer/ time on computer being rated<br><img data-src=\"/img/CA/xfEcDxV.png\" alt=\"\"><br>Since ratios, geometric mean is proper</p>\n<h4 id=\"Power-Benchmark\"><a href=\"#Power-Benchmark\" class=\"headerlink\" title=\"Power Benchmark\"></a>Power Benchmark</h4><ul>\n<li>Power consumption of server at different workload levels(不同的工作量下，平均的工作效能)</li>\n<li>Report power consumption of servers at different workload levels, divided into 10% increments</li>\n<li>Performance: ssj_ops/sec(每秒完成的事務數)</li>\n<li>Power: Watts (Joules/sec)<br><img data-src=\"/img/CA/RwlzFoo.png\" alt=\"\"></li>\n</ul>\n<p>Amdahl’s Law<br>程式的平行化只能將執行時間縮短到一定程度(因為分散運算的overhead)<br><img data-src=\"/img/CA/vSJTU7L.png\" alt=\"\"></p>\n<p>Low Power at Idle(x)  </p>\n<ul>\n<li>At 100% load: 295W</li>\n<li>At 50% load: 246W (83%)</li>\n<li>At 10% load: 180W (61%)  </li>\n<li>not porportional!</li>\n</ul>\n<p>=&gt; In general, the more load, the better power consumption rate<br>=&gt; Consider designing processors to make power proportional to load</p>\n<h2 id=\"Chap03-Instruction-Set-Architecture-ISA\"><a href=\"#Chap03-Instruction-Set-Architecture-ISA\" class=\"headerlink\" title=\"Chap03 Instruction Set Architecture(ISA)\"></a>Chap03 Instruction Set Architecture(ISA)</h2><p>Definition: Instruction set provides an layer of abstraction to programmers<br><img data-src=\"/img/CA/ev3Iazm.png\" alt=\"\"></p>\n<h3 id=\"ISA-Design-Principle\"><a href=\"#ISA-Design-Principle\" class=\"headerlink\" title=\"ISA Design Principle\"></a>ISA Design Principle</h3><p>makes it easy to build the hardware and the compiler while maximizing performance and minimizing cost.</p>\n<p>Interface Design<br>(portability, compatibility, generality)<br>(convenient at high level , efficient in low level)</p>\n<p><img data-src=\"/img/CA/1nBfGuE.png\" alt=\"\"></p>\n<h3 id=\"Design-Principles\"><a href=\"#Design-Principles\" class=\"headerlink\" title=\"Design Principles\"></a>Design Principles</h3><ul>\n<li>Design Principle 1: Simplicity favors regularity(All operation takes three operands)</li>\n<li>Design Principle 2: Smaller is faster(MIPS only 32 register)</li>\n<li>Design Principle 3: Make the common thing fast(addi)</li>\n<li>Design Principle 4: Good design demands good compromises(妥協)(three format of instruction code)</li>\n</ul>\n<h3 id=\"MIPS-Instruction-Set\"><a href=\"#MIPS-Instruction-Set\" class=\"headerlink\" title=\"MIPS Instruction Set\"></a>MIPS Instruction Set</h3><p>Stanford MIPS be Memory are referenced with byte addresses in MIPScommercialized<br>a kind of RISC精簡指令集(Reduced Instruction Set Computing)</p>\n<ul>\n<li>31個 32-bit integer registers (R0 = const 0)<br><img data-src=\"/img/CA/VRdEFJz.png\" alt=\"\"></li>\n<li>32個 32-bit floating-point registers</li>\n<li>32-bit HI, LO, PC (program counter) – internal, can’t modify<br>Memory are referenced with byte addresses in MIPS<br>1 word = 4 byte</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>指令</th>\n<th>意義</th>\n<th>備註</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>add Ra Rb Rc</td>\n<td>Ra =Rb + Rc</td>\n<td></td>\n</tr>\n<tr>\n<td>load Ra A</td>\n<td>Ra =mem[A]</td>\n<td></td>\n</tr>\n<tr>\n<td>store Ra A</td>\n<td>mem[A] =Ra</td>\n<td>Memory are referenced with byte addresses in MIPS</td>\n</tr>\n<tr>\n<td></td>\n<td>Data transfer instructions</td>\n<td></td>\n</tr>\n<tr>\n<td>lw ＄t0, 8 (＄s3)</td>\n<td>t0 = mem[8+reg[＄s3]]</td>\n<td>lw dest offset base, offset must be constant</td>\n</tr>\n<tr>\n<td>Ex. <img data-src=\"/img/CA/c0XyK9w.png\" alt=\"\"></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>sw ＄t0, 8 (＄s3)</td>\n<td>mem[8+reg[＄s3]]= ＄t0</td>\n<td></td>\n</tr>\n<tr>\n<td>addi ＄s3, ＄s3, 4</td>\n<td>add constant value</td>\n<td></td>\n</tr>\n<tr>\n<td>srl ＄10, ＄16, 4</td>\n<td>＄t2 = ＄s0 &gt;&gt; 4 bits</td>\n<td></td>\n</tr>\n<tr>\n<td>sll ＄10, ＄16, 4</td>\n<td>＄t2 = ＄s0 &lt;&lt; 4 bits</td>\n<td></td>\n</tr>\n<tr>\n<td>rs fill 0 because unused</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>or ＄t0, ＄t1, ＄t2</td>\n<td>＄t0 = ＄t1 | ＄t2</td>\n<td></td>\n</tr>\n<tr>\n<td>ori ＄6, ＄6, 0x00ff</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>and ＄t0, ＄t1, ＄t2</td>\n<td>＄t0 = ＄t1 &amp; ＄t2</td>\n<td></td>\n</tr>\n<tr>\n<td>andi ＄6, ＄6, 0x0000</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>nor ＄t0, ＄t1, ＄t3</td>\n<td>t0 = ~(＄t1</td>\n<td>＄ t3)</td>\n</tr>\n<tr>\n<td>(nor ＄t0, ＄t1, ＄t3) = ~(＄t1), if ＄t3 =</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>~(A) = 1 if A = 0</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>~(A) = 0 if A != 0</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<pre><code>lui $t0, 1010101010101010 # load upper immediate\n// that is: 1010101010101010 0000000000000000</code></pre><p>mult rs, rt<br>Multiplying two 32-bit numbers can yield a 64-bit number.<br>higher 32 bits are stored in HI<br>lower 32 bits are stored in LO<br>MFHI rd –  ＄rd = HI<br>MFLO rd –  ＄rd = LO  </p>\n<p>div rs, rt | rs / rt<br>Quotient stored in Lo<br>Remainder stored in Hi   </p>\n<p>Branch instructions<br>Conditional branches<br>beq reg1, reg2, L1<br>Go to statement L1 if [reg1] == [reg2]<br>bne reg1, reg2, L2<br>Go to statement L2 if [reg1] != [reg2]<br>Unconditional branches<br>j L1<br><img data-src=\"/img/CA/ugemo7v.png\" alt=\"\">   </p>\n<p>jr(jump register)<br>unconditional jump to the address stored in a register<br>Provides full 32bits address<br><img data-src=\"/img/CA/BQpStOr.png\" alt=\"\"></p>\n<p>Use J for calling subroutines<br>Use Jal for calling functions<br>Use Jr for ending a subroutine by jumping to the return address (ra)</p>\n<p>slt reg1, reg2, reg3<br>=&gt; if reg1 = (reg2 &lt; reg3) ? 1 : 0;</p>\n<p><code>move</code> copies a value from one register to another</p>\n<p>Byte order: Big Endian vs. Little Endian<br>Big endian(順序, 一般數字寫法): byte 0 is most significant bits e.g., IBM/360/370, Motorola 68K, MIPS, Sparc, HP PA<br>Little endian(逆序): byte 0 is least significant bits e.g., Intel 80x86, DEC Vax, DEC Alpha<br><img data-src=\"/img/CA/RiImWlu.png\" alt=\"\"></p>\n<p>Alignment<br>object on the address that is multiple of their size(一次讀取大小的倍數)<br><img data-src=\"/img/CA/kaceTao.png\" alt=\"\"></p>\n<p>Stored-Program Concept<br>Computers built on 2 key principles:  </p>\n<p>1) Instructions are represented as numbers<br>2) entire programs can be stored in memory to be read or written just like numbers  </p>\n<p>Shift is faster than multiplication<br>=&gt; Multiplying by 4 is the same as shifting left by 2:</p>\n<p>•Fetch instruction from mem [PC]<br>•without decision making instruction<br>    •next instruction = mem [PC + instruction_size]</p>\n<h3 id=\"MIPS-Instruction-Format\"><a href=\"#MIPS-Instruction-Format\" class=\"headerlink\" title=\"MIPS Instruction Format\"></a>MIPS Instruction Format</h3><p>instruction code - 32bit<br><img data-src=\"/img/CA/MeKGSy9.png\" alt=\"\"></p>\n<h4 id=\"R-type\"><a href=\"#R-type\" class=\"headerlink\" title=\"R-type\"></a>R-type</h4><ul>\n<li>rs, rt = source register, rd = destination register<ul>\n<li>32個register =&gt; 5bit</li>\n</ul>\n</li>\n<li>shamt: shift amount<ul>\n<li>filling 0 when unused</li>\n</ul>\n</li>\n<li>func: function field(same operate code with different function code can looked as different argument) <ul>\n<li>Ex. <code>add</code>,<code>addu</code>, <code>addi</code></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"I-type\"><a href=\"#I-type\" class=\"headerlink\" title=\"I-type\"></a>I-type</h4><p>for those have <strong>constant</strong> argument(immediate value)<br><img data-src=\"/img/CA/MCxQVaC.png\" alt=\"\"><br>first example, rs = ＄2(source) rt = ＄1(destination)</p>\n<p>PC addressing mode<br>New PC = PC + 4(auto go to next instruction) + Immediate x 4</p>\n<p>if want to jump farther than 16bit </p>\n<pre><code>bne s1 s2 L2 \njump L1 # equals to beq s1 s2 L1, but jump can go farther\nL2:</code></pre><h4 id=\"J-type\"><a href=\"#J-type\" class=\"headerlink\" title=\"J-type\"></a>J-type</h4><p><img data-src=\"/img/CA/j.png\" alt=\"j type\"></p>\n<p>goto absolute address(I-type is relative address)<br>equal to immediate in the middle of the program counter address<br><img data-src=\"/img/CA/add2.png\" alt=\"\"></p>\n<h3 id=\"指令表\"><a href=\"#指令表\" class=\"headerlink\" title=\"指令表\"></a>指令表</h3><p><img data-src=\"/img/CA/com.png\" alt=\"\"><br><img data-src=\"/img/CA/comm2.png\" alt=\"\"><br><img data-src=\"/img/CA/comm3.png\" alt=\"\"></p>\n<h3 id=\"function\"><a href=\"#function\" class=\"headerlink\" title=\"function\"></a>function</h3><p>pass value</p>\n<ul>\n<li><p>＄a0 ~ ＄a3 : 4 arguments </p>\n<ul>\n<li>if # of parameters is larger than 4 – store to the stack</li>\n</ul>\n</li>\n<li><p>＄v0 ~ ＄v1 : 2 return values<br>preserve register values of caller</p>\n</li>\n<li><p>stack</p>\n</li>\n</ul>\n<p>Caller-save register </p>\n<ul>\n<li>Caller saved register: 由Caller負責清理或存入stack frame<ul>\n<li>hold temporary quantities that need not be preserved across calls</li>\n<li>Caller先備份，所以callee便可直接使用caller-save register</li>\n<li>＄t0~t7</li>\n</ul>\n</li>\n<li>Callee saved register: 由Callee負責清理或存入stack<ul>\n<li>hold values that should be preserved across calls(caller還需要用, callee用的時候要先備份)</li>\n<li>callee should save them and restore them before returning to the caller</li>\n<li>＄s0~s7</li>\n</ul>\n</li>\n</ul>\n<p>call function: <code>jal</code> procedure_address (jump and link)</p>\n<ul>\n<li>Store the return address (PC + 4) at ＄ra</li>\n<li>set PC = procedure_address<br>return : jr ＄ra</li>\n</ul>\n<p>Frame pointer points to the first word of the procedure frame</p>\n<p>procedure call stack<br><img data-src=\"\" alt=\"\"></p>\n<p><img data-src=\"\" alt=\"memory layout\"><br>Memory layout</p>\n<ul>\n<li>text: code</li>\n<li>static data: global variable<ul>\n<li>＄gp is the offset of static data</li>\n</ul>\n</li>\n<li>dynamic data: heap<ul>\n<li>malloc, new</li>\n</ul>\n</li>\n<li>stack: automatic storage</li>\n</ul>\n<p>Caller Steps</p>\n<ol>\n<li>pass the argument ＄a0,＄a1 </li>\n<li>save caller-saved registers</li>\n<li>jal</li>\n</ol>\n<p>Callee Steps</p>\n<ol>\n<li>establish stack frame ＄sp<br>subi ＄sp, ＄sp <frame-size></li>\n<li>saved callee saved registers<br>Ex. ＄ra, ＄fp, ＄s0-＄s7</li>\n<li>establish frame pointer ＄fp<br>Add ＄fp, ＄sp, <frame-size>-4</li>\n<li>Do Something</li>\n<li>put returned values in ＄v0, ＄v1</li>\n<li>restore(load) callee-saved registers</li>\n<li>pop the stack</li>\n<li>return: jr ＄ra</li>\n</ol>\n<p><img data-src=\"/img/CA/reg.png\" alt=\"registers\"></p>\n<p><img data-src=\"\" alt=\"nested procedure\"></p>\n<p>ASCII (American Standard Code for Information Interchange)<br>8 bits/character</p>\n<pre><code>`lb sb`</code></pre><p>Unicode (Universal Encoding)<br>16 bits/character</p>\n<pre><code>`lf sf`</code></pre><p>performance</p>\n<figure class=\"highlight angelscript\"><table><tr><td class=\"code\"><pre><span class=\"line\">Clear1(<span class=\"built_in\">int</span> <span class=\"built_in\">array</span>[ ], <span class=\"built_in\">int</span> size)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"built_in\">int</span> i;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (i=<span class=\"number\">0</span>, i&lt; size; i+= <span class=\"number\">1</span>)</span><br><span class=\"line\">\t<span class=\"built_in\">array</span>[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">Clear <span class=\"number\">2</span>(<span class=\"built_in\">int</span> *<span class=\"built_in\">array</span>, <span class=\"built_in\">int</span> size)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"built_in\">int</span> *p,</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (p = &amp;<span class=\"built_in\">array</span>[<span class=\"number\">0</span>]; p &lt; &amp;<span class=\"built_in\">array</span>[size]; p = p+<span class=\"number\">1</span>)</span><br><span class=\"line\">\t*p = <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Clear2 &gt; Clear1<br>pointer is faster than fetching value</p>\n<figure class=\"highlight mipsasm\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">move </span>＄<span class=\"built_in\">t0</span>, ＄<span class=\"built_in\">a0</span> \t\t\t<span class=\"comment\"># p = &amp;array[0]</span></span><br><span class=\"line\"><span class=\"keyword\">sll </span>＄<span class=\"built_in\">t1</span>, ＄<span class=\"built_in\">a1</span>, <span class=\"number\">2</span> \t\t<span class=\"comment\"># t1 = size x 4</span></span><br><span class=\"line\"><span class=\"keyword\">add </span>＄<span class=\"built_in\">t2</span>, ＄<span class=\"built_in\">a0</span>, ＄<span class=\"built_in\">t1</span>\t\t<span class=\"comment\"># t2 = &amp;array[size]</span></span><br><span class=\"line\"><span class=\"symbol\">Loop2:</span> </span><br><span class=\"line\"><span class=\"keyword\">sw </span>＄<span class=\"built_in\">zero</span>, <span class=\"number\">0</span>(＄<span class=\"built_in\">t0</span>) \t\t<span class=\"comment\"># memory[p] = 0</span></span><br><span class=\"line\"><span class=\"keyword\">addi </span>＄<span class=\"built_in\">t0</span>, ＄<span class=\"built_in\">t0</span>, <span class=\"number\">4</span> \t\t<span class=\"comment\"># p= p+4</span></span><br><span class=\"line\"><span class=\"keyword\">slt </span>＄<span class=\"built_in\">t3</span>, ＄<span class=\"built_in\">t0</span>, ＄<span class=\"built_in\">t2</span> \t\t<span class=\"comment\"># compare p &amp; array[size]</span></span><br><span class=\"line\"><span class=\"keyword\">bne </span>＄<span class=\"built_in\">t3</span>, ＄<span class=\"built_in\">zero</span>, Loop2</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Synchronization\"><a href=\"#Synchronization\" class=\"headerlink\" title=\"Synchronization\"></a>Synchronization</h2><p>SWAP: <strong>atomically</strong> interchange a value in a register for a value in memory; nothing else can interpose itself between the read and the write to the memory location</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\">＄S4 = 1;</span><br><span class=\"line\">Swap (＄S4, <span class=\"keyword\">Lock</span>);</span><br><span class=\"line\">If <span class=\"keyword\">Lock</span> = <span class=\"number\">0</span> <span class=\"comment\"># change success</span></span><br><span class=\"line\">enter <span class=\"keyword\">critical</span> <span class=\"keyword\">section</span>;</span><br></pre></td></tr></table></figure>\n\n<p>Load linked: load value from rs(atomic)</p>\n<pre><code>`ll rt offset(rs)`</code></pre><p>Store conditional</p>\n<pre><code>`sc rt offset(rs)`</code></pre><p>if location not changed since the ll , rt return 1, 否則回傳 0</p>\n<figure class=\"highlight mipsasm\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">swap </span><span class=\"built_in\">s4</span> <span class=\"keyword\">and </span><span class=\"built_in\">s1</span></span><br><span class=\"line\"><span class=\"symbol\">try:</span><span class=\"keyword\">add </span>＄<span class=\"built_in\">t0</span>,＄<span class=\"built_in\">zero</span>,＄<span class=\"built_in\">s4</span> <span class=\"comment\">;copy exchange value</span></span><br><span class=\"line\">\t<span class=\"keyword\">ll </span>＄<span class=\"built_in\">t1</span>,<span class=\"number\">0</span>(＄<span class=\"built_in\">s1</span>) <span class=\"comment\">;load s1 to t1</span></span><br><span class=\"line\">\t<span class=\"keyword\">sc </span>＄<span class=\"built_in\">t0</span>,<span class=\"number\">0</span>(＄<span class=\"built_in\">s1</span>) <span class=\"comment\">;store t0 to s1</span></span><br><span class=\"line\">\t<span class=\"keyword\">beq </span>＄<span class=\"built_in\">t0</span>,＄<span class=\"built_in\">zero</span>,try <span class=\"comment\">;if store fail, try again</span></span><br><span class=\"line\">\t<span class=\"keyword\">add </span>＄<span class=\"built_in\">s4</span>,＄<span class=\"built_in\">zero</span>,＄<span class=\"built_in\">t1</span> <span class=\"comment\">;put load value in ＄s4</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"MIPS-Addresssing-Mode\"><a href=\"#MIPS-Addresssing-Mode\" class=\"headerlink\" title=\"MIPS Addresssing Mode\"></a>MIPS Addresssing Mode</h3><ol>\n<li>Register addressing<br>運算對象在register<br><code>add</code></li>\n<li>Immediate addressing<br>運算對象是constant<br><code>addi</code> </li>\n<li>Base addressing(The argument is at MEM)<br>運算對象在memory<br><code>lw</code><br>way to get address<ol>\n<li> .data # define data<br> xyz:<br> .word 1 # some data here<br> …<br> .text # program code<br> …<br> lw $5,xyz # equals to lw ＄5, offset(＄gp)</li>\n<li> la ＄6, xyz # ＄6 = &amp;xyz</li>\n</ol>\n</li>\n<li>PC-relative addressing<br><code>beq</code> </li>\n<li>Pseudodirect addressing<br><code>j 100</code></li>\n</ol>\n<h3 id=\"from-program-to-memory\"><a href=\"#from-program-to-memory\" class=\"headerlink\" title=\"from program to memory\"></a>from program to memory</h3><pre><code>Program -----------&gt; assembly ---------&gt; object file:\n          compiler            assembler\nmachine language module + library module ---------&gt;  \n                                           linker  \nmachine code program ---------&gt; memory\n                       loader</code></pre><p><strong>Assembler</strong><br>Symbol table: translate variables into memory address<br>Psudoinstruction: common variation of assembly language instructions<br><strong>Linker</strong><br><img data-src=\"/img/CA/exeheader.png\" alt=\"executable \"></p>\n<p>generate executables file header  </p>\n<ol>\n<li>Place code and data modules symbolically in memory.</li>\n<li>Determine the addresses of data and instruction labels.</li>\n<li>Patch both the internal and external references</li>\n</ol>\n<p><strong>Loader</strong><br>determine the size of the text and data segment<br>Creates an address space<br>Copies the instructions and data to memory<br>Initializes the machine registers and sets the stack pointer<br>Jump to a start-up routine</p>\n<p><strong>Dynamically Linked Libraries (DLL)</strong><br>Loading the whole library even if all of the library is not used =&gt; libraries are not linked and loaded until the program is run, and use lazy procedure linkage</p>\n<h3 id=\"java\"><a href=\"#java\" class=\"headerlink\" title=\"java\"></a>java</h3><p><img data-src=\"/img/CA/java.png\" alt=\"java\"><br>Java bytecode: use instruction set designed to interpret Java programs<br>Just In Time Compiler (JIT): compiler that operates at runtime, translating bytecodes into the native code of the compiler<br>Java Virtual Machine (JVM): The program that interprets Java bytecodes</p>\n<h3 id=\"ARM\"><a href=\"#ARM\" class=\"headerlink\" title=\"ARM\"></a>ARM</h3><p>AMD64 (2003): extended architecture to 64 bits<br>AMD64 (announced 2007): SSE5 instructions<br>Intel declined to follow…</p>\n<p>Similar to MIPS<br><img data-src=\"/img/CA/armmips.png\" alt=\"arm mips\"><br>do anything for code density<br>include<br>32-bit ARM instruction<br>16-bit Thumb instruction<br>8-bit Java Instruction Set  </p>\n<p>Current program status register: Top four bits of CPSR<br>use for performance<br>N : Negative<br>Z : Zero<br>C : Carry<br>V : Overflow  </p>\n<p>Conditional execution<br>ADDEQ r0,r1,r2 =&gt; r1-r2, If zero flag set then do r0 = r1 + r2</p>\n<h3 id=\"Intel-x86-ISA\"><a href=\"#Intel-x86-ISA\" class=\"headerlink\" title=\"Intel x86 ISA\"></a>Intel x86 ISA</h3><p>History: skipped</p>\n<p>| IA-32                      | MIPS            |<br>| ————————– | ————— | —————– |<br>| RISC                       | RISC            |<br>| general purpose register   | 8               | 32                |<br>| operand operation          | 2 or 3          | 3                 |<br>| operations to be performed | Register-memory | register-register |<br>| addressing modes           | more            | less              |<br>| encoding                   | variable-length | fixed-length      |</p>\n<p>simple instruction -&gt; higher performance<br>because complex instructions are hard to implement, slow down all instructions</p>\n<p>assembly code for high performance(x)<br>More lines of code -&gt; more errors and less productivity</p>\n<h2 id=\"Chap04-Building-Single-Cycle-Datapath-and-Control-Unit\"><a href=\"#Chap04-Building-Single-Cycle-Datapath-and-Control-Unit\" class=\"headerlink\" title=\"Chap04 Building Single-Cycle Datapath and Control Unit\"></a>Chap04 Building Single-Cycle Datapath and Control Unit</h2><p>How to Design a Processor</p>\n<ol>\n<li>Analyze instruction set =&gt; datapath(include PC,… functional units) requirements</li>\n<li>Select set of datapath components and establish clocking methodology</li>\n<li>Assemble datapath</li>\n<li>Analyze <strong>implementation of each instruction</strong> to determine control points</li>\n<li>Assemble the control logic</li>\n</ol>\n<p>Details</p>\n<ol>\n<li>register-tranfer暫存器傳輸<br>RTL (Register Transfer Languages)<br>Ex. ADDUR[rd] &lt;–R[rs] + R[rt], PC &lt;–PC + 4  </li>\n<li>Combinational Elements: outputs only depend on input<br>Ex. ALU, MUX, Adder<br>Storage Elements: outputs depend on input and state(clock)<br>Ex. Flip-Flop, register, memory<br>Register<br><img data-src=\"/img/CA/regis.png\" alt=\"register graph\"><br>Edge-triggered clocking<br>until next edge would the value change</li>\n<li></li>\n</ol>\n<p><strong>Instruction Fetch Unit</strong>: mem[PC]<br>Sequential code<br>PC &lt;- PC + 4<br>Branch and Jump<br>PC &lt;- Target addr<br><strong>Memory Operations</strong><br>Mem[R[rs] + SignExt[imm16]]<br>sign extension: increasing the number of bits without   changing value (Ex. 10 0010 -&gt; 0010 0010)<br>4.<br>Control Unit: set control flag to change the operation with the change of operation code<br>Control Flags</p>\n<ul>\n<li>MemWr: write memory</li>\n<li>MemtoReg: 0 =&gt; use ALU output 1 =&gt; use Mem value</li>\n<li>RegDst: 1 =&gt; “rd” when 3 operand are all register; 0 =&gt; “rt”</li>\n<li>RegWr: write to register</li>\n<li>ALUsrc: 1=&gt; immed; 0=&gt;regB</li>\n<li>ALUctr: “add”, “sub”</li>\n<li>PCSrc: 1=&gt; PC = PC + 4; 0=&gt; PC = branch target address</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>instruction</th>\n<th>MemWr</th>\n<th>MemtoReg</th>\n<th>RegDst</th>\n<th>RegWr</th>\n<th>ALUsrc</th>\n<th>ALUctr</th>\n<th>PCSrc</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Add</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n<td>add</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Load</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>1</td>\n<td>1</td>\n<td>add</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Store</td>\n<td>1</td>\n<td>x</td>\n<td>x</td>\n<td>0</td>\n<td>1</td>\n<td>add</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Branch</td>\n<td>0</td>\n<td>x</td>\n<td>x</td>\n<td>0</td>\n<td>0</td>\n<td>sub</td>\n<td>value(after sub)</td>\n</tr>\n<tr>\n<td>Ori</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>1</td>\n<td>or</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Jump</td>\n<td>?</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<ol start=\"5\">\n<li>use k-map to make control flags simple<br><img data-src=\"/img/CA/aluctrl.png\" alt=\"ALU control\"><br>I-type: (00) add for load/store and (01) sub for beqand (10) for ori<br>R-type: (11, need to check funct field)</li>\n</ol>\n<p><img data-src=\"/img/CA/table1.png\" alt=\"alu ctr truth table\"><br><img data-src=\"/img/CA/tabl2.png\" alt=\"truth table for main\"></p>\n<p><img data-src=\"/img/CA/table3.png\" alt=\"functional unit used by instructions\"></p>\n<h2 id=\"Chap05-Pipeline\"><a href=\"#Chap05-Pipeline\" class=\"headerlink\" title=\"Chap05 Pipeline\"></a>Chap05 Pipeline</h2><p>when CPI=1:</p>\n<ul>\n<li>Load is too slow! -&gt; Long Cycle Time</li>\n<li>Real memory is not so nice as our idealized memory<ul>\n<li>cannot always get the job done in one (short) cycle</li>\n</ul>\n</li>\n</ul>\n<p>Pipelining doesn’t help <strong>latency</strong> of single task, it helps <strong>throughput</strong> of entire workload<br>Multiple tasks operating simultaneously <strong>using different resources</strong><br>Pipeline rate <strong>limited by slowest pipeline stage</strong><br>Unbalanced lengths of pipe stages reduces speedup(切平均，增加效能最多)</p>\n<p>Break the instruction into smaller steps(5 steps in MIPS):<br>Execute each step in 1 clock cycle</p>\n<ul>\n<li>Instruction Fetch(Ifetch)</li>\n<li>Instruction Decode and Register Fetch(Reg/Dec)</li>\n<li>Execution, Memory Address Computation, or Branch Completion(Exec)</li>\n<li>Memory Access or R-type instruction completion(Mem)</li>\n<li>Write back to register(Wr)</li>\n</ul>\n<p>simplified: IF ID EX MEM WB</p>\n<p>The Four Stages of R-type: without Mem<br>The Four Stages of store: without Wr<br>The Three Stages of Beq: without Mem, Wr</p>\n<p><img data-src=\"/img/CA/pipe.png\" alt=\"pipe 2ns\"><br>Ideal speedup from pipelining == # of pipeline stages<br>(分成N段-&gt;理想上，速度提升N倍)</p>\n<h4 id=\"Pipeline-Hazards\"><a href=\"#Pipeline-Hazards\" class=\"headerlink\" title=\"Pipeline Hazards\"></a>Pipeline Hazards</h4><ul>\n<li>structural hazards: use the same resource at the same time(Single Memory)<br><img data-src=\"/img/CA/pipe2.png\" alt=\"structure hazard\"></li>\n<li>data hazards: use item before it is ready(load, update)</li>\n<li>control hazards: make a decision before condition is evaluated(branch)<br>Can always resolve hazards by waiting<br>Need to detect and resolve hazards</li>\n</ul>\n<p>Way to Solve Hazards</p>\n<ol>\n<li>structural<ol>\n<li>Stall(put bubbles that do nothing)<br><img data-src=\"/img/CA/stall.png\" alt=\"stall\"> </li>\n<li>Split instruction and data memory(2 different memories)</li>\n<li>Delay R-type’s Write by One Cycle()</li>\n</ol>\n</li>\n<li>data<ol>\n<li>“Forward” result from one stage to another(算出答案後，在放入記憶體前，就先給之後的instruction使用)(可以同時read and save) </li>\n<li>Load-use data hazard: can’t solved by forward, can solved by reordering instructions, otherwise, need stall  <img data-src=\"/img/CA/forward.png\" alt=\"lw hazard\">   <img data-src=\"/img/CA/forward2.png\" alt=\"loaduse-&gt;forward\"></li>\n</ol>\n</li>\n<li>control<ol>\n<li>stall until the outcome of the branch is known</li>\n<li>solve branch earlier: put in enough extra hardware so that we can test registers, calculate the branch address, and update the PC</li>\n<li>Predict: flush if the assumption is wrong<br>dynamic scheme: history of branch (90% success)</li>\n<li>performance: 13% of branch instructions executed in SPECint2000, the CPI is slowdown of 1.13 versus the ideal case</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"Pipelined-datapath\"><a href=\"#Pipelined-datapath\" class=\"headerlink\" title=\"Pipelined datapath\"></a>Pipelined datapath</h3><p>Pipeline registers<br><img data-src=\"/img/CA/pipegraph.png\" alt=\"4bar\"></p>\n<ul>\n<li>The registers are named between two stages</li>\n</ul>\n<h3 id=\"control-in-each-stage\"><a href=\"#control-in-each-stage\" class=\"headerlink\" title=\"control in each stage\"></a>control in each stage</h3><p>Pass control signals along just like the data<br><img data-src=\"/img/CA/pipectrl.png\" alt=\"control\"></p>\n",
            "tags": [
                "計算機結構"
            ]
        }
    ]
}