<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://gitqwerty777.github.io</id>
    <title>QWERTY • Posts by &#34;機器學習&#34; tag</title>
    <link href="http://gitqwerty777.github.io" />
    <updated>2015-05-01T04:37:47.000Z</updated>
    <category term="C#" />
    <category term="CodingStyle" />
    <category term="Emacs" />
    <category term="編輯器" />
    <category term="CFR" />
    <category term="電腦對局理論" />
    <category term="指令" />
    <category term="機器學習" />
    <category term="perceptron" />
    <category term="readme" />
    <category term="文件" />
    <category term="github" />
    <category term="artificial intelligence" />
    <category term="search" />
    <category term="First-Order Logic" />
    <category term="大數" />
    <category term="程式" />
    <category term="C++" />
    <category term="Hexo" />
    <category term="網誌" />
    <category term="Markdown" />
    <category term="CleanCode" />
    <category term="重構" />
    <category term="TDD" />
    <category term="設計模式" />
    <category term="CMake" />
    <category term="Makefile" />
    <category term="Linux" />
    <category term="Todo" />
    <category term="註解" />
    <category term="經濟學" />
    <category term="策略" />
    <category term="競爭" />
    <category term="博弈論" />
    <category term="計算機結構" />
    <category term="人工智慧" />
    <category term="圍棋" />
    <category term="象棋" />
    <category term="蒙地卡羅" />
    <category term="Alpha-Beta搜尋" />
    <category term="強化學習" />
    <category term="計算機網路" />
    <category term="boost" />
    <category term="函式庫" />
    <category term="編譯" />
    <category term="gcc" />
    <category term="g++" />
    <category term="clang" />
    <category term="最佳化" />
    <category term="推薦系統" />
    <category term="FM" />
    <category term="FFM" />
    <category term="SVM" />
    <category term="Embedding" />
    <category term="自然語言處理" />
    <category term="外國用語" />
    <category term="萌典" />
    <category term="opencc" />
    <category term="PTT" />
    <category term="vuejs" />
    <category term="linux" />
    <category term="c" />
    <category term="compile" />
    <category term="gdb" />
    <category term="c語言" />
    <category term="cpp" />
    <category term="除錯" />
    <category term="git" />
    <category term="VMWare" />
    <category term="虛擬機" />
    <category term="IFTTT" />
    <category term="自動化" />
    <category term="備份" />
    <category term="webhook" />
    <category term="簡報" />
    <category term="軟體" />
    <category term="PowerPoint" />
    <category term="Latex" />
    <category term="JavaScript" />
    <category term="CSS" />
    <category term="Unity" />
    <category term="fcitx" />
    <category term="嘸蝦米" />
    <category term="輸入法" />
    <category term="硬碟" />
    <category term="記憶體" />
    <category term="效能" />
    <category term="錯誤" />
    <category term="makefile" />
    <category term="備忘錄" />
    <category term="存檔" />
    <category term="統計" />
    <category term="byobu" />
    <category term="screen" />
    <category term="tmux" />
    <category term="reactjs" />
    <category term="javascript" />
    <category term="WideAndDeep" />
    <category term="Google" />
    <category term="觀察者" />
    <category term="訂閱" />
    <category term="委託" />
    <category term="正規表示式(RegExp)" />
    <category term="上下文無關文法(CFG)" />
    <category term="hexo" />
    <category term="blog" />
    <category term="theme" />
    <category term="feature" />
    <category term="revealJS" />
    <category term="markdown" />
    <category term="rss" />
    <category term="facebook" />
    <category term="youtube" />
    <category term="ptt" />
    <category term="bilibili" />
    <category term="pixiv" />
    <category term="crawler" />
    <category term="SEO" />
    <category term="google" />
    <category term="html" />
    <category term="amazon" />
    <category term="webhost" />
    <category term="ssl" />
    <category term="漢字" />
    <category term="中文" />
    <category term="異體字" />
    <category term="unicode" />
    <category term="unity" />
    <category term="演算法" />
    <category term="隨機排序" />
    <category term="洗牌" />
    <category term="Fisher-Yates" />
    <category term="證明" />
    <category term="python" />
    <entry>
        <id>http://gitqwerty777.github.io/natural-language-processing2/</id>
        <title>自然語言處理(下)</title>
        <link rel="alternate" href="http://gitqwerty777.github.io/natural-language-processing2/"/>
        <content type="html">&lt;h2 id=&#34;Chap08-Syntax-and-Grammars&#34;&gt;&lt;a href=&#34;#Chap08-Syntax-and-Grammars&#34; class=&#34;headerlink&#34; title=&#34;Chap08 Syntax and Grammars&#34;&gt;&lt;/a&gt;Chap08 Syntax and Grammars&lt;/h2&gt;&lt;p&gt;Grammar    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;represent certain knowledges of what we know about a language&lt;/li&gt;
&lt;li&gt;General criteria&lt;ul&gt;
&lt;li&gt;linguistic naturalness&lt;/li&gt;
&lt;li&gt;mathematical power&lt;/li&gt;
&lt;li&gt;computational effectiveness&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Context-free grammars(CFG)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alias&lt;ul&gt;
&lt;li&gt;Phrase structure grammars&lt;/li&gt;
&lt;li&gt;Backus-Naur form&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;More powerful than finite state machine&lt;/li&gt;
&lt;li&gt;Rules &lt;ul&gt;
&lt;li&gt;Terminals &lt;ul&gt;
&lt;li&gt;words&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Non-terminals &lt;ul&gt;
&lt;li&gt;Noun phrase, Verb phrase …&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Generate strings in the language&lt;/li&gt;
&lt;li&gt;Reject strings not in the language  &lt;/li&gt;
&lt;li&gt;LHS → RHS&lt;ul&gt;
&lt;li&gt;LHS: Non-terminals &lt;/li&gt;
&lt;li&gt;RHS: Non-terminals or Terminals&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Context Free&lt;ul&gt;
&lt;li&gt;probability of a subtree does not depend on words not dominated by the subtree(subtree出現的機率和上下文無關)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;a id=&#34;more&#34;&gt;&lt;/a&gt;

&lt;p&gt;Evaluation  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Does it undergenerate?&lt;ul&gt;
&lt;li&gt;rules cannot completely explain language&lt;/li&gt;
&lt;li&gt;not a problem if the goal is to produce a language&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Does it overgenerate?&lt;ul&gt;
&lt;li&gt;rules overly explain the language&lt;/li&gt;
&lt;li&gt;not a problem if the goal is to recognize or understand well-formed(correct) language&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Does it assign appropriate structures to the strings that it generates?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Parsing  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;take a string and a grammar&lt;/li&gt;
&lt;li&gt;assigning trees that covers all and only words in input strings&lt;/li&gt;
&lt;li&gt;return parse tree for that string&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;English Grammar Fragment  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sentences&lt;/li&gt;
&lt;li&gt;Noun Phrases&lt;ul&gt;
&lt;li&gt;Ex. NP → det Nominal&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;head: central criticial noun in NP&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;important in statistical parsing&lt;/li&gt;
&lt;li&gt;after det(冠詞), before pp(介系詞片語) &lt;img data-src=&#34;/img/NLP/np-parse.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Agreement&lt;ul&gt;
&lt;li&gt;a part of overgenerate&lt;/li&gt;
&lt;li&gt;This flight(○)&lt;/li&gt;
&lt;li&gt;This flights(×)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Verb Phrases&lt;ul&gt;
&lt;li&gt;head verb with arguments&lt;/li&gt;
&lt;li&gt;Subcategorization(分類)&lt;ul&gt;
&lt;li&gt;categorize according to VP rules&lt;/li&gt;
&lt;li&gt;a part of overgenerate&lt;/li&gt;
&lt;li&gt;Prefer&lt;ul&gt;
&lt;li&gt;I prefer [to leave earlier]TO-VP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Told&lt;ul&gt;
&lt;li&gt;I was told [United has a flight]S&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overgenerate Solution&lt;br&gt;    - transform into multiple rules&lt;br&gt;        - NP → Single_Det Single_Nominal&lt;br&gt;        - NP → 複數_Det 複數_Nominal&lt;br&gt;        - Will generate a lot of rules!&lt;br&gt;    - out of CFG framework&lt;br&gt;        - Chomsky hierarchy of languages &lt;img data-src=&#34;/img/NLP/modelclass.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3poLndpa2lwZWRpYS5vcmcvd2lraS8lRTklOTklODQlRTYlQTAlODclRTglQUYlQUQlRTglQTglODA=&#34;&gt;Indexed Grammar&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Indexed grammars and languages problem &lt;img data-src=&#34;/img/NLP/index-example.png&#34; alt=&#34;&#34;&gt; &lt;/li&gt;
&lt;li&gt;recognized by nested stack automata &lt;img data-src=&#34;/img/NLP/index-grammar.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Treebanks-and-headfinding&#34;&gt;&lt;a href=&#34;#Treebanks-and-headfinding&#34; class=&#34;headerlink&#34; title=&#34;Treebanks and headfinding&#34;&gt;&lt;/a&gt;Treebanks and headfinding&lt;/h3&gt;&lt;p&gt;critical to the development of statistical parsers&lt;/p&gt;
&lt;p&gt;Treebanks  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;corpora with parse trees&lt;ul&gt;
&lt;li&gt;created by automatic parser and human annotators&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ex. Penn Treebank&lt;/li&gt;
&lt;li&gt;Grammar&lt;ul&gt;
&lt;li&gt;Treebanks implicitly define a grammar&lt;ul&gt;
&lt;li&gt;Simply make all subtrees fit the rules&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;parse tree tend to be very flat to avoid recursion&lt;ul&gt;
&lt;li&gt;Penn Treebank has ~4500 different rules for VPs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Head Finding  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use tree traversal rules specific to each non-terminal in the grammar&lt;/li&gt;
&lt;li&gt;先向右再向左 &lt;img data-src=&#34;/img/NLP/head-np.png&#34; alt=&#34;&#34;&gt;&lt;!--重要--&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Dependency-Grammars&#34;&gt;&lt;a href=&#34;#Dependency-Grammars&#34; class=&#34;headerlink&#34; title=&#34;Dependency Grammars&#34;&gt;&lt;/a&gt;Dependency Grammars&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;every possible parse is a tree &lt;img data-src=&#34;/img/NLP/dependency-parse.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;every node is a word &lt;/li&gt;
&lt;li&gt;every link is dependency relations between words &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advantage&lt;ul&gt;
&lt;li&gt;Deals well with long-distance word order languages &lt;ul&gt;
&lt;li&gt;structure is flexible&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parsing is much faster than CFG&lt;/li&gt;
&lt;li&gt;Tree can be used by later applications&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Approaches&lt;!--重要--&gt;&lt;ul&gt;
&lt;li&gt;Optimization-based approaches &lt;ul&gt;
&lt;li&gt;search for the tree that matches some criteria the best&lt;/li&gt;
&lt;li&gt;spanning tree algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Shift-reduce approaches&lt;ul&gt;
&lt;li&gt;greedily take actions based on the current word and state&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Summary  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Constituency(顧客, words that behave as a single unit) is a key phenomena easily captured with CFG rules&lt;ul&gt;
&lt;li&gt;But agreement and subcategorization make problems&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap09-Syntactic-Parsing&#34;&gt;&lt;a href=&#34;#Chap09-Syntactic-Parsing&#34; class=&#34;headerlink&#34; title=&#34;Chap09 Syntactic Parsing&#34;&gt;&lt;/a&gt;Chap09 Syntactic Parsing&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Top-Down Search &lt;img data-src=&#34;/img/NLP/top-down.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;Search trees among possible answers  &lt;!--- But suggests trees that are not consistent with words--&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bottom-Up Parsing&lt;ul&gt;
&lt;li&gt;Only forms trees that can fit the words &lt;!-- global tree may not form answer(S) --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mixed parsing strategy&lt;ul&gt;
&lt;li&gt;looks like Binomial Search&lt;/li&gt;
&lt;li&gt;The number of rules tried at each deicision of the analysis (branching factor)&lt;ul&gt;
&lt;li&gt;top-down parsing: categories of LHS(Left Hand Side) word&lt;/li&gt;
&lt;li&gt;bottom-up parsing: categories of left most RHS(Right Hand Side) word&lt;ul&gt;
&lt;li&gt;倒推：從最左邊可以倒推的字開始&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;backtracking methods are doomed because of two inter-related problems  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1)Structural and lexical ambiguity&lt;ul&gt;
&lt;li&gt;PP(介系詞片語) attachment&lt;ul&gt;
&lt;li&gt;PP can attach to [sentences, verb phrases, noun phrases, and adjectival phrases]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;coordination&lt;ul&gt;
&lt;li&gt;P and Q or R &lt;ul&gt;
&lt;li&gt;P and (Q or R)&lt;/li&gt;
&lt;li&gt;(P and Q) or R&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;noun-noun compounding&lt;ul&gt;
&lt;li&gt;town widget hammer&lt;ul&gt;
&lt;li&gt;((town widget) hammer)&lt;/li&gt;
&lt;li&gt;(town (widget hammer))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solution&lt;ul&gt;
&lt;li&gt;how to determine the intended structure?&lt;/li&gt;
&lt;li&gt;how to store the partial structures?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(2)Shared subproblems&lt;ul&gt;
&lt;li&gt;naïve backtracking will lead to duplicated work(不一定會對，所以會一直backtrack…)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dynamic Programming  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avoid doing repeated work&lt;/li&gt;
&lt;li&gt;Solve in polynomial time&lt;/li&gt;
&lt;li&gt;approaches&lt;ul&gt;
&lt;li&gt;CKY&lt;/li&gt;
&lt;li&gt;Earley&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;CKY-bottom-up&#34;&gt;&lt;a href=&#34;#CKY-bottom-up&#34; class=&#34;headerlink&#34; title=&#34;CKY(bottom-up)&#34;&gt;&lt;/a&gt;CKY(bottom-up)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;transform rules into Chomsky-Normal Form &lt;img data-src=&#34;/img/NLP/cnf-transform.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;build a table &lt;ul&gt;
&lt;li&gt;A spanning from i to j in the input is in [i,j]&lt;/li&gt;
&lt;li&gt;A → BC == [i,j] → [i,k] [k,j]&lt;/li&gt;
&lt;li&gt;entire string = [0, n] &lt;ul&gt;
&lt;li&gt;expected to be S&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iterate all possible k &lt;img data-src=&#34;/img/NLP/CKY-table2.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;[i,j] = [i,i+1]x[i+1, j], [i,i+2]x[i+2,j] ……&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;fill the table a column at a time, from left to right, bottom to top &lt;img data-src=&#34;/img/NLP/CKY-table3.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;Ex. [1,3] = [1,2]Det + [2,3] Nomimal, Noun = NP&lt;/li&gt;
&lt;li&gt;Ex. &lt;img data-src=&#34;/img/NLP/CKY-ex.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Algorithm &lt;img data-src=&#34;/img/NLP/CKY-algo.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Performance&lt;ul&gt;
&lt;li&gt;a lot of elements unrelated to the answer&lt;/li&gt;
&lt;li&gt;can use online search to fill table (from left to right)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Earley&#34;&gt;&lt;a href=&#34;#Earley&#34; class=&#34;headerlink&#34; title=&#34;Earley&#34;&gt;&lt;/a&gt;Earley&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;parser that exploits chart as data structure&lt;/li&gt;
&lt;li&gt;node = vertex&lt;/li&gt;
&lt;li&gt;arc = edge&lt;ul&gt;
&lt;li&gt;active edge: (a) and (b)&lt;/li&gt;
&lt;li&gt;inactive edge: (c)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;decorated grammar  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(a) “•” in the first &lt;img data-src=&#34;/img/NLP/dot-a.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;• NP VP&lt;/li&gt;
&lt;li&gt;A hypothesis has been made, but has not been verified yet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(b) “•” in the middle &lt;img data-src=&#34;/img/NLP/dot-b.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;NP • VP&lt;/li&gt;
&lt;li&gt;A hypothesis has been partially confirmed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(c) “•” in the last&lt;ul&gt;
&lt;li&gt;NP VP •&lt;/li&gt;
&lt;li&gt;A hypothesis has been wholly confirmed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;representation of edge &lt;img data-src=&#34;/img/NLP/chart-struct.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;initialization &lt;img data-src=&#34;/img/NLP/chart-initialize.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;for each rule A → W, if A is a category that can span a chart (typically S), add &amp;lt;0, 0, A → •W&amp;gt; &lt;img data-src=&#34;/img/NLP/chartchart-init.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;A implies •W from position 0 to 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Housekeeping&lt;ul&gt;
&lt;li&gt;prevent duplicate rules&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fundamental rule  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the chart contains &amp;lt;i, j, A → W1 •B W2&amp;gt; and &amp;lt;j, k, B → W3 •&amp;gt;, then add edge &amp;lt;i, k, A → W1 B •W2&amp;gt; to the chart &lt;img data-src=&#34;/img/NLP/chart-fund.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Notes&lt;ol&gt;
&lt;li&gt;New edge may be either active or inactive&lt;/li&gt;
&lt;li&gt;does not remove the active edge that has succeeded&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bottom-up rule  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if adding edge &amp;lt;i, j, C → W1 •&amp;gt; to the chart, then for every rule that has the form B → C W2, add &amp;lt;i, i, B → • C W2&amp;gt; &lt;img data-src=&#34;/img/NLP/chart-bottom.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Top-down rule   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If adding edge &amp;lt;i, j, C → W1 •B W2&amp;gt; to the chart, then for each rule B → W, add &amp;lt; j, j, B →•W&amp;gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Full-Syntactic-Parsing&#34;&gt;&lt;a href=&#34;#Full-Syntactic-Parsing&#34; class=&#34;headerlink&#34; title=&#34;Full Syntactic Parsing&#34;&gt;&lt;/a&gt;Full Syntactic Parsing&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;necessary for deep semantic analysis of texts&lt;/li&gt;
&lt;li&gt;not practical for many applications (given typical resources)&lt;ul&gt;
&lt;li&gt;O(n^3) for straight parsing&lt;/li&gt;
&lt;li&gt;O(n^5) for probabilistic versions&lt;/li&gt;
&lt;li&gt;Too slow for real time applications (search engines)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two Alternatives&lt;ul&gt;
&lt;li&gt;Dependency parsing&lt;ul&gt;
&lt;li&gt;Change the underlying grammar formalism&lt;/li&gt;
&lt;li&gt;can get a lot done with just binary relations among the words&lt;/li&gt;
&lt;li&gt;詳見Chap08 dependency grammar&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Partial parsing&lt;ul&gt;
&lt;li&gt;Approximate phrase-structure parsing with finite-state and statistical approaches&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Both of these approaches give up something (syntactic, structure) in return for more robust and efficient parsing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Partial parsing&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For many applications you don’t really need full parse&lt;/li&gt;
&lt;li&gt;For example, if you’re interested in locating all the people, places and organizations  &lt;ul&gt;
&lt;li&gt;base-NP chunking &lt;ul&gt;
&lt;li&gt;[NP The morning flight] from [NP Denvar] has arrived &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two approaches&lt;ul&gt;
&lt;li&gt;Rule-based (hierarchical) transduction(轉導) &lt;!--???--&gt;&lt;ul&gt;
&lt;li&gt;Restrict recursive rules (make the rules flat)&lt;ul&gt;
&lt;li&gt;like NP → NP VP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Group the rules so that RHS of the rules can refer to non-terminals introduced in earlier transducers, but not later ones&lt;/li&gt;
&lt;li&gt;Combine the rules in a group in the same way we did with the rules for spelling changes&lt;/li&gt;
&lt;li&gt;Combine the groups into a cascade&lt;ul&gt;
&lt;li&gt;can be used to find the sequence of flat chunks you’re interested in&lt;/li&gt;
&lt;li&gt;or approximate hierarchical trees you get from full parsing with a CFG&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Typical Architecture ![](/img/NLP/Cascaded Transducers.png)&lt;ul&gt;
&lt;li&gt;Phase 1: Part of speech tags&lt;/li&gt;
&lt;li&gt;Phase 2: Base syntactic phrases&lt;/li&gt;
&lt;li&gt;Phase 3: Larger verb and noun groups&lt;/li&gt;
&lt;li&gt;Phase 4: Sentential level rules&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Statistical sequence labeling&lt;ul&gt;
&lt;li&gt;HMMs&lt;/li&gt;
&lt;li&gt;MEMMs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap10-Statistical-Parsing&#34;&gt;&lt;a href=&#34;#Chap10-Statistical-Parsing&#34; class=&#34;headerlink&#34; title=&#34;Chap10 Statistical Parsing&#34;&gt;&lt;/a&gt;Chap10 Statistical Parsing&lt;/h2&gt;&lt;p&gt;Motivation  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N-gram models and HMM Tagging only allowed us to process sentences linearly&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probabilistic Context Free Grammars&lt;/strong&gt;(PCFG)&lt;ul&gt;
&lt;li&gt;alias: Stochastic context-free grammar(SCFG)&lt;/li&gt;
&lt;li&gt;simplest and most natural probabilistic model for tree structures&lt;/li&gt;
&lt;li&gt;closely related to those for HMMs&lt;/li&gt;
&lt;li&gt;為每一個CFG的規則標示其發生的可能性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Idea  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reduce “right” parse to “most probable parse”&lt;ul&gt;
&lt;li&gt;Argmax P(Parse|Sentence)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A PCFG consists of  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;set of terminals, {wk}&lt;/li&gt;
&lt;li&gt;set of nonterminals, {Ni}&lt;/li&gt;
&lt;li&gt;start symbol N1&lt;/li&gt;
&lt;li&gt;set of rules&lt;ul&gt;
&lt;li&gt;{Ni –&amp;gt; ξj}(ξj is a sequence of terminals and nonterminals)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;probabilities of rules&lt;ul&gt;
&lt;li&gt;total probability of imply Ni to other sequence ξj is 1 &lt;/li&gt;
&lt;li&gt;∀i Σj P(Ni → ξj) = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Probability of sentence according to grammar G &lt;ul&gt;
&lt;li&gt;P($w_{1m}$) = sum of P($w_{1m}$, t) for every possible tree t&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Nj dominates the words wa … wb&lt;ul&gt;
&lt;li&gt;Nj → wa … wb&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assumptions of the Model  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Place Invariance&lt;ul&gt;
&lt;li&gt;probability of a subtree does not depend on its position in the string&lt;/li&gt;
&lt;li&gt;similar to time invariance in HMMs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ancestor Free&lt;ul&gt;
&lt;li&gt;probability of a subtree does not depend on nodes in the derivation outside the subtree(subtree的機率只和subtree內的node有關)&lt;/li&gt;
&lt;li&gt;can simplify probability calculation &lt;img data-src=&#34;/img/NLP/after-assump.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Questions of PCFGs(similar to three questions of HMM)    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assign probabilities to parse trees&lt;ul&gt;
&lt;li&gt;What is the probability of a sentence $w_{1m}$ according to a grammar G&lt;ul&gt;
&lt;li&gt;P(w1m|G)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parsing with probabilities(Decoding)&lt;ul&gt;
&lt;li&gt;What is the most likely parse for a sentence&lt;ul&gt;
&lt;li&gt;argmax_t P(t|w1m,G) &lt;/li&gt;
&lt;li&gt;How to efficiently find the best (or N best) trees &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Training the model (Learning) &lt;ul&gt;
&lt;li&gt;How to set rule probabilities(parameter of grammar model) that maximize the probability of a sentence&lt;ul&gt;
&lt;li&gt;argmax_G P(w1m|G)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simple Probability Model  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;probability of a tree is the product of the probabilities of rules in derivation&lt;/li&gt;
&lt;li&gt;Rule Probabilities&lt;ul&gt;
&lt;li&gt;S → NP &lt;/li&gt;
&lt;li&gt;P(NP | S)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Training the Model&lt;ul&gt;
&lt;li&gt;estimate probability from data&lt;/li&gt;
&lt;li&gt;P(α → β | α) = Count(α→β) / Count(α) = Count(α→β) / Σγ Count(α→γ)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parsing (Decoding)&lt;ul&gt;
&lt;li&gt;trees with highest probability in the model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: Book the dinner flight&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/pm-ex.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/pm-ex2.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;too slow!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dynamic Programming again  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use CKY and Earley to &lt;strong&gt;parse&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Viterbi and HMMs to &lt;strong&gt;get the best parse&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Parameters of a PCFG in Chomsky Normal Form&lt;ul&gt;
&lt;li&gt;P(Nj→NrNs | G) , $n^3$ matrix of parameters&lt;/li&gt;
&lt;li&gt;P(Nj→wk | G), $nV$ parameters&lt;/li&gt;
&lt;li&gt;n is the number of nonterminals &lt;/li&gt;
&lt;li&gt;V is the number of terminals&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Σr,s P(Nj→NrNs) + ΣkP(Nj→wk) = 1&lt;ul&gt;
&lt;li&gt;所有由Nj導出的rule，機率總和必為1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Probabilistic Regular Grammars (PRG)    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;start state N1 &lt;/li&gt;
&lt;li&gt;rules&lt;ul&gt;
&lt;li&gt;Ni → wjNk&lt;/li&gt;
&lt;li&gt;Ni → wj&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PRG is a HMM with [start state] and [finish(sink) state] &lt;img data-src=&#34;/img/NLP/prg-sink.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inside and Outside probability &lt;img data-src=&#34;/img/NLP/prg-graph.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/prg-bf.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/prg-bf2.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Forward(Outside) probability&lt;ul&gt;
&lt;li&gt;$ α&lt;em&gt;i(t) = P(w&lt;/em&gt;{1(t-1)}, X_t = i)$&lt;/li&gt;
&lt;li&gt;everything above a certain node(include the node)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Backward(Inside) probability&lt;ul&gt;
&lt;li&gt;$ β&lt;em&gt;i(t, T) = P(w&lt;/em&gt;{tT} | X_t = i)$&lt;/li&gt;
&lt;li&gt;everything below a certain node&lt;/li&gt;
&lt;li&gt;total probability of generating words $w_t \cdots w_T$, given the root nonterminal $N^i$ and a grammar G&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inside Algorithm (bottom-up)      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P(w_{1m} | G) = P(N_1 → w_{1m} | G) = P(w_{1m} | N^1_{1m}, G) = B_1(1,m)$&lt;ul&gt;
&lt;li&gt;$B_1(1,m)$ is Inside probability&lt;ul&gt;
&lt;li&gt;P(w1~wm are below N1(start symbol))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;base rule&lt;ul&gt;
&lt;li&gt;$ B_j(k, k) = P(w_k | N^j_{kk}, G) = P(N^j → w_k | G)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$ B_j(p, q) = P(w_{pq} | N^j_{pq}, G) = $ &lt;img data-src=&#34;/img/NLP/inside-induction.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;try every possible rules to split Nj, product of *&lt;em&gt;rule probabilty and segments’ inside probabilities *&lt;/em&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;use grid to solve again&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/inside-grid.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;X軸代表起始座標，Y軸代表長度&lt;ul&gt;
&lt;li&gt;(2,3) → flies like ants&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Outside Algorithm (top-down)     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ P(w_{1m} | G) = Σ_j α_j(k, k)P(N^j → w_k$ &lt;img data-src=&#34;/img/NLP/outside-graph.png&#34; alt=&#34;&#34;&gt; &lt;!--為何是sum...--&gt;&lt;ul&gt;
&lt;li&gt;outside probability of wk x (inside) probability of wk  of every Nj&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;basecase &lt;ul&gt;
&lt;li&gt;$ α_1(1, m) = 1, α_j(1,m) = $&lt;/li&gt;
&lt;li&gt;P(N1) = 1, P(Nj outside w1 to wm) = 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;自己的outside probability 等於 &lt;ul&gt;
&lt;li&gt;爸爸的outside probability 乘以 爸爸的inside probability 除以 自己的inside probability&lt;ul&gt;
&lt;li&gt;inside x outside 是固定值？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;爸爸的inside probabiliity 除以 自己的inside probability 就是其兄弟的inside probability&lt;/li&gt;
&lt;li&gt;使用此公式計算 &lt;img data-src=&#34;/img/NLP/inout.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$ α&lt;em&gt;j(p, q)β_j(p, q) = P(w&lt;/em&gt;{1m}, N^j_{pq} | G) $&lt;ul&gt;
&lt;li&gt;某個點的inside 乘 outside = 在某grammar中，出現此句子，且包含此點的機率 &lt;/li&gt;
&lt;li&gt;所有點的總和：在某grammar下，某parse tree(包含所有node)的機率 &lt;img data-src=&#34;/img/NLP/parse-probability.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Outside example: 這些數字理論上算起來會一樣… &lt;img data-src=&#34;/img/NLP/outside-forward.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finding the Most Likely Parse for a Sentence     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;δi(p,q)= the highest inside probability parse of a subtree $N_{pq}^i$&lt;/li&gt;
&lt;li&gt;Initialization &lt;ul&gt;
&lt;li&gt;δi(p,p)= P(Ni → wp)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Induction and Store backtrace&lt;ul&gt;
&lt;li&gt;δi(p,q)= $argmax(j,k,r)P(Ni→NjNk)δj(p,r)δk(r+1,q)$&lt;/li&gt;
&lt;li&gt;找所有可能的切法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Termination&lt;ul&gt;
&lt;li&gt;answer = δ1(1,m)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Training a PCFG&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;find the optimal probabilities among grammar rules&lt;/li&gt;
&lt;li&gt;use EM Training Algorithm to seek the grammar that maximizes the likelihood of the training data&lt;ul&gt;
&lt;li&gt;Inside-Outside Algorithm &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/inoutagain.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;將產生句子的機率視為π，為Nj產生pq的機率 &lt;img data-src=&#34;/img/NLP/pi.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Nj被使用的機率 &lt;img data-src=&#34;/img/NLP/pi2.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Nj被使用，且Nj→NrNs的機率 &lt;img data-src=&#34;/img/NLP/pi3.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Nj→NrNs這條rule被使用的機率=前兩式相除 &lt;img data-src=&#34;/img/NLP/pi4.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Nj→wk &lt;img data-src=&#34;/img/NLP/pi5.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;僅分子差異 &lt;img data-src=&#34;/img/NLP/pi6.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Problems with the Inside-Outside Algorithm    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extremely Slow&lt;ul&gt;
&lt;li&gt;For each sentence, each iteration of training is $O(m^3n^3)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Local Maxima&lt;/li&gt;
&lt;li&gt;Satisfactory learning requires many more nonterminals than are theoretically needed to describe the language&lt;/li&gt;
&lt;li&gt;There is no guarantee that the learned nonterminals will be linguistically motivated&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap11-Dependency-Parsing&#34;&gt;&lt;a href=&#34;#Chap11-Dependency-Parsing&#34; class=&#34;headerlink&#34; title=&#34;Chap11 Dependency Parsing&#34;&gt;&lt;/a&gt;Chap11 Dependency Parsing&lt;/h2&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3N0cC5saW5nZmlsLnV1LnNlL35uaXZyZS9kb2NzL0FDTHNsaWRlcy5wZGY=&#34;&gt;COLING-ACL 2006, Dependency Parsing, by Joachim Nivre and Sandra Kuebler&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL25hYWNsaGx0MjAxMC5pc2kuZWR1L3R1dG9yaWFscy90Ny1zbGlkZXMucGRm&#34;&gt;NAACL 2010, Recent Advances in Dependency Parsing, by Qin Iris. Wang and YueZhang&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3NpdGUvemhlbmdodWFubHAvcHVibGljYXRpb25zL0lKQ05MUDIwMTMtdHV0b3JpYWwtRFAucGRmP2F0dHJlZGlyZWN0cz0wJmQ9MQ==&#34;&gt;IJCNLP 2013, Dependency Parsing: Past, Present, and Future, by Zhenghua Li, Wenliang Chen, Min Zhang&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Dependency Structure vs. Constituency Structure &lt;img data-src=&#34;/img/NLP/parse.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Parsing is one way to deal with the ambiguity problem in&lt;br&gt;natural language&lt;br&gt;dependency syntax is syntactic relations (dependencies) &lt;/p&gt;
&lt;p&gt;Constraint: between word pairs  &lt;img data-src=&#34;/img/NLP/depend.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;    Projective: No crossing links(a word and its dependents form a contiguous substring of the sentence)&lt;br&gt;    An arc (wi , r ,wj ) ∈ A is projective iff wi →∗ wk for all:&lt;br&gt;    i &amp;lt; k &amp;lt; j when i &amp;lt; j&lt;br&gt;    j &amp;lt; k &amp;lt; i when j &amp;lt; i&lt;br&gt;    射出去的那一方也可以射到兩個字中間的任何一字&lt;br&gt;&lt;img data-src=&#34;/img/NLP/depend-ex.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Non-projective Dependency Trees  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Long-distance dependencies  &lt;/li&gt;
&lt;li&gt;With crossing links&lt;/li&gt;
&lt;li&gt;Not so frequent in English&lt;ul&gt;
&lt;li&gt;All the dependency trees from Penn Treebank are projective&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Common in other languages with free word order&lt;ul&gt;
&lt;li&gt;Prague(23%) and Czech, German and Dutch&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data Driven Dependency Parsing  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data-driven parsing&lt;ul&gt;
&lt;li&gt;No grammar / rules needed&lt;/li&gt;
&lt;li&gt;Parsing decisions are made based on learned models&lt;/li&gt;
&lt;li&gt;deal with ambiguities well&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/data-driven.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Three approaches&lt;ul&gt;
&lt;li&gt;Graph-based models&lt;/li&gt;
&lt;li&gt;Transition-based models(good in practice)&lt;ul&gt;
&lt;li&gt;Define a transition system for &lt;strong&gt;mapping a sentence to its dependency tree&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Predefine some transition actions&lt;/li&gt;
&lt;li&gt;Learning: predicting the next state transition, by transition history&lt;/li&gt;
&lt;li&gt;Parsing: construct the optimal transition sequence&lt;/li&gt;
&lt;li&gt;Greedy search / beam search&lt;/li&gt;
&lt;li&gt;Features are defined over a richer parsing history&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hybrid models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Comparison   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Graph-based models&lt;ul&gt;
&lt;li&gt;Find the optimal tree from all the possible ones&lt;/li&gt;
&lt;li&gt;Global, exhaustive&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Transition-based models&lt;ul&gt;
&lt;li&gt;Predefine some actions (shift and reduce)&lt;/li&gt;
&lt;li&gt;use stack to hold partially built parses&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Find the optimal action sequence&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Local, Greedy or beam search&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The two models produce different types of errors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hybrid Models  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Three integration methods&lt;ul&gt;
&lt;li&gt;Ensemble approach: parsing time integration (Sagae &amp;amp; Lavie 2006)&lt;/li&gt;
&lt;li&gt;Feature-based integration (Nivre &amp;amp; Mcdonald 2008)&lt;/li&gt;
&lt;li&gt;Single model combination (Zhang &amp;amp; Clark 2008)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gain benefits from both models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/NLP/parse-algo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;Graph-based-dependency-parsing-models&#34;&gt;&lt;a href=&#34;#Graph-based-dependency-parsing-models&#34; class=&#34;headerlink&#34; title=&#34;Graph-based dependency parsing models&#34;&gt;&lt;/a&gt;Graph-based dependency parsing models&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Search for a tree with the highest score&lt;/li&gt;
&lt;li&gt;Define search space&lt;ul&gt;
&lt;li&gt;Exhaustive search&lt;/li&gt;
&lt;li&gt;Features are defined over a limited parsing history&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The score is linear combination of features &lt;ul&gt;
&lt;li&gt;What features we can use? (later)&lt;/li&gt;
&lt;li&gt;What learning approaches can lead us to find the best tree with the highest score (later)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Applicable to both probabilistic and nonprobabilistic models &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Features  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dynamic features&lt;ul&gt;
&lt;li&gt;Take into account the link labels of the surrounding word-pairs when predicting the label of current pair&lt;/li&gt;
&lt;li&gt;Commonly used in sequential labeling&lt;/li&gt;
&lt;li&gt;A word’s children are generated first(先生child, 再找parent), before it modifies another word&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learning Approaches   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local learning approaches&lt;ul&gt;
&lt;li&gt;Learn a local link classifier given of features defined on training data&lt;/li&gt;
&lt;li&gt;example &lt;img data-src=&#34;/img/NLP/local-feature-example.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;3-class classification: No link, left link or right link&lt;/li&gt;
&lt;li&gt;Efficient O(n) local training&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;local training and parsing &lt;img data-src=&#34;/img/NLP/local-train-with-parse.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Learn the weights of features&lt;ul&gt;
&lt;li&gt;Maximum entropy models (Ratnaparkhi 99, Charniak 00)&lt;/li&gt;
&lt;li&gt;Support vector machines (Yamada &amp;amp; Matsumoto 03)&lt;/li&gt;
&lt;li&gt;Use a richer feature set!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Global learning approaches&lt;/li&gt;
&lt;li&gt;Unsupervised/Semi-supervised learning approaches&lt;ul&gt;
&lt;li&gt;Use both annotated training data and un-annotated raw text&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Transition-based-model&#34;&gt;&lt;a href=&#34;#Transition-based-model&#34; class=&#34;headerlink&#34; title=&#34;Transition-based model&#34;&gt;&lt;/a&gt;Transition-based model&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Stack holds partially built parses&lt;/li&gt;
&lt;li&gt;Queue holds unprocessed words&lt;/li&gt;
&lt;li&gt;Actions&lt;ul&gt;
&lt;li&gt;use input words to build output parse&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;parsing-processes&#34;&gt;&lt;a href=&#34;#parsing-processes&#34; class=&#34;headerlink&#34; title=&#34;parsing processes&#34;&gt;&lt;/a&gt;parsing processes&lt;/h4&gt;&lt;p&gt;Arc-eager parser  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 tranition actions&lt;ul&gt;
&lt;li&gt;SHIFT: push stack&lt;/li&gt;
&lt;li&gt;REDUCE: pop stack&lt;/li&gt;
&lt;li&gt;ARC-LEFT: pop stack and add link&lt;/li&gt;
&lt;li&gt;ARC-RIGHT: push stack and add link&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/arc-eager-example.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Time complexity: linear&lt;ul&gt;
&lt;li&gt;every word will be pushed once and popped once(except root)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;parse&lt;ul&gt;
&lt;li&gt;by actions: arcleft → arclect subject, noun, …&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Arc-standard parser  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 actions&lt;ul&gt;
&lt;li&gt;SHIFT: push&lt;/li&gt;
&lt;li&gt;LEFT: pop leftmost stack element and add&lt;/li&gt;
&lt;li&gt;RIGHT: pop rightmost stack element and add&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Also linear time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Non-projectivity  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;neither of parser can solve it&lt;ul&gt;
&lt;li&gt;online reorder&lt;ul&gt;
&lt;li&gt;add extra action: swap&lt;/li&gt;
&lt;li&gt;not linear: $N^2$, but expect to belinear&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Decoding-algorithms&#34;&gt;&lt;a href=&#34;#Decoding-algorithms&#34; class=&#34;headerlink&#34; title=&#34;Decoding algorithms&#34;&gt;&lt;/a&gt;Decoding algorithms&lt;/h4&gt;&lt;p&gt;search action sequence to build the parse&lt;br&gt;scoring action given context&lt;br&gt;Candidate item &amp;lt;S, G, Q&amp;gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;greedy local search&lt;ul&gt;
&lt;li&gt;initialize: Q = input&lt;/li&gt;
&lt;li&gt;goal: S=[root], G=tree, Q=[]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;problem: one error leads to incorrect parse&lt;ul&gt;
&lt;li&gt;Beam search: keep N highest partial states&lt;ul&gt;
&lt;li&gt;use total score of all actions to rank a parse&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Score-Models&#34;&gt;&lt;a href=&#34;#Score-Models&#34; class=&#34;headerlink&#34; title=&#34;Score Models&#34;&gt;&lt;/a&gt;Score Models&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;linear model&lt;/li&gt;
&lt;li&gt;SVM&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap12-Semantic-Representation-and-Computational-Semantics&#34;&gt;&lt;a href=&#34;#Chap12-Semantic-Representation-and-Computational-Semantics&#34; class=&#34;headerlink&#34; title=&#34;Chap12 Semantic Representation and Computational Semantics&#34;&gt;&lt;/a&gt;Chap12 Semantic Representation and Computational Semantics&lt;/h2&gt;&lt;p&gt;Semantic aren’t primarily descriptions of inputs&lt;/p&gt;
&lt;p&gt;Semantic Processing  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reason about the truth&lt;/li&gt;
&lt;li&gt;answer questions based on content&lt;ul&gt;
&lt;li&gt;Touchstone application is often question answering&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;inference to determine the truth that isn’t actually know&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Method    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;principled, theoretically motivated approach&lt;ul&gt;
&lt;li&gt;Computational/Compositional Semantics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;limited, practical approaches that have some hope of being useful&lt;ul&gt;
&lt;li&gt;Information extraction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Information-Extraction&#34;&gt;&lt;a href=&#34;#Information-Extraction&#34; class=&#34;headerlink&#34; title=&#34;Information Extraction&#34;&gt;&lt;/a&gt;Information Extraction&lt;/h3&gt;&lt;p&gt;Information Extraction = segmentation + classification +  association + clustering &lt;img data-src=&#34;/img/NLP/IE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;superficial analysis &lt;ul&gt;
&lt;li&gt;pulls out only the entities, relations and roles related to consuming application&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Similar to chunking&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Compositional-Semantics&#34;&gt;&lt;a href=&#34;#Compositional-Semantics&#34; class=&#34;headerlink&#34; title=&#34;Compositional Semantics&#34;&gt;&lt;/a&gt;Compositional Semantics&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Use First-Order Logic(FOL) representation that accounts for all the entities, roles and relations present in a sentence&lt;/li&gt;
&lt;li&gt;Similar to our approach to full parsing&lt;/li&gt;
&lt;li&gt;Compositional: The meaning of a whole is derived from the meanings of the parts(syntatic) &lt;img data-src=&#34;/img/NLP/syntax-semantic.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Syntax-Driven Semantic Analysis&lt;ul&gt;
&lt;li&gt;The composition of meaning representations is guided by the &lt;strong&gt;syntactic&lt;/strong&gt; components and relations provided by the  grammars&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;FOL&#34;&gt;&lt;a href=&#34;#FOL&#34; class=&#34;headerlink&#34; title=&#34;FOL&#34;&gt;&lt;/a&gt;FOL&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;allow to answer yes/no questions&lt;/li&gt;
&lt;li&gt;allow variable&lt;/li&gt;
&lt;li&gt;allow inference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Events, actions and relationships can be captured with representations that consist of predicates with arguments  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predicates&lt;ul&gt;
&lt;li&gt;Primarily Verbs, VPs, Sentences&lt;/li&gt;
&lt;li&gt;Verbs introduce/refer to events and processes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Arguments &lt;ul&gt;
&lt;li&gt;Primarily Nouns, Nominals, NPs, PPs&lt;/li&gt;
&lt;li&gt;Nouns introduce the things that play roles in those events&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: Mary gave a list to John &lt;ul&gt;
&lt;li&gt;Giving(Mary, John, List)&lt;/li&gt;
&lt;li&gt;Gave: Predicate&lt;/li&gt;
&lt;li&gt;Mary, John, List: Argument&lt;/li&gt;
&lt;li&gt;better representation &lt;img data-src=&#34;/img/NLP/FOL-better.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lambda Forms&lt;ul&gt;
&lt;li&gt;Allow variables to be bound&lt;/li&gt;
&lt;li&gt;λxP(x)(Sally) = P(Sally)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ambiguation  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mismatch between syntax and semantics&lt;ul&gt;
&lt;li&gt;displaced arguments&lt;/li&gt;
&lt;li&gt;complex NPs with quantifiers&lt;ul&gt;
&lt;li&gt;A menu&lt;/li&gt;
&lt;li&gt;Every restaurant &lt;img data-src=&#34;/img/NLP/complicate-NP.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Not every waiter&lt;/li&gt;
&lt;li&gt;Most restaurants&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/complicate-NP-induction.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;still preserving strict compositionality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two (syntax) rules to revise&lt;ul&gt;
&lt;li&gt;The S rule&lt;ul&gt;
&lt;li&gt;S → NP VP, NP.Sem(VP.Sem)&lt;/li&gt;
&lt;li&gt;NP and VP swapped, because S is NP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simple NP’s like proper nouns&lt;ul&gt;
&lt;li&gt;λx.Franco(x)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Store and Retrieve  &lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/ambiguity-of-same-POS.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Retrieving the quantifiers one at a time and placing them in front&lt;/li&gt;
&lt;li&gt;The order determines the meaning &lt;img data-src=&#34;/img/NLP/store.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;retrieve &lt;img data-src=&#34;/img/NLP/retrieve.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Set-Based-Models&#34;&gt;&lt;a href=&#34;#Set-Based-Models&#34; class=&#34;headerlink&#34; title=&#34;Set-Based Models&#34;&gt;&lt;/a&gt;Set-Based Models&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;domain: the set of elements&lt;/li&gt;
&lt;li&gt;entity: elements of domain&lt;/li&gt;
&lt;li&gt;Properties of the elements: sets of elements from the domain&lt;/li&gt;
&lt;li&gt;Relations: sets of tuples of elements from the domain&lt;/li&gt;
&lt;li&gt;FOL&lt;ul&gt;
&lt;li&gt;FOL Terms → elements of the domain&lt;ul&gt;
&lt;li&gt;Med -&amp;gt; “f”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FOL atomic formula → sets, or sets of tuples&lt;ul&gt;
&lt;li&gt;Noisy(Med) is true if “f is in the set of elements that corresponds to the noisy relation&lt;/li&gt;
&lt;li&gt;Near(Med, Rio) is true if “the tuple &amp;lt;f,g&amp;gt; is in the set of tuples that corresponds to “Near” in the interpretation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: Everyone likes a noisy restaurant &lt;img data-src=&#34;/img/NLP/set-based-model.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;There is a particular restaurant out there; it’s a noisy place; everybody likes it 有一家吵雜的餐廳大家都喜歡&lt;/li&gt;
&lt;li&gt;Everybody has at least one noisy restaurant that they like 大家都喜歡一家吵雜的餐廳&lt;/li&gt;
&lt;li&gt;Everybody likes noisy restaurants (i.e., there is no noisy restaurant out there that is disliked by anyone) 大家都喜歡吵雜的餐廳&lt;/li&gt;
&lt;li&gt;Using predicates to create &lt;strong&gt;categories&lt;/strong&gt; of concepts &lt;ul&gt;
&lt;li&gt;people and restaurants&lt;/li&gt;
&lt;li&gt;basis for OWL (Web Ontology Language)網絡本體語言&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;before &lt;img data-src=&#34;/img/NLP/uncategories.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;after &lt;img data-src=&#34;/img/NLP/categories.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap13-Lexical-Semantics&#34;&gt;&lt;a href=&#34;#Chap13-Lexical-Semantics&#34; class=&#34;headerlink&#34; title=&#34;Chap13 Lexical Semantics&#34;&gt;&lt;/a&gt;Chap13 Lexical Semantics&lt;/h2&gt;&lt;p&gt;we didn’t do word meaning in compositional semantics&lt;/p&gt;
&lt;p&gt;WordNet  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;meaning and relationship about words&lt;ul&gt;
&lt;li&gt;hypernym(上位詞)&lt;ul&gt;
&lt;li&gt;breakfast → meal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hierarchies &lt;img data-src=&#34;/img/NLP/wordnet-hierarchy.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our semantics examples, we used various FOL predicates to capture various aspects of events, including the notion of roles&lt;br&gt;Havers, takers, givers, servers, etc.&lt;/p&gt;
&lt;p&gt;Thematic roles(語義關係) &lt;img data-src=&#34;/img/NLP/thematic-roles.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;semantic generalizations over the specific roles that occur with specific verbs&lt;ul&gt;
&lt;li&gt;provide a shallow level of semantic analysis&lt;/li&gt;
&lt;li&gt;tied to syntactic analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;i.e. Takers, givers, eaters, makers, doers, killers&lt;ul&gt;
&lt;li&gt;They’re all the agents of the actions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AGENTS are often subjects&lt;/li&gt;
&lt;li&gt;In a VP-&amp;gt;V NP rule, the NP is often a THEME&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2 major English resources using thematic data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PropBank&lt;ul&gt;
&lt;li&gt;Layered on the Penn TreeBank&lt;/li&gt;
&lt;li&gt;Small number (25ish) labels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FrameNet&lt;ul&gt;
&lt;li&gt;Based on frame semantics&lt;/li&gt;
&lt;li&gt;Large number of frame-specific labels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[McAdams and crew] covered [the floors] with [checked linoleum].格子花紋油毯&lt;ul&gt;
&lt;li&gt;Arg0 (agent: the causer of the smearing)&lt;/li&gt;
&lt;li&gt;Arg1 (theme: “thing covered”)&lt;/li&gt;
&lt;li&gt;Arg2 (covering: “stuff being smeared”)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;including agent and theme, remaining args are verb specific&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Logical Statements  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example: EAT – Eating(e) ^Agent(e,x)^ Theme(e,y)^Food(y)&lt;ul&gt;
&lt;li&gt;(adding in all the right quantifiers and lambdas)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use WordNet to encode the selection restrictions&lt;/li&gt;
&lt;li&gt;Unfortunately, language is creative&lt;ul&gt;
&lt;li&gt;… ate glass on an empty stomach accompanied only by water and tea&lt;/li&gt;
&lt;li&gt;you &lt;strong&gt;can’t eat gold&lt;/strong&gt; for lunch if you’re hungry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;can we discover a verb’s restrictions by using a corpus and WordNet?    &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Parse sentences and find heads&lt;/li&gt;
&lt;li&gt;Label the thematic roles&lt;/li&gt;
&lt;li&gt;Collect statistics on the co-occurrence of particular headwords with particular thematic roles&lt;/li&gt;
&lt;li&gt;Use the WordNet hypernym structure to &lt;strong&gt;find the most meaningful level to use as a restriction&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;WSD&#34;&gt;&lt;a href=&#34;#WSD&#34; class=&#34;headerlink&#34; title=&#34;WSD&#34;&gt;&lt;/a&gt;WSD&lt;/h3&gt;&lt;p&gt;Word sense disambiguation  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;select right sense for a word &lt;/li&gt;
&lt;li&gt;Semantic selection restrictions can be used to disambiguate&lt;ul&gt;
&lt;li&gt;Ambiguous arguments to unambiguous predicates&lt;/li&gt;
&lt;li&gt;Ambiguous predicates with unambiguous arguments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ambiguous arguments&lt;ul&gt;
&lt;li&gt;Prepare a dish(菜餚)&lt;/li&gt;
&lt;li&gt;Wash a dish(盤子)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ambiguous predicates&lt;ul&gt;
&lt;li&gt;Serve (任職/服務) Denver&lt;/li&gt;
&lt;li&gt;Serve (供應) breakfast&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Methodology   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Supervised Disambiguation&lt;ul&gt;
&lt;li&gt;based on a labeled training set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dictionary-Based Disambiguation&lt;ul&gt;
&lt;li&gt;based on lexical resource like dictionaries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unsupervised Disambiguation&lt;ul&gt;
&lt;li&gt;label training data is expensive &lt;/li&gt;
&lt;li&gt;based on unlabeled corpora&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Upper(human) and Lower(simple model) Bounds&lt;/li&gt;
&lt;li&gt;Pseudoword&lt;ul&gt;
&lt;li&gt;Generate artificial evaluation data for comparison and improvement of text processing algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Supervised ML Approaches  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What’s a tag?&lt;ul&gt;
&lt;li&gt;In WordNet, “bass” in a text has 8 possible tags or labels (bass1 through bass8)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;require very simple representation for training data&lt;ul&gt;
&lt;li&gt;Vectors of sets of feature/value pairs&lt;/li&gt;
&lt;li&gt;need to extract training data by characterization of text surrounding the target&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If you decide to use features that require more analysis (say parse trees) then the ML part may be doing less work (relatively) if these features are truly informative&lt;/li&gt;
&lt;li&gt;Classification&lt;ul&gt;
&lt;li&gt;Naïve Bayes (the right thing to try first)&lt;/li&gt;
&lt;li&gt;Decision lists&lt;/li&gt;
&lt;li&gt;Decision trees&lt;/li&gt;
&lt;li&gt;MaxEnt&lt;/li&gt;
&lt;li&gt;Support vector machines&lt;/li&gt;
&lt;li&gt;Nearest neighbor methods…&lt;/li&gt;
&lt;li&gt;choice of technique depends on features that have been used&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bootstrapping&lt;ul&gt;
&lt;li&gt;Use when don’t have enough data to train a system…&lt;/li&gt;
&lt;li&gt;集中有放回的均勻抽樣&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Naive-Bayes&#34;&gt;&lt;a href=&#34;#Naive-Bayes&#34; class=&#34;headerlink&#34; title=&#34;Naive Bayes&#34;&gt;&lt;/a&gt;Naive Bayes&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Argmax P(sense|feature vector) &lt;img data-src=&#34;/img/NLP/bayesian-decision.png&#34; alt=&#34;&#34;&gt; &lt;/li&gt;
&lt;li&gt;find maximum probabilty of words given possible sk &lt;img data-src=&#34;/img/NLP/bayesian-decision2.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/bayesian-classifier.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;assumption&lt;ul&gt;
&lt;li&gt;bag of words model&lt;ul&gt;
&lt;li&gt;structure and order of words is ignored&lt;/li&gt;
&lt;li&gt;each pair of words in the bag is independent&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;73% correct&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Dictionary-Based-Disambiguation&#34;&gt;&lt;a href=&#34;#Dictionary-Based-Disambiguation&#34; class=&#34;headerlink&#34; title=&#34;Dictionary-Based Disambiguation&#34;&gt;&lt;/a&gt;Dictionary-Based Disambiguation&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;Disambiguation based on sense definitions&lt;/li&gt;
&lt;li&gt;Thesaurus-Based Disambiguation&lt;/li&gt;
&lt;li&gt;Disambiguation based on translations in a second-language corpus&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;sense definition&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;find keywords in definition of a word&lt;ul&gt;
&lt;li&gt;cone&lt;ul&gt;
&lt;li&gt;… pollen-bearing scales or bracts in &lt;strong&gt;trees&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;shape for holding &lt;strong&gt;ice cream&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;50%~70% accuracies&lt;/li&gt;
&lt;li&gt;Alternatives&lt;ul&gt;
&lt;li&gt;Several iterations to determine correct sense&lt;/li&gt;
&lt;li&gt;Combine the dictionary-based and thesaurus-based disambiguation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU3JUI0JUEyJUU1JUJDJTk1JUU1JTg1JUI4&#34;&gt;Thesaurus-Based(索引典)&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt; Disambiguation    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Category can determine which word senses are used&lt;/li&gt;
&lt;li&gt;Each word is assigned one or more subject codes which correspond to its different meanings&lt;ul&gt;
&lt;li&gt;select the most often subject code&lt;/li&gt;
&lt;li&gt;考慮w的context，有多少words的senses與w相同&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Walker’s Algorithm&lt;ul&gt;
&lt;li&gt;50% accuracy for “interest, point, power, state, and terms”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Problems&lt;ul&gt;
&lt;li&gt;general topic categorization, e.g., mouse in computer&lt;/li&gt;
&lt;li&gt;coverage, e.g., Navratilova&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Yarowsky’s Algorithm &lt;img data-src=&#34;/img/NLP/yarowsky-algo.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/yarowsky-algo2.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/yarowsky-algo3.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;categorize sentences&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;categorize words&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;disambiguate by decision rule for Naïve Bayes&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;result &lt;img data-src=&#34;/img/NLP/yarowsky-result.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disambiguation based on translations in a second-language corpus  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the word “interest” has two translations in German&lt;ul&gt;
&lt;li&gt;“Beteiligung” (legal share–50% a interest in the company)&lt;/li&gt;
&lt;li&gt;“Interesse” (attention, concern–her interest in Mathematics)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: … showed interest …&lt;ul&gt;
&lt;li&gt;Look up English-German dictionary, show → zeigen&lt;/li&gt;
&lt;li&gt;Compute R(Interesse, zeigen) and R(Beteiligung, zeigen)&lt;/li&gt;
&lt;li&gt;R(Interesse, zeigen) &amp;gt; R(Beteiligung, zeigen)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Unsupervised-Disambiguation&#34;&gt;&lt;a href=&#34;#Unsupervised-Disambiguation&#34; class=&#34;headerlink&#34; title=&#34;Unsupervised Disambiguation&#34;&gt;&lt;/a&gt;Unsupervised Disambiguation&lt;/h4&gt;&lt;p&gt;P(vj|sk) are estimated using the EM algorithm  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Random initialization of P(vj|sk)(word)&lt;/li&gt;
&lt;li&gt;For each context ci of w, compute P(ci|sk)(sentence)&lt;/li&gt;
&lt;li&gt;Use P(ci|sk) as training data&lt;/li&gt;
&lt;li&gt;Reestimate P(vj|sk)(word)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Surface Representations(features)   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Collocational&lt;ul&gt;
&lt;li&gt;words that appear in specific positions to the right and left of the target word&lt;/li&gt;
&lt;li&gt;limited to the words themselves as well as part of speech&lt;/li&gt;
&lt;li&gt;Example: guitar and bassplayer stand&lt;ul&gt;
&lt;li&gt;[guitar, NN, and, CJC, player, NN, stand, VVB]&lt;/li&gt;
&lt;li&gt;In other words, a vector consisting of [position n word, position n part-of-speech…]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Co-occurrence&lt;ul&gt;
&lt;li&gt;words that occur regardless of position&lt;/li&gt;
&lt;li&gt;Typically limited to frequency counts&lt;/li&gt;
&lt;li&gt;Assume we’ve settled on a possible vocabulary of 12 words that includes guitarand playerbut not andand stand&lt;/li&gt;
&lt;li&gt;Example: guitar and bassplayer stand&lt;ul&gt;
&lt;li&gt;Assume a 12-word sentence includes guitar and player but not “and” and stand&lt;/li&gt;
&lt;li&gt;[0,0,0,1,0,0,0,0,0,1,0,0]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Applications  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tagging&lt;ul&gt;
&lt;li&gt;translation&lt;/li&gt;
&lt;li&gt;information retrieval&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;different label  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generic thematic roles (aka case roles)&lt;ul&gt;
&lt;li&gt;Agent, instrument, source, goal, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Propbank labels&lt;ul&gt;
&lt;li&gt;Common set of labels ARG0-ARG4, ARGM&lt;/li&gt;
&lt;li&gt;specific to verb semantics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FrameNet frame elements&lt;ul&gt;
&lt;li&gt;Conceptual and frame-specific &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: [Ochocinco] bought [Burke] [a diamond ring]&lt;ul&gt;
&lt;li&gt;generic: Agent, Goal, Theme&lt;/li&gt;
&lt;li&gt;propbank: ARG0, ARG2, ARG1&lt;/li&gt;
&lt;li&gt;framenet: Customer, Recipe, Goods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Semantic Role Labeling  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;automatically identify and label thematic roles&lt;ul&gt;
&lt;li&gt;For each verb in a sentence&lt;ul&gt;
&lt;li&gt;For each constituent&lt;ul&gt;
&lt;li&gt;Decide if it is an argument to that verb&lt;/li&gt;
&lt;li&gt;if it is an argument, determine what kind&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;feature&lt;ul&gt;
&lt;li&gt;from parse and lexical item&lt;/li&gt;
&lt;li&gt;“path” &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Lexical-Acquisition&#34;&gt;&lt;a href=&#34;#Lexical-Acquisition&#34; class=&#34;headerlink&#34; title=&#34;Lexical Acquisition&#34;&gt;&lt;/a&gt;Lexical Acquisition&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Verb Subcategorization&lt;ul&gt;
&lt;li&gt;the syntactic means by which verbs express their arguments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Attachment Ambiguity&lt;ul&gt;
&lt;li&gt;The children ate the cake with their hands&lt;/li&gt;
&lt;li&gt;The children ate the cake with blue icing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SelectionalPreferences&lt;ul&gt;
&lt;li&gt;The semantic categorization of a verb’s arguments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semantic Similarity (refer to IR course)&lt;ul&gt;
&lt;li&gt;Semantic similarity between words&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Verb-Subcategorization&#34;&gt;&lt;a href=&#34;#Verb-Subcategorization&#34; class=&#34;headerlink&#34; title=&#34;Verb Subcategorization&#34;&gt;&lt;/a&gt;Verb Subcategorization&lt;/h4&gt;&lt;p&gt;a particular set of syntactic categories that a verb can appear with is called a &lt;strong&gt;subcategorization frame&lt;/strong&gt; &lt;img data-src=&#34;/img/NLP/subcategorization.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Brent’s subcategorization frame learner  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cues: Define a regular pattern of words and syntactic categories&lt;ol&gt;
&lt;li&gt;ε: error rate of assigning frame f to verb v based on cue cj&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Hypothesis Testing: Define null hypothesis H0: “the frame is not appropriate for the verb” &lt;ol&gt;
&lt;li&gt;Reject this hypothesis if the cue cj indicates with high probability that our H0 is wrong&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example&lt;br&gt;Cues  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;regular pattern for subcategorization frame “NP NP”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(OBJ | SUBJ_OBJ | CAP) (PUNC |CC)&lt;br&gt;Null hypothesis testing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Verb vi occurs a total of n times in the corpus and there are m &amp;lt; n occurrences with a cue for frame fj&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reject the null hypothesis H0 that vi does not accept fj with the following probability of error &lt;img data-src=&#34;/img/NLP/brent-null-hypothesis.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Brent’s system does well at precision, but not well at recall&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Manning’s system&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;solve this problem by using a tagger and running the cue detection on the output of the tagger&lt;/li&gt;
&lt;li&gt;learn a lot of subcategorization frames, even those it is low-reliability&lt;/li&gt;
&lt;li&gt;still low performance &lt;/li&gt;
&lt;li&gt;improve : use prior knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PCFG prefers to parse common construction  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(A|prep, verb, np1, np2, w) ~= P(A|prep, verb, np1, np2)&lt;ul&gt;
&lt;li&gt;Do not count the word outside of frame&lt;/li&gt;
&lt;li&gt;w: words outside of “verb np1(prep np2)”&lt;/li&gt;
&lt;li&gt;A: random variable representing attachment decision&lt;/li&gt;
&lt;li&gt;V(A): verb or np1&lt;/li&gt;
&lt;li&gt;Counter example&lt;ul&gt;
&lt;li&gt;Fred saw a movie with Arnold Schwarzenegger&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;P(A|prep, verb, np1, np2, noun1, noun2) ~= P(A|prep, verb, noun1, noun2)&lt;ul&gt;
&lt;li&gt;noun1 = head of np1, noun2 = head of np2&lt;/li&gt;
&lt;li&gt;total parameters: $10^{13}$ = #(prep) x #(verb) x #(noun) x #(noun) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;P(A= noun | prep, verb, noun1) vs. P(A= verb | prep, verb, noun1)&lt;ul&gt;
&lt;li&gt;compare probability to be verb and probability to be noun&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Technique: Alternative to reduce parameters   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Condition probabilities on fewer things&lt;/li&gt;
&lt;li&gt;Condition probabilities on more general things&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model asks the following questions  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VAp: Is there a PP headed by p and following the verb v which attaches to v(VAp=1) or not (VAp=0)?&lt;/li&gt;
&lt;li&gt;NAp: Is there a PP headed by p and following the noun n which attaches to n (NAp=1) or not (NAp=0)?&lt;/li&gt;
&lt;li&gt;(1) Determine the attachment of a PP that is immediately following an object noun, i.e. compute the probability of NAp=1&lt;/li&gt;
&lt;li&gt;In order for the first PP headed by the preposition p to attach to the verb, both VAp=1 and NAp=0&lt;ul&gt;
&lt;li&gt;calculate likelihood ratio between V and N &lt;img data-src=&#34;/img/NLP/likelihood-ratio-vn.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;maximum estimation&lt;ul&gt;
&lt;li&gt;P(VA = 1 | v) = C(v, p) / C(v)&lt;/li&gt;
&lt;li&gt;P(NA = 1 | n) = C(n, p) / C(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Estimation of PP attachment counts&lt;ul&gt;
&lt;li&gt;Sure Noun Attach&lt;ul&gt;
&lt;li&gt;If a noun is followed by a PP but no preceding verb, increment C(prep attached to noun) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sure Verb Attach&lt;ul&gt;
&lt;li&gt;if a passive verb is followed by a PP other than a “by” phrase, increment C(prep attached to verb) &lt;/li&gt;
&lt;li&gt;if a PP follows both a noun phrase and a verb but the noun phrase is a pronoun, increment C(prep attached to verb)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ambiguous Attach&lt;ul&gt;
&lt;li&gt;if a PP follows both a noun and a verb, see if the probabilities based on the attachment decided by previous way&lt;/li&gt;
&lt;li&gt;otherwise increment both attachment counters by 0.5&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/attach-example.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Sparse data is a major cause of the difference between the human and program performance(attachment indeterminacy不確定性)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using Semantic Information  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;condition on semantic tags of verb &amp;amp; noun&lt;ul&gt;
&lt;li&gt;Sue bought a plant with Jane(human)&lt;/li&gt;
&lt;li&gt;Sue bought a plant with yellow leaves(object)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assumption&lt;br&gt;The noun phrase serves as the subject of the relative clause&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;collect “ subject-verb” and “verb-object” pairs.(training part)  &lt;/li&gt;
&lt;li&gt;compute t-score (testing part) &lt;ul&gt;
&lt;li&gt;t-score &amp;gt; 0.10 (significant)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;P (relative clause attaches to x | main verb of clause =v) &amp;gt; P (relative clause attaches to y | main verb of clause=v)&lt;br&gt;↔ P (x= subject/object | v) &amp;gt; P (y= subject/ object|v)&lt;/p&gt;
&lt;p&gt;Selectional Preferences  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most verbs prefer particular type of arguments&lt;ul&gt;
&lt;li&gt;eat → object (food item)&lt;/li&gt;
&lt;li&gt;think → subject (people)&lt;/li&gt;
&lt;li&gt;bark → subject (dog)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Aspects of meaning of a word can be inferred&lt;ul&gt;
&lt;li&gt;Susan had never eaten a fresh &lt;strong&gt;durian&lt;/strong&gt; before (food item)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Selectional preferences can be used to rank different parses of a sentence&lt;/li&gt;
&lt;li&gt;Selectional preference strength&lt;ul&gt;
&lt;li&gt;how strongly the verb constrains its direct object&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/selection-strength.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;KL divergence between the prior distribution of direct objects of general verb and the distribution of direct objects of specific verb&lt;/li&gt;
&lt;li&gt;2 assumptions&lt;ul&gt;
&lt;li&gt;only the head noun of the object is considered&lt;/li&gt;
&lt;li&gt;rather than dealing with individual nouns, we look at classes of nouns&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Selectional association&lt;ul&gt;
&lt;li&gt;Selectional Association between a verb and a class is this class’s contribution to S(v) / the overall preference strength S(v) &lt;img data-src=&#34;/img/NLP/selectional-association.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;There is also a rule for assigning association strengths to nouns instead of noun classes&lt;ul&gt;
&lt;li&gt;If noun belongs to several classes, then its choose the highest association strength among all classes &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;estimating the probability that a direct object in noun class c occurs given a verb v&lt;ul&gt;
&lt;li&gt;A(interrupt, chair) = max(A(interrupt, people), A(interrupt, furniture)) = A(interrupt, people)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example &lt;img data-src=&#34;/img/NLP/selectional-example.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;eat prefers fooditem &lt;ul&gt;
&lt;li&gt;A(eat, food)=1.08 → very specific&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;seehas a uniform distribution&lt;ul&gt;
&lt;li&gt;A(see, people)=A(see, furniture)=A(see, food)=A(see, action)=0 → no selectional preference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;find disprefers action item&lt;ul&gt;
&lt;li&gt;A(find, action)=-0.13 → less specific&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Semantic Similarity  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assessing semantic similarity between a new word and other already known words&lt;/li&gt;
&lt;li&gt;Vector Space vs Probabilistic&lt;/li&gt;
&lt;li&gt;Vector Space&lt;ul&gt;
&lt;li&gt;Words can be expressed in different spaces: document space, word spaceand modifier space&lt;/li&gt;
&lt;li&gt;Similarity measures for binary vectors: matching coefficient, Dice coefficient, Jaccard(or Tanimoto) coefficient, Overlap coefficientand cosine&lt;/li&gt;
&lt;li&gt;Similarity measures for the real-valued vector space: cosine, Euclidean Distance, normalized correlation coefficient&lt;ul&gt;
&lt;li&gt;cosine assumes a Euclidean space which is not well-motivated when dealing with word counts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/similarity-measure.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Probabilistic Measures&lt;ul&gt;
&lt;li&gt;viewing word counts by representing them as probability distributions&lt;/li&gt;
&lt;li&gt;compare two probability distributions using&lt;ul&gt;
&lt;li&gt;KL Divergence&lt;/li&gt;
&lt;li&gt;Information Radius(Irad)&lt;/li&gt;
&lt;li&gt;L1Norm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap14-Computational-Discourse&#34;&gt;&lt;a href=&#34;#Chap14-Computational-Discourse&#34; class=&#34;headerlink&#34; title=&#34;Chap14 Computational Discourse&#34;&gt;&lt;/a&gt;Chap14 Computational Discourse&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Level&lt;/th&gt;
&lt;th&gt;Well-formedness constraints&lt;/th&gt;
&lt;th&gt;Types of ambiguity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Lexical&lt;/td&gt;
&lt;td&gt;Rules of inflection and derivation&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;structural, morpheme boundaries, morpheme identity&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Syntactic&lt;/td&gt;
&lt;td&gt;Grammar rules&lt;/td&gt;
&lt;td&gt;structural, POS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Semantic&lt;/td&gt;
&lt;td&gt;Selection restrictions&lt;/td&gt;
&lt;td&gt;word sense, quantifier scope&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU4JUFGJUFEJUU3JTk0JUE4JUU1JUFEJUE2&#34;&gt;Pragmatic&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;conversation principles&lt;/td&gt;
&lt;td&gt;pragmatic function&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Computational Discourse  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discourse(語篇)&lt;ul&gt;
&lt;li&gt;A group of sentences with the same coherence relation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Coherence relation&lt;ul&gt;
&lt;li&gt;the 2nd sentence offers the reader an explaination or cause for the 1st sentence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Entity-based Coherence&lt;ul&gt;
&lt;li&gt;relationships with the entities, introducing them and following them in a focused way&lt;/li&gt;
&lt;li&gt;Discourse Segmentation&lt;ul&gt;
&lt;li&gt;Divide a document into a linear sequence of multiparagraph passages&lt;/li&gt;
&lt;li&gt;Academic article&lt;ul&gt;
&lt;li&gt;Abstract&lt;/li&gt;
&lt;li&gt;Introduction&lt;/li&gt;
&lt;li&gt;Methodology&lt;/li&gt;
&lt;li&gt;Results&lt;/li&gt;
&lt;li&gt;Conclusion&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;http://www.wannabehacks.co.uk/images/Inverted_pyramid_in_comprehensive_form.jpg&#34; alt=&#34;Inverted Pyramid&#34;&gt;&lt;/li&gt;
&lt;li&gt;Applications&lt;ul&gt;
&lt;li&gt;News&lt;/li&gt;
&lt;li&gt;Summarize different segments of a document&lt;/li&gt;
&lt;li&gt;Extract information from inside a single discourse segment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TextTiling (Hearst,1997)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tokenization&lt;ul&gt;
&lt;li&gt;Each space-delimited word in the input is converted to lower-case&lt;/li&gt;
&lt;li&gt;Words in a stop list of function words are thrown out&lt;/li&gt;
&lt;li&gt;The remaining words are morphologically stemmed&lt;/li&gt;
&lt;li&gt;The stemmed words are grouped into pseudo-sentencesof length w = 20&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lexical score determination&lt;ul&gt;
&lt;li&gt;compute a lexical cohesion(結合) score between pseudo-sentences&lt;ul&gt;
&lt;li&gt;score: average similarity of words in the pseudo-sentences before gap to pseudo-sentences after the gap(??)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Boundary identification    &lt;ul&gt;
&lt;li&gt;Compute a depth score for each gap&lt;/li&gt;
&lt;li&gt;Boundaries are assigned at any valley which is deeper than a cutoff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Coherence Relations  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Result&lt;ul&gt;
&lt;li&gt;The Tin Woodman was caught in the rain. His joints rusted&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Explanation&lt;ul&gt;
&lt;li&gt;John hid Bill’s car keys. He was drunk&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parallel&lt;ul&gt;
&lt;li&gt;The Scarecrow wanted some brains&lt;/li&gt;
&lt;li&gt;The Tin Woodman wanted a heart&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Elaboration(詳細論述)&lt;ul&gt;
&lt;li&gt;Dorothy was from Kansas&lt;/li&gt;
&lt;li&gt;She lived in the midst of the great Kansas prairies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Occasion(起因)&lt;ul&gt;
&lt;li&gt;Dorothy picked up the oil-can&lt;/li&gt;
&lt;li&gt;She oiled the Tin Woodman’s joints&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Coherence Relation Assignment  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discourse parsing&lt;/li&gt;
&lt;li&gt;Open problems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cue-Phrase-Based Algorithm  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Using cue phrases&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Segment the text into discourse segments&lt;/li&gt;
&lt;li&gt;Classify the relationship between each consecutive discourse&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cue phrase&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;connectives, which are often conjunctions or adverbs &lt;ul&gt;
&lt;li&gt;because, although, but, for example, yet, with, and&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;discourse uses vs. sentential uses&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;With&lt;/strong&gt; its distant orbit, Mars exhibits frigid weather conditions. (因為長距離的運行軌道，火星天氣酷寒)&lt;/li&gt;
&lt;li&gt;We can see Mars &lt;strong&gt;with&lt;/strong&gt; an ordinary telescope&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;img data-src=&#34;/img/NLP/discourse-relation.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Temporal Relation  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ordered in time (Asynchronous)&lt;ul&gt;
&lt;li&gt;before, after …&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;overlapped (Synchronous)&lt;ul&gt;
&lt;li&gt;at the same time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Contingency Relation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因果關係，附帶條件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Comparison Relation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;difference between two arguments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Expansion Relation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;expands the information for one argument in the other one or continues the narrative flow&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implicit Relation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discourse marker is absent&lt;/li&gt;
&lt;li&gt;颱風來襲，學校停止上課&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Chinese Relation Words &lt;img data-src=&#34;/img/NLP/chinese-coherence-relation.png&#34; alt=&#34;&#34;&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ambiguous Discourse Markers &lt;ul&gt;
&lt;li&gt;而：而且, 然而, 因而&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Reference-Resolution&#34;&gt;&lt;a href=&#34;#Reference-Resolution&#34; class=&#34;headerlink&#34; title=&#34;Reference Resolution&#34;&gt;&lt;/a&gt;Reference Resolution&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/NLP/reference-resolution.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evoke&lt;ul&gt;
&lt;li&gt;When a referent is first mentioned in a discourse, we say that a representation for it is &lt;strong&gt;evoked into&lt;/strong&gt; the model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Access&lt;ul&gt;
&lt;li&gt;Upon subsequent mention, this representation is &lt;strong&gt;accessed from&lt;/strong&gt; the model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Five Types of Referring Expressions  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Indefinite Noun Phrases(不定名詞)&lt;ul&gt;
&lt;li&gt;marked with the determiner a, some, this …&lt;/li&gt;
&lt;li&gt;Create a new internal symbol and add to the current world model&lt;ul&gt;
&lt;li&gt;Mayumi has bought a new automobile&lt;/li&gt;
&lt;li&gt;automobile(g123)&lt;/li&gt;
&lt;li&gt;new(g123)&lt;/li&gt;
&lt;li&gt;owns(mayumi, g123)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;non-specific sense to describe an object&lt;ul&gt;
&lt;li&gt;Mayumi wantsto buy a new XJE&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;whole classes of objects&lt;ul&gt;
&lt;li&gt;A new automobiletypically requires repair twice in the first 12 months&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;collect one or more properties&lt;ul&gt;
&lt;li&gt;The Macho GTE XL is a new automobile&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Question and commands&lt;ul&gt;
&lt;li&gt;Is her automobile in a parking placenear the exit?&lt;/li&gt;
&lt;li&gt;Put her automobile into a parking placenear the exit!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Definite Noun Phrases(定名詞)&lt;ul&gt;
&lt;li&gt;simple referential and generic uses(the same as indefinite)&lt;/li&gt;
&lt;li&gt;indicate an individual by description that they satisfy&lt;ul&gt;
&lt;li&gt;The manufacturer &lt;strong&gt;of this automobile&lt;/strong&gt; should be indicted&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pronouns(代名詞)&lt;ul&gt;
&lt;li&gt;reference backs to entities that have been introduced by previous nounphrases in a discourse&lt;/li&gt;
&lt;li&gt;non-referential noun phrase&lt;ul&gt;
&lt;li&gt;non-exist object&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;logical variable&lt;ul&gt;
&lt;li&gt;No male driveradmits that heis incompetent &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;something that is available from the context of utterance, but has not been explicitly mentioned before&lt;ul&gt;
&lt;li&gt;Here they come, late again!&lt;/li&gt;
&lt;li&gt;Can’t easily know who are “they”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Anaphora&lt;ul&gt;
&lt;li&gt;Number Agreements&lt;ul&gt;
&lt;li&gt;John has a Ford Falcon. It is red&lt;/li&gt;
&lt;li&gt;John has three Ford Falcons. They are red&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Person Agreement(人稱)&lt;/li&gt;
&lt;li&gt;Gender Agreement&lt;/li&gt;
&lt;li&gt;Selection Restrictions&lt;ul&gt;
&lt;li&gt;verb and its arguments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Demonstratives (指示詞)&lt;ul&gt;
&lt;li&gt;this, that&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Names&lt;ul&gt;
&lt;li&gt;Full name &amp;gt; long definite description &amp;gt; short definite description &amp;gt; last name&amp;gt; first name &amp;gt; distal demonstrative &amp;gt; proximate demonstrative &amp;gt; NP &amp;gt; stressed pronoun &amp;gt; unstressed pronoun&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Information Status  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Referential forms used to provide new or old information&lt;/li&gt;
&lt;li&gt;givenness hierarchy &lt;img data-src=&#34;/img/NLP/givenness-hierarchy.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Definite-indefinite is a clue to given-new status&lt;ul&gt;
&lt;li&gt;The sales managere(given) employed a foreign distributor(new)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If there are ambiguous noun phrases in a sentence, then it extracts the presuppositions to provide extra constraints&lt;/li&gt;
&lt;li&gt;When some new information is added to knowledge base, check if it is consistent with what we already know&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Active model of understanding  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given a text, build up predictions or expectations about new information and actively compare these with successive input to resolve ambiguities&lt;/li&gt;
&lt;li&gt;Construct a proof of the information provided in a sentence from the existing world knowledge and plausible inference rules illustrated&lt;/li&gt;
&lt;li&gt;the inference are not sensitive to the order&lt;ul&gt;
&lt;li&gt;if the proposition that the disc is heavy is inferred, then it is not changed after the discourse has finished&lt;/li&gt;
&lt;li&gt;Solution: describe the propositions in temporal order&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Script: encapsulate a sequence of actions that belong together into a script&lt;figure class=&#34;highlight dns&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;automobile_buying:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&amp;lt;&amp;#123;customer(C), automobile(&lt;span class=&#34;keyword&#34;&gt;A&lt;/span&gt;), dealer(D), garage(G)&amp;#125;,&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;	&amp;lt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;		goes(C, G),&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;		test_drives(C, &lt;span class=&#34;keyword&#34;&gt;A&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;		orders(C, &lt;span class=&#34;keyword&#34;&gt;A&lt;/span&gt;, D),&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;		delivers(D, &lt;span class=&#34;keyword&#34;&gt;A&lt;/span&gt;, C),&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;		drives(C, &lt;span class=&#34;keyword&#34;&gt;A&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;	&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;參考資料&#34;&gt;&lt;a href=&#34;#參考資料&#34; class=&#34;headerlink&#34; title=&#34;參考資料&#34;&gt;&lt;/a&gt;參考資料&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;HHChen 課堂講義&lt;/li&gt;
&lt;/ul&gt;
</content>
        <category term="機器學習" />
        <category term="自然語言處理" />
        <category term="統計" />
        <updated>2015-05-01T04:37:47.000Z</updated>
    </entry>
    <entry>
        <id>http://gitqwerty777.github.io/natural-language-processing/</id>
        <title>自然語言處理(上)</title>
        <link rel="alternate" href="http://gitqwerty777.github.io/natural-language-processing/"/>
        <content type="html">&lt;h2 id=&#34;Chap01-Introduction&#34;&gt;&lt;a href=&#34;#Chap01-Introduction&#34; class=&#34;headerlink&#34; title=&#34;Chap01 Introduction&#34;&gt;&lt;/a&gt;Chap01 Introduction&lt;/h2&gt;&lt;h3 id=&#34;Applications-of-NLP&#34;&gt;&lt;a href=&#34;#Applications-of-NLP&#34; class=&#34;headerlink&#34; title=&#34;Applications of NLP&#34;&gt;&lt;/a&gt;Applications of NLP&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Machine translation&lt;ul&gt;
&lt;li&gt;google translate&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Speech recognition&lt;ul&gt;
&lt;li&gt;Siri&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Smart input method&lt;ul&gt;
&lt;li&gt;ㄐㄅㄈㄏ → 加倍奉還&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sentiment(情感) analysis&lt;/li&gt;
&lt;li&gt;Information retrieval&lt;/li&gt;
&lt;li&gt;Question Anwering&lt;ul&gt;
&lt;li&gt;Turing Test&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optical character recognition (OCR)&lt;a id=&#34;more&#34;&gt;&lt;/a&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Critical-Problems-in-NLP&#34;&gt;&lt;a href=&#34;#Critical-Problems-in-NLP&#34; class=&#34;headerlink&#34; title=&#34;Critical Problems in NLP&#34;&gt;&lt;/a&gt;Critical Problems in NLP&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Ambiguity(不明確性)&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The most important thing in NLP&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Lexical(字辭)&lt;ul&gt;
&lt;li&gt;&lt;code&gt;current&lt;/code&gt;: noun or adjective&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bank&lt;/code&gt; (noun): money or river&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Syntactic(語法)&lt;ul&gt;
&lt;li&gt;&lt;code&gt;[saw [the boy] [in the park]]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[saw [the boy in the park]]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semantic(語義)&lt;ul&gt;
&lt;li&gt;“John kissed his wife, and so did Sam”. (Sam kissed John’s wife or his own?)&lt;/li&gt;
&lt;li&gt;agent(施事) vs. patient(受事)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ill-form(bad form)&lt;ul&gt;
&lt;li&gt;typo&lt;/li&gt;
&lt;li&gt;grammatical errors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Robustness&lt;ul&gt;
&lt;li&gt;various domain&lt;/li&gt;
&lt;li&gt;網路語言：取材於方言俗語、各門外語、縮略語、諧音、甚至以符號合併以達至象形效果等等&lt;ul&gt;
&lt;li&gt;emoticon(表情符號)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Main-Topics-in-Large-Scale-NLP-Design&#34;&gt;&lt;a href=&#34;#Main-Topics-in-Large-Scale-NLP-Design&#34; class=&#34;headerlink&#34; title=&#34;Main Topics in Large-Scale NLP Design&#34;&gt;&lt;/a&gt;Main Topics in Large-Scale NLP Design&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Knowledge representation&lt;ul&gt;
&lt;li&gt;organize and describe linguistic knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge strategies&lt;ul&gt;
&lt;li&gt;use knowledge for efficient parsing, ambiguity resolution, ill-formed recovery&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge acquisition&lt;ul&gt;
&lt;li&gt;setup and maintain knowledge base systematically and cost-effectively&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge integration&lt;ul&gt;
&lt;li&gt;consider various knowledge sources effectively&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Models&#34;&gt;&lt;a href=&#34;#Models&#34; class=&#34;headerlink&#34; title=&#34;Models&#34;&gt;&lt;/a&gt;Models&lt;/h3&gt;&lt;p&gt;用演算法來轉換文字結構，以產生最後結果   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;State machines&lt;/li&gt;
&lt;li&gt;Rule-based approaches&lt;/li&gt;
&lt;li&gt;Logical formalisms&lt;/li&gt;
&lt;li&gt;Probabilistic models&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Approaches&#34;&gt;&lt;a href=&#34;#Approaches&#34; class=&#34;headerlink&#34; title=&#34;Approaches&#34;&gt;&lt;/a&gt;Approaches&lt;/h3&gt;&lt;p&gt;NLP start from 1960, &lt;strong&gt;statictics method&lt;/strong&gt; wins after 1995&lt;/p&gt;
&lt;p&gt;Rule-based approach&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advantages&lt;ul&gt;
&lt;li&gt;No need database&lt;/li&gt;
&lt;li&gt;Easy to incorporate with knowledge&lt;/li&gt;
&lt;li&gt;Better generalization to a unseen domain&lt;/li&gt;
&lt;li&gt;Explainable and traceable&lt;ul&gt;
&lt;li&gt;easy to understand&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Disadvantages&lt;ul&gt;
&lt;li&gt;Hard to maintain consistency (at different situation)&lt;/li&gt;
&lt;li&gt;Hard to handle uncertain knowledge (define uncertainty factor)&lt;ul&gt;
&lt;li&gt;irregular information&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Not easy to avoid redundancy&lt;/li&gt;
&lt;li&gt;Knowledge acquisition is time consuming&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Corpus(語料庫)-based approach  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advantages&lt;ul&gt;
&lt;li&gt;Knowledge acquisition can be automatically achieved by the computer&lt;/li&gt;
&lt;li&gt;Uncertain knowledge can be objectively quantified(知識可被量化)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consistency and completeness&lt;/strong&gt; are easy to obtain&lt;/li&gt;
&lt;li&gt;Well established statistical theories and technique are available&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Disadvantages&lt;ul&gt;
&lt;li&gt;Generalization is poor for small-size database&lt;/li&gt;
&lt;li&gt;Unable to reasoning&lt;/li&gt;
&lt;li&gt;Hard to identify the effect of each parameter&lt;/li&gt;
&lt;li&gt;Build database is time consuming&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Corpus&lt;ul&gt;
&lt;li&gt;Brown Corpus (1M words),Birmingham Corpus (7.5M words), LOB Corpus (1M words), etc&lt;/li&gt;
&lt;li&gt;Corpora(語料庫(複數)) of special domains or style&lt;ul&gt;
&lt;li&gt;Newspaper, Bible, etc&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Information in Corpora&lt;ul&gt;
&lt;li&gt;pure-text corpus&lt;ul&gt;
&lt;li&gt;language usage of real world, word distribution, co-occurrence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;tagged corpus&lt;ul&gt;
&lt;li&gt;parts of speech, structures, features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hybrid approach   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use rule-based approach when &lt;ul&gt;
&lt;li&gt;there are rules that have good coverage&lt;ul&gt;
&lt;li&gt;it can be governed by a small number of rules&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;extensional knowledge is important to the system&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use corpus-based approach when&lt;ul&gt;
&lt;li&gt;Knowledge needed to solve the problem is huge and intricate&lt;/li&gt;
&lt;li&gt;A good model or formulation exists&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Implementation&#34;&gt;&lt;a href=&#34;#Implementation&#34; class=&#34;headerlink&#34; title=&#34;Implementation&#34;&gt;&lt;/a&gt;Implementation&lt;/h3&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3d3dy5ubHRrLm9yZy8=&#34;&gt;Natural Language Toolkit(NLTK)&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;: Open source Python modules, linguistic data and documentation for research and development in natural language processing&lt;/p&gt;
&lt;p&gt;features  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Corpus readers&lt;/li&gt;
&lt;li&gt;Tokenizers&lt;ul&gt;
&lt;li&gt;whitespace, newline, blankline, word, treebank, sexpr, regexp, Punkt sentence segmenter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stemmers&lt;ul&gt;
&lt;li&gt;Porter, Lancaster, regexp&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Taggers&lt;ul&gt;
&lt;li&gt;regexp, n-gram, backoff, Brill, HMM, TnT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Chunkers&lt;ul&gt;
&lt;li&gt;regexp, n-gram, named-entity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Metrics&lt;ul&gt;
&lt;li&gt;accuracy, precision, recall, windowdiff, distance metrics, inter-annotator agreement coefficients, word association measures, rank correlation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Estimation&lt;ul&gt;
&lt;li&gt;uniform, maximum likelihood, Lidstone, Laplace, expected likelihood, heldout, cross-validation, Good-Turing, Witten-Bell&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Miscellaneous&lt;ul&gt;
&lt;li&gt;unification, chatbots, many utilities&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap02-Overall-Pictures&#34;&gt;&lt;a href=&#34;#Chap02-Overall-Pictures&#34; class=&#34;headerlink&#34; title=&#34;Chap02 Overall Pictures&#34;&gt;&lt;/a&gt;Chap02 Overall Pictures&lt;/h2&gt;&lt;p&gt;&lt;img data-src=&#34;/img/NLP/overview.png&#34; alt=&#34;overview&#34;&gt;&lt;/p&gt;
&lt;p&gt;Knowledge Categories     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Phonology(聲音，資料來源)&lt;/li&gt;
&lt;li&gt;Morphology(詞性)&lt;/li&gt;
&lt;li&gt;Syntax(句構)&lt;/li&gt;
&lt;li&gt;Semantics(語義)&lt;/li&gt;
&lt;li&gt;Pragmatics(句子關聯，語用學)&lt;/li&gt;
&lt;li&gt;Discourse(篇章分析，話語)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Morphology-Structure-of-words&#34;&gt;&lt;a href=&#34;#Morphology-Structure-of-words&#34; class=&#34;headerlink&#34; title=&#34;Morphology(Structure of words)&#34;&gt;&lt;/a&gt;Morphology(Structure of words)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;part-of-speech(POS) tagging(詞性標註, lexical category)&lt;/li&gt;
&lt;li&gt;find the roots of words&lt;ul&gt;
&lt;li&gt;e.g., going → go, cats → cat&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Syntax-structure-of-sentences&#34;&gt;&lt;a href=&#34;#Syntax-structure-of-sentences&#34; class=&#34;headerlink&#34; title=&#34;Syntax(structure of sentences)&#34;&gt;&lt;/a&gt;Syntax(structure of sentences)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Context-Free Grammars(CFG) &lt;img data-src=&#34;/img/NLP/cfg.png&#34; alt=&#34;parse tree&#34;&gt;&lt;/li&gt;
&lt;li&gt;Chomsky Normal Form(CNF)&lt;ul&gt;
&lt;li&gt;can only use following two rules &lt;ol&gt;
&lt;li&gt;&lt;code&gt;non-terminal → terminal&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;non-terminal → non-terminal non-terminal&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;dependency&lt;ul&gt;
&lt;li&gt;local dependency&lt;ul&gt;
&lt;li&gt;words near together would probably have the same syntax rule&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;long-distance dependency &lt;ul&gt;
&lt;li&gt;wh-movement(疑問詞移位)&lt;ul&gt;
&lt;li&gt;What did Jennifer buy? → 什麼 (助動詞) 珍妮佛 買了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分裂句 Right-node raising&lt;ul&gt;
&lt;li&gt;[[she would have bought] and [he might sell]] shares&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Argument-cluster coordination&lt;ul&gt;
&lt;li&gt;I give [[you an apple] and [him a pear]]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;challenge for some statistical NLP approaches (like n-grams)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Semantics-meaning-of-individual-sentences&#34;&gt;&lt;a href=&#34;#Semantics-meaning-of-individual-sentences&#34; class=&#34;headerlink&#34; title=&#34;Semantics(meaning of individual sentences)&#34;&gt;&lt;/a&gt;Semantics(meaning of individual sentences)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;semantic roles  &lt;ul&gt;
&lt;li&gt;agent(主詞)&lt;/li&gt;
&lt;li&gt;patient(受詞)&lt;/li&gt;
&lt;li&gt;instrument(工具)&lt;/li&gt;
&lt;li&gt;goal(目標)&lt;/li&gt;
&lt;li&gt;Beneficiary(受益)&lt;/li&gt;
&lt;li&gt;He threw the book(patient) at me(goal)&lt;/li&gt;
&lt;li&gt;John sold the car for a friend(beneficiary)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Subcategorizations(次分類)&lt;ul&gt;
&lt;li&gt;及物、不及物動詞&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semantics can be divided into two parts&lt;ul&gt;
&lt;li&gt;Lexical Semantics&lt;ul&gt;
&lt;li&gt;上下位，同義(反義)，部分-整體&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Composition Semantics&lt;ul&gt;
&lt;li&gt;合起來的意義與單一字意義不同&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Implementation&lt;ul&gt;
&lt;li&gt;WordNet®(large lexical database of English)&lt;/li&gt;
&lt;li&gt;Thesaurus(索引典)&lt;/li&gt;
&lt;li&gt;同義詞詞林&lt;/li&gt;
&lt;li&gt;廣義知網中文詞知識庫(E-HowNet)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9mcmFtZW5ldC5pY3NpLmJlcmtlbGV5LmVkdS9mbmRydXBhbC9hYm91dA==&#34;&gt;FrameNet&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;FrameNet&#34;&gt;&lt;a href=&#34;#FrameNet&#34; class=&#34;headerlink&#34; title=&#34;FrameNet&#34;&gt;&lt;/a&gt;FrameNet&lt;/h4&gt;&lt;p&gt;A dictionary of more than 10,000 word senses, 170,000 manually annotated sentences&lt;/p&gt;
&lt;p&gt;Frame Semantics(Charles J. Fillmore)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the meanings of most words can be more understood by semantic frame&lt;/li&gt;
&lt;li&gt;Including description of a type of event, relation, or entity and the participants in it&lt;/li&gt;
&lt;li&gt;Example: &lt;code&gt;apply_heat&lt;/code&gt; frame&lt;ul&gt;
&lt;li&gt;When one of these words appear, this frame will be applied&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Fry(炸)&lt;/code&gt;, &lt;code&gt;bake(烘)&lt;/code&gt;, &lt;code&gt;boil(煮)&lt;/code&gt;, a`nd broil(烤)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Frame elements: Cook, Food, Heating_instrument and Container&lt;ul&gt;
&lt;li&gt;a person doing the cooking (Cook)&lt;/li&gt;
&lt;li&gt;the food that is to be cooked (Food)&lt;/li&gt;
&lt;li&gt;something to hold the food while cooking (Container)&lt;/li&gt;
&lt;li&gt;a source of heat (Heating_instrument)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;[&lt;code&gt;Cook&lt;/code&gt; the boys] … GRILL [&lt;code&gt;Food&lt;/code&gt; fish] [&lt;code&gt;Heating_instrument&lt;/code&gt; on an open fire]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Pragmatics-how-sentences-relate-to-each-other&#34;&gt;&lt;a href=&#34;#Pragmatics-how-sentences-relate-to-each-other&#34; class=&#34;headerlink&#34; title=&#34;Pragmatics(how sentences relate to each other)&#34;&gt;&lt;/a&gt;Pragmatics(how sentences relate to each other)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;explain what the speaker really expressed&lt;/li&gt;
&lt;li&gt;Understand the scope of &lt;ul&gt;
&lt;li&gt;quantifiers&lt;/li&gt;
&lt;li&gt;speech acts&lt;/li&gt;
&lt;li&gt;discourse analysis&lt;/li&gt;
&lt;li&gt;anaphoric relations(首語重複)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Anaphora(首語重複) and Coreference(指代)&lt;ul&gt;
&lt;li&gt;張三是老師,他教學很認真,同時,他也是一個好爸爸。&lt;/li&gt;
&lt;li&gt;Type/Instance: “老師”/“張三”, “一個好爸爸”/“張三”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;crucial to &lt;strong&gt;information extraction&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Dialogue Tagging &lt;img data-src=&#34;/img/NLP/dialogue_tag.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Discourse-Analysis&#34;&gt;&lt;a href=&#34;#Discourse-Analysis&#34; class=&#34;headerlink&#34; title=&#34;Discourse Analysis&#34;&gt;&lt;/a&gt;Discourse Analysis&lt;/h3&gt;&lt;p&gt;Example  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1a: 佛羅倫斯哪個博物館在1993年的爆炸事件中受到破壞？&lt;/li&gt;
&lt;li&gt;1b: 這個事件哪一天發生？&lt;ul&gt;
&lt;li&gt;問句1b「這個事件」，指的是問句1a「1993年的爆炸事件」&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Summary&#34;&gt;&lt;a href=&#34;#Summary&#34; class=&#34;headerlink&#34; title=&#34;Summary&#34;&gt;&lt;/a&gt;Summary&lt;/h3&gt;&lt;p&gt;From &lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3d3dy5paXMuc2luaWNhLmVkdS50dy9wYWdlL2V2ZW50cy9GSUxFLzEyMDMxMzEwMTA3U2xpZGVzLnBkZg==&#34;&gt;The Three (and a Half) Futures of NLP&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NLP is &lt;strong&gt;Notation Transformation&lt;/strong&gt;(e.g. English → Chinese), with some information(POS, syntatic, senmatic…) added&lt;/li&gt;
&lt;li&gt;Much NLP is engineering&lt;ul&gt;
&lt;li&gt;select and tuning learning performance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge is crucial in language-related research areas, but providing a large scaleknowledge base is difficult and costly&lt;ul&gt;
&lt;li&gt;Knowledge Base&lt;ul&gt;
&lt;li&gt;WordNet&lt;/li&gt;
&lt;li&gt;FrameNet&lt;/li&gt;
&lt;li&gt;Wikipedia&lt;/li&gt;
&lt;li&gt;Dbpedia&lt;/li&gt;
&lt;li&gt;Freebase&lt;/li&gt;
&lt;li&gt;Siri&lt;/li&gt;
&lt;li&gt;Google Knowledge Graph&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hierarchy of transformations(由深至淺)&lt;ul&gt;
&lt;li&gt;pragmatics, writing style&lt;ul&gt;
&lt;li&gt;deeper semantics, discourse&lt;ul&gt;
&lt;li&gt;shallow semantics, co-reference&lt;ul&gt;
&lt;li&gt;syntax, POS(part-of-speech)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分析時由淺至深&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Analysis&#34;&gt;&lt;a href=&#34;#Analysis&#34; class=&#34;headerlink&#34; title=&#34;Analysis&#34;&gt;&lt;/a&gt;Analysis&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;/img/NLP/layer.png&#34; alt=&#34;Layer&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/l1.png&#34; alt=&#34;L1&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/l2.png&#34; alt=&#34;L2&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/l3.png&#34; alt=&#34;L3&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/l4.png&#34; alt=&#34;L4&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;NLP-progress-by-now&#34;&gt;&lt;a href=&#34;#NLP-progress-by-now&#34; class=&#34;headerlink&#34; title=&#34;NLP progress by now&#34;&gt;&lt;/a&gt;NLP progress by now&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;/img/NLP/sub.png&#34; alt=&#34;NLP subclass&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/dowell.png&#34; alt=&#34;NLP do today&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/cantdo.png&#34; alt=&#34;NLP can&amp;#39;t do today&#34;&gt;  &lt;/p&gt;
&lt;h2 id=&#34;Chap03-Collocations-搭配詞&#34;&gt;&lt;a href=&#34;#Chap03-Collocations-搭配詞&#34; class=&#34;headerlink&#34; title=&#34;Chap03 Collocations(搭配詞)&#34;&gt;&lt;/a&gt;Chap03 Collocations(搭配詞)&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;多個單字組合成一個有意義的語詞，其意義無法從各個單字中推得&lt;ul&gt;
&lt;li&gt;e.g. black market&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Subclasses of Collocations&lt;ul&gt;
&lt;li&gt;compound nouns&lt;ul&gt;
&lt;li&gt;telephone box and post office&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;idioms&lt;ul&gt;
&lt;li&gt;kick the bucket(氣絕)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Light verbs(輕動詞)&lt;ul&gt;
&lt;li&gt;動詞失去其意義，需要和其他有實質意義的詞作搭配&lt;/li&gt;
&lt;li&gt;e.g. The man took a walk(walk, not take) vs The man took a radio(take)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Verb particle constructions(語助詞) or Phrasal Verbs(詞組動詞, 短語動詞, V + 介系詞)&lt;ul&gt;
&lt;li&gt;take in = deceive, look sth. up&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;proper names&lt;ul&gt;
&lt;li&gt;San Francisco&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Terminology(專有名詞)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Classification&lt;ul&gt;
&lt;li&gt;Fixed expressions&lt;ul&gt;
&lt;li&gt;in short (O)&lt;/li&gt;
&lt;li&gt;in shorter or in very short(X)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semi-fixed expressions(可用變化形)&lt;ul&gt;
&lt;li&gt;non-decomposable idioms&lt;ul&gt;
&lt;li&gt;kick the bucket (O)&lt;/li&gt;
&lt;li&gt;he kicks the bucket(O)&lt;/li&gt;
&lt;li&gt;the bucket was kicked (X)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;compound nominals&lt;ul&gt;
&lt;li&gt;car park, car parks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Proper names&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Syntactically-Flexible Expressions&lt;ul&gt;
&lt;li&gt;decomposable idioms&lt;ul&gt;
&lt;li&gt;let the cat out of the bag&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;verb-particle constructions&lt;/li&gt;
&lt;li&gt;light verbs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Institutionalized Phrases (習慣用法)&lt;ul&gt;
&lt;li&gt;salt and pepper(○) pepper and salt(×)&lt;/li&gt;
&lt;li&gt;traffic light&lt;/li&gt;
&lt;li&gt;kindle excitement(點燃激情)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Collocation-detection&#34;&gt;&lt;a href=&#34;#Collocation-detection&#34; class=&#34;headerlink&#34; title=&#34;Collocation detection&#34;&gt;&lt;/a&gt;Collocation detection&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;by Frequency&lt;/li&gt;
&lt;li&gt;by Mean and Variance of the distance between focal word (焦點詞) and collocating word(搭配詞)&lt;/li&gt;
&lt;li&gt;Hypothesis Testing&lt;/li&gt;
&lt;li&gt;Mutual Information&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;By-Frequency&#34;&gt;&lt;a href=&#34;#By-Frequency&#34; class=&#34;headerlink&#34; title=&#34;By Frequency&#34;&gt;&lt;/a&gt;By Frequency&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;找出現機率大的bigrams&lt;ul&gt;
&lt;li&gt;not always significant&lt;/li&gt;
&lt;li&gt;篩選可能為組合詞的詞性組合(Ex. adj+N) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The collocations found &lt;img data-src=&#34;/img/NLP/freandtag.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;What if a word have two possible collocations?(strong force, powerful force) &lt;img data-src=&#34;/img/NLP/frecomp.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;By-Mean-and-Variance-of-the-distance-between-focal-word-焦點詞-and-collocating-word-搭配詞&#34;&gt;&lt;a href=&#34;#By-Mean-and-Variance-of-the-distance-between-focal-word-焦點詞-and-collocating-word-搭配詞&#34; class=&#34;headerlink&#34; title=&#34;By Mean and Variance of the distance between focal word(焦點詞) and collocating word(搭配詞)&#34;&gt;&lt;/a&gt;By Mean and Variance of the distance between focal word(焦點詞) and collocating word(搭配詞)&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;many collocations consist of more flexible relationships&lt;ul&gt;
&lt;li&gt;frequency is not suitable&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Define a collocational window&lt;ol&gt;
&lt;li&gt;e.g., 3-4 words before/after&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;assemble every word pair as a bigram&lt;ol&gt;
&lt;li&gt;e.g., A B C D → AB, AC, AD, BC, BD, CD&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;computes the mean and variance of the offset between the two words&lt;ol&gt;
&lt;li&gt;變異數愈低，代表兩個字之間的位置關聯愈固定 &lt;img data-src=&#34;/img/NLP/meanvar.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;z-score $z = {freq - \mu \over \sigma}$: the strength of a word pair&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Hypothesis-Testing-假設檢定&#34;&gt;&lt;a href=&#34;#Hypothesis-Testing-假設檢定&#34; class=&#34;headerlink&#34; title=&#34;Hypothesis Testing(假設檢定)&#34;&gt;&lt;/a&gt;Hypothesis Testing(假設檢定)&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Even high frequency and low variance can be accidental&lt;/li&gt;
&lt;li&gt;null hypothesis(虛無假設, H0) &lt;ul&gt;
&lt;li&gt;設 w1 and w2 is completely independent → w1 and w2 不是搭配詞&lt;ul&gt;
&lt;li&gt;P(w1w2) = P(w1)P(w2) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;假設H0為真，計算這兩個字符合H0的機率P&lt;ul&gt;
&lt;li&gt;若P太低則否決H0(→ 是搭配詞)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two issues&lt;ul&gt;
&lt;li&gt;Look for particular patterns in the data&lt;/li&gt;
&lt;li&gt;How much data we have seen&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;種類包括：t檢驗，Z檢驗，卡方檢驗，F檢驗      &lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;t-test&#34;&gt;&lt;a href=&#34;#t-test&#34; class=&#34;headerlink&#34; title=&#34;t-test&#34;&gt;&lt;/a&gt;t-test&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Test whether &lt;strong&gt;distributions of two groups&lt;/strong&gt; are &lt;strong&gt;statistically different&lt;/strong&gt; or not&lt;ul&gt;
&lt;li&gt;H0 → (w1, w2) has no differnece with normal distribution&lt;/li&gt;
&lt;li&gt;considering &lt;strong&gt;variance&lt;/strong&gt; of the data &lt;img data-src=&#34;/img/NLP/ttest.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;formula &lt;img data-src=&#34;/img/NLP/ttest2.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/ttest3.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Calculate t by alpha level and degree of freedom&lt;ul&gt;
&lt;li&gt;alpha level &lt;code&gt;α&lt;/code&gt;: confidence&lt;ul&gt;
&lt;li&gt;in normal distribution，α = 95%落在mean±1.96std之間, α = 99%落在mean±2.576std之間&lt;/li&gt;
&lt;li&gt;If t-value is larger than 2.576, we say the two groups &lt;strong&gt;are different&lt;/strong&gt; with 99% confidence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;degree of freedom: number of sample-1&lt;ul&gt;
&lt;li&gt;total = number of two groups-2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;t↑ → more difference → more possible to reject null hypothesis → more possible to be collocation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Example: “new” occurs 15,828 times, “companies” 4,675 times, “new companies” 8 times, total 14,307,668 tokens&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Null hypothesis: the occurrences of new and companies are independent(not collocation)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;H0 mean = P(new, companies) = P(new) x P(companies) = $\frac{15828 \times 4678}{14307668^2} = 3.615 \times 10^{-7}$&lt;/li&gt;
&lt;li&gt;H0 var = p(1-p) ~= p when p is small&lt;/li&gt;
&lt;li&gt;tvalue = $\frac{5.591 \times 10^7 - 3.615\times 10^7}{\sqrt{\frac{5.591 \times 10^7}{14307668}}} = 0.999932$&lt;/li&gt;
&lt;li&gt;0.999932 &amp;lt; 2.576, we cannot reject the null hypothesis&lt;ul&gt;
&lt;li&gt;new company are not collocation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the above words are possible collocations &lt;img data-src=&#34;/img/NLP/ttest4.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!--???????--&gt;
&lt;p&gt;Hypothesis testing of differences    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;useful for lexicography &lt;ul&gt;
&lt;li&gt;which word(strong, powerful) is suitable to modify “computer”?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;T-test can be used for &lt;strong&gt;comparison of the means of two normal populations&lt;/strong&gt; &lt;ul&gt;
&lt;li&gt;H0 is that the average difference is 0 (u = 0)&lt;/li&gt;
&lt;li&gt;v1 and v2 are the words we are comparing (e.g., powerful and strong), and w is the collocate of interest(e.g., computers)&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/ttest5.png&#34; alt=&#34;&#34;&gt;)&lt;img data-src=&#34;/img/NLP/ttest6.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;Chi-Square-test&#34;&gt;&lt;a href=&#34;#Chi-Square-test&#34; class=&#34;headerlink&#34; title=&#34;Chi-Square test&#34;&gt;&lt;/a&gt;Chi-Square test&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;T-test assumes that probabilities are normally distributed&lt;ul&gt;
&lt;li&gt;not really&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Chi-Square: compare &lt;strong&gt;observed frequencies&lt;/strong&gt; with &lt;strong&gt;expected frequencies&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;If &lt;strong&gt;difference between observed and expected frequencies&lt;/strong&gt; is large, we can reject H0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example&lt;ul&gt;
&lt;li&gt;expected frequency of “new companies”: $\frac{8+4667}{14307668} \times \frac{8+15820}{14307668} \times 14307668$ = 5.2 &lt;img data-src=&#34;/img/NLP/chi1.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;chi-square value = χ^2 &lt;img data-src=&#34;/img/NLP/chi3.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/chi2.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;When α=0.05, χ^2=3.841&lt;/li&gt;
&lt;li&gt;Because 1.55&amp;lt;3.841, we cannot reject the null hypothesis. new companies is not a good candidate for a collocation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Comparison with T-test&lt;ul&gt;
&lt;li&gt;The 20 bigrams with the highest t scores in the test corpus are also the 20 bigrams with the highest χ^2 scores&lt;/li&gt;
&lt;li&gt;χ^2 is appropriate for large probabilities(t-test is not because of normality assumption)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Application: Translation&lt;ul&gt;
&lt;li&gt;find similarity of word pairs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Likelihood-Ratios&#34;&gt;&lt;a href=&#34;#Likelihood-Ratios&#34; class=&#34;headerlink&#34; title=&#34;Likelihood Ratios&#34;&gt;&lt;/a&gt;Likelihood Ratios&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Advantage compared with Chi-Square test  &lt;ul&gt;
&lt;li&gt;more appropriate for sparse data&lt;/li&gt;
&lt;li&gt;easier to interpret&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Likelihood Ratios within single corpus  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;examine two hypothesis&lt;ul&gt;
&lt;li&gt;H1: occurrence of w2 is independent of the previous occurrence of w1(null hypothesis)&lt;/li&gt;
&lt;li&gt;H2: occurrence of w2 is dependent of the previous occurrence of w1 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;maximum likelihood estimate &lt;img data-src=&#34;/img/NLP/like1.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;using binomial distribution&lt;ul&gt;
&lt;li&gt;$b(k;n, x) = \binom nk x^k \times (1-x)^{n-k}$&lt;/li&gt;
&lt;li&gt;only different at probability&lt;ul&gt;
&lt;li&gt;$L(H_1) = b(c_{12};c_1, p)b(c_2-c_{12}; N-c_1, p)$&lt;/li&gt;
&lt;li&gt;$L(H_2) = b(c_{12};c_1, p_1)b(c_2-c_{12}; N-c_1, p_2)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/likew.png&#34; alt=&#34;likely probability&#34;&gt;&lt;/li&gt;
&lt;li&gt;log of likelihood ratio λ &lt;img data-src=&#34;/img/NLP/like2.png&#34; alt=&#34;log likelihood ratio&#34;&gt;&lt;/li&gt;
&lt;li&gt;use D = -2logλ to examine the significance of two words, which can asymptotically(漸近) chi-square distributed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Likelihood Ratios between two or more corpora   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;useful for the discovery of &lt;strong&gt;subject-specific collocations&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Mutual-Information&#34;&gt;&lt;a href=&#34;#Mutual-Information&#34; class=&#34;headerlink&#34; title=&#34;Mutual Information&#34;&gt;&lt;/a&gt;Mutual Information&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;measure of &lt;strong&gt;how much one word tells us about the other&lt;/strong&gt;(information theory)   &lt;/li&gt;
&lt;li&gt;pointwise mutual information(PMI) &lt;img data-src=&#34;/img/NLP/mutual.png&#34; alt=&#34;PMI formula&#34;&gt;&lt;ul&gt;
&lt;li&gt;MI是在獲得一個隨機變數的資訊之後，觀察另一個隨機變數所獲得的「資訊量」（單位通常為位元）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;mutual information = Expection(PMI) &lt;img data-src=&#34;/img/NLP/newMI.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;works bad in sparse environments&lt;ul&gt;
&lt;li&gt;As the perfectly dependent bigrams get rarer, their mutual information increases → &lt;strong&gt;bad measure of dependence&lt;/strong&gt; &lt;img data-src=&#34;/img/NLP/pmi-depend.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;good measure of independence&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;when perfect independence, I(x, y) = 0&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/pmi-independ.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;New formula: $C(w1w2)I(w1w2)$&lt;ul&gt;
&lt;li&gt;With MI, bigrams composed of low-frequency words will receive a higher score than bigrams composed of high-frequency words&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chain rule for entropy   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H(X,Y) = H(Y|X) + H(X) = H(X|Y) + H(Y)$&lt;/li&gt;
&lt;li&gt;Conditional entropy $H(Y|X)$ expresses how much &lt;strong&gt;extra information&lt;/strong&gt; you still need to supply on average to communicate Y when X is known &lt;img data-src=&#34;/img/NLP/conditional.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;$H(X)-H(X|Y) = H(Y)-H(Y|X)$&lt;ul&gt;
&lt;li&gt;This difference is called the &lt;strong&gt;mutual information between X and Y&lt;/strong&gt;(X, Y共同擁有的information)&lt;/li&gt;
&lt;li&gt;MI is not similar to chi-square &lt;img data-src=&#34;/img/NLP/wrongMI.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Entropy  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entropy: uncertainty of a variable &lt;img data-src=&#34;/img/NLP/entropy1.png&#34; alt=&#34;&#34;&gt;  &lt;/li&gt;
&lt;li&gt;Incorrect model’s cross entropy is larger than correct model’s &lt;img data-src=&#34;/img/NLP/entropy2.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;正確model和猜測model的差別：P(X)logP(X) ↔ P(X)logPM(X)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Entropy Rate: Per-word entropy(= sentence entropy / N) &lt;img data-src=&#34;/img/NLP/entropy_rate.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Cross Entropy: &lt;strong&gt;average informaton&lt;/strong&gt; needed to &lt;strong&gt;identify an event drawn from the set&lt;/strong&gt; between two probability distributions&lt;ul&gt;
&lt;li&gt;交叉熵的意義是用該模型對文本識別的難度，或者從壓縮的角度來看，每個詞平均要用幾個位來編碼&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/entropy_cross.png&#34; alt=&#34;&#34;&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Joint entropy H(X, Y): average information needed to &lt;strong&gt;specify both values&lt;/strong&gt; &lt;img data-src=&#34;/img/NLP/joint.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Case-Study&#34;&gt;&lt;a href=&#34;#Case-Study&#34; class=&#34;headerlink&#34; title=&#34;Case Study&#34;&gt;&lt;/a&gt;Case Study&lt;/h3&gt;&lt;p&gt;Emotion Analysis  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non-verbal Emotional Expressions&lt;/li&gt;
&lt;li&gt;text (raw) and emoticons(表情符號) (tag) form collection&lt;/li&gt;
&lt;li&gt;appearance of an emoticon is a good emotion indicator to sentences&lt;/li&gt;
&lt;li&gt;check the dependency of each word in sentences&lt;/li&gt;
&lt;li&gt;Evaluation&lt;ul&gt;
&lt;li&gt;Use top 200 lexiconentries as features&lt;/li&gt;
&lt;li&gt;Tag={Positive, Negative}&lt;/li&gt;
&lt;li&gt;LIBSVM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap04-N-gram-Model&#34;&gt;&lt;a href=&#34;#Chap04-N-gram-Model&#34; class=&#34;headerlink&#34; title=&#34;Chap04 N-gram Model&#34;&gt;&lt;/a&gt;Chap04 N-gram Model&lt;/h2&gt;&lt;p&gt;N-grams are token sequences of length N&lt;/p&gt;
&lt;p&gt;applications   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Automatic speech recognition&lt;/li&gt;
&lt;li&gt;Author Identification&lt;/li&gt;
&lt;li&gt;Spelling correction&lt;/li&gt;
&lt;li&gt;Grammatical Error Diagnosis&lt;/li&gt;
&lt;li&gt;Machine translation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Counting&#34;&gt;&lt;a href=&#34;#Counting&#34; class=&#34;headerlink&#34; title=&#34;Counting&#34;&gt;&lt;/a&gt;Counting&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Example: &lt;em&gt;I do uh main-mainly business data processing&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;Should we count “uh”(pause) as tokens?&lt;/li&gt;
&lt;li&gt;What about the repetition of “mainly”? Should such do-overs count twice or just once?(重複)&lt;/li&gt;
&lt;li&gt;The answers depend on the application&lt;ul&gt;
&lt;li&gt;“uh” is not needed for query &lt;/li&gt;
&lt;li&gt;“uh” is very useful in dialog management&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Corpora: Google Web Crawl&lt;ul&gt;
&lt;li&gt;1,024,908,267,229 English tokens&lt;/li&gt;
&lt;li&gt;13,588,391 wordform types&lt;/li&gt;
&lt;li&gt;even large dictionaries of English have only around 500k types. Why so many here?&lt;ul&gt;
&lt;li&gt;Numbers&lt;/li&gt;
&lt;li&gt;Misspellings&lt;/li&gt;
&lt;li&gt;Names&lt;/li&gt;
&lt;li&gt;Acronyms(縮寫)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Language-model&#34;&gt;&lt;a href=&#34;#Language-model&#34; class=&#34;headerlink&#34; title=&#34;Language model&#34;&gt;&lt;/a&gt;Language model&lt;/h3&gt;&lt;p&gt;Language models assign a probability to a word sequence&lt;br&gt;Ex. &lt;code&gt;P(the mythical unicorn) = P(the) * P(mythical | the) * P(unicorn | the mythical)&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Markov assumption: the probability of a word depends only on &lt;strong&gt;limited previous words&lt;/strong&gt;     &lt;ul&gt;
&lt;li&gt;Generalization: n previous words, like bigram, trigrams, 4-grams……&lt;/li&gt;
&lt;li&gt;As we increase the value of N, the accuracy of model increases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;N-Gram probabilities come from a training corpus&lt;br&gt;overly narrow corpus: probabilities don’t generalize&lt;br&gt;overly general corpus: probabilities don’t reflect task or domain  &lt;/p&gt;
&lt;p&gt;maximum likelihood estimate  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;maximizes the probability of the training set T given the model M&lt;/li&gt;
&lt;li&gt;Suppose the word “Chinese” occurs 400 times in a corpus&lt;ul&gt;
&lt;li&gt;MLE estimate is 400/1000000 = .004&lt;/li&gt;
&lt;li&gt;makes it most likely that “Chinese” will occur 400 times in a million word corpus&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;P([s] I want englishfood [s]) = P(I|[s]) x P(want|I) x P(english|want) x P(food|english) x P([s]|food) = 0.000031$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Usage&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;capture some knowledge about language&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;World Knowledge&lt;ul&gt;
&lt;li&gt;P(english food|want) = .0011&lt;/li&gt;
&lt;li&gt;P(chinese food|want) = .0065&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;syntax&lt;ul&gt;
&lt;li&gt;P(to|want) = .66&lt;/li&gt;
&lt;li&gt;P(eat| to) = .28&lt;/li&gt;
&lt;li&gt;P(food| to) = 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;discourse&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(i|&amp;lt;s&amp;gt;)&lt;/code&gt; = .25&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Shannon’s Method: use language model to generate random sentences&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shakespeare as a Corpus  &lt;ul&gt;
&lt;li&gt;99.96% of the possible bigrams were never seen (have zero entries in the table)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This is the biggest problem in language modeling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Evaluating-N-Gram-Models&#34;&gt;&lt;a href=&#34;#Evaluating-N-Gram-Models&#34; class=&#34;headerlink&#34; title=&#34;Evaluating N-Gram Models&#34;&gt;&lt;/a&gt;Evaluating N-Gram Models&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Extrinsic(外在的) evaluation&lt;ul&gt;
&lt;li&gt;Compare performance of the application within two models&lt;/li&gt;
&lt;li&gt;time-consuming&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Intrinsic evaluation&lt;ul&gt;
&lt;li&gt;perplexity&lt;ul&gt;
&lt;li&gt;But get poor approximation unless the test data looks just like the training data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;not sufficient to publish&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Standard Method&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train → Test &lt;/li&gt;
&lt;li&gt;A dataset which is different from our training set, but both drawn from the same source&lt;/li&gt;
&lt;li&gt;use evaluation metric(Ex. perplexity)&lt;/li&gt;
&lt;li&gt;Example &lt;ul&gt;
&lt;li&gt;Create a fixed lexicon L, of size V&lt;ul&gt;
&lt;li&gt;At text normalization phase, &lt;strong&gt;any training word not in L changed to UNK&lt;/strong&gt;(unknown word token)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;count UNK like a normal word&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;When testing, also use UNK counts for any word not in training&lt;/li&gt;
&lt;li&gt;The best language model is one that best predicts an unseen test set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;perplexity-複雜度&#34;&gt;&lt;a href=&#34;#perplexity-複雜度&#34; class=&#34;headerlink&#34; title=&#34;perplexity(複雜度)&#34;&gt;&lt;/a&gt;perplexity(複雜度)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Definition  &lt;ul&gt;
&lt;li&gt;notion of surprise&lt;ul&gt;
&lt;li&gt;The more surprised the model is, the lower probability it assigned to the test set&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Minimizing perplexity is the same as maximizing probability&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;probability of a test set, as normalized by the number of words &lt;img data-src=&#34;/img/NLP/perplexity.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;物理意義是單詞的編碼大小&lt;ul&gt;
&lt;li&gt;如果在某個測試語句上，語言模型的perplexity值為2^190，說明該句子的編碼需要190bits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;relate to entropy&lt;ul&gt;
&lt;li&gt;Perplexity(p, q) = $2^{H(p,q)}$   &lt;/li&gt;
&lt;li&gt;p is the test sample distribution, and q is the distribution of language model&lt;/li&gt;
&lt;li&gt;do everything in log space to avoid underflow and calculate faster&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;word-entropy&#34;&gt;&lt;a href=&#34;#word-entropy&#34; class=&#34;headerlink&#34; title=&#34;word entropy&#34;&gt;&lt;/a&gt;word entropy&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;word entropy for English&lt;ul&gt;
&lt;li&gt;11.82 bits per word [Shannon, 1951]&lt;/li&gt;
&lt;li&gt;9.8 bits per word [Grignetti, 1964]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;word entropy in medical language&lt;ul&gt;
&lt;li&gt;11.15 bits per word&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap05-Statistical-Inference&#34;&gt;&lt;a href=&#34;#Chap05-Statistical-Inference&#34; class=&#34;headerlink&#34; title=&#34;Chap05 Statistical Inference&#34;&gt;&lt;/a&gt;Chap05 Statistical Inference&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Statistical Inference：&lt;strong&gt;taking some data&lt;/strong&gt; (generated by unknown distribution) and then &lt;strong&gt;making some inferences(推理，推測)&lt;/strong&gt; about this distribution&lt;/li&gt;
&lt;li&gt;three issues&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dividing the training data into equivalence classes&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finding a good statistical estimator for each equivalence class&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Combining multiple estimators&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Form-Equivalence-Class&#34;&gt;&lt;a href=&#34;#Form-Equivalence-Class&#34; class=&#34;headerlink&#34; title=&#34;Form Equivalence Class&#34;&gt;&lt;/a&gt;Form Equivalence Class&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Classification Problem&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;predict target feature&lt;/strong&gt; based on various &lt;strong&gt;classificatory features&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;reliability v.s. discrimination&lt;ul&gt;
&lt;li&gt;The more classes, the more discrimination, but estimation feature is not reliable&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Independent assumption&lt;ul&gt;
&lt;li&gt;assume data is nearly independent&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Statistical Language Modeling&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/smodel.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Language Model: P(W)&lt;/li&gt;
&lt;li&gt;LM does not depend on acoustics&lt;ul&gt;
&lt;li&gt;the acoutstics probability is constant(calculated by data)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;n-gram model&lt;ul&gt;
&lt;li&gt;assume equivalence classes are previous n-1 words&lt;/li&gt;
&lt;li&gt;Markov Assumption: Only the prior n-1 local context affects the next entry&lt;ul&gt;
&lt;li&gt;(n-1)th Markov Model or n-gram&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Building n-grams&lt;/strong&gt;&lt;ol&gt;
&lt;li&gt;Remove punctuation(標點) and normalize text&lt;/li&gt;
&lt;li&gt;Map out-of-vocabulary words to unknown symbol(UNK)&lt;/li&gt;
&lt;li&gt;Estimate conditional probabilities by joint probabilities&lt;ul&gt;
&lt;li&gt;P(n | n-2, n-1) = P(n-2, n-1, n) / P(n-2, n-1)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Finding-statistical-estimator&#34;&gt;&lt;a href=&#34;#Finding-statistical-estimator&#34; class=&#34;headerlink&#34; title=&#34;Finding statistical estimator&#34;&gt;&lt;/a&gt;Finding statistical estimator&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Goal: derive &lt;strong&gt;probability estimate of target feature&lt;/strong&gt; based on observed data&lt;/li&gt;
&lt;li&gt;Running Example&lt;ul&gt;
&lt;li&gt;From n-gram data P(w1,..,wn), predict P(wn|w1,..,wn-1)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solutions&lt;ul&gt;
&lt;li&gt;Maximum Likelihood Estimation&lt;/li&gt;
&lt;li&gt;Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws&lt;/li&gt;
&lt;li&gt;Held Out Estimation&lt;/li&gt;
&lt;li&gt;Cross-Validation&lt;/li&gt;
&lt;li&gt;Good-Turing Estimation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model combination&lt;ul&gt;
&lt;li&gt;Combine models (unigram, bigram, trigram, …) to use the most precise model available&lt;/li&gt;
&lt;li&gt;interpolation(內插) and back-off(後退)&lt;/li&gt;
&lt;li&gt;use higher order models when model has enough data&lt;/li&gt;
&lt;li&gt;back off to lower order models when there isn’t enough data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Terminology  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ex. &lt;code&gt;[s] a b a b a&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;N = 5 (&lt;code&gt;[s]a,ab,ba,ab,ba&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;B = 3 (&lt;code&gt;[s]a,ab,ba&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;C(w1, w2…) = 某N-gram(Ex. ab)出現次數&lt;/li&gt;
&lt;li&gt;r =  某N-gram出現頻率&lt;/li&gt;
&lt;li&gt;Nr = 有幾個「出現r次的N-gram」&lt;/li&gt;
&lt;li&gt;Tr = 出現r次的N-gram，在test data出現的總次數&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;1-Maximum-Likelihood-Estimation&#34;&gt;&lt;a href=&#34;#1-Maximum-Likelihood-Estimation&#34; class=&#34;headerlink&#34; title=&#34;(1) Maximum Likelihood Estimation&#34;&gt;&lt;/a&gt;(1) Maximum Likelihood Estimation&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;usually unsuitable for NLP &lt;ul&gt;
&lt;li&gt;sparseness of the data(a lot of word sequences with zero probabilities)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use Discounting or Smoothing technique to improve&lt;ul&gt;
&lt;li&gt;Smoothing&lt;ul&gt;
&lt;li&gt;Smoothing is like Robin Hood: Steal from the rich and give to the poor&lt;/li&gt;
&lt;li&gt;no word sequences has 0 probability &lt;img data-src=&#34;/img/NLP/fBBrh6P.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Discounting&lt;ul&gt;
&lt;li&gt;assign some probability to unseen events&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-Laplace’s-Lidstone’s-and-Jeffreys-Perks’-Laws&#34;&gt;&lt;a href=&#34;#2-Laplace’s-Lidstone’s-and-Jeffreys-Perks’-Laws&#34; class=&#34;headerlink&#34; title=&#34;(2) Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws&#34;&gt;&lt;/a&gt;(2) Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Laplace: add 1 to every count &lt;ul&gt;
&lt;li&gt;gives far too much probabilities to unseen events&lt;/li&gt;
&lt;li&gt;Usage: In domains where the number of zeros isn’t so huge&lt;ul&gt;
&lt;li&gt;pilot studies&lt;/li&gt;
&lt;li&gt;document classification&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lidstone and Jeffreys-Perks: add a smaller value λ &amp;lt; 1&lt;ul&gt;
&lt;li&gt;B:number of bins &lt;img data-src=&#34;/img/NLP/lidstone.png&#34; alt=&#34;lid&#34;&gt;&lt;/li&gt;
&lt;li&gt;Expected Likelihood Estimation (ELE)(Jeffreys-Perks Law)&lt;ul&gt;
&lt;li&gt;λ=1/2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3-Held-Out-Estimation&#34;&gt;&lt;a href=&#34;#3-Held-Out-Estimation&#34; class=&#34;headerlink&#34; title=&#34;(3) Held Out Estimation&#34;&gt;&lt;/a&gt;(3) Held Out Estimation&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;compute frequencies in training data and held out data&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/heldout.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;Tr / Nr = Average frequency of training frequency r N-grams&lt;ul&gt;
&lt;li&gt;estimate frequency(value for validation)&lt;/li&gt;
&lt;li&gt;計算出現在training corpus r次的bigrams，在held-out corpus出現的次數稱為Tr。 因為這種bigrams有Nr個，因此平均為Tr / Nr&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Validation&lt;ul&gt;
&lt;li&gt;if the probabilities estimated on training data are close to those on held-out data, it’s a good language model&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://gitqwerty777.github.io/MLfoundation2/#chap15-validation&#34;&gt;參考資料–validation in machine learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Prevent Overtraining(overfit)&lt;ul&gt;
&lt;li&gt;test on different data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Training portion and testing portion (5-10% of total data)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Held out data (validation data)&lt;ul&gt;
&lt;li&gt;available training data: real training data(90%) + held out data(10%)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Instead of presenting a single performance figure, testing result on each smaller sample&lt;ul&gt;
&lt;li&gt;Using t-test to reject the possibility of an accidental difference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-Cross-Validation&#34;&gt;&lt;a href=&#34;#4-Cross-Validation&#34; class=&#34;headerlink&#34; title=&#34;(4) Cross-Validation&#34;&gt;&lt;/a&gt;(4) Cross-Validation&lt;/h4&gt;&lt;p&gt;If data is not enough, use each part of the data both as training data and held out data  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deleted Estimation&lt;ul&gt;
&lt;li&gt;$N_r^a$ = number of n-grams occurring r times in the &lt;strong&gt;a th part&lt;/strong&gt; of the training data&lt;/li&gt;
&lt;li&gt;$T_r^{ab}$ = number of occurs in part b of 「bigrams occurs r times in part a」&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/deleted_estimate.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Split the training data into K sections&lt;/li&gt;
&lt;li&gt;For each section k: hold-out section k and compute counts from remaining K-1 sections; compute Tr(k) &lt;/li&gt;
&lt;li&gt;Estimate probabilities by averaging over all sections&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;estimate frequency of deleted estimation &lt;img data-src=&#34;/img/NLP/del-estimate.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;5-Good-Turing-Estimation&#34;&gt;&lt;a href=&#34;#5-Good-Turing-Estimation&#34; class=&#34;headerlink&#34; title=&#34;(5) Good-Turing Estimation&#34;&gt;&lt;/a&gt;(5) Good-Turing Estimation&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;用出現一次的來預測沒出現過的&lt;/li&gt;
&lt;li&gt;若出現次數&amp;gt;k，不變，否則套用公式&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/goodturing.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;renormalize to sum = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simple Good-Turing&lt;ul&gt;
&lt;li&gt;replace any zeros in the sequence by linear regression: &lt;code&gt;log(Nc) = a+blog(c)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;after good-turing &lt;img data-src=&#34;/img/NLP/gttable.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;explaination from stanford NLP course   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;when use leave-one-out validation, the possibilities of unseen validation data is $\frac{N_1}{N}$(when thing-saw-once is the validation data), the possibilities of validation data have been seen K times is $\frac{(k+1)N_{k+1}}{N}$ &lt;/li&gt;
&lt;li&gt;Josh Goodman’s intuition: assume You are fishing, and caught 10 carp,3 perch,2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish&lt;ul&gt;
&lt;li&gt;P(unseen) = N1/N0 = N1/N = 3/18&lt;/li&gt;
&lt;li&gt;C(trout) = $2 \times N_2/N_1$ = $2 \times (1/3)$ = 2/3&lt;ul&gt;
&lt;li&gt;P(trout) = 2/3 / 18 = 1/27&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;for large k, often get zero estimate, so do not change the count&lt;ul&gt;
&lt;li&gt;C(the) = 200000, C(a) = 190000, $C*(the) = (200001)N_{200001} / N_{200000} = 0 (because N_{200001} = 0)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;6-Absolute-Discounting&#34;&gt;&lt;a href=&#34;#6-Absolute-Discounting&#34; class=&#34;headerlink&#34; title=&#34;(6) Absolute Discounting&#34;&gt;&lt;/a&gt;(6) Absolute Discounting&lt;/h4&gt;&lt;p&gt;從所有非零N-gram中拿出λ，平均分配給所有未出現過的N-gram  &lt;/p&gt;
&lt;h3 id=&#34;Combining-Estimator&#34;&gt;&lt;a href=&#34;#Combining-Estimator&#34; class=&#34;headerlink&#34; title=&#34;Combining Estimator&#34;&gt;&lt;/a&gt;Combining Estimator&lt;/h3&gt;&lt;p&gt;Combination Methods   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple Linear Interpolation(內插)(finite mixture models)&lt;ul&gt;
&lt;li&gt;Ex. trigram, bigram and unigram &lt;img data-src=&#34;/img/NLP/linearde.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;More generally, λ can be a function of (wn-2, wn-1, wn)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;use &lt;a href=&#34;#backward-forward&#34;&gt;Expectation-Maximization (EM) algorithm&lt;/a&gt; to get weights&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;General Linear Interpolation&lt;ul&gt;
&lt;li&gt;general form for a linear interpolation model&lt;/li&gt;
&lt;li&gt;weights are a function of the history &lt;img data-src=&#34;/img/NLP/gli.png&#34; alt=&#34;&#34;&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Katz’s Backing-Off&lt;ul&gt;
&lt;li&gt;choose proper order to train model (base on training data)&lt;ul&gt;
&lt;li&gt;If the n-gram appeared more than k times&lt;ul&gt;
&lt;li&gt;use MLE estimate and discount it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If the n-gram appeared k times or less&lt;ul&gt;
&lt;li&gt;use an estimate from &lt;strong&gt;lower-order n-gram&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;back-off probability &lt;img data-src=&#34;/img/NLP/pbo.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;$P_{Dis}(w_n|w_{n-2},w_{n-1})$ is specific discounted estimate. e.g., Good-Turing or Absolute Discounting &lt;/li&gt;
&lt;li&gt;unseen trigram is estimated by bigram and β &lt;img data-src=&#34;/img/NLP/bosmooth.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;β(wn-2, wn-1)&lt;/strong&gt; and &lt;strong&gt;α&lt;/strong&gt; are chosen so that sum of probabilities = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;more genereal form &lt;img data-src=&#34;/img/NLP/botable.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Most usual approach in large speech recognition: trigram language model, Good-Turing discounting, back-off combination&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;Chap06-Hidden-Markov-Models-HMM&#34;&gt;&lt;a href=&#34;#Chap06-Hidden-Markov-Models-HMM&#34; class=&#34;headerlink&#34; title=&#34;Chap06 Hidden Markov Models(HMM)&#34;&gt;&lt;/a&gt;Chap06 Hidden Markov Models(HMM)&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;statistical tools that are useful for NLP&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;part-of-speech-tagging&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt;We construct “Visible” Markov Models in training, but treat them as Hidden Markov Models when tagging new corpora  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;model a &lt;strong&gt;state sequence&lt;/strong&gt; (perhaps through time) &lt;strong&gt;of random variables&lt;/strong&gt; that have dependencies&lt;ul&gt;
&lt;li&gt;狀態(state)並不是直接可見的，但受狀態影響的某些變量(output symbol)則是可見的&lt;/li&gt;
&lt;li&gt;known value&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;output symbols(words)&lt;/strong&gt; 字詞&lt;/li&gt;
&lt;li&gt;probabilistic function of state relation 和state相關的機率函式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;unknown value&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;state(part-of-speech tags)&lt;/strong&gt; 目前的state，即POS tag&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;rely on 2 assumptions&lt;ul&gt;
&lt;li&gt;Let X=(X1, …, XT) be a sequence of random variables, X is markov chain if&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Limited Horizon&lt;ul&gt;
&lt;li&gt;a word’s tag only depends on &lt;strong&gt;previous&lt;/strong&gt; tag(state只受前一個state影響)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Time Invariant&lt;ul&gt;
&lt;li&gt;the dependency does not change over time(轉移矩陣不變)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Description   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initial state π, state = Q, Observations = O, transition matrix = A, output(observation) matrix = B  &lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/hmm1.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;$a_{ij}$ = probability of state $q_i$ transition to state $q_j$ &lt;/li&gt;
&lt;li&gt;$b_i(k)$ = probability of observe output symbol $O_k$ when state = $q_i$  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-problems-of-HMM&#34;&gt;&lt;a href=&#34;#3-problems-of-HMM&#34; class=&#34;headerlink&#34; title=&#34;3 problems of HMM&#34;&gt;&lt;/a&gt;3 problems of HMM&lt;/h3&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3d3dy41Mm5scC5jbi9obW0tbGVhcm4tYmVzdC1wcmFjdGljZXMtZm91ci1oaWRkZW4tbWFya292LW1vZGVscw==&#34;&gt;中文解說：隱馬可夫鏈&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;評估（Evaluation）：what is probability of the observation sequence given a model? (P(Observes|Model))&lt;ul&gt;
&lt;li&gt;Used in model improvement&lt;/li&gt;
&lt;li&gt;Used in classification&lt;ul&gt;
&lt;li&gt;Word spotting in speech recognition, language identification, speaker identification, author identification……&lt;/li&gt;
&lt;li&gt;Given an observation, compute P(O|model) for all models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Forward algorithm&lt;/strong&gt; to solve it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;解碼（Decoding）：Given an observation sequence and model, what is the &lt;strong&gt;most likely state sequence&lt;/strong&gt;? (P(States|Observes, Model)) 下一個state是什麼&lt;ul&gt;
&lt;li&gt;Used in tagging (tags=hidden states)&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Viterbi algorithm&lt;/strong&gt; to solve it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;學習（Learning）：Given an observation sequence, infer the best model parameters (argmax(Model) P(Model|Observes))&lt;ul&gt;
&lt;li&gt;「fill in model parameters that make the observation sequence most likely」&lt;/li&gt;
&lt;li&gt;Used for building HMM Model from data&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;EM(Baum-Welch, backward-forward algorithm)&lt;/strong&gt; to solve it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;Solutions-of-HMM-problem&#34;&gt;&lt;a href=&#34;#Solutions-of-HMM-problem&#34; class=&#34;headerlink&#34; title=&#34;Solutions of HMM problem&#34;&gt;&lt;/a&gt;Solutions of HMM problem&lt;/h3&gt;&lt;h4 id=&#34;Forward&#34;&gt;&lt;a href=&#34;#Forward&#34; class=&#34;headerlink&#34; title=&#34;Forward&#34;&gt;&lt;/a&gt;Forward&lt;/h4&gt;&lt;p&gt;&lt;a href=&#34;http://www.52nlp.cn/hmm-learn-best-practices-five-forward-algorithm-1&#34; target=&#34;_blank&#34; rel=&#34;noopener external nofollow noreferrer&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/fwformula.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;simply sum of the probability of each possible state sequence &lt;/li&gt;
&lt;li&gt;Direct evaluation&lt;ul&gt;
&lt;li&gt;time complexity = $(2T+1) \times N^{T+1}$ -&amp;gt; too big &lt;img data-src=&#34;/img/NLP/fw.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use dynamic programming&lt;ul&gt;
&lt;li&gt;record the probability of subpaths of the HMM&lt;/li&gt;
&lt;li&gt;The probability of longer subpaths can be calculated from shorter subpaths&lt;/li&gt;
&lt;li&gt;similar to Viterbi: viterbi use MAX() instead of SUM()&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Description:DP  
- ![dp](/img/NLP/dp.png)
- ![dp](/img/NLP/dptable.png)
    - 選最高機率的路徑(將其他路徑的機率加入最高機率) 
    - 例：p(qqqq) = 0.01, p(qrrq) = 0.007 → P(qqqq) = 0.017
--&gt;

&lt;p&gt;Forward Algorithm  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$α_t(i)$ = probability of state = qi at time = t &lt;img data-src=&#34;/img/NLP/forwardalgo.png&#34; alt=&#34;dp&#34;&gt;&lt;/li&gt;
&lt;li&gt;α的求法：將time = t-1 的 α 值，乘上在time = t時會在qi state的機率，並加總 &lt;img data-src=&#34;/img/NLP/forwardfex.png&#34; alt=&#34;dp&#34;&gt;&lt;/li&gt;
&lt;li&gt;順向推出所有可能的state sequence會產生此observation的機率和, 即為此model會產生此observation的機率 &lt;img data-src=&#34;/img/NLP/forwardexample.png&#34; alt=&#34;dp&#34;&gt;&lt;ul&gt;
&lt;li&gt;Σ P($O_1, O_2, O_3$ | possible state sequence) = P($O_1, O_2, O_3$ | Model)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/forwardpseudo.png&#34; alt=&#34;dp&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example:Urn(甕)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;genie has two urns filled with red and blue balls&lt;/li&gt;
&lt;li&gt;genie selects an urn and then draws a ball from it&lt;ul&gt;
&lt;li&gt;The urns are hidden&lt;/li&gt;
&lt;li&gt;The balls are observed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;After a lot of draws&lt;ul&gt;
&lt;li&gt;know the distribution of colors of balls in each urn(B matrix) &lt;/li&gt;
&lt;li&gt;know the genie’s preferences in draw from one urn or the next(A matrix)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;assume output (observation) is Blue Blue Red (BBR)&lt;ul&gt;
&lt;li&gt;Forward: P(BBR|model) = 0.0792 (SUM of all possible states’ probability) &lt;img data-src=&#34;/img/NLP/forward-urn.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Viterbi&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compute &lt;strong&gt;the most possible path&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$v_t(i)$ = &lt;strong&gt;most possible path probability&lt;/strong&gt; from time = 0 to time = t, and state = qi at time = t &lt;img data-src=&#34;/img/NLP/viterbi.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/viterbi-graph.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/viterbi-algo.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Viterbi in Urn example &lt;img data-src=&#34;/img/NLP/urn-cal.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;def viterbi(obs, states, start_p, trans_p, emit_p):
    V = [{}]
    path = {}

    # Initialize base cases (t == 0)
    for y in states:
        V[0][y] = start_p[y] * emit_p[y][obs[0]]
        path[y] = [y]

    # Run Viterbi for t &amp;gt; 0
    for t in range(1,len(obs)):
        V.append({})
        newpath = {}

        for y in states:
            (prob, state) = max([(V[t-1][y0] * trans_p[y0][y] * emit_p[y][obs[t]], y0) for y0 in states]) 
            # ↑ find the most possible state transitting to given state y at time=t
            V[t][y] = prob
            newpath[y] = path[state] + [y] 

        # newpath(at time t) can overwrite path(at time t-1) 
        path = newpath

    (prob, state) = max([(V[len(obs) - 1][y], y) for y in states])
    return (prob, path[state])&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;Backward&#34;&gt;&lt;a href=&#34;#Backward&#34; class=&#34;headerlink&#34; title=&#34;Backward&#34;&gt;&lt;/a&gt;Backward&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Useful for parameter estimation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Description  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Backward variables β, which are the total probability of seeing the rest of the observation sequence($O_t to O_T$) given state qi at time t &lt;img data-src=&#34;/img/NLP/bw-procedure.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/bw-f.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;初始化β：令t=T時刻所有狀態的β為1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;由後往前計算 &lt;img data-src=&#34;/img/NLP/bw-graph.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;如果要計算某observation的概率，只需將t=1的後向變量相加&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Backward-Forward&#34;&gt;&lt;a href=&#34;#Backward-Forward&#34; class=&#34;headerlink&#34; title=&#34;Backward-Forward&#34;&gt;&lt;/a&gt;Backward-Forward&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;We can locally maximize model parameter λ, by an iterative hill-climbing known as Baum-Welch algorithm(=Forward-Backward) (by EM Algorithm structure)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Forward-Backward Algorithm    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;find which** state transitions(A matrix)** and &lt;strong&gt;symbol observaions(B matrix)&lt;/strong&gt; were &lt;strong&gt;probably used the most&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;By &lt;strong&gt;increasing the probability of those&lt;/strong&gt;, we can get a better model which gives a higher probability to the observation sequence&lt;/li&gt;
&lt;li&gt;transition probabilities and path probabilities are both require each other to calculate&lt;ul&gt;
&lt;li&gt;use A matrix to calculate path probabilities&lt;/li&gt;
&lt;li&gt;need path probabilities to update A matrix&lt;/li&gt;
&lt;li&gt;use EM algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;EM algorithm (Expectation-Maximization)    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;迭代算法，它的最大優點是簡單和穩定，但容易陷入局部最優&lt;/li&gt;
&lt;li&gt;(隨機)選擇參數λ0，找出在λ0下最可能的狀態，計算每個訓練樣本的可能結果的概率，再&lt;strong&gt;重新估計新的參數λ&lt;/strong&gt;。經過多次的迭代，直至某個收斂條件滿足為止&lt;/li&gt;
&lt;li&gt;Urn Example&lt;ul&gt;
&lt;li&gt;update transition matrix A ($a_{12}, a_{11}$ … ) &lt;img data-src=&#34;/img/NLP/newtrans.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;P(1→2) = 0.0414 &lt;img data-src=&#34;/img/NLP/1-2.png&#34; alt=&#34;1→2&#34;&gt;&lt;/li&gt;
&lt;li&gt;P(1→1) = 0.0537 &lt;img data-src=&#34;/img/NLP/1-1.png&#34; alt=&#34;1→1&#34;&gt;&lt;/li&gt;
&lt;li&gt;normalize: P(1→2)+P(1→1) = 1, P(1→2) = 0.435 …&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;若state數目多的時候，計算量過大…&lt;ul&gt;
&lt;li&gt;用backward, forward&lt;/li&gt;
&lt;li&gt;前面用forward, 後面用backward &lt;img data-src=&#34;/img/NLP/bf-graph.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Combine forward and backward  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let ξt be the probability of being in state i at time t and state j at time t+1, &lt;strong&gt;given observation and model λ&lt;/strong&gt;&lt;img data-src=&#34;/img/NLP/kesin.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;use not-quite-ξ to get ξ &lt;img data-src=&#34;/img/NLP/nqkesin.png&#34; alt=&#34;&#34;&gt; because &lt;img data-src=&#34;/img/NLP/kesin-formula.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;P(O|λ) → problem1 of HMM 的答案 → 用forward解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;見上方backward, forward同時使用之圖 &lt;img data-src=&#34;/img/NLP/nqkesin-formula.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;ξ可用來計算transition matrix &lt;img data-src=&#34;/img/NLP/newtrans-final.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Summary of Forward-Backward &lt;img data-src=&#34;/img/NLP/fb-algo.png&#34; alt=&#34;&#34;&gt; &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize λ=(A,B)&lt;/li&gt;
&lt;li&gt;Compute α, β, ξ using observations&lt;/li&gt;
&lt;li&gt;Estimate new λ’=(A,B)&lt;/li&gt;
&lt;li&gt;Replace λ with λ’&lt;/li&gt;
&lt;li&gt;If not converged go to 2&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;Chap07-Part-of-Speech-Tagging&#34;&gt;&lt;a href=&#34;#Chap07-Part-of-Speech-Tagging&#34; class=&#34;headerlink&#34; title=&#34;Chap07 Part-of-Speech Tagging&#34;&gt;&lt;/a&gt;Chap07 Part-of-Speech Tagging&lt;/h2&gt;&lt;p&gt;alias: &lt;strong&gt;parts-of-speech&lt;/strong&gt;, &lt;strong&gt;lexical categories&lt;/strong&gt;, &lt;strong&gt;word classes&lt;/strong&gt;, &lt;strong&gt;morphological classes&lt;/strong&gt;, &lt;strong&gt;lexical tags&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Noun, verb, adjective, preposition, adverb, article, interjection, pronoun, conjunction&lt;/li&gt;
&lt;li&gt;preposition(P)&lt;ul&gt;
&lt;li&gt;of, by, to&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;pronoun(PRO)&lt;ul&gt;
&lt;li&gt;I, me, mine&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;determiner(DET)&lt;ul&gt;
&lt;li&gt;the, a, that, those&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Usage  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Speech synthesis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tag before parsing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Information extraction&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finding names, relations, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Machine Translation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Closed class&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the class that is hard to add new words&lt;/li&gt;
&lt;li&gt;Usually function words (short common words which play a role in grammar)&lt;ul&gt;
&lt;li&gt;prepositions: on, under, over,…&lt;/li&gt;
&lt;li&gt;particles: up, down, on, off, …&lt;/li&gt;
&lt;li&gt;determiners: a, an, the, …&lt;/li&gt;
&lt;li&gt;pronouns: she, who, I, …&lt;/li&gt;
&lt;li&gt;conjunctions: and, but, or, …&lt;/li&gt;
&lt;li&gt;auxiliary verbs: can, may should, …&lt;/li&gt;
&lt;li&gt;numerals: one, two, three, third, …&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open class&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;new ones can be created all the time&lt;/li&gt;
&lt;li&gt;For English: Nouns, Verbs, Adjectives, Adverbs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Choosing Tagset: Ex. “Penn TreeBank tagset”, 45 tag&lt;br&gt;&lt;img data-src=&#34;/img/NLP/tagset.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Methods for POS Tagging  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Rule-based tagging&lt;ul&gt;
&lt;li&gt;ENGTWOL: ENGlish TWO Level analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stochastic: Probabilistic sequence models&lt;ul&gt;
&lt;li&gt;HMM (Hidden Markov Model)&lt;/li&gt;
&lt;li&gt;MEMMs (Maximum Entropy Markov Models)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Transformation-Based Tagger (Brill)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;Rule-Based-Tagging&#34;&gt;&lt;a href=&#34;#Rule-Based-Tagging&#34; class=&#34;headerlink&#34; title=&#34;Rule-Based Tagging&#34;&gt;&lt;/a&gt;Rule-Based Tagging&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Assign all possible tags to each word&lt;/li&gt;
&lt;li&gt;Remove tags according to set of rules&lt;ol&gt;
&lt;li&gt;Typically more than 1000 hand-written rules&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;Hidden-Markov-Model-tagging&#34;&gt;&lt;a href=&#34;#Hidden-Markov-Model-tagging&#34; class=&#34;headerlink&#34; title=&#34;Hidden Markov Model tagging&#34;&gt;&lt;/a&gt;Hidden Markov Model tagging&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;special case of Bayesian inference&lt;ul&gt;
&lt;li&gt;Foundational work in computational linguistics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;related to the “noisy channel” model that’s the basis for ASR, OCR and MT&lt;/li&gt;
&lt;li&gt;Decoding view  &lt;ul&gt;
&lt;li&gt;Consider all possible sequences of tags&lt;/li&gt;
&lt;li&gt;choose the tag sequence which is most possible given the observation sequence of n words w1…wn&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Generative view&lt;ul&gt;
&lt;li&gt;This sequence of words must have resulted from some hidden process&lt;/li&gt;
&lt;li&gt;A sequence of tags (states), each of which emitted a word&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$t^n_1$(t hat), which is the most possible tag &lt;img data-src=&#34;/img/NLP/best-t.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;use viterbi to get tag &lt;img data-src=&#34;/img/NLP/viterbi-ex.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Evaluation  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overall error rate with respect to a gold-standard test set&lt;/li&gt;
&lt;li&gt;Error rates on particular tags/words&lt;/li&gt;
&lt;li&gt;Tag confusions, Unknown words…&lt;/li&gt;
&lt;li&gt;Typically accuracy reaches 96~97%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unknown Words&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simplest model&lt;ul&gt;
&lt;li&gt;Unknown words can be of any part of speech, or only in any open class&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Morphological and other cues&lt;ul&gt;
&lt;li&gt;~ed: past tense forms or past participles&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Maximum-entropy-Markov-model-MEMM&#34;&gt;&lt;a href=&#34;#Maximum-entropy-Markov-model-MEMM&#34; class=&#34;headerlink&#34; title=&#34;Maximum entropy Markov model (MEMM)&#34;&gt;&lt;/a&gt;Maximum entropy Markov model (MEMM)&lt;/h3&gt;&lt;p&gt;Maximum Entropy Model  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MaxEnt: multinomial(多項式) logistic regression&lt;/li&gt;
&lt;li&gt;Used for sequence classification/sequence labeling&lt;/li&gt;
&lt;li&gt;Maximum entropy Markov model (MEMM)&lt;ul&gt;
&lt;li&gt;a common MaxEnt classifier&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Classification
- Task
    - observation, Extract useful features, Classify the observation based on these features
- Probabilistic classifier
    - Given an observation, it gives a probability distribution over all classes
- Non-sequential(連續的) Applications
    - Text classification
    - Sentiment analysis
    - Sentence boundary detection
--&gt;


&lt;p&gt;Exponential(log-linear) classifiers &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Combine features linearly&lt;/li&gt;
&lt;li&gt;Use the sum as an exponent &lt;img data-src=&#34;/img/NLP/maxent.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Example &lt;img data-src=&#34;/img/NLP/maxent-ex.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Maximum Entropy Markov Model       &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MaxEnt model&lt;ul&gt;
&lt;li&gt;classifies &lt;strong&gt;a&lt;/strong&gt; observation into &lt;strong&gt;one&lt;/strong&gt; of discrete classes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MEMM&lt;ul&gt;
&lt;li&gt;augmentation(增加) of the basic MaxEnt classifier&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;assign a class to each element in a sequence&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;POS tagging from MaxExt to MEMM   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;include some source of knowledge into the tagging process&lt;/li&gt;
&lt;li&gt;The simplest approach&lt;ul&gt;
&lt;li&gt;run the local classifier and &lt;strong&gt;feature is classifier from the previous word&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Flaw&lt;ul&gt;
&lt;li&gt;It makes a hard decision on each word before moving on the next word&lt;/li&gt;
&lt;li&gt;cannot use information from the later words&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;discriminative model&lt;/strong&gt;(判別模型)   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute the posterior P(Tag|Word) directly to decide tag &lt;img data-src=&#34;/img/NLP/memm.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;求解條件機率分佈 P(y|x) 預測 y → 求P(tag|word)來取得tag &lt;/li&gt;
&lt;li&gt;不考慮聯合機率分佈 P(x, y)&lt;/li&gt;
&lt;li&gt;對於諸如分類和回歸問題，由於不考慮聯合機率分佈，採用判別模型可以取得更好的效果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;HMM and MEMM(順推和逆推的差別) &lt;img data-src=&#34;/img/NLP/hmmandmemm.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlike HMM, MEMM can condition on any &lt;strong&gt;useful feature of observation&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;HMM: state is the fiven value&lt;/li&gt;
&lt;li&gt;MEMM: observation is the given value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;viterbi function for MEMM &lt;img data-src=&#34;/img/NLP/viterbi-new.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/memm-ex.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Transformation-Based-Learning-of-Tags&#34;&gt;&lt;a href=&#34;#Transformation-Based-Learning-of-Tags&#34; class=&#34;headerlink&#34; title=&#34;Transformation-Based Learning of Tags&#34;&gt;&lt;/a&gt;Transformation-Based Learning of Tags&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Tag each word with its most frequent tag&lt;/li&gt;
&lt;li&gt;Construct a list of transformations that &lt;strong&gt;improve the initial tag&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;trigger environment: at the limited number of words before/after &lt;img data-src=&#34;/img/NLP/transformed-learn.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/transformed-algo.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Trigger by tags &lt;/li&gt;
&lt;li&gt;Trigger by word&lt;/li&gt;
&lt;li&gt;Trigger by morphology(詞法學)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&amp;lt;! – ====================分水嶺：尚未分類========================== –&amp;gt;&lt;/p&gt;
&lt;h3 id=&#34;Zipf’s-Law-long-tail-phenomenon&#34;&gt;&lt;a href=&#34;#Zipf’s-Law-long-tail-phenomenon&#34; class=&#34;headerlink&#34; title=&#34;Zipf’s Law (long tail phenomenon)&#34;&gt;&lt;/a&gt;Zipf’s Law (long tail phenomenon)&lt;/h3&gt;&lt;p&gt;a word’s frequency is approximately inversely proportional to its rank in the word distribution list&lt;br&gt;單詞出現的頻率與它在頻率表裡的排名成反比:&lt;br&gt;頻率最高的單詞出現的頻率大約是出現頻率第二位的單詞的2倍&lt;/p&gt;
&lt;h4 id=&#34;Jelinek-Mercer-Smoothing&#34;&gt;&lt;a href=&#34;#Jelinek-Mercer-Smoothing&#34; class=&#34;headerlink&#34; title=&#34;Jelinek-Mercer Smoothing&#34;&gt;&lt;/a&gt;Jelinek-Mercer Smoothing&lt;/h4&gt;&lt;p&gt;interpolate(插值) between bigram and unigram&lt;br&gt;because if p(eat the) = 0 and p(eat thou) = 0&lt;br&gt;it still must consider that  p(eat the) &amp;gt; p(eat thou)&lt;br&gt;because p(the) &amp;gt; p(thou)&lt;br&gt;so p(eat the) = N * p(the | eat) + (1-N) * p(the | thou) &lt;/p&gt;
&lt;h2 id=&#34;Language-Model-Applications&#34;&gt;&lt;a href=&#34;#Language-Model-Applications&#34; class=&#34;headerlink&#34; title=&#34;Language Model: Applications&#34;&gt;&lt;/a&gt;Language Model: Applications&lt;/h2&gt;&lt;h3 id=&#34;Query-Likelihood-Model&#34;&gt;&lt;a href=&#34;#Query-Likelihood-Model&#34; class=&#34;headerlink&#34; title=&#34;Query Likelihood Model&#34;&gt;&lt;/a&gt;Query Likelihood Model&lt;/h3&gt;&lt;p&gt;given a query 𝑞, rank the probability 𝑝(𝑑|q)&lt;br&gt;&lt;img data-src=&#34;/img/NLP/cfKf6I3.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;So the following arguments are equivalent:&lt;br&gt;1.𝑝𝑑𝑞: find the document 𝑑 that is most likely to be relevant to 𝑞&lt;br&gt;2.𝑝𝑞𝑑: find the document 𝑑 that is most likely to generate the query 𝑞&lt;/p&gt;
&lt;p&gt;Typically, unigram LMs are used in IR(information retrieval)&lt;br&gt;Retrieval does not depend that much on sentence structure&lt;/p&gt;
&lt;h3 id=&#34;Dependence-Language-Model&#34;&gt;&lt;a href=&#34;#Dependence-Language-Model&#34; class=&#34;headerlink&#34; title=&#34;Dependence Language Model&#34;&gt;&lt;/a&gt;Dependence Language Model&lt;/h3&gt;&lt;p&gt;Relax the independence assumption of unigram LMs&lt;br&gt;Do not assume that the dependency only exist between &lt;strong&gt;adjacent&lt;/strong&gt; words&lt;br&gt;Introduce a hidden variable: “linkage” 𝐿&lt;br&gt;Ex.&lt;br&gt;&lt;img data-src=&#34;/img/NLP/Z8ftSRP.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;skipped….&lt;/p&gt;
&lt;h3 id=&#34;Proximity-Language-Model&#34;&gt;&lt;a href=&#34;#Proximity-Language-Model&#34; class=&#34;headerlink&#34; title=&#34;Proximity Language Model&#34;&gt;&lt;/a&gt;Proximity Language Model&lt;/h3&gt;&lt;p&gt;Proximity: how close the query terms appear in a document&lt;br&gt;the closer they are, the more likely they are describing the same topic or concept&lt;/p&gt;
&lt;h3 id=&#34;Positional-Language-Model&#34;&gt;&lt;a href=&#34;#Positional-Language-Model&#34; class=&#34;headerlink&#34; title=&#34;Positional Language Model&#34;&gt;&lt;/a&gt;Positional Language Model&lt;/h3&gt;&lt;p&gt;Position: define a LM for each position of a document, instead of the entire document&lt;br&gt;Words closer to a position will contribute more to the language model of this position&lt;/p&gt;
&lt;h3 id=&#34;Speech-Recognition&#34;&gt;&lt;a href=&#34;#Speech-Recognition&#34; class=&#34;headerlink&#34; title=&#34;Speech Recognition&#34;&gt;&lt;/a&gt;Speech Recognition&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;The “origin” of language models&lt;/li&gt;
&lt;li&gt;used to restrict the search space of possible word sequences&lt;/li&gt;
&lt;li&gt;requires higher order models: knowing previous acoustic is important!&lt;/li&gt;
&lt;li&gt;Speed is important!&lt;/li&gt;
&lt;li&gt;N-gram LM with modified Kneser-Ney smoothing is extensively used&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Machine-Translation-MT&#34;&gt;&lt;a href=&#34;#Machine-Translation-MT&#34; class=&#34;headerlink&#34; title=&#34;Machine Translation (MT)&#34;&gt;&lt;/a&gt;Machine Translation (MT)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Decoding: given the probability model(s), find the best translation&lt;/li&gt;
&lt;li&gt;Similar role as in speech recognition: &lt;strong&gt;eliminate unlikely word sequences&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Higher order Kneser-Ney smoothed n-gram LM is widely used&lt;/li&gt;
&lt;li&gt;NNLM-style models tend to outperform standard back-off LMs&lt;/li&gt;
&lt;li&gt;Also significantly speeded up in (Delvin et al, 2014)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;參考資料&#34;&gt;&lt;a href=&#34;#參考資料&#34; class=&#34;headerlink&#34; title=&#34;參考資料&#34;&gt;&lt;/a&gt;參考資料&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;HHChen 課堂講義&lt;/li&gt;
&lt;li&gt;SDLin 講義&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9jbGFzcy5jb3Vyc2VyYS5vcmcvbmxwLw==&#34;&gt;Stanford NLP course&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;www.52nlp.cn&#34;&gt;52nlp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
        <category term="機器學習" />
        <category term="自然語言處理" />
        <category term="統計" />
        <updated>2015-03-07T03:00:47.000Z</updated>
    </entry>
    <entry>
        <id>http://gitqwerty777.github.io/MLtechnique/</id>
        <title>機器學習技法</title>
        <link rel="alternate" href="http://gitqwerty777.github.io/MLtechnique/"/>
        <content type="html">&lt;blockquote&gt;
&lt;p&gt;尚未寫完&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&#34;more&#34;&gt;&lt;/a&gt;
&lt;h2 id=&#34;Chap01-SVM&#34;&gt;&lt;a href=&#34;#Chap01-SVM&#34; class=&#34;headerlink&#34; title=&#34;Chap01 SVM&#34;&gt;&lt;/a&gt;Chap01 SVM&lt;/h2&gt;&lt;p&gt;All line is the same using PLA, but WHICH line is best? &lt;img data-src=&#34;/img/ML/bestline.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ 可以容忍的誤差愈大愈好(最近的點與分隔線的距離愈遠愈好) &lt;img data-src=&#34;/img/ML/circle.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ fat hyperplane (large &lt;strong&gt;margin&lt;/strong&gt;)(分隔線可以多寬) &lt;img data-src=&#34;/img/ML/fat.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;若只有兩個點，必通過兩點連線之中垂線 &lt;img data-src=&#34;/img/ML/funtime1.png&#34; alt=&#34;&#34;&gt; &lt;/p&gt;
&lt;h3 id=&#34;Standard-large-margin-hyperplane-problem&#34;&gt;&lt;a href=&#34;#Standard-large-margin-hyperplane-problem&#34; class=&#34;headerlink&#34; title=&#34;Standard large-margin hyperplane problem&#34;&gt;&lt;/a&gt;Standard large-margin hyperplane problem&lt;/h3&gt;&lt;p&gt;max fatness(w) (max margin)&lt;br&gt;= min distance($x_n$, w) (n = 1 ~ N)&lt;br&gt;= min distance($x_n$, b, w) – (1)&lt;br&gt;因為 w0 不列入計算(w0 = 常數項參數 = 截距b, x0 必為 1)&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/ML/wtxx.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;$w^tx$ = 0 → x除去x0成為x’ → $w^tx’$ + b = 0&lt;br&gt;設x’, x’’都在$w^tx’$ + b = 0平面上，$w^tx’$ = -b, $w^tx’’$ = -b → $w^t(x’’-x’)$ = 0&lt;br&gt;$w^t$ 垂直於 $w^tx’$ + b = 0平面&lt;br&gt;distance(x, b, w) = x 到 平面的距離 &lt;img data-src=&#34;/img/ML/distancexbw.png&#34; alt=&#34;&#34;&gt;   &lt;/p&gt;
&lt;p&gt;單一data的distance: 因 $y_n(w^t x_n + b) &amp;gt; 0$&lt;br&gt;$distance(x_n, b, w) = (1/|w|) * y_n(w^t x_n + b)$ – (2)  &lt;/p&gt;
&lt;p&gt;specialize&lt;br&gt;令 $min y_n(w^t x_n + b) = 1$&lt;br&gt;→ $distance(x_n, b, w) = 1/|w|$&lt;br&gt;由式子(1),(2)可得 max margin = min distance(x_n, b, w) = 1/|w|&lt;br&gt;條件為 $min y_n(w^t x_n + b) = 1$&lt;/p&gt;
&lt;p&gt;放鬆條件&lt;br&gt;$y_n(w_t x_n + b) &amp;gt;= 1$ is necessary constraints for $ min y_n(w_t x_n + b) = 1$&lt;br&gt;if all y_n(w_t x_n + b) &amp;gt; p &amp;gt; 1 -&amp;gt; can generate more optimal answer (b/p, w/p) -&amp;gt; distance 1/|w| is smaller&lt;br&gt;so y_n(w_t x_n + b) &amp;gt;= 1 → y_n(w_t x_n + b) = 1  &lt;/p&gt;
&lt;p&gt;max 1/|w| -&amp;gt; min 1/2 * w_t *  w&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;standard min bw&#34;&gt;&lt;/p&gt;
&lt;p&gt;can solve w by solve N inequality&lt;/p&gt;
&lt;h3 id=&#34;Support-Vector-Machine-SVM&#34;&gt;&lt;a href=&#34;#Support-Vector-Machine-SVM&#34; class=&#34;headerlink&#34; title=&#34;Support Vector Machine(SVM)&#34;&gt;&lt;/a&gt;Support Vector Machine(SVM)&lt;/h3&gt;&lt;p&gt;只須找最近的點即可算出w&lt;br&gt;support vector: bounary data(胖線的邊界點)&lt;/p&gt;
&lt;p&gt;gradient? : not easy with constraints&lt;br&gt;but we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(convex) quadratic objective function(b, w)&lt;/li&gt;
&lt;li&gt;linear constraints (b, w)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;can use quadratic programming (QP, 二次規劃): easy &lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;bw&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;QP&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;QP value&#34;&gt;&lt;/p&gt;
&lt;p&gt;Linear Hard-Margin(all wxy &amp;gt; 0) SVM&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;regular&#34;&gt;&lt;br&gt;the same as ‘weight-decay regularization’ within Ein = 0&lt;/p&gt;
&lt;p&gt;Restricts Dichotomies(堅持胖的線): if margin &amp;lt; p, no answer&lt;br&gt;fewer dichotomies -&amp;gt; small VC dim -&amp;gt; better generalization&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;circle&#34;&gt;&lt;br&gt;最大胖度有極限：在圓上，最大 根號3/2&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;dvc ap&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;compare&#34;&gt;&lt;br&gt;can be good with transform&lt;br&gt;控制複雜度的方法！&lt;br&gt;控制複雜度的方法！&lt;/p&gt;
&lt;h3 id=&#34;Chap02-dual-SVM&#34;&gt;&lt;a href=&#34;#Chap02-dual-SVM&#34; class=&#34;headerlink&#34; title=&#34;Chap02 dual SVM&#34;&gt;&lt;/a&gt;Chap02 dual SVM&lt;/h3&gt;&lt;p&gt;if d_trans is big, or infinite?(非常複雜的轉換)&lt;br&gt;find: SVM without dependence of d_trans&lt;br&gt;去除計算(b, w)與轉換函式複雜度的關係&lt;br&gt;(QP of d_trans+1 variables -&amp;gt; N variables)    &lt;/p&gt;
&lt;p&gt;Use lagrange multiplier: Dual SVM: 將lambda視為變數來解&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;lagrange function&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;SVM=&#34;&gt;&lt;br&gt;若不符條件，則L()的sigma部分是正的，MaxL()為無限大&lt;br&gt;若符條件，則L()的sigma部分最大值為0 -&amp;gt; MaxL() = 1/2w^tw&lt;br&gt;所以Min(MaxL()) = Min(1/2w^tw)&lt;/p&gt;
&lt;p&gt;交換max min的位置，可求得原問題的下限&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;可以得到strong duality(強對偶關係，=)？&lt;br&gt;若在二次規劃滿足constraint qualification  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;convex&lt;/li&gt;
&lt;li&gt;feasible(有解)&lt;/li&gt;
&lt;li&gt;linear constraints&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;則存在 primal-dual 最佳解(對左右邊均是最佳解)&lt;/p&gt;
&lt;p&gt;Dual SVM 最佳解為？&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;微分b=0&#34;&gt;&lt;br&gt;b可以被消除(=0)&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;微分wi=0&#34;&gt;&lt;br&gt;w代入得&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;w=&#34;&gt;&lt;/p&gt;
&lt;p&gt;Karush-Kuhn-Tucker (KKT) conditions&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;KKT&#34;&gt;&lt;br&gt;primal-inner -&amp;gt; an = 0 或 1-yn(wtzn+b) = 0(complementary slackness)&lt;br&gt;=&amp;gt; at optimal all ‘Lagrange terms’ disappear&lt;/p&gt;
&lt;p&gt;用KKT求得a(原lambda)之後，即可得原來的(b, w)&lt;br&gt;w = sigma anynzn&lt;br&gt;b 僅有邊界(primal feasible)，但b = yn - wtzn when an &amp;gt; 0 (primal-inner，代表必在SVM邊界上)&lt;/p&gt;
&lt;p&gt;重訂support vector的條件(a_n &amp;gt;0)&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;sv &amp;lt; sv&#34;&gt;&lt;br&gt;=&amp;gt; b, w 都可以只用SV求到&lt;/p&gt;
&lt;p&gt;SVM:找到有用的點(SV)&lt;/p&gt;
&lt;h4 id=&#34;Standard-hard-margin-SVM-dual&#34;&gt;&lt;a href=&#34;#Standard-hard-margin-SVM-dual&#34; class=&#34;headerlink&#34; title=&#34;Standard hard-margin SVM dual&#34;&gt;&lt;/a&gt;Standard hard-margin SVM dual&lt;/h4&gt;&lt;p&gt;經過整理&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;min 1/2&#34;&gt;&lt;br&gt;現在有N個a, 並有N+1個條件了&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;QP&#34;&gt;&lt;br&gt;when N is big, qn,m is dense array and very big(N &amp;gt;30000, use &amp;gt;3G ram)&lt;br&gt;use special QP solver&lt;/p&gt;
&lt;h4 id=&#34;SVM-和-PLA-比較&#34;&gt;&lt;a href=&#34;#SVM-和-PLA-比較&#34; class=&#34;headerlink&#34; title=&#34;SVM 和 PLA 比較&#34;&gt;&lt;/a&gt;SVM 和 PLA 比較&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;pla&#34;&gt;&lt;br&gt;w = linear combination of data =&amp;gt; w represented by data&lt;br&gt;SVM: represent w by only SV&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;dual&#34;&gt;&lt;br&gt;Primal: 對(b,w)做特別縮放&lt;br&gt;Dual: 找到SV 和其 lagrange multiplier&lt;/p&gt;
&lt;p&gt;問題：q_n,m 需要做O(d_trans)的運算，如何避免？&lt;/p&gt;
&lt;h3 id=&#34;Chap03-Kernel-SVM&#34;&gt;&lt;a href=&#34;#Chap03-Kernel-SVM&#34; class=&#34;headerlink&#34; title=&#34;Chap03 Kernel SVM&#34;&gt;&lt;/a&gt;Chap03 Kernel SVM&lt;/h3&gt;&lt;p&gt;(z_n^T)(z_m)如何算更快&lt;/p&gt;
&lt;p&gt;轉換+內積 -&amp;gt; Kernel function&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;xxxxxxx&#34;&gt;&lt;br&gt;use O(d) instead of O(d^2)&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;kernel&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;b=&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;gsvm =&#34;&gt;&lt;br&gt;用kernel簡化！(gsvm -&amp;gt; 無w)&lt;/p&gt;
&lt;h4 id=&#34;Kernel-Hard-Margin-SVM&#34;&gt;&lt;a href=&#34;#Kernel-Hard-Margin-SVM&#34; class=&#34;headerlink&#34; title=&#34;Kernel Hard-Margin SVM&#34;&gt;&lt;/a&gt;Kernel Hard-Margin SVM&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;kernel svm algo&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;polynomial-Kernel&#34;&gt;&lt;a href=&#34;#polynomial-Kernel&#34; class=&#34;headerlink&#34; title=&#34;polynomial Kernel&#34;&gt;&lt;/a&gt;polynomial Kernel&lt;/h4&gt;&lt;p&gt;簡化的kernel: 對應到同等大小，不同幾何特性(如內積)的空間&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;kernel&#34;&gt;&lt;br&gt;r影響SV的選擇&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;SELECTED sv&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;high dim&#34;&gt;&lt;br&gt;可以快速做高次轉換(和二次相同複雜度)&lt;/p&gt;
&lt;p&gt;特例: linear只需用 K1 = (0+1xtx)^1&lt;/p&gt;
&lt;h4 id=&#34;infinite-Kernel&#34;&gt;&lt;a href=&#34;#infinite-Kernel&#34; class=&#34;headerlink&#34; title=&#34;infinite Kernel&#34;&gt;&lt;/a&gt;infinite Kernel&lt;/h4&gt;&lt;p&gt;taylor展開&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;k(x,x&amp;#39;)&#34;&gt;&lt;/p&gt;
&lt;p&gt;無限維度的Gaussian Kernel (Radial Basis Funtion(RBF))&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;g&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;support vector mechanism&#34;&gt;&lt;br&gt;large =&amp;gt; sharp Gaussians =&amp;gt; ‘overfit’?&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;overfit &#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;Kernel選擇&#34;&gt;&lt;a href=&#34;#Kernel選擇&#34; class=&#34;headerlink&#34; title=&#34;Kernel選擇&#34;&gt;&lt;/a&gt;Kernel選擇&lt;/h4&gt;&lt;p&gt;linear kernel: 等於沒有轉換，linear first, 計算快&lt;br&gt;polynomial: 轉換過，限制小，strong physical control, 維度太大K會趨向極端值&lt;br&gt;-&amp;gt; 平常只用不大的維度&lt;br&gt;infinite dimension:&lt;br&gt;most powerful&lt;br&gt;less numerical difficulty than poly(僅兩次式)&lt;br&gt;one parameter only&lt;br&gt;cons: mysterious – no w , and too powerful&lt;/p&gt;
&lt;p&gt;define new kernel is hard:&lt;br&gt;Mercer’s condition:&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;mercer&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;Chap04-Soft-Margin-SVM&#34;&gt;&lt;a href=&#34;#Chap04-Soft-Margin-SVM&#34; class=&#34;headerlink&#34; title=&#34;Chap04 Soft-Margin SVM&#34;&gt;&lt;/a&gt;Chap04 Soft-Margin SVM&lt;/h3&gt;&lt;p&gt;overfit reason: transform &amp;amp; hard-margin(全分開)&lt;/p&gt;
&lt;p&gt;Soft-Margin – 容忍錯誤，有錯誤penalty，只有對的需要符合條件&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;soft1&#34;&gt;&lt;/p&gt;
&lt;p&gt;缺點：&lt;br&gt;No QP anymore&lt;br&gt;error大小:離fat boundary的距離 &lt;/p&gt;
&lt;p&gt;改良：求最小(犯錯的點與boundary的距離和)(linear constraint, can use QP)&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;soft2&#34;&gt;&lt;/p&gt;
&lt;p&gt;parameter C: large when want less violate margin&lt;br&gt;small when want large margin, tolerate some violation&lt;/p&gt;
&lt;p&gt;Soft-margin Dual: 將條件加入min中&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;dual&#34;&gt;&lt;br&gt;化簡後得到和dual svm相同的式子(不同條件)&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;化簡後&#34;&gt;&lt;br&gt;C is exactly the upper bound of an&lt;/p&gt;
&lt;h3 id=&#34;Kernel-Soft-Margin-SVM&#34;&gt;&lt;a href=&#34;#Kernel-Soft-Margin-SVM&#34; class=&#34;headerlink&#34; title=&#34;Kernel Soft Margin SVM&#34;&gt;&lt;/a&gt;Kernel Soft Margin SVM&lt;/h3&gt;&lt;p&gt;more flexible: always solvable&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;algo&#34;&gt;&lt;/p&gt;
&lt;p&gt;(3)-&amp;gt;solve b:&lt;br&gt;若as &amp;lt; C(unbounded, free), 則b的求法和hard-margin一樣&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;compare b = &#34;&gt;&lt;/p&gt;
&lt;p&gt;但soft-margin還是會overfit…&lt;/p&gt;
&lt;p&gt;physical meaning&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;&#34;&gt;&lt;br&gt;not SV(an = 0): C-an != 0 -&amp;gt; En = 0&lt;br&gt;unbounded SV(0 &amp;lt; an &amp;lt; C，口) -&amp;gt; En = 0 -&amp;gt; on fat boundary&lt;br&gt;bounded SV(an = C, △) -&amp;gt; En &amp;gt;= 0(有違反，不在boundary上)&lt;br&gt;-&amp;gt; 只有bounded SV才可違反&lt;/p&gt;
&lt;p&gt;difficult to optimize(C, r)&lt;/p&gt;
&lt;h4 id=&#34;SVM-validation&#34;&gt;&lt;a href=&#34;#SVM-validation&#34; class=&#34;headerlink&#34; title=&#34;SVM validation&#34;&gt;&lt;/a&gt;SVM validation&lt;/h4&gt;&lt;p&gt;leave-one-out error &amp;lt;= #SV/N&lt;br&gt;若移除non-SV的點，則得出的g不變&lt;br&gt;-&amp;gt; 可以靠此特性做參數選擇(不選#SV太大的)&lt;/p&gt;
&lt;h3 id=&#34;Chap05-Kernel-Logistic-SVM&#34;&gt;&lt;a href=&#34;#Chap05-Kernel-Logistic-SVM&#34; class=&#34;headerlink&#34; title=&#34;Chap05 Kernel Logistic SVM&#34;&gt;&lt;/a&gt;Chap05 Kernel Logistic SVM&lt;/h3&gt;&lt;p&gt;實用library: linear:LIBLINEAR nonlinear:LIBSVM  &lt;/p&gt;
&lt;p&gt;將E替代 -&amp;gt; 像是 L2 regularization&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;缺點：不能QP, 不能微分(難解)&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;compare&#34;&gt;&lt;br&gt;large margin &amp;lt;=&amp;gt; fewer choices &amp;lt;=&amp;gt; L2 regularization of short w&lt;br&gt;soft margin &amp;lt;=&amp;gt; special err&lt;br&gt;larger C(in soft-margin or in regularization) &amp;lt;=&amp;gt; smaller lagrange multiplier &amp;lt;=&amp;gt; less regularization  &lt;/p&gt;
&lt;p&gt;We can extend SVM to other learning models!&lt;/p&gt;
&lt;p&gt;look (wtzn + b) as linear score(f(x) in PLA)&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;red-blue&#34;&gt;&lt;br&gt;we can have Err_svm is upper bound of Err0/1&lt;br&gt;(hinge error measure)&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;three graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;errwsce&#34;&gt;&lt;br&gt;Err_sce: 與svm相似的一個logistic regression&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;errbound&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;three compare&#34;&gt;&lt;br&gt;L2 logistic regression is similar to SVM,&lt;br&gt;所以SVM可以用來approximate Logistic regression?&lt;br&gt;-&amp;gt; SVM當作Log regression的起始點? 沒有比較快(SVM優點)&lt;br&gt;-&amp;gt; 將SVM答案當作Log的近似解(return theta(wx + b))? 沒有log reg的意義(maximum likelyhood)&lt;br&gt;=&amp;gt; 加兩個自由度，return theta(A*(wx+b) + B)&lt;br&gt;-&amp;gt; often A &amp;gt; 0(同方向), B~=0(無位移)&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;NEW LOGREG&#34;&gt;&lt;br&gt;將原本的SVM視為一種轉換&lt;/p&gt;
&lt;p&gt;Platt’s Model&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;PLATT&#34;&gt;&lt;br&gt;kernel SVM在Z空間的解 – 用Log Reg微調後 –&amp;gt; 用來近似Log Reg在Z空間的解(並不是在z空間最好的解)&lt;/p&gt;
&lt;p&gt;solve LogReg to get(A, B)&lt;/p&gt;
&lt;p&gt;能使用kernel的關鍵：w為z的線性組合&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;svm pla logreg by sgd&#34;&gt;&lt;/p&gt;
&lt;p&gt;Representer Theorem: 若解L2-正規化問題，最佳w必為z的線性組合&lt;br&gt;將w分為(與z垂直)+(與z平行), 希望w_垂直 = 0&lt;br&gt;證：(原本的w) 和 (與z平行的w) 所得的err是一樣的(因為w_垂直 * z = 0)&lt;br&gt;且w平行比較短&lt;br&gt;所以min w 必(與z平行)&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;&#34;&gt;&lt;br&gt;結果：L2的linear model都可以用kernel解！&lt;/p&gt;
&lt;p&gt;將w = sum(B*z) = sum(B*Kernel)代入logistic regression&lt;br&gt;-&amp;gt; 解B&lt;/p&gt;
&lt;p&gt;Kernel Logistic Regression(KLR)&lt;br&gt;= linear model of B&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;special regularizer&#34;&gt;&lt;br&gt;把 kernel當作轉換, kernel當作regularizer&lt;br&gt;= linear model of w&lt;br&gt;with embedded-in-kernel transform &amp;amp; L2 regularizer&lt;br&gt;把 kernel內部(z)當作轉換(?), L2-regularizer&lt;/p&gt;
&lt;p&gt;警告：算出的B不會有很多零&lt;/p&gt;
&lt;p&gt;soft margin SVM ~= L2 LOG REG, special error measure:hinge&lt;br&gt;在z空間解log reg -&amp;gt; 用representor theorem 轉換為一般log reg, 有代價&lt;/p&gt;
&lt;h2 id=&#34;Chap-06-Support-Vector-Regression-SVR&#34;&gt;&lt;a href=&#34;#Chap-06-Support-Vector-Regression-SVR&#34; class=&#34;headerlink&#34; title=&#34;Chap 06 Support Vector Regression(SVR)&#34;&gt;&lt;/a&gt;Chap 06 Support Vector Regression(SVR)&lt;/h2&gt;&lt;p&gt;ridge regression : 有regularized的regression&lt;br&gt;如何加入kernel?&lt;/p&gt;
&lt;p&gt;Kernel Ridge Regression&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;solve ridge&#34;&gt;&lt;br&gt;用representor theorem代入後得到regularization term 和 regression term&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;梯度&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;B=&#34;&gt;&lt;br&gt;因為kernal必為為psd，所以B必有解 O(N^3)&lt;/p&gt;
&lt;p&gt;g(x) = wz = sum(bz)z = sum(bk)&lt;/p&gt;
&lt;p&gt;與linear的比較：&lt;br&gt;kernel自由度高&lt;br&gt;linear為O(d^3+d^2N)&lt;br&gt;kernel和資料量有關，為O(N^3)，檔案大時不快&lt;/p&gt;
&lt;p&gt;LS(least-squares)SVM = kernel ridge regression:&lt;br&gt;和一般regression boundary差不多，但SV很多(B dense)&lt;br&gt;=&amp;gt; 代表計算時間長&lt;br&gt;=&amp;gt; 找一個sparse B?&lt;/p&gt;
&lt;p&gt;tube regression:&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;tube&#34;&gt;&lt;br&gt;insensitive error:容忍一小段的差距(在誤差內err = 0，若超過, err只算超過的部分)&lt;br&gt;error增加的速度變慢&lt;/p&gt;
&lt;p&gt;學SVM，解QP, 用DUAL, KKT-&amp;gt;sparse&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;mimicking&#34;&gt;&lt;br&gt;regulizer 和 超過tube上界的值，超過tube下界的值&lt;/p&gt;
&lt;p&gt;參數：C(violation重視程度), tube範圍&lt;/p&gt;
&lt;p&gt;作dual: lagrange multiplier + KKT condition&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;dual --&#34;&gt;&lt;/p&gt;
&lt;p&gt;在tube裡面的點：B=0&lt;br&gt;=&amp;gt; 只要tube夠寬，B為sparse&lt;/p&gt;
&lt;h3 id=&#34;Linear-SVM-Summary&#34;&gt;&lt;a href=&#34;#Linear-SVM-Summary&#34; class=&#34;headerlink&#34; title=&#34;Linear, SVM Summary&#34;&gt;&lt;/a&gt;Linear, SVM Summary&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;linear&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;SVM&#34;&gt;&lt;br&gt;first row: less used due to worse performance&lt;br&gt;third row: less used due to dense B&lt;br&gt;fourth row: popular in LIBSVM&lt;/p&gt;
&lt;h2 id=&#34;Chap07-Blending-and-Bagging&#34;&gt;&lt;a href=&#34;#Chap07-Blending-and-Bagging&#34; class=&#34;headerlink&#34; title=&#34;Chap07 Blending and Bagging&#34;&gt;&lt;/a&gt;Chap07 Blending and Bagging&lt;/h2&gt;&lt;p&gt;Selection: rely on only once hypothesis&lt;br&gt;Aggregation: mix or combine hypothesiss&lt;br&gt;select trust-worthy from their usual performance&lt;br&gt;=&amp;gt; validation&lt;br&gt;mix the prediction =&amp;gt; vote with different weight of ballot&lt;br&gt;combine predictions conditionally(when some situation, give more ballots to friend t)&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;real function&#34;&gt;&lt;/p&gt;
&lt;p&gt;Aggregation可做到：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;feature transform(?), 將hypothesis變強&lt;/li&gt;
&lt;li&gt;regularization(?)&lt;br&gt;控制 油門 和 煞車&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;two lines&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;uniform blending: 一種model一票，取平均&lt;br&gt;證明可以比原本的Eout小: &lt;img data-src=&#34;&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;一個演算法A的表現，可以用其hypothesis set中的”共識”來表示，等於共識的表現，加上共識的變異數，uniform blending就是將某些在A的hypothesis取平均(變成新的演算法A’)來減少A’的變異數&lt;br&gt;expected performance of A = expected deviation to consensus + performance of consensus&lt;/p&gt;
&lt;p&gt;linear blending: 加權(線性)平均，權重&amp;gt;0&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;linear bledning for regression&#34;&gt;&lt;br&gt;求類似linear regression的式子: 兩段式學習，先算出許多g，再做  linear regression -&amp;gt; 得到答案G&lt;br&gt;限制：權重a&amp;gt;0 -&amp;gt; 將error rate大的model反過來用(error rate = 99%, 取其相反答案即可將error rate = 1%)   &lt;/p&gt;
&lt;p&gt;any blending(stacking): 可用non-linear model(???)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;算出g1-, g2- ...   
phi-1 = (g1-, g2-, ...)   
transform validation data to Z = (phi-1(x), y)   
compuate g = AnyModel(Z, Y)   
return G = g(phi(x))
phi = (g1, g2 ... )&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;比較：linear blending&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;compuate a = AnyModel(Z, Y)   
return G = a * phi(x)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;learning: 邊學邊合，&lt;/p&gt;
&lt;p&gt;bootstrapping: 從有限的資料模擬出新的資料&lt;br&gt;bootstrap data: 從原本資料選擇N筆資料(可重複)&lt;br&gt;Virtual aggregation&lt;br&gt;bootstrap aggregation(bagging): 由bootstrap data訓練g，而非原資料&lt;br&gt;-&amp;gt; meta algorithm for [base algorithm(可使用不同演算法)]&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;BAGGING pocket in action&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;Chap08-Adaptive-Boosting&#34;&gt;&lt;a href=&#34;#Chap08-Adaptive-Boosting&#34; class=&#34;headerlink&#34; title=&#34;Chap08 Adaptive Boosting&#34;&gt;&lt;/a&gt;Chap08 Adaptive Boosting&lt;/h2&gt;&lt;p&gt;教小學生辨認蘋果:&lt;br&gt;由一個演算法提供[會混淆的資料]&lt;br&gt;由其他hypothesis提出一個不同的小規則來區分&lt;/p&gt;
&lt;p&gt;給不同的data權重，會混淆的占較大比例，取min Ein = avg(Wn * err(xn, yn))，可用SVM, lin_reg, log_reg解Wn&lt;/p&gt;
&lt;p&gt;gt = argmin(sum(ut * err))&lt;br&gt;gt+1 = argmin(sum(ut+1 * err))&lt;/p&gt;
&lt;p&gt;找完gt後，gt+1應該要找和gt不相似的-&amp;gt;找ut+1使gt的err rate接近0.5(隨機)。&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;construct to make gt random-like&#34;&gt;&lt;/p&gt;
&lt;p&gt;err rate = 錯誤資料權重和 / (錯誤資料權重和 + 正確資料權重和) = 1/2&lt;br&gt;=&amp;gt; 希望 正確資料權重和 = 錯誤資料權重和&lt;br&gt;在gt中正確的資料, 權重要乘(err rate)&lt;br&gt;在gt中錯誤的資料, 權重要乘(1-err rate)&lt;br&gt;如此一來兩者之和將會相等&lt;/p&gt;
&lt;p&gt;若scale factor = S = sqrt((1-err rate) / err rate)&lt;br&gt;incorrect *= S&lt;br&gt;correct *= 1/S&lt;br&gt;若 S&amp;gt;1:&lt;br&gt;→ err rate &amp;lt;= 1/2&lt;br&gt;→ incorrect↑, correct↓, close to 1/2&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;preliminary algorithm&#34;&gt;&lt;br&gt;u1 可設所有為1/N，得到min Ein&lt;br&gt;G 設uniform會使成績變差&lt;/p&gt;
&lt;p&gt;Adaptive Boosting(皮匠法)&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;ADA BOOST&#34;&gt;&lt;br&gt;邊做邊算at&lt;/p&gt;
&lt;p&gt;希望愈好的gt，at愈大&lt;br&gt;-&amp;gt; 設at = ln(St) (S = scale)&lt;br&gt;if(err rate == 1/2) -&amp;gt; St = 1 -&amp;gt; at = 0&lt;br&gt;if(err rate == 0) -&amp;gt; St = inf -&amp;gt; at = inf&lt;/p&gt;
&lt;p&gt;只要err rate &amp;lt; 1/2 , 就可以參與投票：群眾的力量&lt;/p&gt;
&lt;p&gt;adapative boosting 的 algorithm 選擇(不需強演算法):&lt;br&gt;decision stump: 三個參數：which feature, threshold(線), direction(ox)，可以使Ein &amp;lt;= 1/2&lt;/p&gt;
&lt;h2 id=&#34;Chap09-Decision-Tree&#34;&gt;&lt;a href=&#34;#Chap09-Decision-Tree&#34; class=&#34;headerlink&#34; title=&#34;Chap09 Decision Tree&#34;&gt;&lt;/a&gt;Chap09 Decision Tree&lt;/h2&gt;&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Traditional learning model that realize conditional aggregation&lt;br&gt;模仿人類決策過程&lt;/p&gt;
&lt;p&gt;Path View:&lt;br&gt;G = sum(q * g)&lt;br&gt;q = condition (is x on this path?)&lt;br&gt;g = base hypothesis, only constant, leaf in tree&lt;/p&gt;
&lt;p&gt;Recursive View:&lt;br&gt;G(x) = sum([b(x) == c] * Gc(x))&lt;br&gt;G: full tree&lt;br&gt;b: branching criteria&lt;br&gt;Gc: sub-tree hypothesis&lt;/p&gt;
&lt;p&gt;advantage: human-explainable, simple, efficient, missing feature handle, categorical features easily, multiclass easily&lt;br&gt;disadvantage: heuristic, little theoretical&lt;br&gt;Ex. C&amp;amp;RT, C4.5, J48…&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;basic decision tree algo&#34;&gt;&lt;br&gt;four choices: number of branches, branching&lt;br&gt;criteria, termination criteria, &amp;amp; base hypothesis&lt;/p&gt;
&lt;p&gt;C&amp;amp;RT(Classification and Regression Tree):&lt;br&gt;Tree which is fully-grown with constant leaves&lt;br&gt;C = 2(binary tree)，可用decision stump&lt;br&gt;gt(x) = 在此分類下output最有可能(出現最多次的yn or yn平均)&lt;br&gt;-&amp;gt; 分得愈純愈好(同一類的output皆相同)&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;more simple choices - argmin&#34;&gt;&lt;br&gt;impurity = 變異數 or 出現最多次的yn的比率&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;for classification error&#34;&gt;&lt;br&gt;popular to use :&lt;br&gt;Gini for classification&lt;br&gt;regression error for regression&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;basic C&amp;amp;RT&#34;&gt;&lt;br&gt;terminate criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;all yn is the same: impurity = 0&lt;/li&gt;
&lt;li&gt;all xn the same: cannot cut&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;if all xn different: Ein = 0&lt;br&gt;low-level tree built with small D -&amp;gt; overfit &lt;/p&gt;
&lt;p&gt;regularizer: number of leaves&lt;br&gt;argmin(Ein(G) + c * number of leaves(G))&lt;br&gt;實作：一次剪一片葉子，選最好的  &lt;/p&gt;
&lt;p&gt;相較數字的feature, 處理類型問題較簡單  &lt;/p&gt;
&lt;p&gt;Surrogate(代理) branch:&lt;br&gt;找一些與最好切法相近的，若data features missing, 則使用之&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;圖&#34;&gt;&lt;br&gt;與adaboost相比：片段切割，只在自身subtree切&lt;/p&gt;
&lt;h2 id=&#34;Chap10-Random-Forest&#34;&gt;&lt;a href=&#34;#Chap10-Random-Forest&#34; class=&#34;headerlink&#34; title=&#34;Chap10 Random Forest&#34;&gt;&lt;/a&gt;Chap10 Random Forest&lt;/h2&gt;&lt;p&gt;Random Forest = bagging + fully-grown random-subspace random-combination C&amp;amp;RT decision tree&lt;/p&gt;
&lt;p&gt;highly parallel, 減少 decision tree的variance  &lt;/p&gt;
&lt;h3 id=&#34;增加decision-tree-diversity&#34;&gt;&lt;a href=&#34;#增加decision-tree-diversity&#34; class=&#34;headerlink&#34; title=&#34;增加decision tree diversity&#34;&gt;&lt;/a&gt;增加decision tree diversity&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;random sample features from x(random subspace of X)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;-&amp;gt; efficient, can be used for any learning models&lt;br&gt;10000個features, 只用100個維度來learn&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;將 x 作 低維度random projection -&amp;gt; 產生新的feature(斜線切割), random combination&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;Out-of-bag&#34;&gt;&lt;a href=&#34;#Out-of-bag&#34; class=&#34;headerlink&#34; title=&#34;Out-of-bag&#34;&gt;&lt;/a&gt;Out-of-bag&lt;/h3&gt;&lt;p&gt;out-of-bag: not sampled after N drawings&lt;br&gt;N個data抽N次，沒被抽到機率 ~= 1/e&lt;br&gt;=&amp;gt; 將沒抽到的DATA作g的validation(通常不做，因為g只為G的其中之一)&lt;br&gt;=&amp;gt; 將沒抽到的DATA作G的validation，Eoob = sum(err(G-(xn))) (G-不包含用到xn的g)&lt;br&gt;&lt;img data-src=&#34;&#34; alt=&#34;Eoob(G)&#34;&gt;&lt;br&gt;Eoob: self-validation&lt;/p&gt;
&lt;h3 id=&#34;Feature-Selection&#34;&gt;&lt;a href=&#34;#Feature-Selection&#34; class=&#34;headerlink&#34; title=&#34;Feature Selection&#34;&gt;&lt;/a&gt;Feature Selection&lt;/h3&gt;&lt;p&gt;want to remove redundant, irrelevant features…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;learn a subset-transform&lt;/strong&gt; for the final hypothesis&lt;/p&gt;
&lt;p&gt;advantage: interpretability, remove ‘feature noise’, efficient&lt;br&gt;disadvantage: total computation time increase, ‘select feature overfit’, mis-interpretability(過度解釋)&lt;/p&gt;
&lt;p&gt;decision tree: built-in feature selection&lt;/p&gt;
&lt;p&gt;idea: rate importance of every features&lt;br&gt;linear model: 看w的大小&lt;br&gt;non-linear model: not easy to estimate&lt;/p&gt;
&lt;p&gt;idea: random test&lt;br&gt;put some random value into feature, check performance↓，下降愈多代表愈重要&lt;/p&gt;
&lt;p&gt;random value &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;by original P(X = x)&lt;/li&gt;
&lt;li&gt;bootstrap, &lt;strong&gt;permutation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;performance: 算很久&lt;br&gt;importance(i) = Eoob(G, D) - Eoob(G, Dp) (Dp = data with permutation in xn_i)&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;&#34; alt=&#34;strength-correlation&#34;&gt;&lt;br&gt;strength-correlation decomposition&lt;br&gt;s = average voting margin(投票最多-投票第二多…) with G&lt;br&gt;p = gt之間的相似度&lt;br&gt;bias-variance decomposition&lt;/p&gt;
&lt;h2 id=&#34;Chap11-Gradient-Boost-Decision-Tree&#34;&gt;&lt;a href=&#34;#Chap11-Gradient-Boost-Decision-Tree&#34; class=&#34;headerlink&#34; title=&#34;Chap11 Gradient Boost Decision Tree&#34;&gt;&lt;/a&gt;Chap11 Gradient Boost Decision Tree&lt;/h2&gt;&lt;h2 id=&#34;Chap12-Neural-Network&#34;&gt;&lt;a href=&#34;#Chap12-Neural-Network&#34; class=&#34;headerlink&#34; title=&#34;Chap12 Neural Network&#34;&gt;&lt;/a&gt;Chap12 Neural Network&lt;/h2&gt;</content>
        <category term="機器學習" />
        <updated>2014-11-21T13:40:00.000Z</updated>
    </entry>
    <entry>
        <id>http://gitqwerty777.github.io/MLfoundation2/</id>
        <title>機器學習基石(下)</title>
        <link rel="alternate" href="http://gitqwerty777.github.io/MLfoundation2/"/>
        <content type="html">&lt;h2 id=&#34;Chap09-Linear-Regression&#34;&gt;&lt;a href=&#34;#Chap09-Linear-Regression&#34; class=&#34;headerlink&#34; title=&#34;Chap09 Linear Regression&#34;&gt;&lt;/a&gt;Chap09 Linear Regression&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;將w向量(參數)最佳化&lt;ul&gt;
&lt;li&gt;直接用計算出的 Wx&lt;/li&gt;
&lt;li&gt;perceptron → output 只有 +1/-1&lt;/li&gt;
&lt;li&gt;regression → output 為數字&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;找最少誤差 &lt;img data-src=&#34;/img/ML/einout2.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;wx 和 y 的誤差平方&lt;/li&gt;
&lt;li&gt;Ein: 取平均, Eout: 取期望值 &lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/ML/residual.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;a id=&#34;more&#34;&gt;&lt;/a&gt;

&lt;p&gt;轉換成矩陣形式(最後一行的X, y, w都是矩陣)&lt;br&gt;&lt;img data-src=&#34;/img/ML/einmatrix.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ein(w)函數性質  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;continuous&lt;/li&gt;
&lt;li&gt;differeniable&lt;/li&gt;
&lt;li&gt;convex(凸)     &lt;/li&gt;
&lt;li&gt;可以找到最低點(使Ein微分 = 0的w) &lt;img data-src=&#34;/img/ML/ein=0.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;因為 A 為 $X^tX$ 的形式，必為symmetric matrix，可直接做微分 &lt;img data-src=&#34;/img/ML/vectorwdiff.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;計算W &lt;img data-src=&#34;/img/ML/invertiblesingular.png&#34; alt=&#34;&#34;&gt;   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若A invertible，可直接求inverse&lt;/li&gt;
&lt;li&gt;若非，則用X十字架(&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTW9vcmUlRTIlODAlOTNQZW5yb3NlX3BzZXVkb2ludmVyc2U=&#34;&gt;pseudo inverse&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linear-regression-algorithm-easy&#34;&gt;&lt;a href=&#34;#linear-regression-algorithm-easy&#34; class=&#34;headerlink&#34; title=&#34;linear regression algorithm(easy)&#34;&gt;&lt;/a&gt;linear regression algorithm(easy)&lt;/h3&gt;&lt;p&gt; &lt;img data-src=&#34;/img/ML/regressionalgo.png&#34; alt=&#34;linear regression algorithm&#34;&gt;&lt;/p&gt;
&lt;p&gt;Is it “learning algorithm”?    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No → 直接計算所得的解&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Yes&lt;/strong&gt; → good Ein and Eout(finite $d_vc$), pseudo-inverse&lt;ul&gt;
&lt;li&gt;可視為迭代進行(用矩陣一次算出)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;“Simpler-than-VC”-Guarantee&#34;&gt;&lt;a href=&#34;#“Simpler-than-VC”-Guarantee&#34; class=&#34;headerlink&#34; title=&#34;“Simpler-than-VC” Guarantee&#34;&gt;&lt;/a&gt;“Simpler-than-VC” Guarantee&lt;/h3&gt;&lt;p&gt;可證Ein會小於????&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/ML/9-3.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;預測值 $\hat{y} = Xw = XX^{-1}y = Hy$&lt;br&gt;定義 hat matrix $H = XX^{-1}$ &lt;/p&gt;
&lt;h4 id=&#34;Hat-Matrix-in-Geometry&#34;&gt;&lt;a href=&#34;#Hat-Matrix-in-Geometry&#34; class=&#34;headerlink&#34; title=&#34;Hat Matrix in Geometry&#34;&gt;&lt;/a&gt;Hat Matrix in Geometry&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;/img/ML/9-1.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;預測值 $\hat{y}$ 被限制在X的span上 ($\hat{y} = WX$)&lt;/li&gt;
&lt;li&gt;最小誤差會出現在y - $\hat{y}$ 與 span of X 垂直時&lt;/li&gt;
&lt;li&gt;H 將向量投影至span of X上&lt;/li&gt;
&lt;li&gt;I-H : 投影至與 span of X 垂直的向量 (即為誤差: y - $\hat{y}$)&lt;ul&gt;
&lt;li&gt;可以發現 I-H 對角線上的值之和 trace(I - H) = N - (d + 1)&lt;ul&gt;
&lt;li&gt;其物理意義為在N維空間投影至d+1維空間&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;with noise &lt;img data-src=&#34;/img/ML/9-4.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;f(x) + noise → y&lt;ul&gt;
&lt;li&gt;f(x) 為正確的 y&lt;/li&gt;
&lt;li&gt;noise * (I-H) = y - $\hat{y}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;可算出Ein和noise的關係: N變大時, Eout↓, Ein↑, noise level收斂在σ^2 &lt;img data-src=&#34;/img/ML/9-5.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/ML/9-6.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/ML/9-7.png&#34; alt=&#34;&#34;&gt; &lt;ul&gt;
&lt;li&gt;Eout可用類似方法證明&lt;/li&gt;
&lt;li&gt;expected generalization error(= Eout - Ein): 2(d+1)/N &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Usage&#34;&gt;&lt;a href=&#34;#Usage&#34; class=&#34;headerlink&#34; title=&#34;Usage&#34;&gt;&lt;/a&gt;Usage&lt;/h3&gt;&lt;p&gt;Run Linear Regression Algo(efficient) and set initial w = $w^T_{lin}$ to speed up the perceptron learning     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;誤差大小(看面積): Square Error &amp;gt; 0/1 Error &lt;img data-src=&#34;/img/ML/9-8.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;regression產生的 W 比 classification 的誤差更大，但計算時間較短 &lt;img data-src=&#34;/img/ML/9-9.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap-10-Logistic-Regression&#34;&gt;&lt;a href=&#34;#Chap-10-Logistic-Regression&#34; class=&#34;headerlink&#34; title=&#34;Chap 10 Logistic Regression&#34;&gt;&lt;/a&gt;Chap 10 Logistic Regression&lt;/h2&gt;&lt;p&gt;Heart attack prediction&lt;br&gt;Not every people with bad condition will have heart attack&lt;br&gt;→ only P(Heart attack | x) probability&lt;br&gt;→ look it as noise&lt;/p&gt;
&lt;p&gt;若機率 P(+1|X) &amp;gt; 1/2，則當成 +1，其他output當作noise &lt;img data-src=&#34;/img/ML/10-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;‘soft’ binary classification: f(x) = P(+1|x)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ideal data: probabilty ↔ we have only noisy data(+1 or -1)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the same data as perceptron, but different target function&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Logistic-Hypothesis&#34;&gt;&lt;a href=&#34;#Logistic-Hypothesis&#34; class=&#34;headerlink&#34; title=&#34;Logistic Hypothesis&#34;&gt;&lt;/a&gt;Logistic Hypothesis&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;smooth, monotonic, sigmoid(S形) function &lt;img data-src=&#34;/img/ML/10-3.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/ML/10-2.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;0 &amp;lt;= θ(x) &amp;lt;= 1&lt;/li&gt;
&lt;li&gt;θ(x) + θ(-x) = 1 &lt;/li&gt;
&lt;li&gt;θ(-∞) = 0, θ(0) = 0.5, θ(∞) = 1&lt;/li&gt;
&lt;li&gt;令 $h(x) = θ(w^Tx)$ &lt;img data-src=&#34;/img/ML/10-4.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Likelihood&lt;br&gt;if h ~= f, [h 產生 y 的機率] 接近 [f 產生 y 的機率(其值通常大於產生其他output的機率)]&lt;br&gt;猜測：若能找到h, 其產生的output和f很像的話(也就是和data的output很像), 那h也會和f很像 → 成功學習&lt;br&gt;g = argmax_h likelihood(h) &lt;img data-src=&#34;/img/ML/10-6.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;Cross-Entropy-Error&#34;&gt;&lt;a href=&#34;#Cross-Entropy-Error&#34; class=&#34;headerlink&#34; title=&#34;Cross-Entropy Error&#34;&gt;&lt;/a&gt;Cross-Entropy Error&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;因為 h(x) = θ(wTx)&lt;ul&gt;
&lt;li&gt;h(x) = h(-x)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最高可能性 = max Π h(ynxn)&lt;ul&gt;
&lt;li&gt;Π = 連乘 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;= max ln Π θ(ynwxn)&lt;ul&gt;
&lt;li&gt;轉換成θ，取ln &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;= max Σ ln θ(ynwxn)&lt;ul&gt;
&lt;li&gt;ln Π = Σ ln &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;= min 1/N x Σ -lnθ(ynwxn) &lt;img data-src=&#34;/img/ML/10-7.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;乘1/N, 加上min和負號 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;代入θ公式, Ein = &lt;img data-src=&#34;/img/ML/10-8.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;此函式即為 Cross-Entropy Error &lt;img data-src=&#34;/img/ML/10-9.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Minimize-Cross-Entropy-Error&#34;&gt;&lt;a href=&#34;#Minimize-Cross-Entropy-Error&#34; class=&#34;headerlink&#34; title=&#34;Minimize Cross-Entropy Error&#34;&gt;&lt;/a&gt;Minimize Cross-Entropy Error&lt;/h4&gt;&lt;p&gt;find ∇Ein(w) = 0 to find min(Ein) &lt;img data-src=&#34;/img/ML/10-11.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使所有θ項都為0&lt;ul&gt;
&lt;li&gt;only if all ynwxn &amp;gt;&amp;gt; 0&lt;/li&gt;
&lt;li&gt;需要linear-seqerable data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Σ(-ynxn) = 0&lt;ul&gt;
&lt;li&gt;non-linear equation of w&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;→ 都不好算 &lt;/p&gt;
&lt;p&gt;solve it by PLA  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若有錯則更新，正確則不變($w^t = w^{t+1}$) &lt;img data-src=&#34;/img/ML/10-12.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;加入參數η，為更新的幅度倍率(本來為1) &lt;img data-src=&#34;/img/ML/10-13.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;v 代表原本的式子&lt;/li&gt;
&lt;li&gt;用 (xn, yn) 更新的大小： $θ(-y_nw^Tx_n)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;PLA-smoothing&#34;&gt;&lt;a href=&#34;#PLA-smoothing&#34; class=&#34;headerlink&#34; title=&#34;PLA smoothing&#34;&gt;&lt;/a&gt;PLA smoothing&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/ML/10-14.png&#34; alt=&#34;&#34;&gt;   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更新時就是在往Ein較低的方向走&lt;ul&gt;
&lt;li&gt;v為方向, η為幅度 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Greedy&lt;ul&gt;
&lt;li&gt;每次更新時調整η, 使Ein最小 &lt;img data-src=&#34;/img/ML/10-17.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;η夠小的時候，可用泰勒展開式 &lt;img data-src=&#34;/img/ML/10-15.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;估算出的greedy更新公式 &lt;img data-src=&#34;/img/ML/10-16.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gradient Descent &lt;img data-src=&#34;/img/ML/10-18.png&#34; alt=&#34;&#34;&gt;   &lt;ul&gt;
&lt;li&gt;最優的v是與梯度相反的方向，如果一條直線的斜率k&amp;gt;0，說明向右是上升的方向，應該向左走 &lt;/li&gt;
&lt;li&gt;距離谷底較遠（位置較高）時，步幅(η)大些比較好；接近谷底時，步幅小些比較好&lt;ul&gt;
&lt;li&gt;梯度的數值大小間接反映距離谷底的遠近 &lt;img data-src=&#34;/img/ML/10-19.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;希望步幅與梯度大小成正比&lt;ul&gt;
&lt;li&gt;wt+1 ← wt - η∇Ein(wt)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Update Time  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;decide wt+1 by all data → O(N) time&lt;/li&gt;
&lt;li&gt;Can logistic regression with O(1) time per iteration(like PLA)?&lt;br&gt;→ use one of data instead of all data&lt;br&gt;→ &lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Stochastic-Gradient-Descent-SGD&#34;&gt;&lt;a href=&#34;#Stochastic-Gradient-Descent-SGD&#34; class=&#34;headerlink&#34; title=&#34;Stochastic Gradient Descent (SGD)&#34;&gt;&lt;/a&gt;Stochastic Gradient Descent (SGD)&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;/img/ML/11-7.png&#34; alt=&#34;stochastic gradient descent&#34;&gt;      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隨機選其中一筆資料來對 w 進行更新&lt;ul&gt;
&lt;li&gt;進行足夠多的更新後，平均的隨機梯度與平均的真實梯度近似相等&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;η often use 0.1&lt;/li&gt;
&lt;li&gt;compare PLA and SGD &lt;img data-src=&#34;/img/ML/11-8.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;SGD is more flexible than PLA&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap-11-Linear-Models&#34;&gt;&lt;a href=&#34;#Chap-11-Linear-Models&#34; class=&#34;headerlink&#34; title=&#34;Chap 11 Linear Models&#34;&gt;&lt;/a&gt;Chap 11 Linear Models&lt;/h2&gt;&lt;p&gt;can linear regression or logistic regression help linear classification? &lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/ML/11-1.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/ML/11-2.png&#34; alt=&#34;&#34;&gt; ys: classification correctness score&lt;/p&gt;
&lt;p&gt;Error functions &lt;img data-src=&#34;/img/ML/11-3.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;small 0/1 error $\nRightarrow$ small square error&lt;/li&gt;
&lt;li&gt;small err SQR → small err 0/1&lt;/li&gt;
&lt;li&gt;small Err CE → small Err 0/1&lt;ul&gt;
&lt;li&gt;** small cross entropy error implies small classification error **&lt;img data-src=&#34;/img/ML/11-4.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/ML/11-5.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;small err SCE(scaled cross entropy) ↔ small err 0/1 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/ML/11-6.png&#34; alt=&#34;&#34;&gt;   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PLA&lt;ol&gt;
&lt;li&gt;優點：在數據線性可分時高效且準確&lt;/li&gt;
&lt;li&gt;缺點：只有在數據線性可分時才可行，否則需要借助POCKET算法（沒有理論保證）&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;linear regression&lt;ol&gt;
&lt;li&gt;優點：最簡單的優化（直接利用矩陣運算工具）&lt;/li&gt;
&lt;li&gt;缺點：ys 的值較大時，與 0/1 error 相差較大&lt;/li&gt;
&lt;li&gt;線性回歸得到的結果w可作為其他算法的初始值&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;logistic regression&lt;ol&gt;
&lt;li&gt;優點：比較容易優化（梯度下降）&lt;/li&gt;
&lt;li&gt;缺點：ys 是非常小的負數時，與 0/1 error 相差較大&lt;/li&gt;
&lt;li&gt;實際中，logistic回歸用於分類的效果優於線性回歸的方法和POCKET算法&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;Multiclass-Classification-meta-algorithms&#34;&gt;&lt;a href=&#34;#Multiclass-Classification-meta-algorithms&#34; class=&#34;headerlink&#34; title=&#34;Multiclass Classification - meta algorithms&#34;&gt;&lt;/a&gt;Multiclass Classification - meta algorithms&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;One-Versus-All (OVA) Decomposition&lt;ol&gt;
&lt;li&gt;對每個分類做logistic regression(共N個)，選分數y最高的&lt;/li&gt;
&lt;li&gt;優點：prediction有效率，學習時可平行處理&lt;/li&gt;
&lt;li&gt;缺點：output種類很多時，數據往往非常不平衡(x 遠大於 o)，會嚴重影響訓練準確性&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTXVsdGlub21pYWxfbG9naXN0aWNfcmVncmVzc2lvbg==&#34;&gt;multinomial logistic regression&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt; 考慮了這個問題&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;One versus One (OVO) &lt;img data-src=&#34;/img/ML/11-9.png&#34; alt=&#34;&#34;&gt;&lt;ol&gt;
&lt;li&gt;共有 N(N-1)/2 個 perceptron，投票決定&lt;/li&gt;
&lt;li&gt;優點： training有效率(每個perceptron較小), 可以使用 binary classification 的方法&lt;/li&gt;
&lt;li&gt;缺點： O(K^2) space&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;Chap-12-Nonlinear-Transformation&#34;&gt;&lt;a href=&#34;#Chap-12-Nonlinear-Transformation&#34; class=&#34;headerlink&#34; title=&#34;Chap 12 Nonlinear Transformation&#34;&gt;&lt;/a&gt;Chap 12 Nonlinear Transformation&lt;/h2&gt;&lt;p&gt;座標系轉換  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;將非線性的h(x)轉成線性 &lt;ul&gt;
&lt;li&gt;circular separable in X → linear separable in Z &lt;img data-src=&#34;/img/ML/12-2.png&#34; alt=&#34;&#34;&gt;  &lt;img data-src=&#34;/img/ML/12-1.png&#34; alt=&#34;&#34;&gt; &lt;/li&gt;
&lt;li&gt;可在X做出任何二次曲線的Z &lt;img data-src=&#34;/img/ML/12-3.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;transform data X to Z to train easily &lt;img data-src=&#34;/img/ML/12-4.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;(xn, yn) → (zn = ϕ(xn), yn)&lt;/li&gt;
&lt;li&gt;train w by (z, y)&lt;/li&gt;
&lt;li&gt;g(x) = sign(ϕ(x)w) (= sign(wz))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;代價&lt;ul&gt;
&lt;li&gt;O(Q^d)&lt;ul&gt;
&lt;li&gt;Q次方座標系, d個參數(x, y)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$d_{vc}$ 隨 Q 成長&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;力量愈強，代價愈大(可能會overfit)(見Chap13)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有效學習的條件&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Ein(g) 約等於 Eout(g)&lt;/li&gt;
&lt;li&gt;Ein(g)足夠小&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;當模型很簡單時（dvc 很小），我們更容易滿足1. 而不容易滿足2. ；反之，模型很複雜時（dvc很大），更容易滿足2. 而不容易滿足1.&lt;br&gt;→ 次方愈高，hypothesis set 包含愈多、愈複雜，Eout更偏離，也對數據擬合得更充分，Ein 更小 &lt;img data-src=&#34;/img/ML/12_1.png&#34; alt=&#34;&#34;&gt; &lt;/p&gt;
&lt;p&gt;安全的方法: 先算低次方, 若結果已足夠好就不用繼續尋找&lt;br&gt;實務上的機器學習，通常都不會使用太高維度的learning&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;linear model first: simple, efficient, safe, and workable!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3poLndpa2lwZWRpYS5vcmcvemgtdHcvJUU1JThCJTkyJUU4JUFFJUE5JUU1JUJFJUI3JUU1JUE0JTlBJUU5JUExJUI5JUU1JUJDJThG&#34;&gt;Legendre Polynomials Transform&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;: 互為正交的函式(?)，用來做transform效果較好&lt;/p&gt;
&lt;h2 id=&#34;Chap-13-Overfitting&#34;&gt;&lt;a href=&#34;#Chap-13-Overfitting&#34; class=&#34;headerlink&#34; title=&#34;Chap 13 Overfitting&#34;&gt;&lt;/a&gt;Chap 13 Overfitting&lt;/h2&gt;&lt;p&gt;overfitting: *&lt;em&gt;lower Ein, higher Eout *&lt;/em&gt;&lt;br&gt;&lt;img data-src=&#34;/img/ML/13_01.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;右側為overfitting, 左側為underfitting&lt;/p&gt;
&lt;h3 id=&#34;Case-Study&#34;&gt;&lt;a href=&#34;#Case-Study&#34; class=&#34;headerlink&#34; title=&#34;Case Study&#34;&gt;&lt;/a&gt;Case Study&lt;/h3&gt;&lt;p&gt;target function &lt;img data-src=&#34;/img/ML/12_4.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;try 2nd order and 10th order function &lt;img data-src=&#34;/img/ML/12_5.png&#34; alt=&#34;&#34;&gt;     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;左側： H10 performance is not good even if original function is 10th-order&lt;/li&gt;
&lt;li&gt;右側： even if no noise, there are still overfitting in H10&lt;ul&gt;
&lt;li&gt;hypothesis complexity acts like noise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;philosophy: 以退為進&lt;ul&gt;
&lt;li&gt;絕聖棄智，其效百倍，絕巧棄利，error無有&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;雖然H10在N大的時候Eout較低，但在N小的時候Eout非常大 &lt;img data-src=&#34;/img/ML/12_6.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ 資料不夠多(N小)的時候，不能用太複雜的hypothesis&lt;/p&gt;
&lt;p&gt;實驗 noise 對 overfit 的影響  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;noise ε with variance σ^2&lt;ul&gt;
&lt;li&gt;normal distributed iid&lt;/li&gt;
&lt;li&gt;red area has more overfit &lt;img data-src=&#34;/img/ML/13_1.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;造成overfit的原因  &lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;deterministic noise&lt;ol&gt;
&lt;li&gt;最好的hypothesis和target function的差異(depends on H)&lt;/li&gt;
&lt;li&gt;hypothesis complexity愈大，deterministic noise愈小&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;stochastic noise&lt;ol&gt;
&lt;li&gt;N改變時，noise level對noise的影響  &lt;/li&gt;
&lt;li&gt;不是固定值、不能改善&lt;/li&gt;
&lt;li&gt;example: sample error&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;overfit的四個原因  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;data size ↓&lt;/li&gt;
&lt;li&gt;stochastic noise ↑&lt;/li&gt;
&lt;li&gt;deterministic noise ↑&lt;/li&gt;
&lt;li&gt;hypothesis set的power ↑&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;類比成開車 &lt;img data-src=&#34;/img/ML/13-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Overfit是很常發生的！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data Cleaning/Pruning:&lt;br&gt;對於有些”奇怪”的data   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;改成自己認為是對的output (data cleaning)&lt;/li&gt;
&lt;li&gt;移除資料 (data pruning)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Data Hinting:&lt;br&gt;用已有的知識處理原有的data，產生新的data(不是偷看！)，適合於資料量不足時(Ex. 手寫辨識，可稍微旋轉、平移)，要注意新data的比例是否符合現實情況&lt;/p&gt;
&lt;h2 id=&#34;Chap14-Regularization&#34;&gt;&lt;a href=&#34;#Chap14-Regularization&#34; class=&#34;headerlink&#34; title=&#34;Chap14 Regularization&#34;&gt;&lt;/a&gt;Chap14 Regularization&lt;/h2&gt;&lt;p&gt;Regularization： 設條件(constraint) 降低 hypothesis set 的 complexity&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;對H10改動(= H2’)&lt;/th&gt;
&lt;th&gt;complexity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;w3~w10為0&lt;/td&gt;
&lt;td&gt;H2’ == H2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;w0~w10其中3個不為0&lt;/td&gt;
&lt;td&gt;H2 &amp;lt; H2’ &amp;lt; H10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wi的和小於一固定值  H(C)=sum(w^2)&amp;lt;=C&lt;/td&gt;
&lt;td&gt;soft and smooth structure, e.g. H(0) &amp;lt; H(11.26) &amp;lt; H(∞) = H10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;限制參數大小： NP-hard to solve &lt;img data-src=&#34;/img/ML/13-3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Lagrange Multiplier &lt;img data-src=&#34;/img/ML/13-4.png&#34; alt=&#34;&#34;&gt;   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;min(Ein)：朝梯度的反方向&lt;ul&gt;
&lt;li&gt;-▽Ein(W)為更新的向量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;W為原點到指定點的向量&lt;/li&gt;
&lt;li&gt;只找出在紅色圓(限制)內的最佳解，即紅色圓上與$W_{lin}$最近的點&lt;ul&gt;
&lt;li&gt;$W_{lin}$為linear regression的解&lt;/li&gt;
&lt;li&gt;W與-▽Ein(W)平行的時候 &lt;img data-src=&#34;/img/ML/13-5.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以藉由設不同的 λ 來產生 W，此時 λ 和 H(C) 的 C 相似，用來限制參數    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ridge Regression (similar to linear regression) &lt;img data-src=&#34;/img/ML/13-6.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Augmented Error  &lt;ul&gt;
&lt;li&gt;solve min(Eaug) (unconstrained) is easier than solve min(Ein)(constrained) &lt;/li&gt;
&lt;li&gt;積分後得到regularizer$w^Tw$ &lt;img data-src=&#34;/img/ML/13-01.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;wREG = argmin(w) Eaug(w)&lt;/li&gt;
&lt;li&gt;weight-decay&lt;ul&gt;
&lt;li&gt;Penalize large weights using penalties&lt;/li&gt;
&lt;li&gt;λ↑ → perfer shorter w → effective C↓&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;λ 對應到 C &lt;img data-src=&#34;/img/ML/13-9.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;只需一點regularization就有效 &lt;img data-src=&#34;/img/ML/13-8.png&#34; alt=&#34;&#34;&gt; &lt;/p&gt;
&lt;p&gt;regularizer只限制單一hypothesis的complexity，不像VC bound整個hypothesis set都限制，所以Eaug比Ein更接近Eout &lt;img data-src=&#34;/img/ML/14-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Effective VC Dimension of Eaug  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dVC(H) = d + 1 &lt;/li&gt;
&lt;li&gt;實際的 dvc 更小(被λ限制)，但不好證明 &lt;img data-src=&#34;/img/ML/14-2.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;General Regularizers &lt;img data-src=&#34;/img/ML/14-3.png&#34; alt=&#34;&#34;&gt;    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;target-dependent&lt;ul&gt;
&lt;li&gt;用target function的性質來限制&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;plausible(合理的)&lt;ul&gt;
&lt;li&gt;預期比較平滑、簡單的hypothesis，因為noise是較不平滑的&lt;/li&gt;
&lt;li&gt;L1(sparsity regularizer): regularizer = Σ|wq|&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;friendly(easy to use)&lt;ul&gt;
&lt;li&gt;L2(weight-decay regularizer): regularizer = Σ$w_q^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;comparison: error → user-dependent, plausible, friendly&lt;/li&gt;
&lt;li&gt;augmented error = error + regulizer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/ML/14-4.png&#34; alt=&#34;L1 and L2&#34;&gt; L1 useful when need sparse solution(有許多零的w, 因w最終會落到正方形的頂點)，L1即表示限制函數為一次方 &lt;/p&gt;
&lt;p&gt;noise愈多，需要的regularization愈多 ↔ more bumpy road, putting brakes more &lt;img data-src=&#34;/img/ML/14-5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Conclusion:&lt;br&gt;正規化用來減少hypothesis的complexity，避免overfit，用wTw作regulizer(L2)，以λ為參數調整正規化的程度(即L2圓的大小，L1正方形的大小)，通常λ不會太大&lt;/p&gt;
&lt;h2 id=&#34;Chap15-Validation&#34;&gt;&lt;a href=&#34;#Chap15-Validation&#34; class=&#34;headerlink&#34; title=&#34;Chap15 Validation&#34;&gt;&lt;/a&gt;Chap15 Validation&lt;/h2&gt;&lt;p&gt;So Many Models can choose, so use validation to check which is good choice&lt;/p&gt;
&lt;p&gt;selecting by E_in is dangerous(can’t reflect Eout)&lt;br&gt;selecting by E_test is infeasible and cheating(not easy to get test data)&lt;/p&gt;
&lt;p&gt;$E_{val}$: legal cheating     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;將data分成train和validation二部分&lt;/li&gt;
&lt;li&gt;用train學習，用valid測試&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在許多切割(fold)之中，找$E_{val}$最小的hypothesis，並用這個hypothesis和&lt;strong&gt;全部的data&lt;/strong&gt;算出g &lt;img data-src=&#34;/img/ML/15-1.png&#34; alt=&#34;&#34;&gt;   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$g_m^{-}$ 為 validation data 算出的g &lt;img data-src=&#34;/img/ML/14-7.png&#34; alt=&#34;&#34;&gt; &lt;/li&gt;
&lt;li&gt;find balance of validation data size &lt;img data-src=&#34;/img/ML/15-2.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;leave-one-out cross validation ($E_{loocv}$)&lt;ul&gt;
&lt;li&gt;每次只用一個資料作validation(K = 1)&lt;/li&gt;
&lt;li&gt;often called ‘almost unbiased estimate of Eout’ &lt;img data-src=&#34;/img/ML/15-3.png&#34; alt=&#34;&#34;&gt;  &lt;/li&gt;
&lt;li&gt;缺點：計算太多(一個model要train N次, N為資料個數)&lt;/li&gt;
&lt;li&gt;改善：切成n塊(通常5fold, 10fold)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;選擇 - 先選要測試的models，再用validation選出最好的     &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;all training models: select among hypotheses(初賽)&lt;/li&gt;
&lt;li&gt;all validation schemes: select among finalists(複賽)&lt;/li&gt;
&lt;li&gt;all testing methods: just evaluate&lt;/li&gt;
&lt;li&gt;Still use &lt;strong&gt;test result(之前沒用過的data)&lt;/strong&gt; for final benchmark, not best validation result&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;Chap16-Three-Learning-Principles&#34;&gt;&lt;a href=&#34;#Chap16-Three-Learning-Principles&#34; class=&#34;headerlink&#34; title=&#34;Chap16 Three Learning Principles&#34;&gt;&lt;/a&gt;Chap16 Three Learning Principles&lt;/h2&gt;&lt;h3 id=&#34;Occam’s-Razor&#34;&gt;&lt;a href=&#34;#Occam’s-Razor&#34; class=&#34;headerlink&#34; title=&#34;Occam’s Razor&#34;&gt;&lt;/a&gt;Occam’s Razor&lt;/h3&gt;&lt;p&gt;An explanation of the data should be made as simple as possible, but no simpler&lt;br&gt;用最簡單且有效的方法解釋資料&lt;br&gt;&lt;img data-src=&#34;/img/ML/16-1.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;因為愈簡單的H 愈難分資料 → 可以分開資料時，有顯著性(若是用複雜模型，分開是很容易的)&lt;br&gt;→ linear first, always ask whether overfitting&lt;/p&gt;
&lt;h3 id=&#34;Sampling-Bias&#34;&gt;&lt;a href=&#34;#Sampling-Bias&#34; class=&#34;headerlink&#34; title=&#34;Sampling Bias&#34;&gt;&lt;/a&gt;Sampling Bias&lt;/h3&gt;&lt;p&gt;抽樣誤差：抽樣非真正隨機&lt;br&gt;Ex. 1948電話民調，但電話當時昂貴&lt;br&gt;movie recommend system: When data have time sequential, should emphasize later data, do not use random data&lt;/p&gt;
&lt;h3 id=&#34;Data-Snooping&#34;&gt;&lt;a href=&#34;#Data-Snooping&#34; class=&#34;headerlink&#34; title=&#34;Data Snooping&#34;&gt;&lt;/a&gt;Data Snooping&lt;/h3&gt;&lt;p&gt;偷看資料(機器學習 → 人腦學習)，會包含大腦所花的complexity &lt;img data-src=&#34;/img/ML/16-2.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/ML/16-3.png&#34; alt=&#34;&#34;&gt; &lt;/p&gt;
&lt;p&gt;paper1: H1 works well on data D&lt;br&gt;paper2: find H2 and &lt;strong&gt;publish if better than H1 on D&lt;/strong&gt;&lt;br&gt;….&lt;br&gt;→ bad generalization, cause overfit (if you torture the data long enough, it will confess)&lt;br&gt;→ 解決方法：不要先看paper，先提出自己的方法，再和已發表的方法比較&lt;/p&gt;
&lt;h3 id=&#34;Conclusion&#34;&gt;&lt;a href=&#34;#Conclusion&#34; class=&#34;headerlink&#34; title=&#34;Conclusion&#34;&gt;&lt;/a&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Three Related Fields&lt;br&gt;&lt;img data-src=&#34;/img/ML/16-4.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Three Theoretical Bounds&lt;br&gt;&lt;img data-src=&#34;/img/ML/16-5.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Three Linear Models&lt;br&gt;&lt;img data-src=&#34;/img/ML/16-6.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Three Key Tools: Feature Transform, Regularization, Validation&lt;br&gt;&lt;img data-src=&#34;/img/ML/16-7.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Three Future Directions(in &lt;a href=&#34;http://gitqwerty777.github.io/MLtechnique/&#34;&gt;ML techniques&lt;/a&gt;)&lt;br&gt;&lt;img data-src=&#34;/img/ML/16-8.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;p&gt;End~~&lt;/p&gt;
&lt;h2 id=&#34;Reference&#34;&gt;&lt;a href=&#34;#Reference&#34; class=&#34;headerlink&#34; title=&#34;Reference&#34;&gt;&lt;/a&gt;Reference&lt;/h2&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3d3dy5kb3ViYW4uY29tL2RvdWxpc3QvMzM4MTg1My8=&#34;&gt;http://www.douban.com/doulist/3381853/&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3d3dy5jc2llLm50dS5lZHUudHcvfmh0bGluL2NvdXJzZS9tbDE0ZmFsbC8=&#34;&gt;HTLin講義&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9jbGFzcy5jb3Vyc2VyYS5vcmcvbnR1bWxvbmUtMDAy&#34;&gt;Coursera&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
</content>
        <category term="機器學習" />
        <updated>2014-10-31T07:45:45.000Z</updated>
    </entry>
    <entry>
        <id>http://gitqwerty777.github.io/computer-gaming/</id>
        <title>電腦對局理論</title>
        <link rel="alternate" href="http://gitqwerty777.github.io/computer-gaming/"/>
        <content type="html">&lt;!-- RENEW: --&gt;

&lt;blockquote&gt;
&lt;p&gt;註：此為2014年版，且只寫到第八章(因為教授只考到這)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;Introduction&#34;&gt;&lt;a href=&#34;#Introduction&#34; class=&#34;headerlink&#34; title=&#34;Introduction&#34;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;h3 id=&#34;學習電腦對局的用處&#34;&gt;&lt;a href=&#34;#學習電腦對局的用處&#34; class=&#34;headerlink&#34; title=&#34;學習電腦對局的用處&#34;&gt;&lt;/a&gt;學習電腦對局的用處&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;電腦愈聰明，對人類愈有用&lt;/li&gt;
&lt;li&gt;電腦學得的技巧讓人學習&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;為何學棋局&#34;&gt;&lt;a href=&#34;#為何學棋局&#34; class=&#34;headerlink&#34; title=&#34;為何學棋局&#34;&gt;&lt;/a&gt;為何學棋局&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;容易辨別輸贏&lt;/li&gt;
&lt;li&gt;規則簡單(先備知識少)&lt;/li&gt;
&lt;/ol&gt;
&lt;a id=&#34;more&#34;&gt;&lt;/a&gt;

&lt;h3 id=&#34;圖靈測試-Turing-test&#34;&gt;&lt;a href=&#34;#圖靈測試-Turing-test&#34; class=&#34;headerlink&#34; title=&#34;圖靈測試(Turing test)&#34;&gt;&lt;/a&gt;圖靈測試(Turing test)&lt;/h3&gt;&lt;p&gt;If a machine is intelligent, then it cannot be distinguished from a human&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;反過來利用的例子 - CAPTCHA(驗證碼): Completely Automated Public Turing test to tell Computers and Humans Apart&lt;/li&gt;
&lt;li&gt;Wolfram Alpha&lt;ul&gt;
&lt;li&gt;knowledge base of Siri&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Problems  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are all human behaviors intelligent?&lt;/li&gt;
&lt;li&gt;Can human perform every possible intelligent behavior?&lt;br&gt;→ Human Intelligence 和 Intelligence 並不完全相同&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;改變目標&#34;&gt;&lt;a href=&#34;#改變目標&#34; class=&#34;headerlink&#34; title=&#34;改變目標&#34;&gt;&lt;/a&gt;改變目標&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;From Artificial Intelligence to &lt;strong&gt;Machine Intelligence&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;machine intelligence: the thing machine can do better than human do&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;From imitation of human behaviors to doing intelligent behaviors&lt;/li&gt;
&lt;li&gt;From general-purpose intelligence to &lt;strong&gt;domain-dependent&lt;/strong&gt; Expert Systems&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;重大突破&#34;&gt;&lt;a href=&#34;#重大突破&#34; class=&#34;headerlink&#34; title=&#34;重大突破&#34;&gt;&lt;/a&gt;重大突破&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;1912 - End-Game chess playing machine  &lt;/li&gt;
&lt;li&gt;~1970 - Brute Force    &lt;/li&gt;
&lt;li&gt;1975 - Alpha-Beta pruning(Knuth and Moore)   &lt;/li&gt;
&lt;li&gt;1993 - Monte Carlo  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;無關：核心知識&#34;&gt;&lt;a href=&#34;#無關：核心知識&#34; class=&#34;headerlink&#34; title=&#34;無關：核心知識&#34;&gt;&lt;/a&gt;無關：核心知識&lt;/h3&gt;&lt;p&gt;用少部分的核心知識(要記得的事物)推得大多數的知識&lt;br&gt;Ex. 背九九乘法表推得所有多位數乘法&lt;br&gt;建構式數學(X)  &lt;/p&gt;
&lt;h3 id=&#34;對局分類&#34;&gt;&lt;a href=&#34;#對局分類&#34; class=&#34;headerlink&#34; title=&#34;對局分類&#34;&gt;&lt;/a&gt;對局分類&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;研究遊戲之前的必要分析：分類&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By number of players   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single-player games&lt;ul&gt;
&lt;li&gt;puzzles&lt;/li&gt;
&lt;li&gt;Most of them are NP-complete&lt;ul&gt;
&lt;li&gt;or the game will be not fun to play&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two-player games&lt;ul&gt;
&lt;li&gt;Most of them are either P-SPACE-complete(polynomial space usage) or exponential-time-complete&lt;ul&gt;
&lt;li&gt;PSPACE-complete can be thought of as the hardest problems in PSPACE, solution of PSPACE-complete could easily be used to solve any other problem in PSPACE&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multi-player games&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By state information obtained by each player(盤面資訊是否完全)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Perfect-information games&lt;ul&gt;
&lt;li&gt;all players have all the information to make a correct decision&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Imperfect-information games&lt;ul&gt;
&lt;li&gt;some information is only available to selected players, for example you cannot see the opponent’s cards in Poker(不知對手的牌或棋子, Ex. 橋牌)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By rules of games known in advance(是否有特殊規則、是否知道對手的行動)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Complete-information games&lt;ul&gt;
&lt;li&gt;rules of the game are fully known by all players in advance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Incomplete-information games&lt;ul&gt;
&lt;li&gt;partial rules are not given in advance for some players(Ex. 囚犯困境賽局)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3d3dy5lY29uLnVjc2IuZWR1L35nYXJyYXR0L0Vjb24xNzEvTGVjdDE0X1NsaWRlcy5wZGY=&#34;&gt;definition of perfect and complete information in game theory&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By whether players can fully control the playing of the game(是否受隨機性影響)    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stochastic games&lt;ul&gt;
&lt;li&gt;there is an element of chance such as dice rolls &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deterministic games&lt;ul&gt;
&lt;li&gt;players have a full control over the games&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example(not fully sure):  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;perfect-information complete-information deterministic game: chinese chess, go    &lt;/li&gt;
&lt;li&gt;perfect-information complete-information stochastic game: dark chinese chess, 輪盤(Roulette)    &lt;/li&gt;
&lt;li&gt;perfect-information incomplete-information deterministic game: Prisoner’s Dilemma    &lt;/li&gt;
&lt;li&gt;perfect-information incomplete-information stochastic game: ?    &lt;/li&gt;
&lt;li&gt;inperfect-information complete-information deterministic game: ?    &lt;/li&gt;
&lt;li&gt;inperfect-information complete-information stochastic game: monopoly, bridge   &lt;/li&gt;
&lt;li&gt;inperfect-information incomplete-information deterministic game: battleship, bingo    &lt;/li&gt;
&lt;li&gt;inperfect-information incomplete-information stochastic game: most of the table/computer games&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap02-Basic-Search-Algorithms&#34;&gt;&lt;a href=&#34;#Chap02-Basic-Search-Algorithms&#34; class=&#34;headerlink&#34; title=&#34;Chap02 Basic Search Algorithms&#34;&gt;&lt;/a&gt;Chap02 Basic Search Algorithms&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Brute force&lt;/li&gt;
&lt;li&gt;Systematic brute-force search  &lt;ul&gt;
&lt;li&gt;Breadth-first search (BFS)  &lt;/li&gt;
&lt;li&gt;Depth-first search (DFS)  &lt;ul&gt;
&lt;li&gt;Depth-first Iterative-deepening (DFID)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bi-directional search&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Heuristic search: best-first search  &lt;ul&gt;
&lt;li&gt;A*  &lt;ul&gt;
&lt;li&gt;IDA*&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Symbol-Definition&#34;&gt;&lt;a href=&#34;#Symbol-Definition&#34; class=&#34;headerlink&#34; title=&#34;Symbol Definition&#34;&gt;&lt;/a&gt;Symbol Definition&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Node branching factor &lt;code&gt;b&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;degree&lt;/li&gt;
&lt;li&gt;number of neighbor vertexs of a node&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Edge branching factor &lt;code&gt;e&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;number of connected edges of a node&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Depth of a solution &lt;code&gt;d&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;最短深度, &lt;code&gt;D&lt;/code&gt; 為最長深度&lt;/li&gt;
&lt;li&gt;Root深度為0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;b&lt;/code&gt; and &lt;code&gt;e&lt;/code&gt; are average constant number, &lt;code&gt;e&lt;/code&gt; &amp;gt;= &lt;code&gt;b&lt;/code&gt;(兩個點之間可能有多條線)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Brute-force-search&#34;&gt;&lt;a href=&#34;#Brute-force-search&#34; class=&#34;headerlink&#34; title=&#34;Brute-force search&#34;&gt;&lt;/a&gt;Brute-force search&lt;/h3&gt;&lt;p&gt;Used information  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initial state&lt;/li&gt;
&lt;li&gt;method to find adjacent states&lt;/li&gt;
&lt;li&gt;goal-checking method(whether current state is goal)  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pure brute-force search program &lt;img data-src=&#34;/img/TCG/54GbBxV.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隨機走旁邊的一個點&lt;/li&gt;
&lt;li&gt;不記憶走過的路&lt;ul&gt;
&lt;li&gt;May take infinite time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pure Random Algorithm 應用&lt;ul&gt;
&lt;li&gt;驗證碼(e.g. 虛寶)&lt;/li&gt;
&lt;li&gt;純隨機數&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;BFS-Breadth-First-Search&#34;&gt;&lt;a href=&#34;#BFS-Breadth-First-Search&#34; class=&#34;headerlink&#34; title=&#34;BFS(Breadth-First Search)&#34;&gt;&lt;/a&gt;BFS(Breadth-First Search)&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/TCG/mrf0Egx.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;deeper(N): 回傳與N相鄰的點&lt;br&gt;record parent state and backtrace to Find the path &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Space complexity: $O(b^d)$ → Too big!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Time complexity: $O(b^{d-1} * e)$     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;→ costs O(e) to find deeper(N), at most check b^(d-1) times(deeper(leaf) do not return new node)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open list: nodes that are in the queue(candidate nodes)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Closed list: nodes that have been explored(assure not answer, can skip)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Need a good algorithm to check for states in deeper(N) are visited or not&lt;ul&gt;
&lt;li&gt;Hash  &lt;/li&gt;
&lt;li&gt;Binary search&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;not need to have because it won’t guarantee to improve the performance&lt;/li&gt;
&lt;li&gt;if it is possible to have no solution, Need to store nodes that are already visited &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;node： open list → check is goal or not, explore(deeper) → closed list&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Property    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Always finds optimal solution&lt;/li&gt;
&lt;li&gt;Do not fall into loops if goal exists(always “deeper”) &lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Disk-based-algorithm&#34;&gt;&lt;a href=&#34;#Disk-based-algorithm&#34; class=&#34;headerlink&#34; title=&#34;Disk based algorithm&#34;&gt;&lt;/a&gt;Disk based algorithm&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;/img/TCG/i8bbMET.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Solution for huge space complexity&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;disk: store main data&lt;/li&gt;
&lt;li&gt;memory: store buffers&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Store open list(QUEUE) in disk&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Append&lt;/strong&gt; buffered open list to disk when memory is full or QUEUE is empty&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Store closed list in disk and maintain them as sorted&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Merge&lt;/strong&gt; buffered closed list with disk closed list when memory is full   &lt;/li&gt;
&lt;li&gt;delay cheking: check node in the closed list or not before being taken from open list&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Disk-based-algorithms&#34;&gt;&lt;a href=&#34;#Disk-based-algorithms&#34; class=&#34;headerlink&#34; title=&#34;Disk based algorithms&#34;&gt;&lt;/a&gt;Disk based algorithms&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;not too slow&lt;ul&gt;
&lt;li&gt;read large file in sequence&lt;ul&gt;
&lt;li&gt;queue(always retrieve at head and write at end)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;sorting of data in disk&lt;ul&gt;
&lt;li&gt;merge sort between disk list and buffer list&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;very slow&lt;ul&gt;
&lt;li&gt;read file in random order(disk spinning)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;系統為資源和效率(時間、空間、錢)的trade-off&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;DFS&#34;&gt;&lt;a href=&#34;#DFS&#34; class=&#34;headerlink&#34; title=&#34;DFS&#34;&gt;&lt;/a&gt;DFS&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/TCG/65RmOgp.png&#34; alt=&#34;DFSalgo&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;performance mostly depends on &lt;strong&gt;move ordering&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;If first choose the branch include the goal, find answer quick&lt;/li&gt;
&lt;li&gt;get out of long and wrong branches ASAP!&lt;/li&gt;
&lt;li&gt;implement &lt;code&gt;next(current, N)&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;作用：列舉出N的所有鄰居&lt;/li&gt;
&lt;li&gt;回傳下一個N的鄰居，目前列舉到current&lt;/li&gt;
&lt;li&gt;next(null, N) -&amp;gt; return first neighbor of N&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;time complexity: $O(e^D)$&lt;ul&gt;
&lt;li&gt;number of possible branches at depth D&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;space complexity: $O(D)$&lt;ul&gt;
&lt;li&gt;Only need to store current path in the Stack&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Property  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;need to store close list (BFS: do not need to)&lt;/li&gt;
&lt;li&gt;May not find an optimal solution&lt;/li&gt;
&lt;li&gt;Can’t properly implement on disk&lt;ul&gt;
&lt;li&gt;very huge closed list&lt;ul&gt;
&lt;li&gt;Use data compression or bit-operation techniques to store visited nodes&lt;/li&gt;
&lt;li&gt;Need a good heuristic to store the most frequently visited nodes to avoid swapping too often&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;need to check closed list instantly(BFS: can be delayed)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can DFS be paralleled? Computer scientists fails to do so even after 30 years&lt;/li&gt;
&lt;li&gt;Most critical drawback: huge and unpredictable time complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;General-skills-to-improve-searching-algorithm&#34;&gt;&lt;a href=&#34;#General-skills-to-improve-searching-algorithm&#34; class=&#34;headerlink&#34; title=&#34;General skills to improve searching algorithm&#34;&gt;&lt;/a&gt;General skills to improve searching algorithm&lt;/h3&gt;&lt;h4 id=&#34;Iterative-Deepening-ID-逐層加深&#34;&gt;&lt;a href=&#34;#Iterative-Deepening-ID-逐層加深&#34; class=&#34;headerlink&#34; title=&#34;Iterative-Deepening(ID) 逐層加深&#34;&gt;&lt;/a&gt;Iterative-Deepening(ID) 逐層加深&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;inspired from BFS(BFS = BFID)&lt;/li&gt;
&lt;li&gt;限制搜尋時的複雜度，若找不到再放寬限制&lt;/li&gt;
&lt;li&gt;prevent worse cases&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deep First ID(DFID)     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;限制深度 &lt;ul&gt;
&lt;li&gt;找到解立即return &lt;img data-src=&#34;/img/TCG/9X2ZiRm.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/TCG/gmD51AT.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;time complexity using 二項式定理 &lt;img data-src=&#34;/img/TCG/IfDEwFh.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/TCG/d0m27cU.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;M(e, d) ~ $O(e^d)$ when e is sufficiently large&lt;/li&gt;
&lt;li&gt;→ no so much time penalty to use ID when e is big enough&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;關鍵：設定初始限制和限制放寬的大小&lt;/li&gt;
&lt;li&gt;always find optimal solution&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Bi-directional-search&#34;&gt;&lt;a href=&#34;#Bi-directional-search&#34; class=&#34;headerlink&#34; title=&#34;Bi-directional search&#34;&gt;&lt;/a&gt;Bi-directional search&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;/img/TCG/1-1.png&#34; alt=&#34;DFSdir&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;DFSdir(B, G, successor, i)&lt;/code&gt;: DFS with starting states B, goal states G, successor function and &lt;strong&gt;depth limit i&lt;/strong&gt;  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nextdir(current, successor, N)&lt;/code&gt;: returns the state next to the state “current” in successor(N)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;deeper(current, N)&lt;/code&gt; for forward searching&lt;ul&gt;
&lt;li&gt;deeper(N) contains all next states of N&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;prev(current, N)&lt;/code&gt; for backward searching&lt;ul&gt;
&lt;li&gt;prev(N) contains all previous states of N&lt;br&gt;&lt;img data-src=&#34;/img/TCG/1-2.png&#34; alt=&#34;BDS&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Forward Search: store all states H&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Backward Search: find the path from G(goal) to H at depth = limit or limit+1(for odd-lengthed solutions)  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;also use the concept of iterative-deepening&lt;br&gt;&lt;img data-src=&#34;/img/TCG/7iBkfKB.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Time complexity: $O(e^{d/2})$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the number of nodes visited is greatly reduced(compared with original $O(e^d)$)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Space complexity: $O(e^{d/2})$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pay the price of storing state depth(H)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;restrict&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can’t assure to find optimal solution&lt;/li&gt;
&lt;li&gt;need to know what the goals are &lt;ul&gt;
&lt;li&gt;bi-directional search is used when goal is known, only want to find path, like solving 15-puzzle&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Heuristic-啟發式-search&#34;&gt;&lt;a href=&#34;#Heuristic-啟發式-search&#34; class=&#34;headerlink&#34; title=&#34;Heuristic(啟發式) search&#34;&gt;&lt;/a&gt;Heuristic(啟發式) search&lt;/h3&gt;&lt;p&gt;Definition: criteria, methods, or principles for deciding which is the most effective to achieve some goal&lt;br&gt;→ By 經驗法則(so not always have optimal solution)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先走最有可能通往答案的state(good move ordering)&lt;ul&gt;
&lt;li&gt;best-first algorithm : like greedy   &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The unlikely path will be explored further(pruning)  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key: how to pick the next state to explore&lt;/strong&gt;   &lt;ul&gt;
&lt;li&gt;need simple and effective &lt;strong&gt;estimate function&lt;/strong&gt; to discriminate    &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Heuristic-search-–-A&#34;&gt;&lt;a href=&#34;#Heuristic-search-–-A&#34; class=&#34;headerlink&#34; title=&#34;Heuristic search – A*&#34;&gt;&lt;/a&gt;Heuristic search – A*&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;/img/TCG/Vv8N3hj.png&#34; alt=&#34;A*&#34;&gt;&lt;br&gt;line 12: add all possible path that depth = depth + 1   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open list: a priorty queue(PQ) to store paths with costs&lt;/li&gt;
&lt;li&gt;Closed list: store all visited nodes with the smallest cost&lt;ul&gt;
&lt;li&gt;Check for duplicated visits in the closed list only&lt;/li&gt;
&lt;li&gt;A node is inserted if &lt;ul&gt;
&lt;li&gt;it has never been visited before&lt;/li&gt;
&lt;li&gt;being visited, but has smaller cost&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Given a path P&lt;ul&gt;
&lt;li&gt;g(P) = current cost of P&lt;/li&gt;
&lt;li&gt;h(P) = estimation of remaining path to goal(&lt;strong&gt;heuristic cost&lt;/strong&gt; of P)&lt;/li&gt;
&lt;li&gt;f(P) = g(P) + h(P) is the cost function&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Assume all costs are positive, so there is no need to check for falling into a loop  &lt;/li&gt;
&lt;li&gt;cost function所推測的cost不可超過實際的cost，否則不保證找到最佳解&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;if h() never overestimates the actual cost to the goal&lt;/strong&gt; (called admissible可容許), then &lt;strong&gt;A* always finds an optimal solution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;證明？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;h(n)=0 : A* 等同 BFS&lt;/li&gt;
&lt;li&gt;h(n)&amp;lt;目前節點到結束點的距離 : A* 演算法保證找到最短路徑, h(n)越小, 搜尋深度越深(代表花愈多時間)&lt;/li&gt;
&lt;li&gt;h(n)=目前節點到結束點的距離 : A* 演算法僅會尋找最佳路徑, 並且能快速找到結果(最理想情況)&lt;/li&gt;
&lt;li&gt;h(n)&amp;gt;目前節點到結束點的距離 : 不保證能找到最短路徑, 但計算比較快&lt;/li&gt;
&lt;li&gt;h(n)與g(n)高度相關 : A* 演算法此時成為Best-First Search&lt;br&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL2Jsb2cubWluc3RyZWwuaWR2LnR3LzIwMDQvMTIvc3Rhci1hbGdvcml0aG0uaHRtbA==&#34;&gt;http://blog.minstrel.idv.tw/2004/12/star-algorithm.html&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Question:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What disk based techniques can be used?&lt;/li&gt;
&lt;li&gt;Why do we need a non-trivial h(P) that is admissible?&lt;/li&gt;
&lt;li&gt;How to design an admissible cost function?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;DFS-with-threshold&#34;&gt;&lt;a href=&#34;#DFS-with-threshold&#34; class=&#34;headerlink&#34; title=&#34;DFS with threshold&#34;&gt;&lt;/a&gt;DFS with threshold&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DFScost(N, f, threshold)&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;starting state N &lt;/li&gt;
&lt;li&gt;cost function f&lt;/li&gt;
&lt;li&gt;cuts off a path if cost bigger than threshold &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;DFS1&lt;/code&gt;: Use &lt;code&gt;next1(current,N)&lt;/code&gt; find neighbors of N (in the order of low cost to high cost)&lt;br&gt;&lt;img data-src=&#34;/img/TCG/csd9mLf.png&#34; alt=&#34;dfs1&#34;&gt;&lt;br&gt;&lt;code&gt;DFS2&lt;/code&gt;: Use a priority queue instead of using a stack in &lt;code&gt;DFScost&lt;/code&gt;&lt;br&gt;&lt;img data-src=&#34;/img/TCG/jthjSm8.png&#34; alt=&#34;dfs2&#34;&gt;&lt;br&gt;It may be costly to maintain a priority queue&lt;/p&gt;
&lt;h3 id=&#34;IDA-DFID-A&#34;&gt;&lt;a href=&#34;#IDA-DFID-A&#34; class=&#34;headerlink&#34; title=&#34;IDA* = DFID + A*&#34;&gt;&lt;/a&gt;IDA* = DFID + A*&lt;/h3&gt;&lt;p&gt;用A*的cost作為DFS的threshold&lt;br&gt;&lt;img data-src=&#34;/img/TCG/PJ2bPrX.png&#34; alt=&#34;&#34;&gt; &lt;/p&gt;
&lt;p&gt;Ex. 15 puzzle&lt;br&gt;all posibilities: $16! \leq 2.1 \times 10^{13}$&lt;br&gt;g(P): the number of moves made so far&lt;br&gt;h(P): &lt;strong&gt;Manhattan distance&lt;/strong&gt; between the current board and the goal&lt;br&gt;Manhattan distance from (i, j) to (i’, j’) is |i’ - i| + |j’ - j| (admissible)   &lt;/p&gt;
&lt;h3 id=&#34;basic-thought-for-a-problem&#34;&gt;&lt;a href=&#34;#basic-thought-for-a-problem&#34; class=&#34;headerlink&#34; title=&#34;basic thought for a problem&#34;&gt;&lt;/a&gt;basic thought for a problem&lt;/h3&gt;&lt;p&gt;&lt;em&gt;What you should think about before playing a game&lt;/em&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Needed to &lt;ul&gt;
&lt;li&gt;Find an optimal solution?&lt;/li&gt;
&lt;li&gt;batch operations?&lt;/li&gt;
&lt;li&gt;disk based algorithms?&lt;/li&gt;
&lt;li&gt;Search in parallel?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Balancing&lt;/strong&gt; in resource usage:&lt;ul&gt;
&lt;li&gt;memorize past results vs efforts to search again(time and space)&lt;/li&gt;
&lt;li&gt;The efforts to compute a better heuristic(time to think a heuristic?)&lt;/li&gt;
&lt;li&gt;The amount of resources spent in implementing a better heuristic and the amount of resources spent in searching(complexity of heuristic function)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For specific algorithm&lt;ul&gt;
&lt;li&gt;heuristic : How to design a good and non-trivial heuristic function?&lt;/li&gt;
&lt;li&gt;DFS : How to get a better move ordering?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Can these techniques be applied to two-person game?&lt;/p&gt;
&lt;h3 id=&#34;algorithm整理&#34;&gt;&lt;a href=&#34;#algorithm整理&#34; class=&#34;headerlink&#34; title=&#34;algorithm整理&#34;&gt;&lt;/a&gt;algorithm整理&lt;/h3&gt;&lt;p&gt;| Name      | Time Complexity | Space Complexity | OptimalSolution    | UseDisk | Description               |&lt;br&gt;| ——— | ————— | —————- | —————— | ——- |&lt;br&gt;| brute     | $∞$             | $O(1)$           | No                 | No      |&lt;br&gt;| BFS       | $O(b^d)$        | $O(b^{d-1} * e)$ | Yes                | Needed  |&lt;br&gt;| DFS       | $O(e^d)$        | $O(d)$           | No                 | NoNeed  |&lt;br&gt;| Heuristic | N\A             | N\A              | Yes, if admissible | –      | Ex. A*                    |&lt;br&gt;| BDS       | $O(e^{d/2})$    | $O(e^{d/2})$     | No                 | Needed  | DFS + bidiretional search |&lt;br&gt;| DFID      | $O(e^d)$        | $O(d)$           | Yes                | NoNeed  | DFS + ID                  |&lt;br&gt;| IDA*      | N\A             | N\A              | Yes                | N\A     | DFID + A*                 |&lt;/p&gt;
&lt;h2 id=&#34;Chap03-Heuristic-Search-with-Pre-Computed-Databases&#34;&gt;&lt;a href=&#34;#Chap03-Heuristic-Search-with-Pre-Computed-Databases&#34; class=&#34;headerlink&#34; title=&#34;Chap03 Heuristic Search with Pre-Computed Databases&#34;&gt;&lt;/a&gt;Chap03 Heuristic Search with Pre-Computed Databases&lt;/h2&gt;&lt;p&gt;new form of heuristic called &lt;strong&gt;pattern databases&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the subgoals can be divided&lt;ul&gt;
&lt;li&gt;Can sget better admissible cost function by &lt;strong&gt;sum of costs of the subgoals&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Make use of the fact that computers can memorize lots of patterns&lt;ul&gt;
&lt;li&gt;使用已經計算過的 pattern 來做出更好、更接近real cost的heuristic function &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using 15 puzzle as example &lt;img data-src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/15-puzzle.svg/480px-15-puzzle.svg.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;State space can be divided into two subsets: even and odd permutations&lt;/li&gt;
&lt;li&gt;$f_1$ is number of inversions in a permutation &lt;code&gt;X1X2...XN&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;inversion is a distinct pair Xi &amp;gt; Xj such that i &amp;lt; j(後面有幾個數比自己小) &lt;/li&gt;
&lt;li&gt;Example: &lt;code&gt;10,8,12,3,7,6,2,1,14,4,11,15,13,9,5&lt;/code&gt; has 9+7+9+2+5+4+1+0+5+0+2+3+2+1 inversions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$f_2$ is the row number that empty cell is(空的那一格在哪一行)&lt;/li&gt;
&lt;li&gt;f = $f_1$ + $f_2$&lt;/li&gt;
&lt;li&gt;Slide a tile never change the parity    &lt;ul&gt;
&lt;li&gt;Proof: skip(a lot of)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Solving Result&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1-MIPS machine&lt;/li&gt;
&lt;li&gt;30 CPU minutes in 1985 &lt;/li&gt;
&lt;li&gt;using IDA* with Manhattan distance heuristic&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Non-additive-pattern-databases&#34;&gt;&lt;a href=&#34;#Non-additive-pattern-databases&#34; class=&#34;headerlink&#34; title=&#34;Non-additive pattern databases&#34;&gt;&lt;/a&gt;Non-additive pattern databases&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;原本cost funtion為15片個別的distance之和，若能一次計算多片的distance？&lt;/li&gt;
&lt;li&gt;linear conflict: 靠很近不代表步數少(如[2, 1, 3, 4]交換至[1, 2, 3, 4]並不只兩步)&lt;ul&gt;
&lt;li&gt;有可能移成pattern時，反而使其他片遠離&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/TCG/4-1.png&#34; alt=&#34;linear conflict&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fringe(初級知識)&lt;ul&gt;
&lt;li&gt;subset of selected tiles called &lt;strong&gt;pattern&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;tiles not selected is “don’t-care tile”, all looked as the same&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If there are 7 selected tiles, including empty cell  &lt;ul&gt;
&lt;li&gt;16!/9! = 57657600 possible pattern size&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/TCG/4-2.png&#34; alt=&#34;prefrin&#34;&gt;&lt;br&gt;goal fringe: 選擇的方塊都和goal的位置一樣&lt;br&gt;&lt;img data-src=&#34;/img/TCG/4-3.png&#34; alt=&#34;goalfrin&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;precompute the minimum number of moves(&lt;strong&gt;fringe number&lt;/strong&gt;) to make goal fringe&lt;ul&gt;
&lt;li&gt;goal fringe: 找給定的選擇方塊，在任何pattern中，最小需要移動成最終目標的步數&lt;/li&gt;
&lt;li&gt;We can solve it because the pattern size is relatively small&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pro’s&lt;ul&gt;
&lt;li&gt;pattern size↑, fringe number↑, which means better estimation&lt;ul&gt;
&lt;li&gt;because estimate number it is closer to the real answer    &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Con’s    &lt;ul&gt;
&lt;li&gt;Pattern with a larger size&lt;ul&gt;
&lt;li&gt;consuming lots of memory and time&lt;/li&gt;
&lt;li&gt;limited by source&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;not optimal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Property   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Divide and Conquer  &lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Reduce a 15-puzzle problem into a 8-puzzle &lt;img data-src=&#34;/img/TCG/4-4.png&#34; alt=&#34;15-8&#34;&gt;&lt;/li&gt;
&lt;li&gt;魔術方塊 – 分成六面&lt;/li&gt;
&lt;li&gt;Cannot easily combine&lt;ul&gt;
&lt;li&gt;affect tiles that have reached the goal in the subproblem when solving the remains&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Used as heuristic function(admissible)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;More-than-one-patterns&#34;&gt;&lt;a href=&#34;#More-than-one-patterns&#34; class=&#34;headerlink&#34; title=&#34;More than one patterns&#34;&gt;&lt;/a&gt;More than one patterns&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;How to Find better patterns for fringes?&lt;ul&gt;
&lt;li&gt;→ Can we combine smaller patterns to form bigger patterns?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For different pattern databases P1, P2, P3 …  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;patterns may not be disjoint, may be overlapping&lt;/li&gt;
&lt;li&gt;The heuristic function we can use is&lt;ul&gt;
&lt;li&gt;$h(P_1, P_2, P_3 … ) = max{h(P_1),h(P_2),h(P_3) …}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How to make heuristics and the patterns disjoint?  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;patterns should be disjoint to add them together(see below)&lt;ul&gt;
&lt;li&gt;Though patterns are disjoint, their costs are not disjoint&lt;ul&gt;
&lt;li&gt;Some moves are counted more than once&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f(P1) + f(P2) is admissible if  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;f() is disjoint with respect to P1 and P2&lt;/li&gt;
&lt;li&gt;both f(P1) and f(P2) are admissible&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For Manhattan distance heuristic  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each region is a tile&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Divide the board into several disjoint regions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;They are disjoint&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;only count the number of moves made by each region&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;doesn’t count cross-region moves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Refinement&lt;br&gt;Partition the board into disjoint regions using the tiles in a region of the goal arrangement as a pattern&lt;br&gt;&lt;img data-src=&#34;/img/TCG/4-5.png&#34; alt=&#34;aabb&#34;&gt;&lt;br&gt;&lt;strong&gt;只算每個region內的片所移動的步數和，作為新定義的fringe number&lt;/strong&gt;&lt;br&gt;如此一來，就可以將每個region的cost相加而保持admissible&lt;/p&gt;
&lt;h3 id=&#34;Disjoint-pattern&#34;&gt;&lt;a href=&#34;#Disjoint-pattern&#34; class=&#34;headerlink&#34; title=&#34;Disjoint pattern&#34;&gt;&lt;/a&gt;Disjoint pattern&lt;/h3&gt;&lt;p&gt;A heuristic function f() is disjoint with respect to two patterns P1 and P2 if  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;P1 and P2 have no common cells&lt;/li&gt;
&lt;li&gt;The solutions corresponding to f(P1) and f(P2) do not interfere each other&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Revised fringe number f’(p): for each fringe arrangement F, the &lt;strong&gt;minimum&lt;/strong&gt; number of &lt;strong&gt;fringe-only&lt;/strong&gt; moves to make goal fringe&lt;/p&gt;
&lt;h3 id=&#34;Result&#34;&gt;&lt;a href=&#34;#Result&#34; class=&#34;headerlink&#34; title=&#34;Result&#34;&gt;&lt;/a&gt;Result&lt;/h3&gt;&lt;p&gt;Solves the 15 puzzle problem using fringe that is more than &lt;strong&gt;2000&lt;/strong&gt; times faster than the previous result by using the Manhattan distance  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The average Manhattan distance is 76.078 moves in 24-puzzle    &lt;/li&gt;
&lt;li&gt;The average value for the disjoint database heuristic is 81.607 moves in 24-puzzle   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;only small refinement on heuristic function would make performance far better&lt;/strong&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other heuristics   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pairwise distance&lt;ul&gt;
&lt;li&gt;partition the board into many 2-tiles so that the sum of cost is &lt;strong&gt;maximized&lt;/strong&gt;&lt;br&gt;For an $n^2 - 1$ puzzle, we have $O(n^4)$ different combinations&lt;br&gt;using&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;What-else-can-be-done&#34;&gt;&lt;a href=&#34;#What-else-can-be-done&#34; class=&#34;headerlink&#34; title=&#34;What else can be done?&#34;&gt;&lt;/a&gt;What else can be done?&lt;/h3&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Better way of partitioning&lt;/li&gt;
&lt;li&gt;Is it possible to generalize this result to other problem domains?&lt;/li&gt;
&lt;li&gt;Decide ratio of the time used in searching and the time used in retrieving pre-computed knowledge&lt;ul&gt;
&lt;li&gt;memorize vs compute&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;Chap-04-Two-Player-Perfect-Information-Games-Introductions&#34;&gt;&lt;a href=&#34;#Chap-04-Two-Player-Perfect-Information-Games-Introductions&#34; class=&#34;headerlink&#34; title=&#34;Chap 04 Two-Player Perfect Information Games Introductions&#34;&gt;&lt;/a&gt;Chap 04 Two-Player Perfect Information Games Introductions&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Conclusion: decision complexity is more important than state-space complexity   &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;trade-off between &lt;strong&gt;knowledge-based&lt;/strong&gt; methods and &lt;strong&gt;brute-force&lt;/strong&gt; methods&lt;/p&gt;
&lt;p&gt;Domain: 2-person &lt;strong&gt;zero-sum games&lt;/strong&gt; with perfect information&lt;br&gt;Zero-sum means one player’s loss is exactly the other player’s gain, and vice versa.&lt;/p&gt;
&lt;h3 id=&#34;Definition&#34;&gt;&lt;a href=&#34;#Definition&#34; class=&#34;headerlink&#34; title=&#34;Definition&#34;&gt;&lt;/a&gt;Definition&lt;/h3&gt;&lt;p&gt;Game-theoretic value: the outcome of a game when all participants play optimally&lt;br&gt;Game-theoretic value for most games are unknown or are only known for some legal positions.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Ultra-weakly solved&lt;/td&gt;
&lt;td&gt;在初始盤面可知，遊戲中先行者或後行者誰有必勝、或必不敗之策略&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Weakly solved&lt;/td&gt;
&lt;td&gt;for the initial position a strategy has been determined to achieve the game-theoretic value(知道必不敗之策略為何)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Strongly solved&lt;/td&gt;
&lt;td&gt;a strategy has been determined for all legal positions(任何合法情況都能知道最佳策略)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;State-space complexity of a game: the &lt;strong&gt;number of the legal positions&lt;/strong&gt; in a game(可能的盤面)&lt;br&gt;Game-tree complexity(decision complexity) of a game: the &lt;strong&gt;number of the leaf nodes&lt;/strong&gt; in a solution search tree(可能的走法)  &lt;/p&gt;
&lt;p&gt;A fair game: the game-theoretic value is draw and both players have roughly equal probability on making a mistake.  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper-scissor-stone&lt;/li&gt;
&lt;li&gt;Roll a dice and compare who gets a larger number&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Initiative(主動): the right to move first  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A convergent game: the size of the state space decreases as the game progresses  &lt;ul&gt;
&lt;li&gt;Example: Checkers  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A divergent game: the size of the state space increases as the game progresses  &lt;ul&gt;
&lt;li&gt;Example: Connect-5 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A game may be convergent at one stage and then divergent at other stage.&lt;ul&gt;
&lt;li&gt;Ex. Go, Tic-Tac-Toe&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Threats are something like forced moved or moves you have little choices.&lt;br&gt;Threats are moves with predictable counter-moves&lt;/p&gt;
&lt;h3 id=&#34;Classification&#34;&gt;&lt;a href=&#34;#Classification&#34; class=&#34;headerlink&#34; title=&#34;Classification&#34;&gt;&lt;/a&gt;Classification&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/TCG/5-1.png&#34; alt=&#34;4&#34;&gt;&lt;/p&gt;
&lt;p&gt;Questions to be researched&lt;br&gt;Can perfect knowledge obtained from solved games be translated into rules and strategies which human beings can assimilate?&lt;br&gt;Are such rules generic, or do they constitute a multitude of ad hoc recipes?&lt;br&gt;Can methods be transferred between games?  &lt;/p&gt;
&lt;p&gt;Connection games&lt;br&gt;Connect-four (6 * 7)&lt;br&gt;Qubic (4 * 4 * 4)&lt;br&gt;Renju - Does not allow the First player to play certain moves, An asymmetric game.&lt;br&gt;mnk-Game: a game playing on a board of m rows and n columns with the goal of obtaining a straight line of length k.&lt;br&gt;Variations: First ply picks only one stone, the rest picks two stones in a ply. -&amp;gt; Connect 6. &lt;/p&gt;
&lt;p&gt;Hex (10 * 10 or 11 * 11)&lt;br&gt;Exactly one of the players can win.&lt;br&gt;solved on a 6 * 6 board in 1994.&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/TCG/5-2.png&#34; alt=&#34;Hex&#34;&gt;&lt;/p&gt;
&lt;p&gt;Proof on exactly one player win&lt;br&gt;Assume there is no winner&lt;br&gt;&lt;img data-src=&#34;/img/TCG/5-3.png&#34; alt=&#34;block&#34;&gt;&lt;br&gt;blue should totally block red at some place -&amp;gt; blue will connect!  &lt;/p&gt;
&lt;p&gt;let R be the set of red cells that can be reached by chains from rightmost column&lt;br&gt;R does not contain a cell of the leftmost column; otherwise we have a contradiction&lt;br&gt;let N(R) be the blue cells that can be reached by chains originated from the rightmost column.&lt;br&gt;N(R) must contain a cell in the top and bottom row , Otherwise, R contains all cells in the First/bottom row, which is a contradiction.&lt;br&gt;N(R) must be connected. Otherwise, R can advance further. Hence N(R) is a blue winning chain.&lt;/p&gt;
&lt;h3 id=&#34;Strategy-stealing-argument&#34;&gt;&lt;a href=&#34;#Strategy-stealing-argument&#34; class=&#34;headerlink&#34; title=&#34;Strategy-stealing argument&#34;&gt;&lt;/a&gt;Strategy-stealing argument&lt;/h3&gt;&lt;p&gt;made by John Nash in 1949&lt;br&gt;後手無一般化的必勝法&lt;br&gt;若後手有必勝法，則先手可以先隨機下一子(並無視之)，再照著後手的下法&lt;br&gt;後手必勝的下法包含了第一手，則再隨機下一子，將其視為第一子&lt;br&gt;限制：不能有和，下子不會有害，symmetric，history independent，&lt;/p&gt;
&lt;p&gt;Assume the initial board position is B0&lt;br&gt;f(B) has a value only when it is a legal position for the second player.&lt;br&gt;rev(x): interchange colors of pieces in a board or ply x.&lt;br&gt;always has exactly one winner  &lt;/p&gt;
&lt;p&gt;Not Solved&lt;br&gt;Chess DEEP BLUE beat the human World Champion in 1997&lt;br&gt;Chinese chess Professional 7-dan in 2007&lt;br&gt;Shogi&lt;br&gt;Claimed to be professional 2-dan in 2007&lt;br&gt;Defeat a 68-year old 1993 Meijin during 2011 and 2012&lt;/p&gt;
&lt;p&gt;Go&lt;br&gt;Recent success and breakthrough using Monte Carlo UCT based methods.&lt;br&gt;Amateur 1 dan in 2010.&lt;br&gt;Amateur 3 dan in 2011.&lt;br&gt;The program Zen beat a 9-dan professional master at March 17, 2012&lt;br&gt;  First game: Five stone handicap and won by 11 points&lt;br&gt;  Second game: four stones handicap and won by 20 points&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/TCG/5-4.png&#34; alt=&#34;table of complexity&#34;&gt;&lt;/p&gt;
&lt;p&gt;possible to use heuristics to prune tremendously when the structure of the game is well studied&lt;/p&gt;
&lt;p&gt;Methods to solve games&lt;br&gt;Brute-force methods  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Retrograde analysis(倒推)&lt;/li&gt;
&lt;li&gt;Enhanced transposition-table methods(?)&lt;br&gt;Knowledge-based methods  &lt;/li&gt;
&lt;li&gt;Threat-space search and lambda-search&lt;/li&gt;
&lt;li&gt;Proof-number search&lt;/li&gt;
&lt;li&gt;Depth-First proof-number search&lt;/li&gt;
&lt;li&gt;Pattern search&lt;ul&gt;
&lt;li&gt;search threat patterns, which are collections of cells in a position&lt;/li&gt;
&lt;li&gt;A threat pattern can be thought of as representing the relevant area on the board&lt;br&gt;Recent advancements  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Monte Carlo UCT based game tree simulation&lt;ul&gt;
&lt;li&gt;Monte Carlo method has a root from statistic&lt;/li&gt;
&lt;li&gt;Biased sampling&lt;/li&gt;
&lt;li&gt;Using methods from machine learning&lt;/li&gt;
&lt;li&gt;Combining domain knowledge with statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A majority vote algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;low state-space complexity have mainly been solved with brute-force methods.&lt;br&gt;Nine Men’s Morris&lt;/p&gt;
&lt;p&gt;low game-tree-complexities have mainly been solved with knowledge-based methods.&lt;br&gt;by intelligent (heuristic) searching with help of databases&lt;br&gt;Go-Moku, Renju, and k-in-a-row games&lt;/p&gt;
&lt;p&gt;The First player has advantages.&lt;br&gt;Two kinds of positions&lt;br&gt;P-positions: the previous player can force a win.&lt;br&gt;N-positions: the next player can force a win.&lt;/p&gt;
&lt;p&gt;First player to have a forced win, just one of the moves that make P-position.&lt;br&gt;second player to have a forced win, all of the moves must lead to(造成) N-positions&lt;/p&gt;
&lt;p&gt;At small boards, the second player is able to draw or even to win for certain games.&lt;/p&gt;
&lt;p&gt;Try to obtain a small advantage by using the initiative.&lt;br&gt;The opponent must react adequately on the moves played by the other player.&lt;br&gt;Force the opponent to always play the moves you expected.&lt;/p&gt;
&lt;p&gt;Offsetting the initiative&lt;/p&gt;
&lt;p&gt;一子棋 by 張系國 棋王 -&amp;gt; 先手優勢極大，隨著棋子增加，所需贏的步數就愈少。&lt;/p&gt;
&lt;p&gt;讓子&lt;br&gt;Ex. Go k = 7.5 in 2011&lt;/p&gt;
&lt;p&gt;Enforce rules so that the first player cannot win by selective patterns.&lt;br&gt;Ex. Renju&lt;/p&gt;
&lt;p&gt;The one-move-equalization rule: one player plays an opening move and the other player then has to decide which color to&lt;br&gt;play for the reminder of the game.&lt;br&gt;. Hex.&lt;br&gt;. Second-player will win.&lt;/p&gt;
&lt;p&gt;The First move plays one stone, the rest plays two stones each.&lt;br&gt;Can’t prove it is fair&lt;/p&gt;
&lt;p&gt;The first player uses less resource.&lt;br&gt;For example: using less time.&lt;br&gt;Ex. Chinese chess.&lt;/p&gt;
&lt;p&gt;1990’s prediction at 2000&lt;br&gt;&lt;img data-src=&#34;/img/TCG/5-5.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;2000’s prediction at 2010&lt;br&gt;&lt;img data-src=&#34;/img/TCG/5-6.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;Chap-05-Computer-chess-programming-by-Shannon&#34;&gt;&lt;a href=&#34;#Chap-05-Computer-chess-programming-by-Shannon&#34; class=&#34;headerlink&#34; title=&#34;Chap 05 Computer chess programming by Shannon&#34;&gt;&lt;/a&gt;Chap 05 Computer chess programming by Shannon&lt;/h2&gt;&lt;p&gt;C.E. Shannon&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1916 ~ 2001.&lt;/li&gt;
&lt;li&gt;The founding father of Information theory.&lt;/li&gt;
&lt;li&gt;The founding father of digital circuit design.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ground breaking paper for computer game playing: “Programming a Computer for Playing Chess”, 1950.&lt;br&gt;Presented many novel ideas that are still being used today.(太神啦！)  &lt;/p&gt;
&lt;h3 id=&#34;Analysis&#34;&gt;&lt;a href=&#34;#Analysis&#34; class=&#34;headerlink&#34; title=&#34;Analysis&#34;&gt;&lt;/a&gt;Analysis&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;typical 30 legal moves in one ply(下子)  &lt;/li&gt;
&lt;li&gt;typical game last about 40 moves  &lt;ul&gt;
&lt;li&gt;will be 10^120 variations  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;possible legal position(state space complexity) is roughly 10^43&lt;/li&gt;
&lt;li&gt;CPU speed in 1950 is 10^6 per second current CPU speed is 10^9 per second, still not fast enough to brute force it&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But it is possible to enumerate small endgames&lt;br&gt;3~6 piece endgame roughly 7.75*10^9 positions  &lt;/p&gt;
&lt;h3 id=&#34;Three-phases-of-chess&#34;&gt;&lt;a href=&#34;#Three-phases-of-chess&#34; class=&#34;headerlink&#34; title=&#34;Three phases of chess&#34;&gt;&lt;/a&gt;Three phases of chess&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Opening &lt;ul&gt;
&lt;li&gt;Development of pieces to good position&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Middle&lt;ul&gt;
&lt;li&gt;after opening until few pieces&lt;/li&gt;
&lt;li&gt;pawn structure &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;End game &lt;ul&gt;
&lt;li&gt;concerning usage of pawns&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Different principles of play apply in the different phases&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;Evaluating-Function&#34;&gt;&lt;a href=&#34;#Evaluating-Function&#34; class=&#34;headerlink&#34; title=&#34;Evaluating Function&#34;&gt;&lt;/a&gt;Evaluating Function&lt;/h3&gt;&lt;p&gt;position p, include board status, which side to move, history of moves&lt;br&gt;history -&amp;gt; castling&lt;br&gt;&lt;img data-src=&#34;/img/TCG/6-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Perfect evaluating function f(p):&lt;br&gt;f(p) = 1 for a won position.&lt;br&gt;f(p) = 0 for a drawn position.&lt;br&gt;f(p) = -1 for a lost position.&lt;br&gt;Perfect evaluating function is impossible for most games, and is &lt;strong&gt;not fun or educational&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Factors considered in approximate evaluating functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The relative values of differences in materials.&lt;ul&gt;
&lt;li&gt;The values of queen, rook, bishop, knight and pawn are about 9, 5, 3, 3, and 1, respectively.&lt;/li&gt;
&lt;li&gt;How to determine good relative values? Static values verse dynamic values?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Position of pieces&lt;ul&gt;
&lt;li&gt;Mobility: the freedom to move your pieces.&lt;/li&gt;
&lt;li&gt;at center , or at corner&lt;/li&gt;
&lt;li&gt;Doubled rooks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pawn structure: the relative positions of the pawns.&lt;ul&gt;
&lt;li&gt;Backward pawn: a pawn that is behind the pawn of the same color on an adjacent file that cannot advance without losing of itself.&lt;/li&gt;
&lt;li&gt;Isolated pawn: A pawn that has no friend pawn on the adjacent file.&lt;/li&gt;
&lt;li&gt;Doubled pawn: two pawns of the same color on the same file&lt;/li&gt;
&lt;li&gt;these three are all bad pawn&lt;/li&gt;
&lt;li&gt;Passed pawns: pawns that have no opposing pawns to prevent&lt;/li&gt;
&lt;li&gt;Pawns on opposite colour squares from bishop.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;King safety.&lt;/li&gt;
&lt;li&gt;Threat and attack.&lt;ul&gt;
&lt;li&gt;Attacks on pieces which give one player an option of exchanging&lt;/li&gt;
&lt;li&gt;Pins(小盯大) which mean here immobilizing pins where the pinned piece is of value not greater than the pinning piece&lt;/li&gt;
&lt;li&gt;Commitments -&amp;gt; 需要保護其他子&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/TCG/6-2.png&#34; alt=&#34;three pawn&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Putting “right” coeffcients for diffferent factors&lt;br&gt;Dynamic setting in practical situations.&lt;/p&gt;
&lt;p&gt;evaluating function can be only applied in&lt;br&gt;relatively quiescent positions.&lt;/p&gt;
&lt;p&gt;not in the middle of material exchanging.&lt;br&gt;not being checked&lt;/p&gt;
&lt;p&gt;max-min strategy&lt;br&gt;In your move, you try to maximize your f(p).&lt;br&gt;In the opponent’s move, he tries to minimize f(p).&lt;/p&gt;
&lt;p&gt;A strategy in which all variations are considered out to a&lt;br&gt;definite number of moves and the move then determined from&lt;br&gt;a max-min formula is called type A strategy.&lt;/p&gt;
&lt;p&gt;Stalemate&lt;br&gt;Winning by making the opponent having no legal next move.&lt;br&gt;suicide move is not legal, and stalemate results in&lt;br&gt;a draw if it is not currently in check.&lt;/p&gt;
&lt;p&gt;Zugzwang(強制被動): In certain positions, a player is at a disadvantage if he is the next player to move.&lt;br&gt;&lt;img data-src=&#34;/img/TCG/6-3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Programming&lt;br&gt;    - Special rules of games&lt;br&gt;    - Methods of winning&lt;br&gt;    - Basic data structure for positions.&lt;br&gt;    - check for possible legal moves&lt;br&gt;    - Evaluating function.&lt;/p&gt;
&lt;p&gt;Forced variations(迫著)&lt;br&gt;one player has little or no choices in playing&lt;/p&gt;
&lt;p&gt;type B strategy&lt;br&gt;the machine must &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;examine forceful variations out as far as possible and evaluate only at reasonable positions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;select the variations to be explored by some process&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;| 1 if any piece is attacked by a piece of lower value,&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;  g(P) =    /    or by more pieces then defences of if any check exists&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;\    on a square controlled by opponent.
 | 0 otherwise.&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Using this function, variations could be explored until g(P)=0,&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;effective branching factor&lt;/strong&gt; is about 2 to 3.&lt;br&gt;Chinese chess has a larger real branching factor, but its average effective branching factor is also about 2 to 3.&lt;/p&gt;
&lt;p&gt;“style” of play by the machine can&lt;br&gt;be changed very easily by altering some of the coeffcients and&lt;br&gt;numerical factors involved in the evaluating function&lt;/p&gt;
&lt;p&gt;A chess master, on the other hand, has available knowledge of hundreds or perhaps thousands of standard situations, stock&lt;br&gt;combinations, and common manoeuvres based on pins, forks, discoveries, promotions, etc.&lt;br&gt;In a given position he recognizes some similarity to a familiar situation and this directs his mental calculations along the lines with greater probability of success.&lt;/p&gt;
&lt;p&gt;Need to re-think the goal of writing a computer program that&lt;br&gt;plays games.&lt;br&gt;To discover intelligence:&lt;br&gt;What is considered intelligence for computers may not be considered so for human.&lt;br&gt;To have fun:&lt;br&gt;A very strong program may not be a program that gives you the most pleasure.&lt;br&gt;To Find ways to make computers more helpful to human.&lt;br&gt;Techniques or (machine) intelligence discovered may be useful to computers performing other tasks&lt;/p&gt;
&lt;h2 id=&#34;Chap-06-Alpha-Beta-Pruning&#34;&gt;&lt;a href=&#34;#Chap-06-Alpha-Beta-Pruning&#34; class=&#34;headerlink&#34; title=&#34;Chap 06 Alpha-Beta Pruning&#34;&gt;&lt;/a&gt;Chap 06 Alpha-Beta Pruning&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;standard searching procedure for 2-person perfect-information zero sum games&lt;/li&gt;
&lt;li&gt;terminal position&lt;ul&gt;
&lt;li&gt;a position whose (win/loss/draw) value can be know&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Dewey-decimal-system&#34;&gt;&lt;a href=&#34;#Dewey-decimal-system&#34; class=&#34;headerlink&#34; title=&#34;Dewey decimal system&#34;&gt;&lt;/a&gt;Dewey decimal system&lt;/h3&gt;&lt;p&gt;杜威分類法 &lt;img data-src=&#34;/img/TCG/7-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;Min-Max-method&#34;&gt;&lt;a href=&#34;#Min-Max-method&#34; class=&#34;headerlink&#34; title=&#34;Min-Max method&#34;&gt;&lt;/a&gt;Min-Max method&lt;/h3&gt;&lt;p&gt;假設持白子，數字為白子的evaluating function, 在下白子時，取分數最高(max)的，在下黑子時，取分數最低(min)的 &lt;img data-src=&#34;/img/TCG/7-2.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/TCG/7-3.png&#34; alt=&#34;max layer function F&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;Nega-max-method&#34;&gt;&lt;a href=&#34;#Nega-max-method&#34; class=&#34;headerlink&#34; title=&#34;Nega-max method&#34;&gt;&lt;/a&gt;Nega-max method&lt;/h3&gt;&lt;p&gt;將下黑子的分數取負號(即為黑子的分數，因為是零和遊戲)&lt;br&gt;這樣每一層都取最大分數即可&lt;br&gt;&lt;img data-src=&#34;/img/TCG/7-4.png&#34; alt=&#34;negamax algorithm&#34;&gt;&lt;/p&gt;
&lt;p&gt;優點是實作較快，程式碼簡潔 &lt;/p&gt;
&lt;h3 id=&#34;Alpha-Beta-cut-off&#34;&gt;&lt;a href=&#34;#Alpha-Beta-cut-off&#34; class=&#34;headerlink&#34; title=&#34;Alpha-Beta cut off&#34;&gt;&lt;/a&gt;Alpha-Beta cut off&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;current search window(score bound) = [α, β]&lt;/li&gt;
&lt;li&gt;If α &amp;gt; β, no need to do further search in current branch &lt;/li&gt;
&lt;li&gt;initial alpha = -∞, beta = ∞&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/TCG/7-5.png&#34; alt=&#34;Alpha Cut off&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只要發現對手有一種反擊方式，使結果比其他手的結果還差，就砍掉這一手(branch)&lt;/li&gt;
&lt;li&gt;2.1 can cut off 2.x&lt;ul&gt;
&lt;li&gt;before 2.1 , window = [15, ∞]&lt;/li&gt;
&lt;li&gt;after 2.1 , window = [15, 10]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We want to choose the biggest value at root for lower bound, so 2.x is all cut off&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/TCG/7-6.png&#34; alt=&#34;Beta Cut off&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只要對手發現自己有一種反擊方式，使結果比其他手的結果還差(α)，就砍掉這一手(branch)&lt;/li&gt;
&lt;li&gt;1.2.1 can cut off 1.2.x&lt;ul&gt;
&lt;li&gt;beofre 1.2.1 , 1 bound is [-∞, 10]&lt;/li&gt;
&lt;li&gt;now 1.2 bound is [15, 10]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We want to choose smallest value at 1 for upper bound, 1.2.x is all cut off&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以砍所有子孫 &lt;img data-src=&#34;/img/TCG/7-7.png&#34; alt=&#34;Deep Cut off&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2.1.1 is cut off   &lt;ul&gt;
&lt;li&gt;root bound = [15, ∞]&lt;/li&gt;
&lt;li&gt;2.1.1 = [-∞, 7]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/TCG/7-8.png&#34; alt=&#34;alpha-beta cut off Algorithm&#34;&gt;&lt;br&gt;f = white move, find max to be lower bound, do beta cut off&lt;br&gt;g = black move, find min to be upper bound, do alpha cut off&lt;br&gt;&lt;img data-src=&#34;/img/TCG/7-9.png&#34; alt=&#34;example&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/TCG/7-10.png&#34; alt=&#34;F2&#34;&gt;&lt;br&gt;window變號，回傳的score也要變號&lt;br&gt;t = -F(pi, -beta, -m)&lt;/p&gt;
&lt;h3 id=&#34;Analysis-for-AB-pruning&#34;&gt;&lt;a href=&#34;#Analysis-for-AB-pruning&#34; class=&#34;headerlink&#34; title=&#34;Analysis for AB pruning&#34;&gt;&lt;/a&gt;Analysis for AB pruning&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;different move orderings&lt;/strong&gt; give very different cut branches&lt;br&gt;愈快找到最佳解，可以砍的branch愈多&lt;/p&gt;
&lt;p&gt;critical nodes 一定會搜到(cut off之前至少需搜完一個子branch) &lt;img data-src=&#34;/img/TCG/7-11.png&#34; alt=&#34;Critical Node&#34;&gt;&lt;/p&gt;
&lt;p&gt;perfect-ordering tree: 每個branch的第一個child就是最佳解&lt;br&gt;Theorem: 若是perfect-ordering tree, AB pruning 會剛好走過所有 critical nodes&lt;br&gt;Proof:&lt;br&gt;Three Types of critial nodes  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;定義a_i = 第i層的node是第幾個child(杜威分類)&lt;/li&gt;
&lt;li&gt;a_j = 第一個「不是第一個child」的node(如果有的話)&lt;ul&gt;
&lt;li&gt;a_j-1 = a_j+1 = 1&lt;ul&gt;
&lt;li&gt;小於j的node都是1&lt;/li&gt;
&lt;li&gt;而且因為是critial node，所以a_j的child一定是1(其他會被砍掉)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;a_l = the last layer&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;root and all node = 1(最左邊, 1, 1.1, 1.1.1 …)&lt;/li&gt;
&lt;li&gt;l-j = even&lt;ol&gt;
&lt;li&gt;j = l (type1 的全部兒子(除了最左邊))  &lt;/li&gt;
&lt;li&gt;j &amp;lt; l (type3 的全部兒子)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;l-j = odd&lt;ol&gt;
&lt;li&gt;j+1 = l (type2.1 的第一個兒子)&lt;/li&gt;
&lt;li&gt;j+1 &amp;lt; l (type2.2的第一個兒子)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/TCG/7-13.png&#34; alt=&#34;Three Types of critial nodes&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/TCG/7-14.png&#34; alt=&#34;Proof&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can calculate the least number of nodes to be searched &lt;img data-src=&#34;/img/TCG/7-15.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/TCG/7-16.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;when there’re some early terminate nodes &lt;img data-src=&#34;/img/TCG/7-18.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;l = even → x.1.x.1… = b0(q1b2)q3…&lt;br&gt;            1.x.1.x… = (q0b1)(q2b3)…(q0b1 = 第一個孩子的全child，若無child，則為(1-qi)*0)&lt;/p&gt;
&lt;p&gt;Perfect ordering is not always best when tree are not balanced &lt;img data-src=&#34;/img/TCG/7-17.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ When &lt;strong&gt;“relative” ordering of children&lt;/strong&gt;(not perfect order!) are good enough, there are some cut-off  &lt;/p&gt;
&lt;p&gt;Theorem: 若知道所有的分數，就可以最佳化alpha-beta pruning(計算的點最少，cut最多)&lt;br&gt;→ 不過如果能算出來就不用search了…&lt;/p&gt;
&lt;h3 id=&#34;Variations-of-alpha-beta-search&#34;&gt;&lt;a href=&#34;#Variations-of-alpha-beta-search&#34; class=&#34;headerlink&#34; title=&#34;Variations of alpha-beta search&#34;&gt;&lt;/a&gt;Variations of alpha-beta search&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Fail hard alpha-beta cut(Original) : F2 &lt;img data-src=&#34;/img/TCG/7-19.png&#34; alt=&#34;&#34;&gt; &lt;ul&gt;
&lt;li&gt;returned value in [α, β] &lt;img data-src=&#34;/img/TCG/7-20.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fail soft alpha-beta cut(Variation): F3  &lt;img data-src=&#34;/img/TCG/7-21.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;Find “better” value when the value is out of the search window&lt;/li&gt;
&lt;li&gt;m is the value in this branch(not related to α)&lt;ul&gt;
&lt;li&gt;use max(m, alpha) to get window &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;return original value m instead of α or β when cut off, which is more precise than fail-hard &lt;img data-src=&#34;/img/TCG/7-22.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Failed-high &lt;ul&gt;
&lt;li&gt;return value &amp;gt; β&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Failed-low&lt;ul&gt;
&lt;li&gt;return value &amp;lt; α&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Comparison  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;fail-hard&lt;ul&gt;
&lt;li&gt;return max{4000,200,v} &lt;img data-src=&#34;/img/TCG/7-23.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;fail-soft&lt;ul&gt;
&lt;li&gt;return max{200,v} &lt;img data-src=&#34;/img/TCG/7-24.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;fail-soft provides more information when the true value is out of search window&lt;ul&gt;
&lt;li&gt;can record better value to be used later when this position is revisited&lt;/li&gt;
&lt;li&gt;F3 saves about 7% of time than that of F2 when a transposition table is used to save and re-use searched results&lt;/li&gt;
&lt;li&gt;記錄F3傳回的值，可減少重複計算的時間，因為下一手的樹在下兩層，大部分node皆相同&lt;ul&gt;
&lt;li&gt;if p1 is searched, p2 does not need to search again &lt;img data-src=&#34;/img/TCG/7-25.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Questions&#34;&gt;&lt;a href=&#34;#Questions&#34; class=&#34;headerlink&#34; title=&#34;Questions&#34;&gt;&lt;/a&gt;Questions&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;What move ordering is good?&lt;ul&gt;
&lt;li&gt;search the best possible move first&lt;/li&gt;
&lt;li&gt;cut off a branch with more nodes first&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What is the effect of using iterative-deepening alpha-beta cut off?&lt;/li&gt;
&lt;li&gt;How about searching game graph instead of game tree?&lt;/li&gt;
&lt;li&gt;Can some nodes be visited more than once?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Pruning-Techinique&#34;&gt;&lt;a href=&#34;#Pruning-Techinique&#34; class=&#34;headerlink&#34; title=&#34;Pruning Techinique&#34;&gt;&lt;/a&gt;Pruning Techinique&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Exact algorithms: by mathematical proof&lt;ul&gt;
&lt;li&gt;Alpha-Beta pruning&lt;/li&gt;
&lt;li&gt;Scout(in Chap07)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Approximated heuristics: pruned branches with low probability to be solution&lt;ul&gt;
&lt;li&gt;in very bad position(盤面太差)&lt;/li&gt;
&lt;li&gt;a little hope to gain back the advantage(無法逆轉)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap07-Scout-and-Proof-Number-Search&#34;&gt;&lt;a href=&#34;#Chap07-Scout-and-Proof-Number-Search&#34; class=&#34;headerlink&#34; title=&#34;Chap07 Scout and Proof Number Search&#34;&gt;&lt;/a&gt;Chap07 Scout and Proof Number Search&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Suppose we get at least score s at the First branch&lt;ul&gt;
&lt;li&gt;want to find whether second branch can get score over s or not&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Is there a way to search a tree approximately?&lt;/strong&gt;  &lt;/p&gt;
&lt;h3 id=&#34;SCOUT&#34;&gt;&lt;a href=&#34;#SCOUT&#34; class=&#34;headerlink&#34; title=&#34;SCOUT&#34;&gt;&lt;/a&gt;SCOUT&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Invented by Judea Pearl in 1980&lt;/li&gt;
&lt;li&gt;first time: search approximately&lt;ul&gt;
&lt;li&gt;if there is better value, search again&lt;/li&gt;
&lt;li&gt;first search can provide useful information in the second search &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TEST whether Tb can return score &amp;gt; v &lt;img data-src=&#34;/img/TCG/test-algo.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;if p is max node → success with only one subbranch &amp;gt; v&lt;/li&gt;
&lt;li&gt;if p is min node → success with all subbranches &amp;gt; v&lt;/li&gt;
&lt;li&gt;If success, then search Tb. else, &lt;strong&gt;no need to search Tb&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;algorithm &lt;img data-src=&#34;/img/TCG/scout-algo.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;scout first branch and test other branch&lt;ul&gt;
&lt;li&gt;if test success, update the value by scout this branch&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;recursive procedure&lt;ul&gt;
&lt;li&gt;Every ancestor of you may initiate a TEST to visit you&lt;ul&gt;
&lt;li&gt;will be visited at most d times(= depth)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Time Complexity  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;not guarantee&lt;/strong&gt;(but most of the time) that the visited nodes number are less than alpha-beta&lt;ul&gt;
&lt;li&gt;may search a branch two times&lt;/li&gt;
&lt;li&gt;may pay many visits to a node that is cut off by alpha-beta&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TEST: Ω(b^(d/2))&lt;ul&gt;
&lt;li&gt;but has small argument and will be very small at the best situation &lt;img data-src=&#34;/img/TCG/nodes-visited.png&#34; alt=&#34;node visited&#34;&gt;&lt;ul&gt;
&lt;li&gt;if the first subbranch has the best value, then TEST scans the tree fast&lt;/li&gt;
&lt;li&gt;move ordering is very important&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Comparison&lt;ul&gt;
&lt;li&gt;alpha-beta&lt;ul&gt;
&lt;li&gt;cut off comes from bounds of search windows(by ancestors)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;scout&lt;ul&gt;
&lt;li&gt;cut off from previous branches’ score(by brothers)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Performance  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SCOUT favors “skinny” game trees&lt;ul&gt;
&lt;li&gt;Show great improvements on depth &amp;gt; 3 for games with &lt;strong&gt;small branching factors&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;On depth = 5, it saves over 40% of time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AB + scout gets average 10~20% improvement than only AB&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Null(Zero) window search    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using alpha-beta search with the window [m,m + 1]&lt;ul&gt;
&lt;li&gt;result will be failed-high or failed-low&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Failed-high means return value &amp;gt; m + 1&lt;ul&gt;
&lt;li&gt;Equivalent to TEST(p; m;&amp;gt;) is true&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Failed-low means return value &amp;lt; m&lt;ul&gt;
&lt;li&gt;Equivalent to TEST(p; m;&amp;gt;) is false&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Using searching window is better than using a single bound in SCOUT&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/TCG/nega-scout.png&#34; alt=&#34;&#34;&gt;    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;depth &amp;lt; 3 → no alpha-beta pruning → return value is exact value(no need to search again)&lt;/li&gt;
&lt;li&gt;first-time search → do null window search(scout)&lt;/li&gt;
&lt;li&gt;research → do normal window a-b pruning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Refinements  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use information from previous search&lt;ul&gt;
&lt;li&gt;When a subtree is re-searched, restart from the position that the value is returned in first search&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Change move ordering&lt;ul&gt;
&lt;li&gt;Reorder the moves by priority list&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Proof-Number-Search&#34;&gt;&lt;a href=&#34;#Proof-Number-Search&#34; class=&#34;headerlink&#34; title=&#34;Proof Number Search&#34;&gt;&lt;/a&gt;Proof Number Search&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;https://chessprogramming.wikispaces.com/Proof-number+search#Pseudo%20Code&#34; alt=&#34;參考資料: chessprogramming: proof-number search&#34;&gt;&lt;/p&gt;
&lt;p&gt;binary valued game tree    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;2-player game tree with either 0 or 1 on the leaves&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;and-or tree: min → and, max → or&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;most proving node for node u&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;node that if its value is 1, then the value of u is 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;most disproving node for node u&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;node that if its value is 0, then the value of u is 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;proof(u): minimum number of nodes to visit to make u = 1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;disproof(u): minimum number of nodes to visit to make u = 0&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If value(u) is unknown, then proof(u) is the cost of evaluating u  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If value(u) is 1, then proof(u) = 0&lt;/li&gt;
&lt;li&gt;If value(u) is 0, then proof(u) = ∞&lt;/li&gt;
&lt;li&gt;proof number can be calculate by search childrens &lt;img data-src=&#34;/img/TCG/proof-number.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;disproof number → reverse calculate method of proof number&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Usage  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;find child u that have min{proof(root); disproof(root)}&lt;/li&gt;
&lt;li&gt;if we try to &lt;strong&gt;prove&lt;/strong&gt; it&lt;ul&gt;
&lt;li&gt;pick a child with the &lt;strong&gt;least proof number&lt;/strong&gt; for a &lt;strong&gt;MAX node&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;pick &lt;strong&gt;any node that has a chance to be proved&lt;/strong&gt; for a &lt;strong&gt;MIN node&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;if we try to &lt;strong&gt;disprove&lt;/strong&gt; it&lt;ul&gt;
&lt;li&gt;pick a child with the &lt;strong&gt;least disproof number&lt;/strong&gt; for a &lt;strong&gt;MIN node&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;pick &lt;strong&gt;any node that has a chance to be disproved&lt;/strong&gt; for a &lt;strong&gt;MAX node&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;used in open game tree or an endgame tree because some proof or disproof number is known&lt;ul&gt;
&lt;li&gt;1 → proved to win, 0 → proved to lose &lt;/li&gt;
&lt;li&gt;or used to achieve sub-goal in games&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- why smallest number because proof need all 1? --&gt;
&lt;p&gt;Proof-Number search algorithm &lt;img data-src=&#34;/img/TCG/pn-algo.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;keep update number by bottom-up&lt;ol&gt;
&lt;li&gt;compare proof number and disproof number of root&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;find the leaf to prove or disprove&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Multi-value game tree  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;value in [0, 1]&lt;/li&gt;
&lt;li&gt;$proof_v(u)$: the minimum number of leaves needed to visited to make u &amp;gt;= v&lt;ul&gt;
&lt;li&gt;proof(u) = $proof_1(u)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$disproof_v(u)$: the minimum number of leaves needed to visited to make u &amp;lt; v&lt;ul&gt;
&lt;li&gt;disproof(u) = $disproof_1(u)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;use binary search to set upper bound of the value &lt;img data-src=&#34;/img/TCG/multivalue-pn-algo.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap08-Monte-Carlo-Game-Tree-Search&#34;&gt;&lt;a href=&#34;#Chap08-Monte-Carlo-Game-Tree-Search&#34; class=&#34;headerlink&#34; title=&#34;Chap08 Monte-Carlo Game Tree Search&#34;&gt;&lt;/a&gt;Chap08 Monte-Carlo Game Tree Search&lt;/h2&gt;&lt;h3 id=&#34;original-ideas&#34;&gt;&lt;a href=&#34;#original-ideas&#34; class=&#34;headerlink&#34; title=&#34;original ideas&#34;&gt;&lt;/a&gt;original ideas&lt;/h3&gt;&lt;p&gt;Algorithm $MCS_{pure}$ &lt;img data-src=&#34;img/TCG/random-games.png&#34; alt=&#34;&#34;&gt;    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For each possible next move&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;play this move and then play a lot of random games(play every moves as random)&lt;/li&gt;
&lt;li&gt;calculate average score&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose move with best score&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Original version: GOBBLE in 1993  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Performance is not good compared to other Go programs(alpha-beta)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Enhanced versions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adding the idea of minimax tree search&lt;/li&gt;
&lt;li&gt;Adding more domain knowledge&lt;/li&gt;
&lt;li&gt;Adding more searching techniques&lt;/li&gt;
&lt;li&gt;Building theoretical foundations from statistics, and on-line and off-line learning&lt;/li&gt;
&lt;li&gt;results&lt;ul&gt;
&lt;li&gt;MoGo&lt;ul&gt;
&lt;li&gt;Beat a professional human 8 dan(段) with a 8-stone handicap at January 2008&lt;/li&gt;
&lt;li&gt;Judged to be in a “professional level” for 9 x 9 Go in 2009&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Zen&lt;ul&gt;
&lt;li&gt;close to amateur 3-dan in 2011&lt;/li&gt;
&lt;li&gt;Beat a 9-dan professional master with handicaps at March 17, 2012&lt;ul&gt;
&lt;li&gt;First game: Five stone handicap and won by 11 points&lt;/li&gt;
&lt;li&gt;Second game: four stones handicap and won by 20 points&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disadvantage  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;average score search != minimax tree search&lt;ul&gt;
&lt;li&gt;$MCS_{pure}$ prefer right branch, but it’s min value is low &lt;img data-src=&#34;/img/TCG/minmax-and-avergae.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;First-Refinement-Monte-Carlo-based-tree-search&#34;&gt;&lt;a href=&#34;#First-Refinement-Monte-Carlo-based-tree-search&#34; class=&#34;headerlink&#34; title=&#34;First Refinement: Monte-Carlo based tree search&#34;&gt;&lt;/a&gt;First Refinement: Monte-Carlo based tree search&lt;/h3&gt;&lt;p&gt;Intuition   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Best First tree growing&lt;ul&gt;
&lt;li&gt;Expand one level of best leaf(which has largest score) &lt;img data-src=&#34;/img/TCG/mct-ex2.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;if number of simulations is not enough, it can’t be a good simulation&lt;ul&gt;
&lt;li&gt;on a MIN node, if not enough children are probed for enough number of times, you may miss a very bad branch&lt;/li&gt;
&lt;li&gt;take &lt;strong&gt;simulation count&lt;/strong&gt; into consideration&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/TCG/MCT.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/TCG/mct-ex1.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;h3 id=&#34;Second-Refinement-UCT&#34;&gt;&lt;a href=&#34;#Second-Refinement-UCT&#34; class=&#34;headerlink&#34; title=&#34;Second Refinement: UCT&#34;&gt;&lt;/a&gt;Second Refinement: UCT&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Effcient sampling  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Original: equally distributed among all legal moves&lt;/li&gt;
&lt;li&gt;Biased sampling: sample some moves more often than others&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Observations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some moves are bad and do not need further exploring&lt;ul&gt;
&lt;li&gt;Need to consider extremely bad luck sitiation&lt;ul&gt;
&lt;li&gt;e.g. often “randomly” choose bad move and get bad score&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTXVsdGktYXJtZWRfYmFuZGl0&#34;&gt;K-arm bandit problem&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assume you have K slot machines each with a different payoff, i.e., expected value of returns ui, and an unknown distribution&lt;/li&gt;
&lt;li&gt;Assume you can bet on the machines N times, what is the best strategy to get the largest returns?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ideas&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try each machine a few, but enough, times and record their returns&lt;ul&gt;
&lt;li&gt;For the machines that currently have the best returns, play more often later&lt;/li&gt;
&lt;li&gt;For the machines that currently return poorly, give them a chance sometimes to check their distributions are really bad or not&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;UCB: Upper Confidence Bound &lt;img data-src=&#34;img/TCG/UCB.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Meaning&lt;ul&gt;
&lt;li&gt;For a MAX node, Wi is the number of win’s for the MAX player&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For a MIN node, Wi is the number of win’s for the MIN player&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;When N is approaching logN, then UCB is nothing but the current winning rate plus a constant&lt;/li&gt;
&lt;li&gt;When N getting larger, UCB will approachthe real winning rate&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Expand for the move with the highest UCB value&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;only compare UCB scores among children of a node&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;It is meaningless to compare scores of nodes that are not siblings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Using argument c to keep a balance between&lt;ul&gt;
&lt;li&gt;Exploitation: exploring the best move so far&lt;/li&gt;
&lt;li&gt;Exploration: exploring other moves to see if they can be proved to be better &lt;img data-src=&#34;/img/TCG/ucb-ex1.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;alternative&lt;ul&gt;
&lt;li&gt;consider the variance of scores in each branch &lt;img data-src=&#34;/img/TCG/UCB2.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;UCT: Upper Confidence Bound for Tree  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Maintain the UCB value for each node in the game tree&lt;ul&gt;
&lt;li&gt;Pick path such that each node in this path has a largest UCB score among all of its siblings&lt;/li&gt;
&lt;li&gt;Pick the leaf node in the path which has been visited more than a certain amount of times to expand&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Usable when the “density of goals” is suffciently large  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When there is only a unique goal, Monte-Carlo based simulation may not be useful&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;new MCT algorithm(with UCT) &lt;img data-src=&#34;/img/TCG/mct-uct.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;Implementation-hints&#34;&gt;&lt;a href=&#34;#Implementation-hints&#34; class=&#34;headerlink&#34; title=&#34;Implementation hints&#34;&gt;&lt;/a&gt;Implementation hints&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;/img/TCG/uct-imp.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/TCG/uct-imp2.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/TCG/uct-imp3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;When-to-use-Monte-Carlo&#34;&gt;&lt;a href=&#34;#When-to-use-Monte-Carlo&#34; class=&#34;headerlink&#34; title=&#34;When to use Monte-Carlo&#34;&gt;&lt;/a&gt;When to use Monte-Carlo&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;huge branching number &lt;/li&gt;
&lt;li&gt;cannot easily compute good evaluating function&lt;/li&gt;
&lt;li&gt;Mostly used in Go, Bridge(?)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Rule of Go(圍棋)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ko(打劫): 不能有重複盤面&lt;/li&gt;
&lt;li&gt;可以跳過，不能下自殺步&lt;/li&gt;
&lt;li&gt;Komi: 先手讓子&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Implementation  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;partition stones into strings(使用共同氣的子) by DFS&lt;/li&gt;
&lt;li&gt;check empty intersection is an eye or not(check neighbors and limits)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Domain-independent-refinements&#34;&gt;&lt;a href=&#34;#Domain-independent-refinements&#34; class=&#34;headerlink&#34; title=&#34;Domain independent refinements&#34;&gt;&lt;/a&gt;Domain independent refinements&lt;/h3&gt;&lt;p&gt;Main considerations   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avoid doing un-needed computations&lt;/li&gt;
&lt;li&gt;Increase the speed of convergence&lt;/li&gt;
&lt;li&gt;Avoid early mis-judgement&lt;/li&gt;
&lt;li&gt;Avoid extreme bad cases&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Refinements  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Progressive pruning  &lt;ul&gt;
&lt;li&gt;Cut hopeless nodes early&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;All moves at first(AMAF)&lt;ul&gt;
&lt;li&gt;Increase the speed of convergence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Node expansion&lt;ul&gt;
&lt;li&gt;Grow only nodes with a potential&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Temperature&lt;ul&gt;
&lt;li&gt;Introduce randomness&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Depth-i enhancement&lt;ul&gt;
&lt;li&gt;With regard to Line 1, the initial phase, exhaustively enumerate all possibilities&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Progressive-pruning&#34;&gt;&lt;a href=&#34;#Progressive-pruning&#34; class=&#34;headerlink&#34; title=&#34;Progressive pruning&#34;&gt;&lt;/a&gt;Progressive pruning&lt;/h4&gt;&lt;p&gt;Each move has a mean value m and a standard deviation σ  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Left expected outcome ml = m - rd * σ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Right expected outcome mr = m + rd * σ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;rd is argument&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A move M1 is &lt;strong&gt;statistically inferior&lt;/strong&gt; to another move M2 if M1.mr &amp;lt; M2.ml&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Two moves M1 and M2 are &lt;strong&gt;statistically equal&lt;/strong&gt; if M1.σ &amp;lt; σe and M2.σ &amp;lt; σe and no move is statistically inferior to the other&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;σe is argument which called standard deviation for equality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Remarks  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;only compare nodes that are of the same parent&lt;/li&gt;
&lt;li&gt;compare their raw scores not their UCB values&lt;ul&gt;
&lt;li&gt;If you use UCB scores, then the mean and standard deviation of a move are those calculated only from its un-pruned children&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;prune statistically inferior moves after enough number of times of simulation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This process is stopped when  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there is only one move left&lt;/li&gt;
&lt;li&gt;the moves left are statistically equal&lt;/li&gt;
&lt;li&gt;a maximal threshold(like 10000 multiplied by the number of legal moves) of iterations is reached&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Two different pruning rules  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hard: a pruned move cannot be a candidate later on&lt;/li&gt;
&lt;li&gt;Soft: a move pruned at a given time &lt;strong&gt;can be a candidate later on&lt;/strong&gt; if its value is no longer statistically inferior to a currently active move&lt;ul&gt;
&lt;li&gt;Periodically check whether to reactive it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Arguments  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Selection of rd &lt;img data-src=&#34;/img/TCG/uct-result2.png&#34; alt=&#34;&#34;&gt;   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The greater rd is&lt;/li&gt;
&lt;li&gt;the less pruned the moves are&lt;/li&gt;
&lt;li&gt;the better the algorithm performs&lt;/li&gt;
&lt;li&gt;the slower at each play&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Selection of σe &lt;img data-src=&#34;/img/TCG/uct-result1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The smaller σe is&lt;/li&gt;
&lt;li&gt;the fewer equalities there are&lt;/li&gt;
&lt;li&gt;the better the algorithm performs&lt;/li&gt;
&lt;li&gt;the slower at each play&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;rd plays an important role in the move pruning process&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;σe is less sensitive&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Another trick is progressive widening or progressive un-pruning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A node is effective if enough simulations are done on it and its values are good&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can set threshold on whether to expand the best path, for exmaple&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;enough simulations are done&lt;/li&gt;
&lt;li&gt;score is good enough&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;All-moves-at-first-AMAF&#34;&gt;&lt;a href=&#34;#All-moves-at-first-AMAF&#34; class=&#34;headerlink&#34; title=&#34;All moves at first(AMAF)&#34;&gt;&lt;/a&gt;All moves at first(AMAF)&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;score is used for &lt;strong&gt;all moves the same player played in a random game&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;in this example, after simulate r→v→y→u→w, w which  has parent v and u which has parent r will be updated, too &lt;img data-src=&#34;/img/TCG/amaf.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advantage&lt;ul&gt;
&lt;li&gt;speeding up the experiments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Drawback&lt;ul&gt;
&lt;li&gt;not the same move - move in early game is not equal to late game &lt;/li&gt;
&lt;li&gt;Recapturing&lt;ul&gt;
&lt;li&gt;Order of moves is important for certain games(圍棋)&lt;/li&gt;
&lt;li&gt;Modification: if several moves are played at the same place because of captures, modify the statistics only for the player who played first &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Refinement: RAVE    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let v1(m) be the score of a move m without using AMAF&lt;/li&gt;
&lt;li&gt;Let v2(m) be the score of a move m with AMAF&lt;/li&gt;
&lt;li&gt;Observations&lt;ul&gt;
&lt;li&gt;v1(m) is good when suffcient number of simulations are starting with m&lt;/li&gt;
&lt;li&gt;v2(m) is a &lt;strong&gt;good guess for the true score&lt;/strong&gt; of the move m&lt;ul&gt;
&lt;li&gt;when &lt;strong&gt;approaching the end of a game&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;when &lt;strong&gt;too few simulations starting with m&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Rapid Action Value Estimate (RAVE)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;revised score $v3(m) = a \times v1(m) + (1-a) \times v2(m)$&lt;/li&gt;
&lt;li&gt;can dynamically change a as the game goes&lt;ul&gt;
&lt;li&gt;For example: a = min{1, Nm/10000}, where Nm is simulation times start from m&lt;ul&gt;
&lt;li&gt;This means when Nm reaches 10000, then no RAVE is used&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Works out better than setting a = 0(i.e. pure AMAF)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Node-expansion&#34;&gt;&lt;a href=&#34;#Node-expansion&#34; class=&#34;headerlink&#34; title=&#34;Node expansion&#34;&gt;&lt;/a&gt;Node expansion&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;May decide to expand potentially good nodes judging from the&lt;br&gt;current statistics&lt;/li&gt;
&lt;li&gt;All ends: expand all possible children of a newly added node&lt;/li&gt;
&lt;li&gt;Visit count: delay the expansion of a node until it is visited a certain number of times&lt;/li&gt;
&lt;li&gt;Transition probability: delay the expansion of a node until its \score” or estimated visit count is high comparing to its siblings&lt;/li&gt;
&lt;li&gt;Use the current value, variance and parent’s value to derive a good estimation using statistical methods&lt;br&gt;Expansion policy with some transition probability is much better than the \all ends” or \pure visit count” policy&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ##Chap09 Other way to increase performance --&gt;

&lt;h2 id=&#34;Reference&#34;&gt;&lt;a href=&#34;#Reference&#34; class=&#34;headerlink&#34; title=&#34;Reference&#34;&gt;&lt;/a&gt;Reference&lt;/h2&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3d3dy5paXMuc2luaWNhLmVkdS50dy9+dHNoc3UvdGNnLw==&#34;&gt;TSHsu講義 2014年版&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
</content>
        <category term="電腦對局理論" />
        <category term="機器學習" />
        <category term="人工智慧" />
        <category term="圍棋" />
        <category term="象棋" />
        <category term="蒙地卡羅" />
        <category term="Alpha-Beta搜尋" />
        <category term="強化學習" />
        <updated>2014-09-26T11:41:48.000Z</updated>
    </entry>
    <entry>
        <id>http://gitqwerty777.github.io/MLfoundation1/</id>
        <title>機器學習基石(上)</title>
        <link rel="alternate" href="http://gitqwerty777.github.io/MLfoundation1/"/>
        <content type="html">&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2NvdXJzZS9udHVtbG9uZQ==&#34;&gt;原版&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;的講義做得十分精美，可以很快了解&lt;/p&gt;
&lt;h2 id=&#34;Chap01-Introduction&#34;&gt;&lt;a href=&#34;#Chap01-Introduction&#34; class=&#34;headerlink&#34; title=&#34;Chap01 Introduction&#34;&gt;&lt;/a&gt;Chap01 Introduction&lt;/h2&gt;&lt;p&gt;課堂討論：學習的定義    &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;從不會到會 &lt;/li&gt;
&lt;li&gt;從會到更進步、熟練&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/ML/0FPIeqh.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;課堂討論：學習的方法    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;以「樹的定義」為例  &lt;/li&gt;
&lt;li&gt;如何寫出「能判斷是否是樹」的程式？ &lt;ol&gt;
&lt;li&gt;define trees and hand-program: difficult&lt;/li&gt;
&lt;li&gt;learn from data by observation and recognize: more easier(機器「自己」學習)&lt;a id=&#34;more&#34;&gt;&lt;/a&gt;
&lt;img data-src=&#34;/img/ML/BuqSVKs.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;課堂討論：兩種學習方法  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;電腦: learn from data -&amp;gt; get knowledge by observing  &lt;/li&gt;
&lt;li&gt;人腦: learn from teachers -&amp;gt; get the essence of the knowledge(can computer do that?)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-eassence-of-ML&#34;&gt;&lt;a href=&#34;#key-eassence-of-ML&#34; class=&#34;headerlink&#34; title=&#34;key eassence of ML&#34;&gt;&lt;/a&gt;key eassence of ML&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;存在「&lt;strong&gt;潛藏模式&lt;/strong&gt;」可以學習&lt;ul&gt;
&lt;li&gt;若認為有「潛藏模式」，才需要學習  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;無法簡單定義&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;有可提供學習的&lt;strong&gt;資料&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;ML使用時機&#34;&gt;&lt;a href=&#34;#ML使用時機&#34; class=&#34;headerlink&#34; title=&#34;ML使用時機&#34;&gt;&lt;/a&gt;ML使用時機&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;人類無法操作&lt;ul&gt;
&lt;li&gt;火星探索&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;難以定義的問題&lt;ul&gt;
&lt;li&gt;視覺/聽覺辨識  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;需要快速判斷&lt;ul&gt;
&lt;li&gt;股票炒短線程式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;大量資料&lt;ul&gt;
&lt;li&gt;個人化使用者體驗&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ML應用&#34;&gt;&lt;a href=&#34;#ML應用&#34; class=&#34;headerlink&#34; title=&#34;ML應用&#34;&gt;&lt;/a&gt;ML應用&lt;/h3&gt;&lt;p&gt;推薦系統&lt;br&gt;將物品分解成各個porperty factors，形成vector，並與自己的喜好vector比較  &lt;/p&gt;
&lt;h3 id=&#34;formalize-the-learning-problem&#34;&gt;&lt;a href=&#34;#formalize-the-learning-problem&#34; class=&#34;headerlink&#34; title=&#34;formalize the learning problem&#34;&gt;&lt;/a&gt;formalize the learning problem&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;target funcion &lt;code&gt;f&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;unknown pattern to be learned   &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;data &lt;code&gt;D&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;training examples&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hypothesis set &lt;code&gt;h&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;candidate functions to be choosed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hypothesis &lt;code&gt;g&lt;/code&gt; &lt;ul&gt;
&lt;li&gt;best candidate function which is learned from data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;use algorithm(A) with data(D) and hypothesis set(H) to get g &lt;img data-src=&#34;/img/ML/c5XEqoy.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Machine Learning:&lt;br&gt;&lt;br&gt;use data to compute hypothesis &lt;code&gt;g&lt;/code&gt; that approximates target &lt;code&gt;f&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;Differences&#34;&gt;&lt;a href=&#34;#Differences&#34; class=&#34;headerlink&#34; title=&#34;Differences&#34;&gt;&lt;/a&gt;Differences&lt;/h3&gt;&lt;h4 id=&#34;Machine-Learning-amp-Data-Mining&#34;&gt;&lt;a href=&#34;#Machine-Learning-amp-Data-Mining&#34; class=&#34;headerlink&#34; title=&#34;Machine Learning &amp;amp; Data Mining&#34;&gt;&lt;/a&gt;Machine Learning &amp;amp; Data Mining&lt;/h4&gt;&lt;p&gt;ML: the same as above&lt;br&gt;DM: use &lt;strong&gt;huge&lt;/strong&gt; data to &lt;strong&gt;find property&lt;/strong&gt; that is interesting&lt;/p&gt;
&lt;h4 id=&#34;Machine-Learning-amp-Artificial-Intelligence&#34;&gt;&lt;a href=&#34;#Machine-Learning-amp-Artificial-Intelligence&#34; class=&#34;headerlink&#34; title=&#34;Machine Learning &amp;amp; Artificial Intelligence&#34;&gt;&lt;/a&gt;Machine Learning &amp;amp; Artificial Intelligence&lt;/h4&gt;&lt;p&gt;AI -&amp;gt; compute something that shows intelligent behavior&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ML can realize AI&lt;/strong&gt;&lt;br&gt;traditional AI -&amp;gt; game tree&lt;br&gt;ML -&amp;gt; learning (techiniques) from board data&lt;/p&gt;
&lt;h4 id=&#34;Machine-Learning-amp-Statistics&#34;&gt;&lt;a href=&#34;#Machine-Learning-amp-Statistics&#34; class=&#34;headerlink&#34; title=&#34;Machine Learning &amp;amp; Statistics&#34;&gt;&lt;/a&gt;Machine Learning &amp;amp; Statistics&lt;/h4&gt;&lt;p&gt;Statistics: use data to make inference about an unknown process&lt;br&gt;-&amp;gt; many &lt;strong&gt;useful tools for ML&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;課堂討論：Big Data     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As data getting bigger, the way to deal with data has to be changed.(such as distributed computation)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;not&lt;/strong&gt; a new topic&lt;/li&gt;
&lt;li&gt;marketing buzz word&lt;br&gt;課堂討論：Maching Learning &amp;amp; Neural Network  &lt;/li&gt;
&lt;li&gt;A technique used in early AI and ML&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap-02-Perceptron-感知器&#34;&gt;&lt;a href=&#34;#Chap-02-Perceptron-感知器&#34; class=&#34;headerlink&#34; title=&#34;Chap 02 Perceptron(感知器)&#34;&gt;&lt;/a&gt;Chap 02 Perceptron(感知器)&lt;/h2&gt;&lt;h3 id=&#34;yes-no-question-by-grading&#34;&gt;&lt;a href=&#34;#yes-no-question-by-grading&#34; class=&#34;headerlink&#34; title=&#34;yes/no question by grading&#34;&gt;&lt;/a&gt;yes/no question by grading&lt;/h3&gt;&lt;p&gt;用feature(特質)來分隔兩種不同的結果    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;x: input&lt;/li&gt;
&lt;li&gt;w: hypothesis&lt;/li&gt;
&lt;li&gt;x是在d維度空間的點(d個features)，w為分隔此空間的線(平面)的法向量 &lt;img data-src=&#34;/img/ML/pla-w.png&#34; alt=&#34;&#34;&gt; &lt;/li&gt;
&lt;li&gt;以二維空間為例：w產生的線分隔兩邊 &lt;img data-src=&#34;/img/ML/MOzf2UK.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;也就是h(x)的正負，w所在的那一側為正 &lt;img data-src=&#34;/img/ML/joxwtUt.png&#34; alt=&#34;&#34;&gt;   &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;select-g-from-h&#34;&gt;&lt;a href=&#34;#select-g-from-h&#34; class=&#34;headerlink&#34; title=&#34;select g from h&#34;&gt;&lt;/a&gt;select g from h&lt;/h3&gt;&lt;p&gt;Difficult: h is infinite&lt;br&gt;Idea: 從某一條線開始，進行更改(local search)&lt;/p&gt;
&lt;h3 id=&#34;Perception-Learning-Algorithm-PLA&#34;&gt;&lt;a href=&#34;#Perception-Learning-Algorithm-PLA&#34; class=&#34;headerlink&#34; title=&#34;Perception Learning Algorithm(PLA)&#34;&gt;&lt;/a&gt;Perception Learning Algorithm(PLA)&lt;/h3&gt;&lt;p&gt;A fault confessed is half redressed(知錯能改)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;find a mistake(which sign is wrong) &lt;img data-src=&#34;/img/ML/u0KFPyS.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;correct the mistake &lt;img data-src=&#34;/img/ML/Mow3SlT.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;if real ans = +, new w = w + x(使w靠近正的點)&lt;/li&gt;
&lt;li&gt;if real ans = -, new w = w - x(使w遠離負的點) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;keep doing until no mistake &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;question&lt;br&gt;同乘$y_nx_n$ &lt;img data-src=&#34;/img/ML/KKHE36Z.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;可看出錯誤變少：正確的時候，$w_nx_n$和$y_n$同號，所以$w_nx_ny_n$是正的    &lt;/p&gt;
&lt;h3 id=&#34;linear-seperability&#34;&gt;&lt;a href=&#34;#linear-seperability&#34; class=&#34;headerlink&#34; title=&#34;linear seperability&#34;&gt;&lt;/a&gt;linear seperability&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/ML/5L1kwEZ.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;linear seperable&lt;ul&gt;
&lt;li&gt;exist perfect w makes $sign(y) = sign(w_nx_n)$, n = 0~N&lt;/li&gt;
&lt;li&gt;用直線(平面)必可分成無錯誤的兩塊  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;if Data is linear seperable, then PLA can generate w to make no mistake &lt;/li&gt;
&lt;li&gt;每次改動使$w_f$(正解)和$w_t$的內積變大，也就是愈來愈接近 &lt;img data-src=&#34;/img/ML/unBVfjt.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;但成長速度有限 &lt;img data-src=&#34;/img/ML/LHtRvcu.png&#34; alt=&#34;&#34;&gt;    &lt;ul&gt;
&lt;li&gt;$|W_t| &amp;lt;= sqrt(t) max(X_n)$&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/ML/J66FCPC.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;question&lt;br&gt;&lt;img data-src=&#34;/img/ML/0szpVwP.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;PLA-Guarantee&#34;&gt;&lt;a href=&#34;#PLA-Guarantee&#34; class=&#34;headerlink&#34; title=&#34;PLA Guarantee&#34;&gt;&lt;/a&gt;PLA Guarantee&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/ML/9qQxERz.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;advantage&lt;ul&gt;
&lt;li&gt;simple to implement&lt;/li&gt;
&lt;li&gt;fast&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;disadvantage&lt;ul&gt;
&lt;li&gt;not fully sure how long it will take&lt;/li&gt;
&lt;li&gt;assume linear seperable&lt;ul&gt;
&lt;li&gt;What if no linear seperate?(in reality)&lt;/li&gt;
&lt;li&gt;選出犯錯最少的&lt;/li&gt;
&lt;li&gt;這是個NP-HARD問題… &lt;img data-src=&#34;/img/ML/oRWuGAO.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Pocket-Algorithm-a-little-modified-by-PLA&#34;&gt;&lt;a href=&#34;#Pocket-Algorithm-a-little-modified-by-PLA&#34; class=&#34;headerlink&#34; title=&#34;Pocket Algorithm(a little modified by PLA)&#34;&gt;&lt;/a&gt;Pocket Algorithm(a little modified by PLA)&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/ML/XkWjmux.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;greedy &lt;ul&gt;
&lt;li&gt;may not be the best answer: 可能是局部最佳解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;slower than PLA(need to compare Wt+1 and Wt)  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap03-types-of-learning&#34;&gt;&lt;a href=&#34;#Chap03-types-of-learning&#34; class=&#34;headerlink&#34; title=&#34;Chap03 types of learning&#34;&gt;&lt;/a&gt;Chap03 types of learning&lt;/h2&gt;&lt;h3 id=&#34;Different-Output-Space&#34;&gt;&lt;a href=&#34;#Different-Output-Space&#34; class=&#34;headerlink&#34; title=&#34;Different Output Space&#34;&gt;&lt;/a&gt;Different Output Space&lt;/h3&gt;&lt;p&gt;Binary Classification  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;yes/no&lt;/li&gt;
&lt;li&gt;core problem to build tools&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Multiclass Classification(N output class)    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression(迴歸分析)&lt;ul&gt;
&lt;li&gt;output 為一數字&lt;/li&gt;
&lt;li&gt;Ex. temperature, stock price&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;core problem to build statistic tools&lt;/strong&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Structured Learning&lt;ul&gt;
&lt;li&gt;output $y$ = structures with &lt;strong&gt;implicit class definition&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;too many class → structure&lt;/li&gt;
&lt;li&gt;Ex. Speech parse tree, sequence tagging(標詞性), protein folding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Different-Data-Label&#34;&gt;&lt;a href=&#34;#Different-Data-Label&#34; class=&#34;headerlink&#34; title=&#34;Different Data Label&#34;&gt;&lt;/a&gt;Different Data Label&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Supervised&lt;/strong&gt; Learning(監督式學習)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data with pairs of input and output&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unsupervised Learning  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;doesn’t have output data(沒正確答案)&lt;/li&gt;
&lt;li&gt;clustering(分群問題)&lt;ul&gt;
&lt;li&gt;density estimation(find traffic dangerous areas)&lt;/li&gt;
&lt;li&gt;unusual detection(find unusual data)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;usually used in data mining &lt;img data-src=&#34;/img/ML/Jz6fiwk.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Semi-Supervised  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;given small amount of data with output, find output of other data&lt;ul&gt;
&lt;li&gt;Ex. facebook face identifier&lt;/li&gt;
&lt;li&gt;leverage unlabeled data to avoid ‘expensive’ labeling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reinforcement Learning(增強學習)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;natural way of learning(行為學派)&lt;ul&gt;
&lt;li&gt;learn with &lt;strong&gt;‘seqentially implicit output’&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;if output is good, give reinforcement&lt;ul&gt;
&lt;li&gt;probability of this input increases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;if output is bad, give pushnishment&lt;ul&gt;
&lt;li&gt;probability of this input decreases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ex. &lt;ul&gt;
&lt;li&gt;train a dog&lt;/li&gt;
&lt;li&gt;online ADs&lt;/li&gt;
&lt;li&gt;chess AI&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;和gene algorithm類似&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Different-Protocol&#34;&gt;&lt;a href=&#34;#Different-Protocol&#34; class=&#34;headerlink&#34; title=&#34;Different Protocol&#34;&gt;&lt;/a&gt;Different Protocol&lt;/h3&gt;&lt;p&gt;Batch Learning    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;learn from known data&lt;ul&gt;
&lt;li&gt;duck feeding(填鴨式)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;very common protocol&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Online Learning  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sequential, passive data(不斷的得到新資料)&lt;/li&gt;
&lt;li&gt;Every datum can improve &lt;code&gt;g&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;PLA, reinforcement learning is often used with online learning&lt;/li&gt;
&lt;li&gt;Ex. spam filter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Active Learning  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;strategically-observed data&lt;/li&gt;
&lt;li&gt;machine can ask question(take &lt;strong&gt;chosen&lt;/strong&gt;(input, output)pair to learn)&lt;ul&gt;
&lt;li&gt;關於自己不會(錯誤)的問題，拿相關的資料來學習&lt;/li&gt;
&lt;li&gt;比對有自信的答案(= 對答案)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Different-Input-Space&#34;&gt;&lt;a href=&#34;#Different-Input-Space&#34; class=&#34;headerlink&#34; title=&#34;Different Input Space&#34;&gt;&lt;/a&gt;Different Input Space&lt;/h3&gt;&lt;p&gt;Feature &amp;lt;-&amp;gt; Input&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Concrete&lt;/strong&gt; Features  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;each input class represents some ‘sophisticated physical meaning’&lt;/li&gt;
&lt;li&gt;input 和 output 有相關(經過人類分類過)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Raw Features(未處理的資料)   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‘simple physical meaing’ -&amp;gt; difficult to learn&lt;/li&gt;
&lt;li&gt;Ex. Digit Recognition&lt;ul&gt;
&lt;li&gt;concrete feature: symmtry, density&lt;/li&gt;
&lt;li&gt;raw feature: matrix of image bits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract Features  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‘no physical learning’ -&amp;gt; the most difficult to learn&lt;/li&gt;
&lt;li&gt;need ‘feature conversion’&lt;/li&gt;
&lt;li&gt;Ex. Rating Prediction Problem&lt;ul&gt;
&lt;li&gt;從歌曲評分抽出feature: 喜好, 歌的性質……  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general machine learning, those three feature types will be used&lt;/p&gt;
&lt;h2 id=&#34;Chap-04-Feasibility-of-Learning&#34;&gt;&lt;a href=&#34;#Chap-04-Feasibility-of-Learning&#34; class=&#34;headerlink&#34; title=&#34;Chap 04 Feasibility of Learning&#34;&gt;&lt;/a&gt;Chap 04 Feasibility of Learning&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;learning will be stricted by limited data(no free lunch)&lt;/li&gt;
&lt;li&gt;learning from D (to infer something outside D) is doomed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Statistics   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Real environment -&amp;gt; unknown&lt;/li&gt;
&lt;li&gt;Sample data -&amp;gt; known&lt;ul&gt;
&lt;li&gt;Can sample represent the real?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;有極小可能無法代表real status&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Hoeffding’s-Inequality&#34;&gt;&lt;a href=&#34;#Hoeffding’s-Inequality&#34; class=&#34;headerlink&#34; title=&#34;Hoeffding’s Inequality&#34;&gt;&lt;/a&gt;Hoeffding’s Inequality&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;v and u are error rate of certain h in sample and real data &lt;img data-src=&#34;/img/ML/PG3e7Jr.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;larger sample size N or looser gap(誤差)&lt;ul&gt;
&lt;li&gt;higher probability to approximate real&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Error between hypothesis and target function&lt;/strong&gt; can be inferred by data &lt;img data-src=&#34;/img/ML/2I9ZSPn.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/ML/AC3KnSC.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;Ein-and-Eout&#34;&gt;&lt;a href=&#34;#Ein-and-Eout&#34; class=&#34;headerlink&#34; title=&#34;Ein and Eout&#34;&gt;&lt;/a&gt;Ein and Eout&lt;/h3&gt;&lt;p&gt;in-sample error(Ein) and out-of-sample error(Eout)&lt;br&gt;Guarantee: for large N, Ein(h) ~= Eout(h) is probably approximately correct (PAC) &lt;img data-src=&#34;/img/ML/colR3kh.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;p&gt;Q: if 150 people flips a coin 5 times, and one of them gets 5 heads.  A: Probability is &amp;gt; 99% &lt;img data-src=&#34;/img/ML/CCrtjgi.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ 做愈多次，遇到的BAD sample(Eout 和 Ein 差很多; sample和實際差距過大)的機率愈大&lt;br&gt;→ Real learning: Algorithm choose the best &lt;code&gt;h&lt;/code&gt; which has lowest Ein(h) among &lt;code&gt;H&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bad Data for a &lt;code&gt;H&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;存在 &lt;code&gt;h&lt;/code&gt; 使 Ein(h) 和 Eout(h) 相差很大 &lt;img data-src=&#34;/img/ML/x6wkDZk.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;由 hoeffding 知道抽到bad data的機率很小&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hypothesis的個數愈多，抽到BAD data的機率愈高 &lt;img data-src=&#34;/img/ML/IK9lYNY.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;安全的data(在任何h都不是bad data)的比例 若很高，則學到的東西可能不好&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若hypothesis set的大小是有限的話，只要N夠大，Eout ~= Ein&lt;br&gt;但perceptron不是finite(有無限多種分隔可選)&lt;/p&gt;
&lt;h2 id=&#34;Chap05-Training-versus-Testing&#34;&gt;&lt;a href=&#34;#Chap05-Training-versus-Testing&#34; class=&#34;headerlink&#34; title=&#34;Chap05 Training versus Testing&#34;&gt;&lt;/a&gt;Chap05 Training versus Testing&lt;/h2&gt;&lt;p&gt;g is similar to f ↔ Eout(g) ~= Ein(g) ~= 0  &lt;/p&gt;
&lt;p&gt;But need train and test &lt;img data-src=&#34;/img/ML/TXVWRpF.png&#34; alt=&#34;&#34;&gt;       &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train: find hypothesis that can fit sample data   &lt;/li&gt;
&lt;li&gt;Test: take &lt;strong&gt;good sample data&lt;/strong&gt; that is similar to exact data  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How to decide the number of hypothesis set&lt;br&gt;&lt;img data-src=&#34;/img/ML/mrA45Zq.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/ML/hsyNq1P.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Cannot both satisfied!&lt;/p&gt;
&lt;p&gt;Todo: Find a finite value $m_H$ can replace infinite M&lt;br&gt;&lt;img data-src=&#34;/img/ML/LOwwaGm.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Idea: M is overestimated, we use classification:&lt;br&gt;how many lines =&amp;gt; how many kinds of line(that makes different output)&lt;br&gt;This method is called Dichotomies(二分法): Mini-hypotheses&lt;br&gt;&lt;img data-src=&#34;/img/ML/8CcPNcS.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;input&lt;/th&gt;
&lt;th&gt;types of lines&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4 (00, 01, 10, 11)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;14 (2 lines that is not &lt;br&gt; linearly seperable)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;N&lt;/td&gt;
&lt;td&gt;effective(N) &amp;lt;= $2^N$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Growth Function $m_H$ = &lt;strong&gt;max number of dichotomies(max number of different outputs)&lt;/strong&gt;&lt;br&gt;&lt;img data-src=&#34;/img/ML/xc50yGO.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;h3 id=&#34;Types-of-Growth-Function&#34;&gt;&lt;a href=&#34;#Types-of-Growth-Function&#34; class=&#34;headerlink&#34; title=&#34;Types of Growth Function&#34;&gt;&lt;/a&gt;Types of Growth Function&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Positive Rays &lt;img data-src=&#34;/img/ML/vmoIwfN.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;$m_H(N)$ = N + 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Positive Intervals &lt;img data-src=&#34;/img/ML/FcLeNhZ.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;$C^{N+1}_2 + 1$ &lt;img data-src=&#34;/img/ML/D4mfUyr.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Convex Sets&lt;ul&gt;
&lt;li&gt;worst case: every point make a circle &lt;img data-src=&#34;/img/ML/tVqlZrK.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;$m_H(N) = 2^N$ -&amp;gt; exists N inputs that can be &lt;strong&gt;shattered(所有output皆可產生)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/ML/eEXFWde.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Now $m_H(N)$ is finite, but exponential&lt;br&gt;Question:Can we find polynomial instead of exponential?&lt;/p&gt;
&lt;h3 id=&#34;Break-Point-of-H&#34;&gt;&lt;a href=&#34;#Break-Point-of-H&#34; class=&#34;headerlink&#34; title=&#34;Break Point of H&#34;&gt;&lt;/a&gt;Break Point of H&lt;/h3&gt;&lt;p&gt;if all possible k inputs can’t be shattered by H&lt;br&gt;k = break point for H &lt;img data-src=&#34;/img/ML/q3wjQSm.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;2D perceptrons: break point at 4&lt;br&gt;3 inputs: exist at least one input that can shatter &lt;img data-src=&#34;/img/ML/perceptron-shatter.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;4 inputs: for all inputs, no shatter  &lt;/p&gt;
&lt;p&gt;If there is no breakpoint, we can only find exponential($2^N$) increase&lt;br&gt;If there is a breakpoint, we can find polynomial($O(N^k)$)increase&lt;br&gt;breakpoint愈小，hypothesis set 成長的速度受到愈多限制(因為無法shatter，所以hypothesis數比exponential小)&lt;/p&gt;
&lt;h2 id=&#34;Chap06-Theory-of-Generalization&#34;&gt;&lt;a href=&#34;#Chap06-Theory-of-Generalization&#34; class=&#34;headerlink&#34; title=&#34;Chap06 Theory of Generalization&#34;&gt;&lt;/a&gt;Chap06 Theory of Generalization&lt;/h2&gt;&lt;p&gt;Q: maximum possible $m_H(N)$ if input number(N) = 3 when breakpoint(k) = 2?&lt;br&gt;A: x1, x2 cannot shatter, and so does x2, x3 and x1, x3 &lt;img data-src=&#34;/img/ML/KE3Xwxf.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ When N &amp;gt; breakpoint, break point restricts $m_H(N)$ a lot!&lt;/p&gt;
&lt;p&gt;idea: prove $m_H(N) \leq$ poly(N) if N &amp;gt; k &lt;/p&gt;
&lt;h3 id=&#34;Bounding-function&#34;&gt;&lt;a href=&#34;#Bounding-function&#34; class=&#34;headerlink&#34; title=&#34;Bounding function&#34;&gt;&lt;/a&gt;Bounding function&lt;/h3&gt;&lt;p&gt;bounding function B(N, k): maximum possible $m_H(N)$ when break point = k&lt;/p&gt;
&lt;p&gt;Table of bounding function(incomplete) &lt;img data-src=&#34;/img/ML/darN0tn.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;B(N, k) = $m_H(N) = 2^N$ when N &amp;lt; k(shatter)&lt;br&gt;B(N, k) &amp;lt; $m_H(N) = 2^N - 1$ when N = k(至少比shatter少一種)&lt;br&gt;When N &amp;gt; k :Using reduce, Ex. B(4,3) &lt;img data-src=&#34;/img/ML/gDjeq7v.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;α: dichotomies on (x1, x2, x3) with x4 paired&lt;br&gt;β: dichotomies on (x1, x2, x3) with x4 no paired&lt;/p&gt;
&lt;p&gt;Because B(4,3) can’t shatter any 3 inputs&lt;br&gt;→ α + β can’t shatter at (x1, x2, x3)&lt;br&gt;→ α + β $\leq$ B(3,3)&lt;/p&gt;
&lt;p&gt;Because B(4,3) can’t shatter any 3 inputs and x4 is already paired&lt;br&gt;→ α can’t shatter any 2 inputs at (x1, x2, x3)&lt;br&gt;→ α $\leq$ B(3,2)&lt;/p&gt;
&lt;p&gt;B(4,3) = 2α + β $\leq$ B(3,3) + B(3,2)&lt;br&gt;Generalized: B(N,k) $\leq$ B(N-1,k) + B(N-1,k-1) &lt;img data-src=&#34;/img/ML/jbksHEC.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;By calculation: $m_H(N) \leq B(N,k) \leq N^{k-1}$  &lt;/p&gt;
&lt;p&gt;Conclusion: $m_H(N)$ is polynomial if break point exists for N &amp;gt;= 2 &amp;amp; k &amp;gt;= 3!!&lt;br&gt;&lt;img data-src=&#34;/img/ML/M8N4HsO.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/ML/OqhVOS4.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;‘&amp;lt;=’ can be ‘=’ actually -&amp;gt; not easy proof(skipped)&lt;/p&gt;
&lt;h3 id=&#34;Vapnik-Chervonenkis-VC-bound&#34;&gt;&lt;a href=&#34;#Vapnik-Chervonenkis-VC-bound&#34; class=&#34;headerlink&#34; title=&#34;Vapnik-Chervonenkis (VC) bound&#34;&gt;&lt;/a&gt;Vapnik-Chervonenkis (VC) bound&lt;/h3&gt;&lt;p&gt;Proof: BAD Bound for General H   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Now Ein(h) finite, but Eout(h) still infinite(Eout的點有無限個)&lt;ol&gt;
&lt;li&gt;use ghost sample data Ein’ to replace(&lt;strong&gt;想像&lt;/strong&gt;再sample一次會產生的Ein’，將這段資料作為eout)&lt;/li&gt;
&lt;li&gt;圖中Ein離Eout很遠，是bad data，只要Ein’在Eout附近，Ein’也會離Eout很遠 &lt;img data-src=&#34;/img/ML/kK29SSC.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Eout 乘1/2，使其成為不等式 &lt;img data-src=&#34;/img/ML/jr6WUKW.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;將bad data相似的hypothesis分在一起 &lt;img data-src=&#34;&#34; alt=&#34;&#34;&gt;&lt;ol&gt;
&lt;li&gt;總共有2N個data(Ein + Ein’) → $m_H(2N)$ &lt;img data-src=&#34;/img/ML/MQ5v22d.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;因為有了$m_H()$函數，變成只考慮固定的hypothesis   &lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Use Hoeffding without Replacement&lt;ol&gt;
&lt;li&gt;可視為2N個點取N個點，sample為Ein，剩下為Ein’(不放回去)&lt;/li&gt;
&lt;li&gt;使用 ‘Hoeffding without Replacement’： 公式和hoeffding 一樣 &lt;img data-src=&#34;/img/ML/0ZC5xI3.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Hoeffding只用於單一hypothesis，所以需要步驟2&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Vapnik-Chervonenkis (VC) bound &lt;img data-src=&#34;/img/ML/tjn5okQ.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ proved that learning with &lt;strong&gt;2D perceptrons&lt;/strong&gt; feasible!&lt;br&gt;&lt;img data-src=&#34;/img/ML/kyXVoYU.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;You need to let everything good to learned well &lt;img data-src=&#34;/img/ML/n8YPfWQ.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;Chap-07-VC-Dimension&#34;&gt;&lt;a href=&#34;#Chap-07-VC-Dimension&#34; class=&#34;headerlink&#34; title=&#34;Chap 07 VC Dimension&#34;&gt;&lt;/a&gt;Chap 07 VC Dimension&lt;/h2&gt;&lt;p&gt;VC Dimension&lt;br&gt;= maximum non-break point = (minimum k) - 1&lt;br&gt;= largest N that can shatter &lt;/p&gt;
&lt;p&gt;2D perceptron review &lt;img data-src=&#34;/img/ML/EOUT=0.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;How does PLA in more than 2 dimension?  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D → 3&lt;/li&gt;
&lt;li&gt;d-dimension perceptron &lt;ul&gt;
&lt;li&gt;d_VC = d+1 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Proof&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;d_VC &amp;gt; d+1 → d+1 can shatter&lt;br&gt;input matrix which is invertible &lt;img data-src=&#34;/img/ML/specificmatrix.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;for any y, we can find w such that sign(Xw) = y → $w = yX^{-1}$ → it can shatter &lt;/li&gt;
&lt;li&gt;d_VC &amp;lt; d+1 → d+2 can’t shatter&lt;br&gt;linear dependence restricts dichotomy &lt;img data-src=&#34;/img/ML/linearrely.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;if row &amp;gt; column, it would cause linear dependence &lt;img data-src=&#34;/img/ML/xd+2=all.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;for any input, we can find some $a_n$ that makes an output can’t happen → no shatter &lt;img data-src=&#34;/img/ML/geneag0.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;freedom&#34;&gt;&lt;a href=&#34;#freedom&#34; class=&#34;headerlink&#34; title=&#34;freedom&#34;&gt;&lt;/a&gt;freedom&lt;/h3&gt;&lt;p&gt;dimension, number of parameters, hypothesis quantity(M) → degrees of freedom&lt;br&gt;d_VC(H) = effitive binary degrees of freedom = powerfulness of H&lt;/p&gt;
&lt;p&gt;The more powerful it is (d_vc bigger), the more probability to get bad data &lt;img data-src=&#34;/img/ML/dvcbigsmall.png&#34; alt=&#34;D_vc&#34;&gt;&lt;br&gt;question:&lt;img data-src=&#34;/img/ML/Qhyperplane.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;比perceptron少一個parameter → d&lt;/p&gt;
&lt;p&gt;penalty for model complexity &lt;img data-src=&#34;/img/ML/smalle.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;model愈強，Ein愈小，和Eout誤差愈大 &lt;img data-src=&#34;/img/ML/modelcomplexity.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;p&gt;number of data(N) should be 10000 d_vc in theory; 10 d_vc is enough in practice, because VC bound is loose &lt;img data-src=&#34;/img/ML/nanddvc.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/ML/hoffedingloose.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;question: &lt;img data-src=&#34;/img/ML/q2.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;all of above(increase power of model)&lt;/p&gt;
&lt;h2 id=&#34;Chap08-Noise-and-Error&#34;&gt;&lt;a href=&#34;#Chap08-Noise-and-Error&#34; class=&#34;headerlink&#34; title=&#34;Chap08 Noise and Error&#34;&gt;&lt;/a&gt;Chap08 Noise and Error&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Noise in y&lt;ul&gt;
&lt;li&gt;Example: good customer mislabeled as bad&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Noise in x&lt;ul&gt;
&lt;li&gt;Example: incorrect feature calculation &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Would get probabilisic output y ≠ h(x) by given P(y|x)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Does VC bound works in noise? Yes, if i.i.d.(Independent and identically distributed) &lt;img data-src=&#34;/img/ML/iid.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ we can view as ‘ideal mini-target’ + noise&lt;br&gt;→ learning goal is to &lt;strong&gt;predict ideal mini-target(which is Y that has high P(Y|X) given X) on often seen inputs(X with high P(X))&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;Eout use expectation instead of Σ , $err$ means pointwise error(only consider a point x) &lt;img data-src=&#34;/img/ML/einout.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;h3 id=&#34;Error-Measure&#34;&gt;&lt;a href=&#34;#Error-Measure&#34; class=&#34;headerlink&#34; title=&#34;Error Measure&#34;&gt;&lt;/a&gt;Error Measure&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/ML/01andsquare.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;classification(0/1 error)&lt;ul&gt;
&lt;li&gt;minimum flipping noise(最少錯誤的output) &lt;/li&gt;
&lt;li&gt;NP-hard to optimize&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;regression use squared error&lt;ul&gt;
&lt;li&gt;minimum gaussian noise(output和正確答案的平方差最小)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Error is *&lt;em&gt;application/user dependent *&lt;/em&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CIA fingerprint login error&lt;ul&gt;
&lt;li&gt;not allow predict 0  to 1 &lt;img data-src=&#34;/img/ML/unbalancedata.png&#34; alt=&#34;&#34;&gt;     &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Supermarket member login error&lt;ul&gt;
&lt;li&gt;not want to predict 1 to 0 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error weight is not the same!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example: pocket  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;modify Ein to $E^w_{in}$(with weight)&lt;/li&gt;
&lt;li&gt;weight愈高的錯誤愈容易被選來修正&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;權重可以套用在許多機器學習的演算法&lt;/p&gt;
&lt;h3 id=&#34;algorithm-choosing&#34;&gt;&lt;a href=&#34;#algorithm-choosing&#34; class=&#34;headerlink&#34; title=&#34;algorithm choosing&#34;&gt;&lt;/a&gt;algorithm choosing&lt;/h3&gt;&lt;p&gt;Algorithmic Error Measures $\hat{err}$   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True&lt;ul&gt;
&lt;li&gt;error cannot be ignored or created&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;plausible(可用性)&lt;ul&gt;
&lt;li&gt;0/1 error&lt;/li&gt;
&lt;li&gt;squared error&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;friendly(較容易的演算法)    &lt;ul&gt;
&lt;li&gt;close form solution(有公式解，如Chap09的linear regression)&lt;/li&gt;
&lt;li&gt;convex objective function(可以持續更新的，如PLA)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\hat{err}$ is key part of many algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/ML/err-flow.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;參考資料&#34;&gt;&lt;a href=&#34;#參考資料&#34; class=&#34;headerlink&#34; title=&#34;參考資料&#34;&gt;&lt;/a&gt;參考資料&lt;/h2&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2NvdXJzZS9udHVtbG9uZQ==&#34;&gt;Coursera機器學習基石&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;br&gt;C老師上課講解&lt;/p&gt;
</content>
        <category term="機器學習" />
        <category term="perceptron" />
        <updated>2014-09-16T11:41:48.000Z</updated>
    </entry>
</feed>
