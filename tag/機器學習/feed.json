{
    "version": "https://jsonfeed.org/version/1",
    "title": "QWERTY • All posts by \"機器學習\" tag",
    "description": "Programming | Computer Science | Thought",
    "home_page_url": "http://gitqwerty777.github.io",
    "items": [
        {
            "id": "http://gitqwerty777.github.io/natural-language-processing2/",
            "url": "http://gitqwerty777.github.io/natural-language-processing2/",
            "title": "自然語言處理(下)",
            "date_published": "2015-05-01T04:37:47.000Z",
            "content_html": "<h2 id=\"Chap08-Syntax-and-Grammars\"><a href=\"#Chap08-Syntax-and-Grammars\" class=\"headerlink\" title=\"Chap08 Syntax and Grammars\"></a>Chap08 Syntax and Grammars</h2><p>Grammar    </p>\n<ul>\n<li>represent certain knowledges of what we know about a language</li>\n<li>General criteria<ul>\n<li>linguistic naturalness</li>\n<li>mathematical power</li>\n<li>computational effectiveness</li>\n</ul>\n</li>\n</ul>\n<p>Context-free grammars(CFG)</p>\n<ul>\n<li>Alias<ul>\n<li>Phrase structure grammars</li>\n<li>Backus-Naur form</li>\n</ul>\n</li>\n<li>More powerful than finite state machine</li>\n<li>Rules <ul>\n<li>Terminals <ul>\n<li>words</li>\n</ul>\n</li>\n<li>Non-terminals <ul>\n<li>Noun phrase, Verb phrase …</li>\n</ul>\n</li>\n<li>Generate strings in the language</li>\n<li>Reject strings not in the language  </li>\n<li>LHS → RHS<ul>\n<li>LHS: Non-terminals </li>\n<li>RHS: Non-terminals or Terminals</li>\n</ul>\n</li>\n<li>Context Free<ul>\n<li>probability of a subtree does not depend on words not dominated by the subtree(subtree出現的機率和上下文無關)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<a id=\"more\"></a>\n\n<p>Evaluation  </p>\n<ol>\n<li>Does it undergenerate?<ul>\n<li>rules cannot completely explain language</li>\n<li>not a problem if the goal is to produce a language</li>\n</ul>\n</li>\n<li>Does it overgenerate?<ul>\n<li>rules overly explain the language</li>\n<li>not a problem if the goal is to recognize or understand well-formed(correct) language</li>\n</ul>\n</li>\n<li>Does it assign appropriate structures to the strings that it generates?</li>\n</ol>\n<p>Parsing  </p>\n<ul>\n<li>take a string and a grammar</li>\n<li>assigning trees that covers all and only words in input strings</li>\n<li>return parse tree for that string</li>\n</ul>\n<p>English Grammar Fragment  </p>\n<ul>\n<li>Sentences</li>\n<li>Noun Phrases<ul>\n<li>Ex. NP → det Nominal</li>\n<li><strong>head: central criticial noun in NP</strong><ul>\n<li>important in statistical parsing</li>\n<li>after det(冠詞), before pp(介系詞片語) <img data-src=\"/img/NLP/np-parse.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Agreement<ul>\n<li>a part of overgenerate</li>\n<li>This flight(○)</li>\n<li>This flights(×)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Verb Phrases<ul>\n<li>head verb with arguments</li>\n<li>Subcategorization(分類)<ul>\n<li>categorize according to VP rules</li>\n<li>a part of overgenerate</li>\n<li>Prefer<ul>\n<li>I prefer [to leave earlier]TO-VP</li>\n</ul>\n</li>\n<li>Told<ul>\n<li>I was told [United has a flight]S</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Overgenerate Solution<br>    - transform into multiple rules<br>        - NP → Single_Det Single_Nominal<br>        - NP → 複數_Det 複數_Nominal<br>        - Will generate a lot of rules!<br>    - out of CFG framework<br>        - Chomsky hierarchy of languages <img data-src=\"/img/NLP/modelclass.png\" alt=\"\"></p>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL3poLndpa2lwZWRpYS5vcmcvd2lraS8lRTklOTklODQlRTYlQTAlODclRTglQUYlQUQlRTglQTglODA=\">Indexed Grammar<i class=\"fa fa-external-link-alt\"></i></span>  </p>\n<ul>\n<li>Indexed grammars and languages problem <img data-src=\"/img/NLP/index-example.png\" alt=\"\"> </li>\n<li>recognized by nested stack automata <img data-src=\"/img/NLP/index-grammar.png\" alt=\"\"></li>\n</ul>\n<h3 id=\"Treebanks-and-headfinding\"><a href=\"#Treebanks-and-headfinding\" class=\"headerlink\" title=\"Treebanks and headfinding\"></a>Treebanks and headfinding</h3><p>critical to the development of statistical parsers</p>\n<p>Treebanks  </p>\n<ul>\n<li>corpora with parse trees<ul>\n<li>created by automatic parser and human annotators</li>\n</ul>\n</li>\n<li>Ex. Penn Treebank</li>\n<li>Grammar<ul>\n<li>Treebanks implicitly define a grammar<ul>\n<li>Simply make all subtrees fit the rules</li>\n</ul>\n</li>\n<li>parse tree tend to be very flat to avoid recursion<ul>\n<li>Penn Treebank has ~4500 different rules for VPs</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Head Finding  </p>\n<ul>\n<li>use tree traversal rules specific to each non-terminal in the grammar</li>\n<li>先向右再向左 <img data-src=\"/img/NLP/head-np.png\" alt=\"\"><!--重要--></li>\n</ul>\n<h3 id=\"Dependency-Grammars\"><a href=\"#Dependency-Grammars\" class=\"headerlink\" title=\"Dependency Grammars\"></a>Dependency Grammars</h3><ul>\n<li>every possible parse is a tree <img data-src=\"/img/NLP/dependency-parse.png\" alt=\"\"><ul>\n<li>every node is a word </li>\n<li>every link is dependency relations between words </li>\n</ul>\n</li>\n<li>Advantage<ul>\n<li>Deals well with long-distance word order languages <ul>\n<li>structure is flexible</li>\n</ul>\n</li>\n<li>Parsing is much faster than CFG</li>\n<li>Tree can be used by later applications</li>\n</ul>\n</li>\n<li>Approaches<!--重要--><ul>\n<li>Optimization-based approaches <ul>\n<li>search for the tree that matches some criteria the best</li>\n<li>spanning tree algorithms</li>\n</ul>\n</li>\n<li>Shift-reduce approaches<ul>\n<li>greedily take actions based on the current word and state</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Summary  </p>\n<ul>\n<li>Constituency(顧客, words that behave as a single unit) is a key phenomena easily captured with CFG rules<ul>\n<li>But agreement and subcategorization make problems</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap09-Syntactic-Parsing\"><a href=\"#Chap09-Syntactic-Parsing\" class=\"headerlink\" title=\"Chap09 Syntactic Parsing\"></a>Chap09 Syntactic Parsing</h2><ul>\n<li>Top-Down Search <img data-src=\"/img/NLP/top-down.png\" alt=\"\"><ul>\n<li>Search trees among possible answers  <!--- But suggests trees that are not consistent with words--></li>\n</ul>\n</li>\n<li>Bottom-Up Parsing<ul>\n<li>Only forms trees that can fit the words <!-- global tree may not form answer(S) --></li>\n</ul>\n</li>\n<li>Mixed parsing strategy<ul>\n<li>looks like Binomial Search</li>\n<li>The number of rules tried at each deicision of the analysis (branching factor)<ul>\n<li>top-down parsing: categories of LHS(Left Hand Side) word</li>\n<li>bottom-up parsing: categories of left most RHS(Right Hand Side) word<ul>\n<li>倒推：從最左邊可以倒推的字開始</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>backtracking methods are doomed because of two inter-related problems  </p>\n<ul>\n<li>(1)Structural and lexical ambiguity<ul>\n<li>PP(介系詞片語) attachment<ul>\n<li>PP can attach to [sentences, verb phrases, noun phrases, and adjectival phrases]</li>\n</ul>\n</li>\n<li>coordination<ul>\n<li>P and Q or R <ul>\n<li>P and (Q or R)</li>\n<li>(P and Q) or R</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>noun-noun compounding<ul>\n<li>town widget hammer<ul>\n<li>((town widget) hammer)</li>\n<li>(town (widget hammer))</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Solution<ul>\n<li>how to determine the intended structure?</li>\n<li>how to store the partial structures?</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>(2)Shared subproblems<ul>\n<li>naïve backtracking will lead to duplicated work(不一定會對，所以會一直backtrack…)</li>\n</ul>\n</li>\n</ul>\n<p>Dynamic Programming  </p>\n<ul>\n<li>Avoid doing repeated work</li>\n<li>Solve in polynomial time</li>\n<li>approaches<ul>\n<li>CKY</li>\n<li>Earley</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"CKY-bottom-up\"><a href=\"#CKY-bottom-up\" class=\"headerlink\" title=\"CKY(bottom-up)\"></a>CKY(bottom-up)</h3><ul>\n<li>transform rules into Chomsky-Normal Form <img data-src=\"/img/NLP/cnf-transform.png\" alt=\"\"></li>\n<li>build a table <ul>\n<li>A spanning from i to j in the input is in [i,j]</li>\n<li>A → BC == [i,j] → [i,k] [k,j]</li>\n<li>entire string = [0, n] <ul>\n<li>expected to be S</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>iterate all possible k <img data-src=\"/img/NLP/CKY-table2.png\" alt=\"\"><ul>\n<li>[i,j] = [i,i+1]x[i+1, j], [i,i+2]x[i+2,j] ……</li>\n</ul>\n</li>\n<li>fill the table a column at a time, from left to right, bottom to top <img data-src=\"/img/NLP/CKY-table3.png\" alt=\"\"><ul>\n<li>Ex. [1,3] = [1,2]Det + [2,3] Nomimal, Noun = NP</li>\n<li>Ex. <img data-src=\"/img/NLP/CKY-ex.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Algorithm <img data-src=\"/img/NLP/CKY-algo.png\" alt=\"\"></li>\n<li>Performance<ul>\n<li>a lot of elements unrelated to the answer</li>\n<li>can use online search to fill table (from left to right)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Earley\"><a href=\"#Earley\" class=\"headerlink\" title=\"Earley\"></a>Earley</h3><ul>\n<li>parser that exploits chart as data structure</li>\n<li>node = vertex</li>\n<li>arc = edge<ul>\n<li>active edge: (a) and (b)</li>\n<li>inactive edge: (c)</li>\n</ul>\n</li>\n</ul>\n<p>decorated grammar  </p>\n<ul>\n<li>(a) “•” in the first <img data-src=\"/img/NLP/dot-a.png\" alt=\"\"><ul>\n<li>• NP VP</li>\n<li>A hypothesis has been made, but has not been verified yet</li>\n</ul>\n</li>\n<li>(b) “•” in the middle <img data-src=\"/img/NLP/dot-b.png\" alt=\"\"><ul>\n<li>NP • VP</li>\n<li>A hypothesis has been partially confirmed</li>\n</ul>\n</li>\n<li>(c) “•” in the last<ul>\n<li>NP VP •</li>\n<li>A hypothesis has been wholly confirmed</li>\n</ul>\n</li>\n<li>representation of edge <img data-src=\"/img/NLP/chart-struct.png\" alt=\"\"></li>\n<li>initialization <img data-src=\"/img/NLP/chart-initialize.png\" alt=\"\"><ul>\n<li>for each rule A → W, if A is a category that can span a chart (typically S), add &lt;0, 0, A → •W&gt; <img data-src=\"/img/NLP/chartchart-init.png\" alt=\"\"><ul>\n<li>A implies •W from position 0 to 0</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Housekeeping<ul>\n<li>prevent duplicate rules</li>\n</ul>\n</li>\n</ul>\n<p>Fundamental rule  </p>\n<ul>\n<li>If the chart contains &lt;i, j, A → W1 •B W2&gt; and &lt;j, k, B → W3 •&gt;, then add edge &lt;i, k, A → W1 B •W2&gt; to the chart <img data-src=\"/img/NLP/chart-fund.png\" alt=\"\"></li>\n<li>Notes<ol>\n<li>New edge may be either active or inactive</li>\n<li>does not remove the active edge that has succeeded</li>\n</ol>\n</li>\n</ul>\n<p>Bottom-up rule  </p>\n<ul>\n<li>if adding edge &lt;i, j, C → W1 •&gt; to the chart, then for every rule that has the form B → C W2, add &lt;i, i, B → • C W2&gt; <img data-src=\"/img/NLP/chart-bottom.png\" alt=\"\"></li>\n</ul>\n<p>Top-down rule   </p>\n<ul>\n<li>If adding edge &lt;i, j, C → W1 •B W2&gt; to the chart, then for each rule B → W, add &lt; j, j, B →•W&gt;</li>\n</ul>\n<h3 id=\"Full-Syntactic-Parsing\"><a href=\"#Full-Syntactic-Parsing\" class=\"headerlink\" title=\"Full Syntactic Parsing\"></a>Full Syntactic Parsing</h3><ul>\n<li>necessary for deep semantic analysis of texts</li>\n<li>not practical for many applications (given typical resources)<ul>\n<li>O(n^3) for straight parsing</li>\n<li>O(n^5) for probabilistic versions</li>\n<li>Too slow for real time applications (search engines)</li>\n</ul>\n</li>\n<li>Two Alternatives<ul>\n<li>Dependency parsing<ul>\n<li>Change the underlying grammar formalism</li>\n<li>can get a lot done with just binary relations among the words</li>\n<li>詳見Chap08 dependency grammar</li>\n</ul>\n</li>\n<li>Partial parsing<ul>\n<li>Approximate phrase-structure parsing with finite-state and statistical approaches</li>\n</ul>\n</li>\n<li>Both of these approaches give up something (syntactic, structure) in return for more robust and efficient parsing</li>\n</ul>\n</li>\n</ul>\n<p>Partial parsing</p>\n<ul>\n<li>For many applications you don’t really need full parse</li>\n<li>For example, if you’re interested in locating all the people, places and organizations  <ul>\n<li>base-NP chunking <ul>\n<li>[NP The morning flight] from [NP Denvar] has arrived </li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Two approaches<ul>\n<li>Rule-based (hierarchical) transduction(轉導) <!--???--><ul>\n<li>Restrict recursive rules (make the rules flat)<ul>\n<li>like NP → NP VP</li>\n</ul>\n</li>\n<li>Group the rules so that RHS of the rules can refer to non-terminals introduced in earlier transducers, but not later ones</li>\n<li>Combine the rules in a group in the same way we did with the rules for spelling changes</li>\n<li>Combine the groups into a cascade<ul>\n<li>can be used to find the sequence of flat chunks you’re interested in</li>\n<li>or approximate hierarchical trees you get from full parsing with a CFG</li>\n</ul>\n</li>\n<li>Typical Architecture ![](/img/NLP/Cascaded Transducers.png)<ul>\n<li>Phase 1: Part of speech tags</li>\n<li>Phase 2: Base syntactic phrases</li>\n<li>Phase 3: Larger verb and noun groups</li>\n<li>Phase 4: Sentential level rules</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Statistical sequence labeling<ul>\n<li>HMMs</li>\n<li>MEMMs</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap10-Statistical-Parsing\"><a href=\"#Chap10-Statistical-Parsing\" class=\"headerlink\" title=\"Chap10 Statistical Parsing\"></a>Chap10 Statistical Parsing</h2><p>Motivation  </p>\n<ul>\n<li>N-gram models and HMM Tagging only allowed us to process sentences linearly</li>\n<li><strong>Probabilistic Context Free Grammars</strong>(PCFG)<ul>\n<li>alias: Stochastic context-free grammar(SCFG)</li>\n<li>simplest and most natural probabilistic model for tree structures</li>\n<li>closely related to those for HMMs</li>\n<li>為每一個CFG的規則標示其發生的可能性</li>\n</ul>\n</li>\n</ul>\n<p>Idea  </p>\n<ul>\n<li>reduce “right” parse to “most probable parse”<ul>\n<li>Argmax P(Parse|Sentence)</li>\n</ul>\n</li>\n</ul>\n<p>A PCFG consists of  </p>\n<ul>\n<li>set of terminals, {wk}</li>\n<li>set of nonterminals, {Ni}</li>\n<li>start symbol N1</li>\n<li>set of rules<ul>\n<li>{Ni –&gt; ξj}(ξj is a sequence of terminals and nonterminals)</li>\n</ul>\n</li>\n<li>probabilities of rules<ul>\n<li>total probability of imply Ni to other sequence ξj is 1 </li>\n<li>∀i Σj P(Ni → ξj) = 1</li>\n</ul>\n</li>\n<li>Probability of sentence according to grammar G <ul>\n<li>P($w_{1m}$) = sum of P($w_{1m}$, t) for every possible tree t</li>\n</ul>\n</li>\n<li>Nj dominates the words wa … wb<ul>\n<li>Nj → wa … wb</li>\n</ul>\n</li>\n</ul>\n<p>Assumptions of the Model  </p>\n<ul>\n<li>Place Invariance<ul>\n<li>probability of a subtree does not depend on its position in the string</li>\n<li>similar to time invariance in HMMs</li>\n</ul>\n</li>\n<li>Ancestor Free<ul>\n<li>probability of a subtree does not depend on nodes in the derivation outside the subtree(subtree的機率只和subtree內的node有關)</li>\n<li>can simplify probability calculation <img data-src=\"/img/NLP/after-assump.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>Questions of PCFGs(similar to three questions of HMM)    </p>\n<ul>\n<li>Assign probabilities to parse trees<ul>\n<li>What is the probability of a sentence $w_{1m}$ according to a grammar G<ul>\n<li>P(w1m|G)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Parsing with probabilities(Decoding)<ul>\n<li>What is the most likely parse for a sentence<ul>\n<li>argmax_t P(t|w1m,G) </li>\n<li>How to efficiently find the best (or N best) trees </li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Training the model (Learning) <ul>\n<li>How to set rule probabilities(parameter of grammar model) that maximize the probability of a sentence<ul>\n<li>argmax_G P(w1m|G)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Simple Probability Model  </p>\n<ul>\n<li>probability of a tree is the product of the probabilities of rules in derivation</li>\n<li>Rule Probabilities<ul>\n<li>S → NP </li>\n<li>P(NP | S)</li>\n</ul>\n</li>\n<li>Training the Model<ul>\n<li>estimate probability from data</li>\n<li>P(α → β | α) = Count(α→β) / Count(α) = Count(α→β) / Σγ Count(α→γ)</li>\n</ul>\n</li>\n<li>Parsing (Decoding)<ul>\n<li>trees with highest probability in the model</li>\n</ul>\n</li>\n<li>Example: Book the dinner flight<ul>\n<li><img data-src=\"/img/NLP/pm-ex.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/pm-ex2.png\" alt=\"\"></li>\n<li>too slow!</li>\n</ul>\n</li>\n</ul>\n<p>Dynamic Programming again  </p>\n<ul>\n<li>use CKY and Earley to <strong>parse</strong></li>\n<li>Viterbi and HMMs to <strong>get the best parse</strong></li>\n<li>Parameters of a PCFG in Chomsky Normal Form<ul>\n<li>P(Nj→NrNs | G) , $n^3$ matrix of parameters</li>\n<li>P(Nj→wk | G), $nV$ parameters</li>\n<li>n is the number of nonterminals </li>\n<li>V is the number of terminals</li>\n</ul>\n</li>\n<li>Σr,s P(Nj→NrNs) + ΣkP(Nj→wk) = 1<ul>\n<li>所有由Nj導出的rule，機率總和必為1</li>\n</ul>\n</li>\n</ul>\n<p>Probabilistic Regular Grammars (PRG)    </p>\n<ul>\n<li>start state N1 </li>\n<li>rules<ul>\n<li>Ni → wjNk</li>\n<li>Ni → wj</li>\n</ul>\n</li>\n<li>PRG is a HMM with [start state] and [finish(sink) state] <img data-src=\"/img/NLP/prg-sink.png\" alt=\"\"></li>\n</ul>\n<p>Inside and Outside probability <img data-src=\"/img/NLP/prg-graph.png\" alt=\"\"> <img data-src=\"/img/NLP/prg-bf.png\" alt=\"\"> <img data-src=\"/img/NLP/prg-bf2.png\" alt=\"\">  </p>\n<ul>\n<li>Forward(Outside) probability<ul>\n<li>$ α<em>i(t) = P(w</em>{1(t-1)}, X_t = i)$</li>\n<li>everything above a certain node(include the node)</li>\n</ul>\n</li>\n<li>Backward(Inside) probability<ul>\n<li>$ β<em>i(t, T) = P(w</em>{tT} | X_t = i)$</li>\n<li>everything below a certain node</li>\n<li>total probability of generating words $w_t \\cdots w_T$, given the root nonterminal $N^i$ and a grammar G</li>\n</ul>\n</li>\n</ul>\n<p>Inside Algorithm (bottom-up)      </p>\n<ul>\n<li>$P(w_{1m} | G) = P(N_1 → w_{1m} | G) = P(w_{1m} | N^1_{1m}, G) = B_1(1,m)$<ul>\n<li>$B_1(1,m)$ is Inside probability<ul>\n<li>P(w1~wm are below N1(start symbol))</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>base rule<ul>\n<li>$ B_j(k, k) = P(w_k | N^j_{kk}, G) = P(N^j → w_k | G)$</li>\n</ul>\n</li>\n<li>$ B_j(p, q) = P(w_{pq} | N^j_{pq}, G) = $ <img data-src=\"/img/NLP/inside-induction.png\" alt=\"\"><ul>\n<li>try every possible rules to split Nj, product of *<em>rule probabilty and segments’ inside probabilities *</em> </li>\n</ul>\n</li>\n<li>use grid to solve again<ul>\n<li><img data-src=\"/img/NLP/inside-grid.png\" alt=\"\"><ul>\n<li>X軸代表起始座標，Y軸代表長度<ul>\n<li>(2,3) → flies like ants</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Outside Algorithm (top-down)     </p>\n<ul>\n<li>$ P(w_{1m} | G) = Σ_j α_j(k, k)P(N^j → w_k$ <img data-src=\"/img/NLP/outside-graph.png\" alt=\"\"> <!--為何是sum...--><ul>\n<li>outside probability of wk x (inside) probability of wk  of every Nj</li>\n</ul>\n</li>\n<li>basecase <ul>\n<li>$ α_1(1, m) = 1, α_j(1,m) = $</li>\n<li>P(N1) = 1, P(Nj outside w1 to wm) = 0</li>\n</ul>\n</li>\n<li>自己的outside probability 等於 <ul>\n<li>爸爸的outside probability 乘以 爸爸的inside probability 除以 自己的inside probability<ul>\n<li>inside x outside 是固定值？</li>\n</ul>\n</li>\n<li>爸爸的inside probabiliity 除以 自己的inside probability 就是其兄弟的inside probability</li>\n<li>使用此公式計算 <img data-src=\"/img/NLP/inout.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>$ α<em>j(p, q)β_j(p, q) = P(w</em>{1m}, N^j_{pq} | G) $<ul>\n<li>某個點的inside 乘 outside = 在某grammar中，出現此句子，且包含此點的機率 </li>\n<li>所有點的總和：在某grammar下，某parse tree(包含所有node)的機率 <img data-src=\"/img/NLP/parse-probability.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Outside example: 這些數字理論上算起來會一樣… <img data-src=\"/img/NLP/outside-forward.png\" alt=\"\"></li>\n</ul>\n<p>Finding the Most Likely Parse for a Sentence     </p>\n<ul>\n<li>δi(p,q)= the highest inside probability parse of a subtree $N_{pq}^i$</li>\n<li>Initialization <ul>\n<li>δi(p,p)= P(Ni → wp)</li>\n</ul>\n</li>\n<li>Induction and Store backtrace<ul>\n<li>δi(p,q)= $argmax(j,k,r)P(Ni→NjNk)δj(p,r)δk(r+1,q)$</li>\n<li>找所有可能的切法</li>\n</ul>\n</li>\n<li>Termination<ul>\n<li>answer = δ1(1,m)</li>\n</ul>\n</li>\n</ul>\n<p>Training a PCFG</p>\n<ul>\n<li>find the optimal probabilities among grammar rules</li>\n<li>use EM Training Algorithm to seek the grammar that maximizes the likelihood of the training data<ul>\n<li>Inside-Outside Algorithm </li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/inoutagain.png\" alt=\"\"></li>\n<li>將產生句子的機率視為π，為Nj產生pq的機率 <img data-src=\"/img/NLP/pi.png\" alt=\"\"></li>\n<li>Nj被使用的機率 <img data-src=\"/img/NLP/pi2.png\" alt=\"\"></li>\n<li>Nj被使用，且Nj→NrNs的機率 <img data-src=\"/img/NLP/pi3.png\" alt=\"\"></li>\n<li>Nj→NrNs這條rule被使用的機率=前兩式相除 <img data-src=\"/img/NLP/pi4.png\" alt=\"\"></li>\n<li>Nj→wk <img data-src=\"/img/NLP/pi5.png\" alt=\"\"><ul>\n<li>僅分子差異 <img data-src=\"/img/NLP/pi6.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>Problems with the Inside-Outside Algorithm    </p>\n<ul>\n<li>Extremely Slow<ul>\n<li>For each sentence, each iteration of training is $O(m^3n^3)$</li>\n</ul>\n</li>\n<li>Local Maxima</li>\n<li>Satisfactory learning requires many more nonterminals than are theoretically needed to describe the language</li>\n<li>There is no guarantee that the learned nonterminals will be linguistically motivated</li>\n</ul>\n<h2 id=\"Chap11-Dependency-Parsing\"><a href=\"#Chap11-Dependency-Parsing\" class=\"headerlink\" title=\"Chap11 Dependency Parsing\"></a>Chap11 Dependency Parsing</h2><p><span class=\"exturl\" data-url=\"aHR0cDovL3N0cC5saW5nZmlsLnV1LnNlL35uaXZyZS9kb2NzL0FDTHNsaWRlcy5wZGY=\">COLING-ACL 2006, Dependency Parsing, by Joachim Nivre and Sandra Kuebler<i class=\"fa fa-external-link-alt\"></i></span><br><span class=\"exturl\" data-url=\"aHR0cDovL25hYWNsaGx0MjAxMC5pc2kuZWR1L3R1dG9yaWFscy90Ny1zbGlkZXMucGRm\">NAACL 2010, Recent Advances in Dependency Parsing, by Qin Iris. Wang and YueZhang<i class=\"fa fa-external-link-alt\"></i></span><br><span class=\"exturl\" data-url=\"aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3NpdGUvemhlbmdodWFubHAvcHVibGljYXRpb25zL0lKQ05MUDIwMTMtdHV0b3JpYWwtRFAucGRmP2F0dHJlZGlyZWN0cz0wJmQ9MQ==\">IJCNLP 2013, Dependency Parsing: Past, Present, and Future, by Zhenghua Li, Wenliang Chen, Min Zhang<i class=\"fa fa-external-link-alt\"></i></span></p>\n<p>Dependency Structure vs. Constituency Structure <img data-src=\"/img/NLP/parse.png\" alt=\"\"><br>Parsing is one way to deal with the ambiguity problem in<br>natural language<br>dependency syntax is syntactic relations (dependencies) </p>\n<p>Constraint: between word pairs  <img data-src=\"/img/NLP/depend.png\" alt=\"\"><br>    Projective: No crossing links(a word and its dependents form a contiguous substring of the sentence)<br>    An arc (wi , r ,wj ) ∈ A is projective iff wi →∗ wk for all:<br>    i &lt; k &lt; j when i &lt; j<br>    j &lt; k &lt; i when j &lt; i<br>    射出去的那一方也可以射到兩個字中間的任何一字<br><img data-src=\"/img/NLP/depend-ex.png\" alt=\"\"></p>\n<p>Non-projective Dependency Trees  </p>\n<ul>\n<li>Long-distance dependencies  </li>\n<li>With crossing links</li>\n<li>Not so frequent in English<ul>\n<li>All the dependency trees from Penn Treebank are projective</li>\n</ul>\n</li>\n<li>Common in other languages with free word order<ul>\n<li>Prague(23%) and Czech, German and Dutch</li>\n</ul>\n</li>\n</ul>\n<p>Data Driven Dependency Parsing  </p>\n<ul>\n<li>Data-driven parsing<ul>\n<li>No grammar / rules needed</li>\n<li>Parsing decisions are made based on learned models</li>\n<li>deal with ambiguities well</li>\n<li><img data-src=\"/img/NLP/data-driven.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Three approaches<ul>\n<li>Graph-based models</li>\n<li>Transition-based models(good in practice)<ul>\n<li>Define a transition system for <strong>mapping a sentence to its dependency tree</strong></li>\n<li>Predefine some transition actions</li>\n<li>Learning: predicting the next state transition, by transition history</li>\n<li>Parsing: construct the optimal transition sequence</li>\n<li>Greedy search / beam search</li>\n<li>Features are defined over a richer parsing history</li>\n</ul>\n</li>\n<li>Hybrid models</li>\n</ul>\n</li>\n</ul>\n<p>Comparison   </p>\n<ul>\n<li>Graph-based models<ul>\n<li>Find the optimal tree from all the possible ones</li>\n<li>Global, exhaustive</li>\n</ul>\n</li>\n<li>Transition-based models<ul>\n<li>Predefine some actions (shift and reduce)</li>\n<li>use stack to hold partially built parses</li>\n<li><strong>Find the optimal action sequence</strong></li>\n<li>Local, Greedy or beam search</li>\n</ul>\n</li>\n<li>The two models produce different types of errors</li>\n</ul>\n<p>Hybrid Models  </p>\n<ul>\n<li>Three integration methods<ul>\n<li>Ensemble approach: parsing time integration (Sagae &amp; Lavie 2006)</li>\n<li>Feature-based integration (Nivre &amp; Mcdonald 2008)</li>\n<li>Single model combination (Zhang &amp; Clark 2008)</li>\n</ul>\n</li>\n<li>Gain benefits from both models</li>\n</ul>\n<p><img data-src=\"/img/NLP/parse-algo.png\" alt=\"\"></p>\n<h3 id=\"Graph-based-dependency-parsing-models\"><a href=\"#Graph-based-dependency-parsing-models\" class=\"headerlink\" title=\"Graph-based dependency parsing models\"></a>Graph-based dependency parsing models</h3><ul>\n<li>Search for a tree with the highest score</li>\n<li>Define search space<ul>\n<li>Exhaustive search</li>\n<li>Features are defined over a limited parsing history</li>\n</ul>\n</li>\n<li>The score is linear combination of features <ul>\n<li>What features we can use? (later)</li>\n<li>What learning approaches can lead us to find the best tree with the highest score (later)</li>\n</ul>\n</li>\n<li>Applicable to both probabilistic and nonprobabilistic models </li>\n</ul>\n<p>Features  </p>\n<ul>\n<li>dynamic features<ul>\n<li>Take into account the link labels of the surrounding word-pairs when predicting the label of current pair</li>\n<li>Commonly used in sequential labeling</li>\n<li>A word’s children are generated first(先生child, 再找parent), before it modifies another word</li>\n</ul>\n</li>\n</ul>\n<p>Learning Approaches   </p>\n<ul>\n<li>Local learning approaches<ul>\n<li>Learn a local link classifier given of features defined on training data</li>\n<li>example <img data-src=\"/img/NLP/local-feature-example.png\" alt=\"\"><ul>\n<li>3-class classification: No link, left link or right link</li>\n<li>Efficient O(n) local training</li>\n</ul>\n</li>\n<li>local training and parsing <img data-src=\"/img/NLP/local-train-with-parse.png\" alt=\"\"></li>\n<li>Learn the weights of features<ul>\n<li>Maximum entropy models (Ratnaparkhi 99, Charniak 00)</li>\n<li>Support vector machines (Yamada &amp; Matsumoto 03)</li>\n<li>Use a richer feature set!</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Global learning approaches</li>\n<li>Unsupervised/Semi-supervised learning approaches<ul>\n<li>Use both annotated training data and un-annotated raw text</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Transition-based-model\"><a href=\"#Transition-based-model\" class=\"headerlink\" title=\"Transition-based model\"></a>Transition-based model</h3><ul>\n<li>Stack holds partially built parses</li>\n<li>Queue holds unprocessed words</li>\n<li>Actions<ul>\n<li>use input words to build output parse</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"parsing-processes\"><a href=\"#parsing-processes\" class=\"headerlink\" title=\"parsing processes\"></a>parsing processes</h4><p>Arc-eager parser  </p>\n<ul>\n<li>4 tranition actions<ul>\n<li>SHIFT: push stack</li>\n<li>REDUCE: pop stack</li>\n<li>ARC-LEFT: pop stack and add link</li>\n<li>ARC-RIGHT: push stack and add link</li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/arc-eager-example.png\" alt=\"\"></li>\n<li>Time complexity: linear<ul>\n<li>every word will be pushed once and popped once(except root)</li>\n</ul>\n</li>\n<li>parse<ul>\n<li>by actions: arcleft → arclect subject, noun, …</li>\n</ul>\n</li>\n</ul>\n<p>Arc-standard parser  </p>\n<ul>\n<li>3 actions<ul>\n<li>SHIFT: push</li>\n<li>LEFT: pop leftmost stack element and add</li>\n<li>RIGHT: pop rightmost stack element and add</li>\n</ul>\n</li>\n<li>Also linear time</li>\n</ul>\n<p>Non-projectivity  </p>\n<ul>\n<li>neither of parser can solve it<ul>\n<li>online reorder<ul>\n<li>add extra action: swap</li>\n<li>not linear: $N^2$, but expect to belinear</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Decoding-algorithms\"><a href=\"#Decoding-algorithms\" class=\"headerlink\" title=\"Decoding algorithms\"></a>Decoding algorithms</h4><p>search action sequence to build the parse<br>scoring action given context<br>Candidate item &lt;S, G, Q&gt;</p>\n<ul>\n<li>greedy local search<ul>\n<li>initialize: Q = input</li>\n<li>goal: S=[root], G=tree, Q=[]</li>\n</ul>\n</li>\n<li>problem: one error leads to incorrect parse<ul>\n<li>Beam search: keep N highest partial states<ul>\n<li>use total score of all actions to rank a parse</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Score-Models\"><a href=\"#Score-Models\" class=\"headerlink\" title=\"Score Models\"></a>Score Models</h4><ul>\n<li>linear model</li>\n<li>SVM</li>\n</ul>\n<h2 id=\"Chap12-Semantic-Representation-and-Computational-Semantics\"><a href=\"#Chap12-Semantic-Representation-and-Computational-Semantics\" class=\"headerlink\" title=\"Chap12 Semantic Representation and Computational Semantics\"></a>Chap12 Semantic Representation and Computational Semantics</h2><p>Semantic aren’t primarily descriptions of inputs</p>\n<p>Semantic Processing  </p>\n<ul>\n<li>reason about the truth</li>\n<li>answer questions based on content<ul>\n<li>Touchstone application is often question answering</li>\n</ul>\n</li>\n<li>inference to determine the truth that isn’t actually know</li>\n</ul>\n<p>Method    </p>\n<ul>\n<li>principled, theoretically motivated approach<ul>\n<li>Computational/Compositional Semantics</li>\n</ul>\n</li>\n<li>limited, practical approaches that have some hope of being useful<ul>\n<li>Information extraction</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Information-Extraction\"><a href=\"#Information-Extraction\" class=\"headerlink\" title=\"Information Extraction\"></a>Information Extraction</h3><p>Information Extraction = segmentation + classification +  association + clustering <img data-src=\"/img/NLP/IE.png\" alt=\"\"></p>\n<ul>\n<li>superficial analysis <ul>\n<li>pulls out only the entities, relations and roles related to consuming application</li>\n</ul>\n</li>\n<li>Similar to chunking</li>\n</ul>\n<h3 id=\"Compositional-Semantics\"><a href=\"#Compositional-Semantics\" class=\"headerlink\" title=\"Compositional Semantics\"></a>Compositional Semantics</h3><ul>\n<li>Use First-Order Logic(FOL) representation that accounts for all the entities, roles and relations present in a sentence</li>\n<li>Similar to our approach to full parsing</li>\n<li>Compositional: The meaning of a whole is derived from the meanings of the parts(syntatic) <img data-src=\"/img/NLP/syntax-semantic.png\" alt=\"\"></li>\n<li>Syntax-Driven Semantic Analysis<ul>\n<li>The composition of meaning representations is guided by the <strong>syntactic</strong> components and relations provided by the  grammars</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"FOL\"><a href=\"#FOL\" class=\"headerlink\" title=\"FOL\"></a>FOL</h4><ul>\n<li>allow to answer yes/no questions</li>\n<li>allow variable</li>\n<li>allow inference</li>\n</ul>\n<p>Events, actions and relationships can be captured with representations that consist of predicates with arguments  </p>\n<ul>\n<li>Predicates<ul>\n<li>Primarily Verbs, VPs, Sentences</li>\n<li>Verbs introduce/refer to events and processes</li>\n</ul>\n</li>\n<li>Arguments <ul>\n<li>Primarily Nouns, Nominals, NPs, PPs</li>\n<li>Nouns introduce the things that play roles in those events</li>\n</ul>\n</li>\n<li>Example: Mary gave a list to John <ul>\n<li>Giving(Mary, John, List)</li>\n<li>Gave: Predicate</li>\n<li>Mary, John, List: Argument</li>\n<li>better representation <img data-src=\"/img/NLP/FOL-better.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Lambda Forms<ul>\n<li>Allow variables to be bound</li>\n<li>λxP(x)(Sally) = P(Sally)</li>\n</ul>\n</li>\n</ul>\n<p>Ambiguation  </p>\n<ul>\n<li>mismatch between syntax and semantics<ul>\n<li>displaced arguments</li>\n<li>complex NPs with quantifiers<ul>\n<li>A menu</li>\n<li>Every restaurant <img data-src=\"/img/NLP/complicate-NP.png\" alt=\"\"></li>\n<li>Not every waiter</li>\n<li>Most restaurants</li>\n<li><img data-src=\"/img/NLP/complicate-NP-induction.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>still preserving strict compositionality</li>\n</ul>\n</li>\n<li>Two (syntax) rules to revise<ul>\n<li>The S rule<ul>\n<li>S → NP VP, NP.Sem(VP.Sem)</li>\n<li>NP and VP swapped, because S is NP</li>\n</ul>\n</li>\n<li>Simple NP’s like proper nouns<ul>\n<li>λx.Franco(x)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Store and Retrieve  <ul>\n<li><img data-src=\"/img/NLP/ambiguity-of-same-POS.png\" alt=\"\"></li>\n<li>Retrieving the quantifiers one at a time and placing them in front</li>\n<li>The order determines the meaning <img data-src=\"/img/NLP/store.png\" alt=\"\"></li>\n<li>retrieve <img data-src=\"/img/NLP/retrieve.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Set-Based-Models\"><a href=\"#Set-Based-Models\" class=\"headerlink\" title=\"Set-Based Models\"></a>Set-Based Models</h3><ul>\n<li>domain: the set of elements</li>\n<li>entity: elements of domain</li>\n<li>Properties of the elements: sets of elements from the domain</li>\n<li>Relations: sets of tuples of elements from the domain</li>\n<li>FOL<ul>\n<li>FOL Terms → elements of the domain<ul>\n<li>Med -&gt; “f”</li>\n</ul>\n</li>\n<li>FOL atomic formula → sets, or sets of tuples<ul>\n<li>Noisy(Med) is true if “f is in the set of elements that corresponds to the noisy relation</li>\n<li>Near(Med, Rio) is true if “the tuple &lt;f,g&gt; is in the set of tuples that corresponds to “Near” in the interpretation</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Example: Everyone likes a noisy restaurant <img data-src=\"/img/NLP/set-based-model.png\" alt=\"\"><ul>\n<li>There is a particular restaurant out there; it’s a noisy place; everybody likes it 有一家吵雜的餐廳大家都喜歡</li>\n<li>Everybody has at least one noisy restaurant that they like 大家都喜歡一家吵雜的餐廳</li>\n<li>Everybody likes noisy restaurants (i.e., there is no noisy restaurant out there that is disliked by anyone) 大家都喜歡吵雜的餐廳</li>\n<li>Using predicates to create <strong>categories</strong> of concepts <ul>\n<li>people and restaurants</li>\n<li>basis for OWL (Web Ontology Language)網絡本體語言</li>\n</ul>\n</li>\n<li>before <img data-src=\"/img/NLP/uncategories.png\" alt=\"\"></li>\n<li>after <img data-src=\"/img/NLP/categories.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap13-Lexical-Semantics\"><a href=\"#Chap13-Lexical-Semantics\" class=\"headerlink\" title=\"Chap13 Lexical Semantics\"></a>Chap13 Lexical Semantics</h2><p>we didn’t do word meaning in compositional semantics</p>\n<p>WordNet  </p>\n<ul>\n<li>meaning and relationship about words<ul>\n<li>hypernym(上位詞)<ul>\n<li>breakfast → meal</li>\n</ul>\n</li>\n<li>hierarchies <img data-src=\"/img/NLP/wordnet-hierarchy.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>In our semantics examples, we used various FOL predicates to capture various aspects of events, including the notion of roles<br>Havers, takers, givers, servers, etc.</p>\n<p>Thematic roles(語義關係) <img data-src=\"/img/NLP/thematic-roles.png\" alt=\"\"></p>\n<ul>\n<li>semantic generalizations over the specific roles that occur with specific verbs<ul>\n<li>provide a shallow level of semantic analysis</li>\n<li>tied to syntactic analysis</li>\n</ul>\n</li>\n<li>i.e. Takers, givers, eaters, makers, doers, killers<ul>\n<li>They’re all the agents of the actions</li>\n</ul>\n</li>\n<li>AGENTS are often subjects</li>\n<li>In a VP-&gt;V NP rule, the NP is often a THEME</li>\n</ul>\n<p>2 major English resources using thematic data</p>\n<ul>\n<li>PropBank<ul>\n<li>Layered on the Penn TreeBank</li>\n<li>Small number (25ish) labels</li>\n</ul>\n</li>\n<li>FrameNet<ul>\n<li>Based on frame semantics</li>\n<li>Large number of frame-specific labels</li>\n</ul>\n</li>\n</ul>\n<p>Example  </p>\n<ul>\n<li>[McAdams and crew] covered [the floors] with [checked linoleum].格子花紋油毯<ul>\n<li>Arg0 (agent: the causer of the smearing)</li>\n<li>Arg1 (theme: “thing covered”)</li>\n<li>Arg2 (covering: “stuff being smeared”)</li>\n</ul>\n</li>\n<li>including agent and theme, remaining args are verb specific</li>\n</ul>\n<p>Logical Statements  </p>\n<ul>\n<li>Example: EAT – Eating(e) ^Agent(e,x)^ Theme(e,y)^Food(y)<ul>\n<li>(adding in all the right quantifiers and lambdas)</li>\n</ul>\n</li>\n<li>Use WordNet to encode the selection restrictions</li>\n<li>Unfortunately, language is creative<ul>\n<li>… ate glass on an empty stomach accompanied only by water and tea</li>\n<li>you <strong>can’t eat gold</strong> for lunch if you’re hungry</li>\n</ul>\n</li>\n</ul>\n<p>can we discover a verb’s restrictions by using a corpus and WordNet?    </p>\n<ol>\n<li>Parse sentences and find heads</li>\n<li>Label the thematic roles</li>\n<li>Collect statistics on the co-occurrence of particular headwords with particular thematic roles</li>\n<li>Use the WordNet hypernym structure to <strong>find the most meaningful level to use as a restriction</strong></li>\n</ol>\n<h3 id=\"WSD\"><a href=\"#WSD\" class=\"headerlink\" title=\"WSD\"></a>WSD</h3><p>Word sense disambiguation  </p>\n<ul>\n<li>select right sense for a word </li>\n<li>Semantic selection restrictions can be used to disambiguate<ul>\n<li>Ambiguous arguments to unambiguous predicates</li>\n<li>Ambiguous predicates with unambiguous arguments</li>\n</ul>\n</li>\n<li>Ambiguous arguments<ul>\n<li>Prepare a dish(菜餚)</li>\n<li>Wash a dish(盤子)</li>\n</ul>\n</li>\n<li>Ambiguous predicates<ul>\n<li>Serve (任職/服務) Denver</li>\n<li>Serve (供應) breakfast</li>\n</ul>\n</li>\n</ul>\n<p>Methodology   </p>\n<ul>\n<li>Supervised Disambiguation<ul>\n<li>based on a labeled training set</li>\n</ul>\n</li>\n<li>Dictionary-Based Disambiguation<ul>\n<li>based on lexical resource like dictionaries</li>\n</ul>\n</li>\n<li>Unsupervised Disambiguation<ul>\n<li>label training data is expensive </li>\n<li>based on unlabeled corpora</li>\n</ul>\n</li>\n<li>Upper(human) and Lower(simple model) Bounds</li>\n<li>Pseudoword<ul>\n<li>Generate artificial evaluation data for comparison and improvement of text processing algorithms</li>\n</ul>\n</li>\n</ul>\n<p>Supervised ML Approaches  </p>\n<ul>\n<li>What’s a tag?<ul>\n<li>In WordNet, “bass” in a text has 8 possible tags or labels (bass1 through bass8)</li>\n</ul>\n</li>\n<li>require very simple representation for training data<ul>\n<li>Vectors of sets of feature/value pairs</li>\n<li>need to extract training data by characterization of text surrounding the target</li>\n</ul>\n</li>\n<li>If you decide to use features that require more analysis (say parse trees) then the ML part may be doing less work (relatively) if these features are truly informative</li>\n<li>Classification<ul>\n<li>Naïve Bayes (the right thing to try first)</li>\n<li>Decision lists</li>\n<li>Decision trees</li>\n<li>MaxEnt</li>\n<li>Support vector machines</li>\n<li>Nearest neighbor methods…</li>\n<li>choice of technique depends on features that have been used</li>\n</ul>\n</li>\n<li>Bootstrapping<ul>\n<li>Use when don’t have enough data to train a system…</li>\n<li>集中有放回的均勻抽樣</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Naive-Bayes\"><a href=\"#Naive-Bayes\" class=\"headerlink\" title=\"Naive Bayes\"></a>Naive Bayes</h4><ul>\n<li>Argmax P(sense|feature vector) <img data-src=\"/img/NLP/bayesian-decision.png\" alt=\"\"> </li>\n<li>find maximum probabilty of words given possible sk <img data-src=\"/img/NLP/bayesian-decision2.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/bayesian-classifier.png\" alt=\"\"></li>\n<li>assumption<ul>\n<li>bag of words model<ul>\n<li>structure and order of words is ignored</li>\n<li>each pair of words in the bag is independent</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>73% correct</li>\n</ul>\n<h4 id=\"Dictionary-Based-Disambiguation\"><a href=\"#Dictionary-Based-Disambiguation\" class=\"headerlink\" title=\"Dictionary-Based Disambiguation\"></a>Dictionary-Based Disambiguation</h4><ol>\n<li>Disambiguation based on sense definitions</li>\n<li>Thesaurus-Based Disambiguation</li>\n<li>Disambiguation based on translations in a second-language corpus</li>\n</ol>\n<p>sense definition</p>\n<ul>\n<li>find keywords in definition of a word<ul>\n<li>cone<ul>\n<li>… pollen-bearing scales or bracts in <strong>trees</strong></li>\n<li>shape for holding <strong>ice cream</strong></li>\n</ul>\n</li>\n<li>50%~70% accuracies</li>\n<li>Alternatives<ul>\n<li>Several iterations to determine correct sense</li>\n<li>Combine the dictionary-based and thesaurus-based disambiguation</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU3JUI0JUEyJUU1JUJDJTk1JUU1JTg1JUI4\">Thesaurus-Based(索引典)<i class=\"fa fa-external-link-alt\"></i></span> Disambiguation    </p>\n<ul>\n<li>Category can determine which word senses are used</li>\n<li>Each word is assigned one or more subject codes which correspond to its different meanings<ul>\n<li>select the most often subject code</li>\n<li>考慮w的context，有多少words的senses與w相同</li>\n</ul>\n</li>\n<li>Walker’s Algorithm<ul>\n<li>50% accuracy for “interest, point, power, state, and terms”</li>\n</ul>\n</li>\n<li>Problems<ul>\n<li>general topic categorization, e.g., mouse in computer</li>\n<li>coverage, e.g., Navratilova</li>\n</ul>\n</li>\n<li>Yarowsky’s Algorithm <img data-src=\"/img/NLP/yarowsky-algo.png\" alt=\"\"> <img data-src=\"/img/NLP/yarowsky-algo2.png\" alt=\"\"> <img data-src=\"/img/NLP/yarowsky-algo3.png\" alt=\"\"><ul>\n<li><ol>\n<li>categorize sentences</li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li>categorize words</li>\n</ol>\n</li>\n<li><ol start=\"3\">\n<li>disambiguate by decision rule for Naïve Bayes</li>\n</ol>\n</li>\n<li>result <img data-src=\"/img/NLP/yarowsky-result.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>Disambiguation based on translations in a second-language corpus  </p>\n<ul>\n<li>the word “interest” has two translations in German<ul>\n<li>“Beteiligung” (legal share–50% a interest in the company)</li>\n<li>“Interesse” (attention, concern–her interest in Mathematics)</li>\n</ul>\n</li>\n<li>Example: … showed interest …<ul>\n<li>Look up English-German dictionary, show → zeigen</li>\n<li>Compute R(Interesse, zeigen) and R(Beteiligung, zeigen)</li>\n<li>R(Interesse, zeigen) &gt; R(Beteiligung, zeigen)</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Unsupervised-Disambiguation\"><a href=\"#Unsupervised-Disambiguation\" class=\"headerlink\" title=\"Unsupervised Disambiguation\"></a>Unsupervised Disambiguation</h4><p>P(vj|sk) are estimated using the EM algorithm  </p>\n<ol>\n<li>Random initialization of P(vj|sk)(word)</li>\n<li>For each context ci of w, compute P(ci|sk)(sentence)</li>\n<li>Use P(ci|sk) as training data</li>\n<li>Reestimate P(vj|sk)(word)</li>\n</ol>\n<p>Surface Representations(features)   </p>\n<ul>\n<li>Collocational<ul>\n<li>words that appear in specific positions to the right and left of the target word</li>\n<li>limited to the words themselves as well as part of speech</li>\n<li>Example: guitar and bassplayer stand<ul>\n<li>[guitar, NN, and, CJC, player, NN, stand, VVB]</li>\n<li>In other words, a vector consisting of [position n word, position n part-of-speech…]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Co-occurrence<ul>\n<li>words that occur regardless of position</li>\n<li>Typically limited to frequency counts</li>\n<li>Assume we’ve settled on a possible vocabulary of 12 words that includes guitarand playerbut not andand stand</li>\n<li>Example: guitar and bassplayer stand<ul>\n<li>Assume a 12-word sentence includes guitar and player but not “and” and stand</li>\n<li>[0,0,0,1,0,0,0,0,0,1,0,0]</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Applications  </p>\n<ul>\n<li>tagging<ul>\n<li>translation</li>\n<li>information retrieval</li>\n</ul>\n</li>\n</ul>\n<p>different label  </p>\n<ul>\n<li>Generic thematic roles (aka case roles)<ul>\n<li>Agent, instrument, source, goal, etc.</li>\n</ul>\n</li>\n<li>Propbank labels<ul>\n<li>Common set of labels ARG0-ARG4, ARGM</li>\n<li>specific to verb semantics</li>\n</ul>\n</li>\n<li>FrameNet frame elements<ul>\n<li>Conceptual and frame-specific </li>\n</ul>\n</li>\n<li>Example: [Ochocinco] bought [Burke] [a diamond ring]<ul>\n<li>generic: Agent, Goal, Theme</li>\n<li>propbank: ARG0, ARG2, ARG1</li>\n<li>framenet: Customer, Recipe, Goods</li>\n</ul>\n</li>\n</ul>\n<p>Semantic Role Labeling  </p>\n<ul>\n<li>automatically identify and label thematic roles<ul>\n<li>For each verb in a sentence<ul>\n<li>For each constituent<ul>\n<li>Decide if it is an argument to that verb</li>\n<li>if it is an argument, determine what kind</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>feature<ul>\n<li>from parse and lexical item</li>\n<li>“path” </li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Lexical-Acquisition\"><a href=\"#Lexical-Acquisition\" class=\"headerlink\" title=\"Lexical Acquisition\"></a>Lexical Acquisition</h3><ul>\n<li>Verb Subcategorization<ul>\n<li>the syntactic means by which verbs express their arguments</li>\n</ul>\n</li>\n<li>Attachment Ambiguity<ul>\n<li>The children ate the cake with their hands</li>\n<li>The children ate the cake with blue icing</li>\n</ul>\n</li>\n<li>SelectionalPreferences<ul>\n<li>The semantic categorization of a verb’s arguments</li>\n</ul>\n</li>\n<li>Semantic Similarity (refer to IR course)<ul>\n<li>Semantic similarity between words</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Verb-Subcategorization\"><a href=\"#Verb-Subcategorization\" class=\"headerlink\" title=\"Verb Subcategorization\"></a>Verb Subcategorization</h4><p>a particular set of syntactic categories that a verb can appear with is called a <strong>subcategorization frame</strong> <img data-src=\"/img/NLP/subcategorization.png\" alt=\"\"></p>\n<p>Brent’s subcategorization frame learner  </p>\n<ol>\n<li>Cues: Define a regular pattern of words and syntactic categories<ol>\n<li>ε: error rate of assigning frame f to verb v based on cue cj</li>\n</ol>\n</li>\n<li>Hypothesis Testing: Define null hypothesis H0: “the frame is not appropriate for the verb” <ol>\n<li>Reject this hypothesis if the cue cj indicates with high probability that our H0 is wrong</li>\n</ol>\n</li>\n</ol>\n<p>Example<br>Cues  </p>\n<ul>\n<li><p>regular pattern for subcategorization frame “NP NP”</p>\n<ul>\n<li>(OBJ | SUBJ_OBJ | CAP) (PUNC |CC)<br>Null hypothesis testing</li>\n</ul>\n</li>\n<li><p>Verb vi occurs a total of n times in the corpus and there are m &lt; n occurrences with a cue for frame fj</p>\n</li>\n<li><p>Reject the null hypothesis H0 that vi does not accept fj with the following probability of error <img data-src=\"/img/NLP/brent-null-hypothesis.png\" alt=\"\"></p>\n</li>\n<li><p>Brent’s system does well at precision, but not well at recall</p>\n</li>\n<li><p>Manning’s system</p>\n<ul>\n<li>solve this problem by using a tagger and running the cue detection on the output of the tagger</li>\n<li>learn a lot of subcategorization frames, even those it is low-reliability</li>\n<li>still low performance </li>\n<li>improve : use prior knowledge</li>\n</ul>\n</li>\n</ul>\n<p>PCFG prefers to parse common construction  </p>\n<ul>\n<li>P(A|prep, verb, np1, np2, w) ~= P(A|prep, verb, np1, np2)<ul>\n<li>Do not count the word outside of frame</li>\n<li>w: words outside of “verb np1(prep np2)”</li>\n<li>A: random variable representing attachment decision</li>\n<li>V(A): verb or np1</li>\n<li>Counter example<ul>\n<li>Fred saw a movie with Arnold Schwarzenegger</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>P(A|prep, verb, np1, np2, noun1, noun2) ~= P(A|prep, verb, noun1, noun2)<ul>\n<li>noun1 = head of np1, noun2 = head of np2</li>\n<li>total parameters: $10^{13}$ = #(prep) x #(verb) x #(noun) x #(noun) </li>\n</ul>\n</li>\n<li>P(A= noun | prep, verb, noun1) vs. P(A= verb | prep, verb, noun1)<ul>\n<li>compare probability to be verb and probability to be noun</li>\n</ul>\n</li>\n</ul>\n<p>Technique: Alternative to reduce parameters   </p>\n<ul>\n<li>Condition probabilities on fewer things</li>\n<li>Condition probabilities on more general things</li>\n</ul>\n<p>The model asks the following questions  </p>\n<ul>\n<li>VAp: Is there a PP headed by p and following the verb v which attaches to v(VAp=1) or not (VAp=0)?</li>\n<li>NAp: Is there a PP headed by p and following the noun n which attaches to n (NAp=1) or not (NAp=0)?</li>\n<li>(1) Determine the attachment of a PP that is immediately following an object noun, i.e. compute the probability of NAp=1</li>\n<li>In order for the first PP headed by the preposition p to attach to the verb, both VAp=1 and NAp=0<ul>\n<li>calculate likelihood ratio between V and N <img data-src=\"/img/NLP/likelihood-ratio-vn.png\" alt=\"\"></li>\n<li>maximum estimation<ul>\n<li>P(VA = 1 | v) = C(v, p) / C(v)</li>\n<li>P(NA = 1 | n) = C(n, p) / C(n)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Estimation of PP attachment counts<ul>\n<li>Sure Noun Attach<ul>\n<li>If a noun is followed by a PP but no preceding verb, increment C(prep attached to noun) </li>\n</ul>\n</li>\n<li>Sure Verb Attach<ul>\n<li>if a passive verb is followed by a PP other than a “by” phrase, increment C(prep attached to verb) </li>\n<li>if a PP follows both a noun phrase and a verb but the noun phrase is a pronoun, increment C(prep attached to verb)</li>\n</ul>\n</li>\n<li>Ambiguous Attach<ul>\n<li>if a PP follows both a noun and a verb, see if the probabilities based on the attachment decided by previous way</li>\n<li>otherwise increment both attachment counters by 0.5</li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/attach-example.png\" alt=\"\"></li>\n<li>Sparse data is a major cause of the difference between the human and program performance(attachment indeterminacy不確定性)</li>\n</ul>\n</li>\n</ul>\n<p>Using Semantic Information  </p>\n<ul>\n<li>condition on semantic tags of verb &amp; noun<ul>\n<li>Sue bought a plant with Jane(human)</li>\n<li>Sue bought a plant with yellow leaves(object)</li>\n</ul>\n</li>\n</ul>\n<p>Assumption<br>The noun phrase serves as the subject of the relative clause</p>\n<ul>\n<li>collect “ subject-verb” and “verb-object” pairs.(training part)  </li>\n<li>compute t-score (testing part) <ul>\n<li>t-score &gt; 0.10 (significant)</li>\n</ul>\n</li>\n</ul>\n<p>P (relative clause attaches to x | main verb of clause =v) &gt; P (relative clause attaches to y | main verb of clause=v)<br>↔ P (x= subject/object | v) &gt; P (y= subject/ object|v)</p>\n<p>Selectional Preferences  </p>\n<ul>\n<li>Most verbs prefer particular type of arguments<ul>\n<li>eat → object (food item)</li>\n<li>think → subject (people)</li>\n<li>bark → subject (dog)</li>\n</ul>\n</li>\n<li>Aspects of meaning of a word can be inferred<ul>\n<li>Susan had never eaten a fresh <strong>durian</strong> before (food item)</li>\n</ul>\n</li>\n<li>Selectional preferences can be used to rank different parses of a sentence</li>\n<li>Selectional preference strength<ul>\n<li>how strongly the verb constrains its direct object</li>\n<li><img data-src=\"/img/NLP/selection-strength.png\" alt=\"\"></li>\n<li>KL divergence between the prior distribution of direct objects of general verb and the distribution of direct objects of specific verb</li>\n<li>2 assumptions<ul>\n<li>only the head noun of the object is considered</li>\n<li>rather than dealing with individual nouns, we look at classes of nouns</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Selectional association<ul>\n<li>Selectional Association between a verb and a class is this class’s contribution to S(v) / the overall preference strength S(v) <img data-src=\"/img/NLP/selectional-association.png\" alt=\"\"></li>\n<li>There is also a rule for assigning association strengths to nouns instead of noun classes<ul>\n<li>If noun belongs to several classes, then its choose the highest association strength among all classes </li>\n</ul>\n</li>\n<li>estimating the probability that a direct object in noun class c occurs given a verb v<ul>\n<li>A(interrupt, chair) = max(A(interrupt, people), A(interrupt, furniture)) = A(interrupt, people)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Example <img data-src=\"/img/NLP/selectional-example.png\" alt=\"\"><ul>\n<li>eat prefers fooditem <ul>\n<li>A(eat, food)=1.08 → very specific</li>\n</ul>\n</li>\n<li>seehas a uniform distribution<ul>\n<li>A(see, people)=A(see, furniture)=A(see, food)=A(see, action)=0 → no selectional preference</li>\n</ul>\n</li>\n<li>find disprefers action item<ul>\n<li>A(find, action)=-0.13 → less specific</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Semantic Similarity  </p>\n<ul>\n<li>assessing semantic similarity between a new word and other already known words</li>\n<li>Vector Space vs Probabilistic</li>\n<li>Vector Space<ul>\n<li>Words can be expressed in different spaces: document space, word spaceand modifier space</li>\n<li>Similarity measures for binary vectors: matching coefficient, Dice coefficient, Jaccard(or Tanimoto) coefficient, Overlap coefficientand cosine</li>\n<li>Similarity measures for the real-valued vector space: cosine, Euclidean Distance, normalized correlation coefficient<ul>\n<li>cosine assumes a Euclidean space which is not well-motivated when dealing with word counts</li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/similarity-measure.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Probabilistic Measures<ul>\n<li>viewing word counts by representing them as probability distributions</li>\n<li>compare two probability distributions using<ul>\n<li>KL Divergence</li>\n<li>Information Radius(Irad)</li>\n<li>L1Norm</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap14-Computational-Discourse\"><a href=\"#Chap14-Computational-Discourse\" class=\"headerlink\" title=\"Chap14 Computational Discourse\"></a>Chap14 Computational Discourse</h2><table>\n<thead>\n<tr>\n<th>Level</th>\n<th>Well-formedness constraints</th>\n<th>Types of ambiguity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Lexical</td>\n<td>Rules of inflection and derivation</td>\n<td></td>\n</tr>\n<tr>\n<td>structural, morpheme boundaries, morpheme identity</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Syntactic</td>\n<td>Grammar rules</td>\n<td>structural, POS</td>\n</tr>\n<tr>\n<td>Semantic</td>\n<td>Selection restrictions</td>\n<td>word sense, quantifier scope</td>\n</tr>\n<tr>\n<td><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU4JUFGJUFEJUU3JTk0JUE4JUU1JUFEJUE2\">Pragmatic<i class=\"fa fa-external-link-alt\"></i></span></td>\n<td>conversation principles</td>\n<td>pragmatic function</td>\n</tr>\n</tbody></table>\n<p>Computational Discourse  </p>\n<ul>\n<li>Discourse(語篇)<ul>\n<li>A group of sentences with the same coherence relation</li>\n</ul>\n</li>\n<li>Coherence relation<ul>\n<li>the 2nd sentence offers the reader an explaination or cause for the 1st sentence</li>\n</ul>\n</li>\n<li>Entity-based Coherence<ul>\n<li>relationships with the entities, introducing them and following them in a focused way</li>\n<li>Discourse Segmentation<ul>\n<li>Divide a document into a linear sequence of multiparagraph passages</li>\n<li>Academic article<ul>\n<li>Abstract</li>\n<li>Introduction</li>\n<li>Methodology</li>\n<li>Results</li>\n<li>Conclusion</li>\n</ul>\n</li>\n<li><img data-src=\"http://www.wannabehacks.co.uk/images/Inverted_pyramid_in_comprehensive_form.jpg\" alt=\"Inverted Pyramid\"></li>\n<li>Applications<ul>\n<li>News</li>\n<li>Summarize different segments of a document</li>\n<li>Extract information from inside a single discourse segment</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>TextTiling (Hearst,1997)  </p>\n<ul>\n<li>Tokenization<ul>\n<li>Each space-delimited word in the input is converted to lower-case</li>\n<li>Words in a stop list of function words are thrown out</li>\n<li>The remaining words are morphologically stemmed</li>\n<li>The stemmed words are grouped into pseudo-sentencesof length w = 20</li>\n</ul>\n</li>\n<li>Lexical score determination<ul>\n<li>compute a lexical cohesion(結合) score between pseudo-sentences<ul>\n<li>score: average similarity of words in the pseudo-sentences before gap to pseudo-sentences after the gap(??)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Boundary identification    <ul>\n<li>Compute a depth score for each gap</li>\n<li>Boundaries are assigned at any valley which is deeper than a cutoff</li>\n</ul>\n</li>\n</ul>\n<p>Coherence Relations  </p>\n<ul>\n<li>Result<ul>\n<li>The Tin Woodman was caught in the rain. His joints rusted</li>\n</ul>\n</li>\n<li>Explanation<ul>\n<li>John hid Bill’s car keys. He was drunk</li>\n</ul>\n</li>\n<li>Parallel<ul>\n<li>The Scarecrow wanted some brains</li>\n<li>The Tin Woodman wanted a heart</li>\n</ul>\n</li>\n<li>Elaboration(詳細論述)<ul>\n<li>Dorothy was from Kansas</li>\n<li>She lived in the midst of the great Kansas prairies</li>\n</ul>\n</li>\n<li>Occasion(起因)<ul>\n<li>Dorothy picked up the oil-can</li>\n<li>She oiled the Tin Woodman’s joints</li>\n</ul>\n</li>\n</ul>\n<p>Coherence Relation Assignment  </p>\n<ul>\n<li>Discourse parsing</li>\n<li>Open problems</li>\n</ul>\n<p>Cue-Phrase-Based Algorithm  </p>\n<ul>\n<li><p>Using cue phrases</p>\n<ul>\n<li>Segment the text into discourse segments</li>\n<li>Classify the relationship between each consecutive discourse</li>\n</ul>\n</li>\n<li><p>Cue phrase</p>\n<ul>\n<li>connectives, which are often conjunctions or adverbs <ul>\n<li>because, although, but, for example, yet, with, and</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>discourse uses vs. sentential uses</p>\n<ul>\n<li><strong>With</strong> its distant orbit, Mars exhibits frigid weather conditions. (因為長距離的運行軌道，火星天氣酷寒)</li>\n<li>We can see Mars <strong>with</strong> an ordinary telescope</li>\n</ul>\n</li>\n<li><p><img data-src=\"/img/NLP/discourse-relation.png\" alt=\"\"></p>\n</li>\n<li><p>Temporal Relation  </p>\n<ul>\n<li>ordered in time (Asynchronous)<ul>\n<li>before, after …</li>\n</ul>\n</li>\n<li>overlapped (Synchronous)<ul>\n<li>at the same time</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Contingency Relation</p>\n<ul>\n<li>因果關係，附帶條件</li>\n</ul>\n</li>\n<li><p>Comparison Relation</p>\n<ul>\n<li>difference between two arguments</li>\n</ul>\n</li>\n<li><p>Expansion Relation</p>\n<ul>\n<li>expands the information for one argument in the other one or continues the narrative flow</li>\n</ul>\n</li>\n<li><p>Implicit Relation</p>\n<ul>\n<li>Discourse marker is absent</li>\n<li>颱風來襲，學校停止上課</li>\n</ul>\n</li>\n<li><p>Chinese Relation Words <img data-src=\"/img/NLP/chinese-coherence-relation.png\" alt=\"\"> </p>\n<ul>\n<li>Ambiguous Discourse Markers <ul>\n<li>而：而且, 然而, 因而</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Reference-Resolution\"><a href=\"#Reference-Resolution\" class=\"headerlink\" title=\"Reference Resolution\"></a>Reference Resolution</h3><p><img data-src=\"/img/NLP/reference-resolution.png\" alt=\"\">  </p>\n<ul>\n<li>Evoke<ul>\n<li>When a referent is first mentioned in a discourse, we say that a representation for it is <strong>evoked into</strong> the model</li>\n</ul>\n</li>\n<li>Access<ul>\n<li>Upon subsequent mention, this representation is <strong>accessed from</strong> the model</li>\n</ul>\n</li>\n</ul>\n<p>Five Types of Referring Expressions  </p>\n<ul>\n<li>Indefinite Noun Phrases(不定名詞)<ul>\n<li>marked with the determiner a, some, this …</li>\n<li>Create a new internal symbol and add to the current world model<ul>\n<li>Mayumi has bought a new automobile</li>\n<li>automobile(g123)</li>\n<li>new(g123)</li>\n<li>owns(mayumi, g123)</li>\n</ul>\n</li>\n<li>non-specific sense to describe an object<ul>\n<li>Mayumi wantsto buy a new XJE</li>\n</ul>\n</li>\n<li>whole classes of objects<ul>\n<li>A new automobiletypically requires repair twice in the first 12 months</li>\n</ul>\n</li>\n<li>collect one or more properties<ul>\n<li>The Macho GTE XL is a new automobile</li>\n</ul>\n</li>\n<li>Question and commands<ul>\n<li>Is her automobile in a parking placenear the exit?</li>\n<li>Put her automobile into a parking placenear the exit!</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Definite Noun Phrases(定名詞)<ul>\n<li>simple referential and generic uses(the same as indefinite)</li>\n<li>indicate an individual by description that they satisfy<ul>\n<li>The manufacturer <strong>of this automobile</strong> should be indicted</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Pronouns(代名詞)<ul>\n<li>reference backs to entities that have been introduced by previous nounphrases in a discourse</li>\n<li>non-referential noun phrase<ul>\n<li>non-exist object</li>\n</ul>\n</li>\n<li>logical variable<ul>\n<li>No male driveradmits that heis incompetent </li>\n</ul>\n</li>\n<li>something that is available from the context of utterance, but has not been explicitly mentioned before<ul>\n<li>Here they come, late again!</li>\n<li>Can’t easily know who are “they”</li>\n</ul>\n</li>\n<li>Anaphora<ul>\n<li>Number Agreements<ul>\n<li>John has a Ford Falcon. It is red</li>\n<li>John has three Ford Falcons. They are red</li>\n</ul>\n</li>\n<li>Person Agreement(人稱)</li>\n<li>Gender Agreement</li>\n<li>Selection Restrictions<ul>\n<li>verb and its arguments</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Demonstratives (指示詞)<ul>\n<li>this, that</li>\n</ul>\n</li>\n<li>Names<ul>\n<li>Full name &gt; long definite description &gt; short definite description &gt; last name&gt; first name &gt; distal demonstrative &gt; proximate demonstrative &gt; NP &gt; stressed pronoun &gt; unstressed pronoun</li>\n</ul>\n</li>\n</ul>\n<p>Information Status  </p>\n<ul>\n<li>Referential forms used to provide new or old information</li>\n<li>givenness hierarchy <img data-src=\"/img/NLP/givenness-hierarchy.png\" alt=\"\"></li>\n<li>Definite-indefinite is a clue to given-new status<ul>\n<li>The sales managere(given) employed a foreign distributor(new)</li>\n</ul>\n</li>\n<li>If there are ambiguous noun phrases in a sentence, then it extracts the presuppositions to provide extra constraints</li>\n<li>When some new information is added to knowledge base, check if it is consistent with what we already know</li>\n</ul>\n<p>Active model of understanding  </p>\n<ul>\n<li>Given a text, build up predictions or expectations about new information and actively compare these with successive input to resolve ambiguities</li>\n<li>Construct a proof of the information provided in a sentence from the existing world knowledge and plausible inference rules illustrated</li>\n<li>the inference are not sensitive to the order<ul>\n<li>if the proposition that the disc is heavy is inferred, then it is not changed after the discourse has finished</li>\n<li>Solution: describe the propositions in temporal order</li>\n</ul>\n</li>\n<li>Script: encapsulate a sequence of actions that belong together into a script<figure class=\"highlight dns\"><table><tr><td class=\"code\"><pre><span class=\"line\">automobile_buying:</span><br><span class=\"line\">&lt;&#123;customer(C), automobile(<span class=\"keyword\">A</span>), dealer(D), garage(G)&#125;,</span><br><span class=\"line\">\t&lt;</span><br><span class=\"line\">\t\tgoes(C, G),</span><br><span class=\"line\">\t\ttest_drives(C, <span class=\"keyword\">A</span>),</span><br><span class=\"line\">\t\torders(C, <span class=\"keyword\">A</span>, D),</span><br><span class=\"line\">\t\tdelivers(D, <span class=\"keyword\">A</span>, C),</span><br><span class=\"line\">\t\tdrives(C, <span class=\"keyword\">A</span>)</span><br><span class=\"line\">\t&gt;</span><br><span class=\"line\">&gt;</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<h2 id=\"參考資料\"><a href=\"#參考資料\" class=\"headerlink\" title=\"參考資料\"></a>參考資料</h2><ul>\n<li>HHChen 課堂講義</li>\n</ul>\n",
            "tags": [
                "機器學習",
                "自然語言處理",
                "統計"
            ]
        },
        {
            "id": "http://gitqwerty777.github.io/natural-language-processing/",
            "url": "http://gitqwerty777.github.io/natural-language-processing/",
            "title": "自然語言處理(上)",
            "date_published": "2015-03-07T03:00:47.000Z",
            "content_html": "<h2 id=\"Chap01-Introduction\"><a href=\"#Chap01-Introduction\" class=\"headerlink\" title=\"Chap01 Introduction\"></a>Chap01 Introduction</h2><h3 id=\"Applications-of-NLP\"><a href=\"#Applications-of-NLP\" class=\"headerlink\" title=\"Applications of NLP\"></a>Applications of NLP</h3><ul>\n<li>Machine translation<ul>\n<li>google translate</li>\n</ul>\n</li>\n<li>Speech recognition<ul>\n<li>Siri</li>\n</ul>\n</li>\n<li>Smart input method<ul>\n<li>ㄐㄅㄈㄏ → 加倍奉還</li>\n</ul>\n</li>\n<li>Sentiment(情感) analysis</li>\n<li>Information retrieval</li>\n<li>Question Anwering<ul>\n<li>Turing Test</li>\n</ul>\n</li>\n<li>Optical character recognition (OCR)<a id=\"more\"></a>\n\n</li>\n</ul>\n<h3 id=\"Critical-Problems-in-NLP\"><a href=\"#Critical-Problems-in-NLP\" class=\"headerlink\" title=\"Critical Problems in NLP\"></a>Critical Problems in NLP</h3><ul>\n<li>Ambiguity(不明確性)<ul>\n<li><strong>The most important thing in NLP</strong></li>\n<li>Lexical(字辭)<ul>\n<li><code>current</code>: noun or adjective</li>\n<li><code>bank</code> (noun): money or river</li>\n</ul>\n</li>\n<li>Syntactic(語法)<ul>\n<li><code>[saw [the boy] [in the park]]</code></li>\n<li><code>[saw [the boy in the park]]</code></li>\n</ul>\n</li>\n<li>Semantic(語義)<ul>\n<li>“John kissed his wife, and so did Sam”. (Sam kissed John’s wife or his own?)</li>\n<li>agent(施事) vs. patient(受事)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>ill-form(bad form)<ul>\n<li>typo</li>\n<li>grammatical errors</li>\n</ul>\n</li>\n<li>Robustness<ul>\n<li>various domain</li>\n<li>網路語言：取材於方言俗語、各門外語、縮略語、諧音、甚至以符號合併以達至象形效果等等<ul>\n<li>emoticon(表情符號)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Main-Topics-in-Large-Scale-NLP-Design\"><a href=\"#Main-Topics-in-Large-Scale-NLP-Design\" class=\"headerlink\" title=\"Main Topics in Large-Scale NLP Design\"></a>Main Topics in Large-Scale NLP Design</h3><ul>\n<li>Knowledge representation<ul>\n<li>organize and describe linguistic knowledge</li>\n</ul>\n</li>\n<li>Knowledge strategies<ul>\n<li>use knowledge for efficient parsing, ambiguity resolution, ill-formed recovery</li>\n</ul>\n</li>\n<li>Knowledge acquisition<ul>\n<li>setup and maintain knowledge base systematically and cost-effectively</li>\n</ul>\n</li>\n<li>Knowledge integration<ul>\n<li>consider various knowledge sources effectively</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Models\"><a href=\"#Models\" class=\"headerlink\" title=\"Models\"></a>Models</h3><p>用演算法來轉換文字結構，以產生最後結果   </p>\n<ul>\n<li>State machines</li>\n<li>Rule-based approaches</li>\n<li>Logical formalisms</li>\n<li>Probabilistic models</li>\n</ul>\n<h3 id=\"Approaches\"><a href=\"#Approaches\" class=\"headerlink\" title=\"Approaches\"></a>Approaches</h3><p>NLP start from 1960, <strong>statictics method</strong> wins after 1995</p>\n<p>Rule-based approach</p>\n<ul>\n<li>Advantages<ul>\n<li>No need database</li>\n<li>Easy to incorporate with knowledge</li>\n<li>Better generalization to a unseen domain</li>\n<li>Explainable and traceable<ul>\n<li>easy to understand</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Disadvantages<ul>\n<li>Hard to maintain consistency (at different situation)</li>\n<li>Hard to handle uncertain knowledge (define uncertainty factor)<ul>\n<li>irregular information</li>\n</ul>\n</li>\n<li>Not easy to avoid redundancy</li>\n<li>Knowledge acquisition is time consuming</li>\n</ul>\n</li>\n</ul>\n<p>Corpus(語料庫)-based approach  </p>\n<ul>\n<li>Advantages<ul>\n<li>Knowledge acquisition can be automatically achieved by the computer</li>\n<li>Uncertain knowledge can be objectively quantified(知識可被量化)</li>\n<li><strong>Consistency and completeness</strong> are easy to obtain</li>\n<li>Well established statistical theories and technique are available</li>\n</ul>\n</li>\n<li>Disadvantages<ul>\n<li>Generalization is poor for small-size database</li>\n<li>Unable to reasoning</li>\n<li>Hard to identify the effect of each parameter</li>\n<li>Build database is time consuming</li>\n</ul>\n</li>\n<li>Corpus<ul>\n<li>Brown Corpus (1M words),Birmingham Corpus (7.5M words), LOB Corpus (1M words), etc</li>\n<li>Corpora(語料庫(複數)) of special domains or style<ul>\n<li>Newspaper, Bible, etc</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Information in Corpora<ul>\n<li>pure-text corpus<ul>\n<li>language usage of real world, word distribution, co-occurrence</li>\n</ul>\n</li>\n<li>tagged corpus<ul>\n<li>parts of speech, structures, features</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Hybrid approach   </p>\n<ul>\n<li>Use rule-based approach when <ul>\n<li>there are rules that have good coverage<ul>\n<li>it can be governed by a small number of rules</li>\n</ul>\n</li>\n<li>extensional knowledge is important to the system</li>\n</ul>\n</li>\n<li>Use corpus-based approach when<ul>\n<li>Knowledge needed to solve the problem is huge and intricate</li>\n<li>A good model or formulation exists</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h3><p><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5ubHRrLm9yZy8=\">Natural Language Toolkit(NLTK)<i class=\"fa fa-external-link-alt\"></i></span>: Open source Python modules, linguistic data and documentation for research and development in natural language processing</p>\n<p>features  </p>\n<ul>\n<li>Corpus readers</li>\n<li>Tokenizers<ul>\n<li>whitespace, newline, blankline, word, treebank, sexpr, regexp, Punkt sentence segmenter</li>\n</ul>\n</li>\n<li>Stemmers<ul>\n<li>Porter, Lancaster, regexp</li>\n</ul>\n</li>\n<li>Taggers<ul>\n<li>regexp, n-gram, backoff, Brill, HMM, TnT</li>\n</ul>\n</li>\n<li>Chunkers<ul>\n<li>regexp, n-gram, named-entity</li>\n</ul>\n</li>\n<li>Metrics<ul>\n<li>accuracy, precision, recall, windowdiff, distance metrics, inter-annotator agreement coefficients, word association measures, rank correlation</li>\n</ul>\n</li>\n<li>Estimation<ul>\n<li>uniform, maximum likelihood, Lidstone, Laplace, expected likelihood, heldout, cross-validation, Good-Turing, Witten-Bell</li>\n</ul>\n</li>\n<li>Miscellaneous<ul>\n<li>unification, chatbots, many utilities</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap02-Overall-Pictures\"><a href=\"#Chap02-Overall-Pictures\" class=\"headerlink\" title=\"Chap02 Overall Pictures\"></a>Chap02 Overall Pictures</h2><p><img data-src=\"/img/NLP/overview.png\" alt=\"overview\"></p>\n<p>Knowledge Categories     </p>\n<ul>\n<li>Phonology(聲音，資料來源)</li>\n<li>Morphology(詞性)</li>\n<li>Syntax(句構)</li>\n<li>Semantics(語義)</li>\n<li>Pragmatics(句子關聯，語用學)</li>\n<li>Discourse(篇章分析，話語)</li>\n</ul>\n<h3 id=\"Morphology-Structure-of-words\"><a href=\"#Morphology-Structure-of-words\" class=\"headerlink\" title=\"Morphology(Structure of words)\"></a>Morphology(Structure of words)</h3><ul>\n<li>part-of-speech(POS) tagging(詞性標註, lexical category)</li>\n<li>find the roots of words<ul>\n<li>e.g., going → go, cats → cat</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Syntax-structure-of-sentences\"><a href=\"#Syntax-structure-of-sentences\" class=\"headerlink\" title=\"Syntax(structure of sentences)\"></a>Syntax(structure of sentences)</h3><ul>\n<li>Context-Free Grammars(CFG) <img data-src=\"/img/NLP/cfg.png\" alt=\"parse tree\"></li>\n<li>Chomsky Normal Form(CNF)<ul>\n<li>can only use following two rules <ol>\n<li><code>non-terminal → terminal</code></li>\n<li><code>non-terminal → non-terminal non-terminal</code></li>\n</ol>\n</li>\n</ul>\n</li>\n<li>dependency<ul>\n<li>local dependency<ul>\n<li>words near together would probably have the same syntax rule</li>\n</ul>\n</li>\n<li>long-distance dependency <ul>\n<li>wh-movement(疑問詞移位)<ul>\n<li>What did Jennifer buy? → 什麼 (助動詞) 珍妮佛 買了</li>\n</ul>\n</li>\n<li>分裂句 Right-node raising<ul>\n<li>[[she would have bought] and [he might sell]] shares</li>\n</ul>\n</li>\n<li>Argument-cluster coordination<ul>\n<li>I give [[you an apple] and [him a pear]]</li>\n</ul>\n</li>\n<li><strong>challenge for some statistical NLP approaches (like n-grams)</strong></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Semantics-meaning-of-individual-sentences\"><a href=\"#Semantics-meaning-of-individual-sentences\" class=\"headerlink\" title=\"Semantics(meaning of individual sentences)\"></a>Semantics(meaning of individual sentences)</h3><ul>\n<li>semantic roles  <ul>\n<li>agent(主詞)</li>\n<li>patient(受詞)</li>\n<li>instrument(工具)</li>\n<li>goal(目標)</li>\n<li>Beneficiary(受益)</li>\n<li>He threw the book(patient) at me(goal)</li>\n<li>John sold the car for a friend(beneficiary)</li>\n</ul>\n</li>\n<li>Subcategorizations(次分類)<ul>\n<li>及物、不及物動詞</li>\n</ul>\n</li>\n<li>Semantics can be divided into two parts<ul>\n<li>Lexical Semantics<ul>\n<li>上下位，同義(反義)，部分-整體</li>\n</ul>\n</li>\n<li>Composition Semantics<ul>\n<li>合起來的意義與單一字意義不同</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Implementation<ul>\n<li>WordNet®(large lexical database of English)</li>\n<li>Thesaurus(索引典)</li>\n<li>同義詞詞林</li>\n<li>廣義知網中文詞知識庫(E-HowNet)</li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9mcmFtZW5ldC5pY3NpLmJlcmtlbGV5LmVkdS9mbmRydXBhbC9hYm91dA==\">FrameNet<i class=\"fa fa-external-link-alt\"></i></span></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"FrameNet\"><a href=\"#FrameNet\" class=\"headerlink\" title=\"FrameNet\"></a>FrameNet</h4><p>A dictionary of more than 10,000 word senses, 170,000 manually annotated sentences</p>\n<p>Frame Semantics(Charles J. Fillmore)  </p>\n<ul>\n<li>the meanings of most words can be more understood by semantic frame</li>\n<li>Including description of a type of event, relation, or entity and the participants in it</li>\n<li>Example: <code>apply_heat</code> frame<ul>\n<li>When one of these words appear, this frame will be applied<ul>\n<li><code>Fry(炸)</code>, <code>bake(烘)</code>, <code>boil(煮)</code>, a`nd broil(烤)</li>\n</ul>\n</li>\n<li>Frame elements: Cook, Food, Heating_instrument and Container<ul>\n<li>a person doing the cooking (Cook)</li>\n<li>the food that is to be cooked (Food)</li>\n<li>something to hold the food while cooking (Container)</li>\n<li>a source of heat (Heating_instrument)</li>\n</ul>\n</li>\n<li>[<code>Cook</code> the boys] … GRILL [<code>Food</code> fish] [<code>Heating_instrument</code> on an open fire]</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Pragmatics-how-sentences-relate-to-each-other\"><a href=\"#Pragmatics-how-sentences-relate-to-each-other\" class=\"headerlink\" title=\"Pragmatics(how sentences relate to each other)\"></a>Pragmatics(how sentences relate to each other)</h3><ul>\n<li>explain what the speaker really expressed</li>\n<li>Understand the scope of <ul>\n<li>quantifiers</li>\n<li>speech acts</li>\n<li>discourse analysis</li>\n<li>anaphoric relations(首語重複)</li>\n</ul>\n</li>\n<li>Anaphora(首語重複) and Coreference(指代)<ul>\n<li>張三是老師,他教學很認真,同時,他也是一個好爸爸。</li>\n<li>Type/Instance: “老師”/“張三”, “一個好爸爸”/“張三”</li>\n</ul>\n</li>\n<li>crucial to <strong>information extraction</strong></li>\n<li>Dialogue Tagging <img data-src=\"/img/NLP/dialogue_tag.png\" alt=\"\"></li>\n</ul>\n<h3 id=\"Discourse-Analysis\"><a href=\"#Discourse-Analysis\" class=\"headerlink\" title=\"Discourse Analysis\"></a>Discourse Analysis</h3><p>Example  </p>\n<ul>\n<li>1a: 佛羅倫斯哪個博物館在1993年的爆炸事件中受到破壞？</li>\n<li>1b: 這個事件哪一天發生？<ul>\n<li>問句1b「這個事件」，指的是問句1a「1993年的爆炸事件」</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>From <span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5paXMuc2luaWNhLmVkdS50dy9wYWdlL2V2ZW50cy9GSUxFLzEyMDMxMzEwMTA3U2xpZGVzLnBkZg==\">The Three (and a Half) Futures of NLP<i class=\"fa fa-external-link-alt\"></i></span></p>\n<ul>\n<li>NLP is <strong>Notation Transformation</strong>(e.g. English → Chinese), with some information(POS, syntatic, senmatic…) added</li>\n<li>Much NLP is engineering<ul>\n<li>select and tuning learning performance</li>\n</ul>\n</li>\n<li>Knowledge is crucial in language-related research areas, but providing a large scaleknowledge base is difficult and costly<ul>\n<li>Knowledge Base<ul>\n<li>WordNet</li>\n<li>FrameNet</li>\n<li>Wikipedia</li>\n<li>Dbpedia</li>\n<li>Freebase</li>\n<li>Siri</li>\n<li>Google Knowledge Graph</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Hierarchy of transformations(由深至淺)<ul>\n<li>pragmatics, writing style<ul>\n<li>deeper semantics, discourse<ul>\n<li>shallow semantics, co-reference<ul>\n<li>syntax, POS(part-of-speech)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>分析時由淺至深</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Analysis\"><a href=\"#Analysis\" class=\"headerlink\" title=\"Analysis\"></a>Analysis</h4><p><img data-src=\"/img/NLP/layer.png\" alt=\"Layer\"><br><img data-src=\"/img/NLP/l1.png\" alt=\"L1\"><br><img data-src=\"/img/NLP/l2.png\" alt=\"L2\"><br><img data-src=\"/img/NLP/l3.png\" alt=\"L3\"><br><img data-src=\"/img/NLP/l4.png\" alt=\"L4\"></p>\n<h4 id=\"NLP-progress-by-now\"><a href=\"#NLP-progress-by-now\" class=\"headerlink\" title=\"NLP progress by now\"></a>NLP progress by now</h4><p><img data-src=\"/img/NLP/sub.png\" alt=\"NLP subclass\"><br><img data-src=\"/img/NLP/dowell.png\" alt=\"NLP do today\"><br><img data-src=\"/img/NLP/cantdo.png\" alt=\"NLP can&#39;t do today\">  </p>\n<h2 id=\"Chap03-Collocations-搭配詞\"><a href=\"#Chap03-Collocations-搭配詞\" class=\"headerlink\" title=\"Chap03 Collocations(搭配詞)\"></a>Chap03 Collocations(搭配詞)</h2><ul>\n<li>多個單字組合成一個有意義的語詞，其意義無法從各個單字中推得<ul>\n<li>e.g. black market</li>\n</ul>\n</li>\n<li>Subclasses of Collocations<ul>\n<li>compound nouns<ul>\n<li>telephone box and post office</li>\n</ul>\n</li>\n<li>idioms<ul>\n<li>kick the bucket(氣絕)  </li>\n</ul>\n</li>\n<li>Light verbs(輕動詞)<ul>\n<li>動詞失去其意義，需要和其他有實質意義的詞作搭配</li>\n<li>e.g. The man took a walk(walk, not take) vs The man took a radio(take)</li>\n</ul>\n</li>\n<li>Verb particle constructions(語助詞) or Phrasal Verbs(詞組動詞, 短語動詞, V + 介系詞)<ul>\n<li>take in = deceive, look sth. up</li>\n</ul>\n</li>\n<li>proper names<ul>\n<li>San Francisco</li>\n</ul>\n</li>\n<li>Terminology(專有名詞)</li>\n</ul>\n</li>\n<li>Classification<ul>\n<li>Fixed expressions<ul>\n<li>in short (O)</li>\n<li>in shorter or in very short(X)</li>\n</ul>\n</li>\n<li>Semi-fixed expressions(可用變化形)<ul>\n<li>non-decomposable idioms<ul>\n<li>kick the bucket (O)</li>\n<li>he kicks the bucket(O)</li>\n<li>the bucket was kicked (X)</li>\n</ul>\n</li>\n<li>compound nominals<ul>\n<li>car park, car parks</li>\n</ul>\n</li>\n<li>Proper names</li>\n</ul>\n</li>\n<li>Syntactically-Flexible Expressions<ul>\n<li>decomposable idioms<ul>\n<li>let the cat out of the bag</li>\n</ul>\n</li>\n<li>verb-particle constructions</li>\n<li>light verbs</li>\n</ul>\n</li>\n<li>Institutionalized Phrases (習慣用法)<ul>\n<li>salt and pepper(○) pepper and salt(×)</li>\n<li>traffic light</li>\n<li>kindle excitement(點燃激情)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Collocation-detection\"><a href=\"#Collocation-detection\" class=\"headerlink\" title=\"Collocation detection\"></a>Collocation detection</h3><ul>\n<li>by Frequency</li>\n<li>by Mean and Variance of the distance between focal word (焦點詞) and collocating word(搭配詞)</li>\n<li>Hypothesis Testing</li>\n<li>Mutual Information</li>\n</ul>\n<h4 id=\"By-Frequency\"><a href=\"#By-Frequency\" class=\"headerlink\" title=\"By Frequency\"></a>By Frequency</h4><ul>\n<li>找出現機率大的bigrams<ul>\n<li>not always significant</li>\n<li>篩選可能為組合詞的詞性組合(Ex. adj+N) </li>\n</ul>\n</li>\n<li>The collocations found <img data-src=\"/img/NLP/freandtag.png\" alt=\"\"></li>\n<li>What if a word have two possible collocations?(strong force, powerful force) <img data-src=\"/img/NLP/frecomp.png\" alt=\"\"></li>\n</ul>\n<h4 id=\"By-Mean-and-Variance-of-the-distance-between-focal-word-焦點詞-and-collocating-word-搭配詞\"><a href=\"#By-Mean-and-Variance-of-the-distance-between-focal-word-焦點詞-and-collocating-word-搭配詞\" class=\"headerlink\" title=\"By Mean and Variance of the distance between focal word(焦點詞) and collocating word(搭配詞)\"></a>By Mean and Variance of the distance between focal word(焦點詞) and collocating word(搭配詞)</h4><ul>\n<li>many collocations consist of more flexible relationships<ul>\n<li>frequency is not suitable</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li>Define a collocational window<ol>\n<li>e.g., 3-4 words before/after</li>\n</ol>\n</li>\n<li>assemble every word pair as a bigram<ol>\n<li>e.g., A B C D → AB, AC, AD, BC, BD, CD</li>\n</ol>\n</li>\n<li>computes the mean and variance of the offset between the two words<ol>\n<li>變異數愈低，代表兩個字之間的位置關聯愈固定 <img data-src=\"/img/NLP/meanvar.png\" alt=\"\"></li>\n</ol>\n</li>\n</ol>\n<ul>\n<li>z-score $z = {freq - \\mu \\over \\sigma}$: the strength of a word pair</li>\n</ul>\n<h4 id=\"Hypothesis-Testing-假設檢定\"><a href=\"#Hypothesis-Testing-假設檢定\" class=\"headerlink\" title=\"Hypothesis Testing(假設檢定)\"></a>Hypothesis Testing(假設檢定)</h4><ul>\n<li>Even high frequency and low variance can be accidental</li>\n<li>null hypothesis(虛無假設, H0) <ul>\n<li>設 w1 and w2 is completely independent → w1 and w2 不是搭配詞<ul>\n<li>P(w1w2) = P(w1)P(w2) </li>\n</ul>\n</li>\n</ul>\n</li>\n<li>假設H0為真，計算這兩個字符合H0的機率P<ul>\n<li>若P太低則否決H0(→ 是搭配詞)</li>\n</ul>\n</li>\n<li>Two issues<ul>\n<li>Look for particular patterns in the data</li>\n<li>How much data we have seen</li>\n</ul>\n</li>\n<li>種類包括：t檢驗，Z檢驗，卡方檢驗，F檢驗      </li>\n</ul>\n<h5 id=\"t-test\"><a href=\"#t-test\" class=\"headerlink\" title=\"t-test\"></a>t-test</h5><ul>\n<li>Test whether <strong>distributions of two groups</strong> are <strong>statistically different</strong> or not<ul>\n<li>H0 → (w1, w2) has no differnece with normal distribution</li>\n<li>considering <strong>variance</strong> of the data <img data-src=\"/img/NLP/ttest.png\" alt=\"\"></li>\n<li>formula <img data-src=\"/img/NLP/ttest2.png\" alt=\"\"> <img data-src=\"/img/NLP/ttest3.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Calculate t by alpha level and degree of freedom<ul>\n<li>alpha level <code>α</code>: confidence<ul>\n<li>in normal distribution，α = 95%落在mean±1.96std之間, α = 99%落在mean±2.576std之間</li>\n<li>If t-value is larger than 2.576, we say the two groups <strong>are different</strong> with 99% confidence</li>\n</ul>\n</li>\n<li>degree of freedom: number of sample-1<ul>\n<li>total = number of two groups-2</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>t↑ → more difference → more possible to reject null hypothesis → more possible to be collocation</strong></li>\n<li>Example: “new” occurs 15,828 times, “companies” 4,675 times, “new companies” 8 times, total 14,307,668 tokens<ul>\n<li><strong>Null hypothesis: the occurrences of new and companies are independent(not collocation)</strong></li>\n<li>H0 mean = P(new, companies) = P(new) x P(companies) = $\\frac{15828 \\times 4678}{14307668^2} = 3.615 \\times 10^{-7}$</li>\n<li>H0 var = p(1-p) ~= p when p is small</li>\n<li>tvalue = $\\frac{5.591 \\times 10^7 - 3.615\\times 10^7}{\\sqrt{\\frac{5.591 \\times 10^7}{14307668}}} = 0.999932$</li>\n<li>0.999932 &lt; 2.576, we cannot reject the null hypothesis<ul>\n<li>new company are not collocation</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>the above words are possible collocations <img data-src=\"/img/NLP/ttest4.png\" alt=\"\"></li>\n</ul>\n<!--???????-->\n<p>Hypothesis testing of differences    </p>\n<ul>\n<li>useful for lexicography <ul>\n<li>which word(strong, powerful) is suitable to modify “computer”?</li>\n</ul>\n</li>\n<li>T-test can be used for <strong>comparison of the means of two normal populations</strong> <ul>\n<li>H0 is that the average difference is 0 (u = 0)</li>\n<li>v1 and v2 are the words we are comparing (e.g., powerful and strong), and w is the collocate of interest(e.g., computers)</li>\n<li><img data-src=\"/img/NLP/ttest5.png\" alt=\"\">)<img data-src=\"/img/NLP/ttest6.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<h5 id=\"Chi-Square-test\"><a href=\"#Chi-Square-test\" class=\"headerlink\" title=\"Chi-Square test\"></a>Chi-Square test</h5><ul>\n<li>T-test assumes that probabilities are normally distributed<ul>\n<li>not really</li>\n</ul>\n</li>\n<li>Chi-Square: compare <strong>observed frequencies</strong> with <strong>expected frequencies</strong><ul>\n<li>If <strong>difference between observed and expected frequencies</strong> is large, we can reject H0</li>\n</ul>\n</li>\n<li>Example<ul>\n<li>expected frequency of “new companies”: $\\frac{8+4667}{14307668} \\times \\frac{8+15820}{14307668} \\times 14307668$ = 5.2 <img data-src=\"/img/NLP/chi1.png\" alt=\"\"></li>\n<li>chi-square value = χ^2 <img data-src=\"/img/NLP/chi3.png\" alt=\"\"> <img data-src=\"/img/NLP/chi2.png\" alt=\"\"></li>\n<li>When α=0.05, χ^2=3.841</li>\n<li>Because 1.55&lt;3.841, we cannot reject the null hypothesis. new companies is not a good candidate for a collocation</li>\n</ul>\n</li>\n<li>Comparison with T-test<ul>\n<li>The 20 bigrams with the highest t scores in the test corpus are also the 20 bigrams with the highest χ^2 scores</li>\n<li>χ^2 is appropriate for large probabilities(t-test is not because of normality assumption)</li>\n</ul>\n</li>\n<li>Application: Translation<ul>\n<li>find similarity of word pairs</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Likelihood-Ratios\"><a href=\"#Likelihood-Ratios\" class=\"headerlink\" title=\"Likelihood Ratios\"></a>Likelihood Ratios</h3><ul>\n<li>Advantage compared with Chi-Square test  <ul>\n<li>more appropriate for sparse data</li>\n<li>easier to interpret</li>\n</ul>\n</li>\n</ul>\n<p>Likelihood Ratios within single corpus  </p>\n<ul>\n<li>examine two hypothesis<ul>\n<li>H1: occurrence of w2 is independent of the previous occurrence of w1(null hypothesis)</li>\n<li>H2: occurrence of w2 is dependent of the previous occurrence of w1 </li>\n</ul>\n</li>\n<li>maximum likelihood estimate <img data-src=\"/img/NLP/like1.png\" alt=\"\"></li>\n<li>using binomial distribution<ul>\n<li>$b(k;n, x) = \\binom nk x^k \\times (1-x)^{n-k}$</li>\n<li>only different at probability<ul>\n<li>$L(H_1) = b(c_{12};c_1, p)b(c_2-c_{12}; N-c_1, p)$</li>\n<li>$L(H_2) = b(c_{12};c_1, p_1)b(c_2-c_{12}; N-c_1, p_2)$</li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/likew.png\" alt=\"likely probability\"></li>\n<li>log of likelihood ratio λ <img data-src=\"/img/NLP/like2.png\" alt=\"log likelihood ratio\"></li>\n<li>use D = -2logλ to examine the significance of two words, which can asymptotically(漸近) chi-square distributed</li>\n</ul>\n</li>\n</ul>\n<p>Likelihood Ratios between two or more corpora   </p>\n<ul>\n<li>useful for the discovery of <strong>subject-specific collocations</strong></li>\n</ul>\n<h3 id=\"Mutual-Information\"><a href=\"#Mutual-Information\" class=\"headerlink\" title=\"Mutual Information\"></a>Mutual Information</h3><ul>\n<li>measure of <strong>how much one word tells us about the other</strong>(information theory)   </li>\n<li>pointwise mutual information(PMI) <img data-src=\"/img/NLP/mutual.png\" alt=\"PMI formula\"><ul>\n<li>MI是在獲得一個隨機變數的資訊之後，觀察另一個隨機變數所獲得的「資訊量」（單位通常為位元）</li>\n</ul>\n</li>\n<li>mutual information = Expection(PMI) <img data-src=\"/img/NLP/newMI.png\" alt=\"\"></li>\n<li>works bad in sparse environments<ul>\n<li>As the perfectly dependent bigrams get rarer, their mutual information increases → <strong>bad measure of dependence</strong> <img data-src=\"/img/NLP/pmi-depend.png\" alt=\"\"></li>\n</ul>\n</li>\n<li><strong>good measure of independence</strong><ul>\n<li>when perfect independence, I(x, y) = 0</li>\n<li><img data-src=\"/img/NLP/pmi-independ.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>New formula: $C(w1w2)I(w1w2)$<ul>\n<li>With MI, bigrams composed of low-frequency words will receive a higher score than bigrams composed of high-frequency words</li>\n</ul>\n</li>\n</ul>\n<p>Chain rule for entropy   </p>\n<ul>\n<li>$H(X,Y) = H(Y|X) + H(X) = H(X|Y) + H(Y)$</li>\n<li>Conditional entropy $H(Y|X)$ expresses how much <strong>extra information</strong> you still need to supply on average to communicate Y when X is known <img data-src=\"/img/NLP/conditional.png\" alt=\"\"></li>\n<li>$H(X)-H(X|Y) = H(Y)-H(Y|X)$<ul>\n<li>This difference is called the <strong>mutual information between X and Y</strong>(X, Y共同擁有的information)</li>\n<li>MI is not similar to chi-square <img data-src=\"/img/NLP/wrongMI.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>Entropy  </p>\n<ul>\n<li>Entropy: uncertainty of a variable <img data-src=\"/img/NLP/entropy1.png\" alt=\"\">  </li>\n<li>Incorrect model’s cross entropy is larger than correct model’s <img data-src=\"/img/NLP/entropy2.png\" alt=\"\"><ul>\n<li>正確model和猜測model的差別：P(X)logP(X) ↔ P(X)logPM(X)</li>\n</ul>\n</li>\n<li>Entropy Rate: Per-word entropy(= sentence entropy / N) <img data-src=\"/img/NLP/entropy_rate.png\" alt=\"\"></li>\n<li>Cross Entropy: <strong>average informaton</strong> needed to <strong>identify an event drawn from the set</strong> between two probability distributions<ul>\n<li>交叉熵的意義是用該模型對文本識別的難度，或者從壓縮的角度來看，每個詞平均要用幾個位來編碼</li>\n<li><img data-src=\"/img/NLP/entropy_cross.png\" alt=\"\">  </li>\n</ul>\n</li>\n<li>Joint entropy H(X, Y): average information needed to <strong>specify both values</strong> <img data-src=\"/img/NLP/joint.png\" alt=\"\"></li>\n</ul>\n<h3 id=\"Case-Study\"><a href=\"#Case-Study\" class=\"headerlink\" title=\"Case Study\"></a>Case Study</h3><p>Emotion Analysis  </p>\n<ul>\n<li>Non-verbal Emotional Expressions</li>\n<li>text (raw) and emoticons(表情符號) (tag) form collection</li>\n<li>appearance of an emoticon is a good emotion indicator to sentences</li>\n<li>check the dependency of each word in sentences</li>\n<li>Evaluation<ul>\n<li>Use top 200 lexiconentries as features</li>\n<li>Tag={Positive, Negative}</li>\n<li>LIBSVM</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap04-N-gram-Model\"><a href=\"#Chap04-N-gram-Model\" class=\"headerlink\" title=\"Chap04 N-gram Model\"></a>Chap04 N-gram Model</h2><p>N-grams are token sequences of length N</p>\n<p>applications   </p>\n<ul>\n<li>Automatic speech recognition</li>\n<li>Author Identification</li>\n<li>Spelling correction</li>\n<li>Grammatical Error Diagnosis</li>\n<li>Machine translation</li>\n</ul>\n<h3 id=\"Counting\"><a href=\"#Counting\" class=\"headerlink\" title=\"Counting\"></a>Counting</h3><ul>\n<li>Example: <em>I do uh main-mainly business data processing</em><ul>\n<li>Should we count “uh”(pause) as tokens?</li>\n<li>What about the repetition of “mainly”? Should such do-overs count twice or just once?(重複)</li>\n<li>The answers depend on the application<ul>\n<li>“uh” is not needed for query </li>\n<li>“uh” is very useful in dialog management</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Corpora: Google Web Crawl<ul>\n<li>1,024,908,267,229 English tokens</li>\n<li>13,588,391 wordform types</li>\n<li>even large dictionaries of English have only around 500k types. Why so many here?<ul>\n<li>Numbers</li>\n<li>Misspellings</li>\n<li>Names</li>\n<li>Acronyms(縮寫)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Language-model\"><a href=\"#Language-model\" class=\"headerlink\" title=\"Language model\"></a>Language model</h3><p>Language models assign a probability to a word sequence<br>Ex. <code>P(the mythical unicorn) = P(the) * P(mythical | the) * P(unicorn | the mythical)</code></p>\n<ul>\n<li>Markov assumption: the probability of a word depends only on <strong>limited previous words</strong>     <ul>\n<li>Generalization: n previous words, like bigram, trigrams, 4-grams……</li>\n<li>As we increase the value of N, the accuracy of model increases</li>\n</ul>\n</li>\n</ul>\n<p>N-Gram probabilities come from a training corpus<br>overly narrow corpus: probabilities don’t generalize<br>overly general corpus: probabilities don’t reflect task or domain  </p>\n<p>maximum likelihood estimate  </p>\n<ul>\n<li>maximizes the probability of the training set T given the model M</li>\n<li>Suppose the word “Chinese” occurs 400 times in a corpus<ul>\n<li>MLE estimate is 400/1000000 = .004</li>\n<li>makes it most likely that “Chinese” will occur 400 times in a million word corpus</li>\n</ul>\n</li>\n<li>P([s] I want englishfood [s]) = P(I|[s]) x P(want|I) x P(english|want) x P(food|english) x P([s]|food) = 0.000031$</li>\n</ul>\n<p>Usage</p>\n<ul>\n<li><p>capture some knowledge about language</p>\n<ul>\n<li>World Knowledge<ul>\n<li>P(english food|want) = .0011</li>\n<li>P(chinese food|want) = .0065</li>\n</ul>\n</li>\n<li>syntax<ul>\n<li>P(to|want) = .66</li>\n<li>P(eat| to) = .28</li>\n<li>P(food| to) = 0</li>\n</ul>\n</li>\n<li>discourse<ul>\n<li><code>P(i|&lt;s&gt;)</code> = .25</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Shannon’s Method: use language model to generate random sentences</p>\n<ul>\n<li>Shakespeare as a Corpus  <ul>\n<li>99.96% of the possible bigrams were never seen (have zero entries in the table)</li>\n</ul>\n</li>\n<li>This is the biggest problem in language modeling</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Evaluating-N-Gram-Models\"><a href=\"#Evaluating-N-Gram-Models\" class=\"headerlink\" title=\"Evaluating N-Gram Models\"></a>Evaluating N-Gram Models</h3><ul>\n<li>Extrinsic(外在的) evaluation<ul>\n<li>Compare performance of the application within two models</li>\n<li>time-consuming</li>\n</ul>\n</li>\n<li>Intrinsic evaluation<ul>\n<li>perplexity<ul>\n<li>But get poor approximation unless the test data looks just like the training data</li>\n</ul>\n</li>\n<li>not sufficient to publish</li>\n</ul>\n</li>\n</ul>\n<p>Standard Method</p>\n<ul>\n<li>Train → Test </li>\n<li>A dataset which is different from our training set, but both drawn from the same source</li>\n<li>use evaluation metric(Ex. perplexity)</li>\n<li>Example <ul>\n<li>Create a fixed lexicon L, of size V<ul>\n<li>At text normalization phase, <strong>any training word not in L changed to UNK</strong>(unknown word token)</li>\n<li><strong>count UNK like a normal word</strong></li>\n</ul>\n</li>\n<li>When testing, also use UNK counts for any word not in training</li>\n<li>The best language model is one that best predicts an unseen test set</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"perplexity-複雜度\"><a href=\"#perplexity-複雜度\" class=\"headerlink\" title=\"perplexity(複雜度)\"></a>perplexity(複雜度)</h3><ul>\n<li>Definition  <ul>\n<li>notion of surprise<ul>\n<li>The more surprised the model is, the lower probability it assigned to the test set</li>\n<li><strong>Minimizing perplexity is the same as maximizing probability</strong></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>probability of a test set, as normalized by the number of words <img data-src=\"/img/NLP/perplexity.png\" alt=\"\"></li>\n<li>物理意義是單詞的編碼大小<ul>\n<li>如果在某個測試語句上，語言模型的perplexity值為2^190，說明該句子的編碼需要190bits</li>\n</ul>\n</li>\n<li>relate to entropy<ul>\n<li>Perplexity(p, q) = $2^{H(p,q)}$   </li>\n<li>p is the test sample distribution, and q is the distribution of language model</li>\n<li>do everything in log space to avoid underflow and calculate faster</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"word-entropy\"><a href=\"#word-entropy\" class=\"headerlink\" title=\"word entropy\"></a>word entropy</h3><ul>\n<li>word entropy for English<ul>\n<li>11.82 bits per word [Shannon, 1951]</li>\n<li>9.8 bits per word [Grignetti, 1964]</li>\n</ul>\n</li>\n<li>word entropy in medical language<ul>\n<li>11.15 bits per word</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap05-Statistical-Inference\"><a href=\"#Chap05-Statistical-Inference\" class=\"headerlink\" title=\"Chap05 Statistical Inference\"></a>Chap05 Statistical Inference</h2><ul>\n<li>Statistical Inference：<strong>taking some data</strong> (generated by unknown distribution) and then <strong>making some inferences(推理，推測)</strong> about this distribution</li>\n<li>three issues<ul>\n<li><strong>Dividing the training data into equivalence classes</strong></li>\n<li><strong>Finding a good statistical estimator for each equivalence class</strong></li>\n<li><strong>Combining multiple estimators</strong></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Form-Equivalence-Class\"><a href=\"#Form-Equivalence-Class\" class=\"headerlink\" title=\"Form Equivalence Class\"></a>Form Equivalence Class</h3><ul>\n<li>Classification Problem<ul>\n<li><strong>predict target feature</strong> based on various <strong>classificatory features</strong></li>\n<li>reliability v.s. discrimination<ul>\n<li>The more classes, the more discrimination, but estimation feature is not reliable</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Independent assumption<ul>\n<li>assume data is nearly independent</li>\n</ul>\n</li>\n<li>Statistical Language Modeling<ul>\n<li><img data-src=\"/img/NLP/smodel.png\" alt=\"\"></li>\n<li>Language Model: P(W)</li>\n<li>LM does not depend on acoustics<ul>\n<li>the acoutstics probability is constant(calculated by data)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>n-gram model<ul>\n<li>assume equivalence classes are previous n-1 words</li>\n<li>Markov Assumption: Only the prior n-1 local context affects the next entry<ul>\n<li>(n-1)th Markov Model or n-gram</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Building n-grams</strong><ol>\n<li>Remove punctuation(標點) and normalize text</li>\n<li>Map out-of-vocabulary words to unknown symbol(UNK)</li>\n<li>Estimate conditional probabilities by joint probabilities<ul>\n<li>P(n | n-2, n-1) = P(n-2, n-1, n) / P(n-2, n-1)</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"Finding-statistical-estimator\"><a href=\"#Finding-statistical-estimator\" class=\"headerlink\" title=\"Finding statistical estimator\"></a>Finding statistical estimator</h3><ul>\n<li>Goal: derive <strong>probability estimate of target feature</strong> based on observed data</li>\n<li>Running Example<ul>\n<li>From n-gram data P(w1,..,wn), predict P(wn|w1,..,wn-1)</li>\n</ul>\n</li>\n<li>Solutions<ul>\n<li>Maximum Likelihood Estimation</li>\n<li>Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws</li>\n<li>Held Out Estimation</li>\n<li>Cross-Validation</li>\n<li>Good-Turing Estimation</li>\n</ul>\n</li>\n<li>Model combination<ul>\n<li>Combine models (unigram, bigram, trigram, …) to use the most precise model available</li>\n<li>interpolation(內插) and back-off(後退)</li>\n<li>use higher order models when model has enough data</li>\n<li>back off to lower order models when there isn’t enough data</li>\n</ul>\n</li>\n</ul>\n<p>Terminology  </p>\n<ul>\n<li>Ex. <code>[s] a b a b a</code><ul>\n<li>N = 5 (<code>[s]a,ab,ba,ab,ba</code>)</li>\n<li>B = 3 (<code>[s]a,ab,ba</code>)</li>\n<li>C(w1, w2…) = 某N-gram(Ex. ab)出現次數</li>\n<li>r =  某N-gram出現頻率</li>\n<li>Nr = 有幾個「出現r次的N-gram」</li>\n<li>Tr = 出現r次的N-gram，在test data出現的總次數</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"1-Maximum-Likelihood-Estimation\"><a href=\"#1-Maximum-Likelihood-Estimation\" class=\"headerlink\" title=\"(1) Maximum Likelihood Estimation\"></a>(1) Maximum Likelihood Estimation</h4><ul>\n<li>usually unsuitable for NLP <ul>\n<li>sparseness of the data(a lot of word sequences with zero probabilities)</li>\n</ul>\n</li>\n<li>Use Discounting or Smoothing technique to improve<ul>\n<li>Smoothing<ul>\n<li>Smoothing is like Robin Hood: Steal from the rich and give to the poor</li>\n<li>no word sequences has 0 probability <img data-src=\"/img/NLP/fBBrh6P.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Discounting<ul>\n<li>assign some probability to unseen events</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"2-Laplace’s-Lidstone’s-and-Jeffreys-Perks’-Laws\"><a href=\"#2-Laplace’s-Lidstone’s-and-Jeffreys-Perks’-Laws\" class=\"headerlink\" title=\"(2) Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws\"></a>(2) Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws</h4><ul>\n<li>Laplace: add 1 to every count <ul>\n<li>gives far too much probabilities to unseen events</li>\n<li>Usage: In domains where the number of zeros isn’t so huge<ul>\n<li>pilot studies</li>\n<li>document classification</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Lidstone and Jeffreys-Perks: add a smaller value λ &lt; 1<ul>\n<li>B:number of bins <img data-src=\"/img/NLP/lidstone.png\" alt=\"lid\"></li>\n<li>Expected Likelihood Estimation (ELE)(Jeffreys-Perks Law)<ul>\n<li>λ=1/2</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"3-Held-Out-Estimation\"><a href=\"#3-Held-Out-Estimation\" class=\"headerlink\" title=\"(3) Held Out Estimation\"></a>(3) Held Out Estimation</h4><ul>\n<li>compute frequencies in training data and held out data</li>\n<li><img data-src=\"/img/NLP/heldout.png\" alt=\"\"><ul>\n<li>Tr / Nr = Average frequency of training frequency r N-grams<ul>\n<li>estimate frequency(value for validation)</li>\n<li>計算出現在training corpus r次的bigrams，在held-out corpus出現的次數稱為Tr。 因為這種bigrams有Nr個，因此平均為Tr / Nr</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Validation<ul>\n<li>if the probabilities estimated on training data are close to those on held-out data, it’s a good language model</li>\n<li><a href=\"http://gitqwerty777.github.io/MLfoundation2/#chap15-validation\">參考資料–validation in machine learning</a></li>\n</ul>\n</li>\n<li>Prevent Overtraining(overfit)<ul>\n<li>test on different data</li>\n</ul>\n</li>\n</ul>\n<p>Training portion and testing portion (5-10% of total data)  </p>\n<ul>\n<li>Held out data (validation data)<ul>\n<li>available training data: real training data(90%) + held out data(10%)</li>\n</ul>\n</li>\n<li>Instead of presenting a single performance figure, testing result on each smaller sample<ul>\n<li>Using t-test to reject the possibility of an accidental difference</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"4-Cross-Validation\"><a href=\"#4-Cross-Validation\" class=\"headerlink\" title=\"(4) Cross-Validation\"></a>(4) Cross-Validation</h4><p>If data is not enough, use each part of the data both as training data and held out data  </p>\n<ul>\n<li>Deleted Estimation<ul>\n<li>$N_r^a$ = number of n-grams occurring r times in the <strong>a th part</strong> of the training data</li>\n<li>$T_r^{ab}$ = number of occurs in part b of 「bigrams occurs r times in part a」</li>\n<li><img data-src=\"/img/NLP/deleted_estimate.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<ol>\n<li>Split the training data into K sections</li>\n<li>For each section k: hold-out section k and compute counts from remaining K-1 sections; compute Tr(k) </li>\n<li>Estimate probabilities by averaging over all sections</li>\n</ol>\n<p>estimate frequency of deleted estimation <img data-src=\"/img/NLP/del-estimate.png\" alt=\"\"></p>\n<h4 id=\"5-Good-Turing-Estimation\"><a href=\"#5-Good-Turing-Estimation\" class=\"headerlink\" title=\"(5) Good-Turing Estimation\"></a>(5) Good-Turing Estimation</h4><ul>\n<li>用出現一次的來預測沒出現過的</li>\n<li>若出現次數&gt;k，不變，否則套用公式</li>\n<li><img data-src=\"/img/NLP/goodturing.png\" alt=\"\"><ul>\n<li>renormalize to sum = 1</li>\n</ul>\n</li>\n<li>Simple Good-Turing<ul>\n<li>replace any zeros in the sequence by linear regression: <code>log(Nc) = a+blog(c)</code></li>\n</ul>\n</li>\n<li>after good-turing <img data-src=\"/img/NLP/gttable.png\" alt=\"\"></li>\n</ul>\n<p>explaination from stanford NLP course   </p>\n<ul>\n<li>when use leave-one-out validation, the possibilities of unseen validation data is $\\frac{N_1}{N}$(when thing-saw-once is the validation data), the possibilities of validation data have been seen K times is $\\frac{(k+1)N_{k+1}}{N}$ </li>\n<li>Josh Goodman’s intuition: assume You are fishing, and caught 10 carp,3 perch,2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish<ul>\n<li>P(unseen) = N1/N0 = N1/N = 3/18</li>\n<li>C(trout) = $2 \\times N_2/N_1$ = $2 \\times (1/3)$ = 2/3<ul>\n<li>P(trout) = 2/3 / 18 = 1/27</li>\n</ul>\n</li>\n<li>for large k, often get zero estimate, so do not change the count<ul>\n<li>C(the) = 200000, C(a) = 190000, $C*(the) = (200001)N_{200001} / N_{200000} = 0 (because N_{200001} = 0)$</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"6-Absolute-Discounting\"><a href=\"#6-Absolute-Discounting\" class=\"headerlink\" title=\"(6) Absolute Discounting\"></a>(6) Absolute Discounting</h4><p>從所有非零N-gram中拿出λ，平均分配給所有未出現過的N-gram  </p>\n<h3 id=\"Combining-Estimator\"><a href=\"#Combining-Estimator\" class=\"headerlink\" title=\"Combining Estimator\"></a>Combining Estimator</h3><p>Combination Methods   </p>\n<ul>\n<li>Simple Linear Interpolation(內插)(finite mixture models)<ul>\n<li>Ex. trigram, bigram and unigram <img data-src=\"/img/NLP/linearde.png\" alt=\"\"><ul>\n<li>More generally, λ can be a function of (wn-2, wn-1, wn)</li>\n</ul>\n</li>\n<li>use <a href=\"#backward-forward\">Expectation-Maximization (EM) algorithm</a> to get weights</li>\n</ul>\n</li>\n<li>General Linear Interpolation<ul>\n<li>general form for a linear interpolation model</li>\n<li>weights are a function of the history <img data-src=\"/img/NLP/gli.png\" alt=\"\"> </li>\n</ul>\n</li>\n<li>Katz’s Backing-Off<ul>\n<li>choose proper order to train model (base on training data)<ul>\n<li>If the n-gram appeared more than k times<ul>\n<li>use MLE estimate and discount it</li>\n</ul>\n</li>\n<li>If the n-gram appeared k times or less<ul>\n<li>use an estimate from <strong>lower-order n-gram</strong></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>back-off probability <img data-src=\"/img/NLP/pbo.png\" alt=\"\"><ul>\n<li>$P_{Dis}(w_n|w_{n-2},w_{n-1})$ is specific discounted estimate. e.g., Good-Turing or Absolute Discounting </li>\n<li>unseen trigram is estimated by bigram and β <img data-src=\"/img/NLP/bosmooth.png\" alt=\"\"></li>\n<li><strong>β(wn-2, wn-1)</strong> and <strong>α</strong> are chosen so that sum of probabilities = 1</li>\n</ul>\n</li>\n<li>more genereal form <img data-src=\"/img/NLP/botable.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>Most usual approach in large speech recognition: trigram language model, Good-Turing discounting, back-off combination</p>\n</blockquote>\n<h2 id=\"Chap06-Hidden-Markov-Models-HMM\"><a href=\"#Chap06-Hidden-Markov-Models-HMM\" class=\"headerlink\" title=\"Chap06 Hidden Markov Models(HMM)\"></a>Chap06 Hidden Markov Models(HMM)</h2><ul>\n<li>statistical tools that are useful for NLP<ul>\n<li><strong>part-of-speech-tagging</strong> </li>\n<li>We construct “Visible” Markov Models in training, but treat them as Hidden Markov Models when tagging new corpora  </li>\n</ul>\n</li>\n<li>model a <strong>state sequence</strong> (perhaps through time) <strong>of random variables</strong> that have dependencies<ul>\n<li>狀態(state)並不是直接可見的，但受狀態影響的某些變量(output symbol)則是可見的</li>\n<li>known value<ul>\n<li><strong>output symbols(words)</strong> 字詞</li>\n<li>probabilistic function of state relation 和state相關的機率函式</li>\n</ul>\n</li>\n<li>unknown value<ul>\n<li><strong>state(part-of-speech tags)</strong> 目前的state，即POS tag</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>rely on 2 assumptions<ul>\n<li>Let X=(X1, …, XT) be a sequence of random variables, X is markov chain if</li>\n</ul>\n<ol>\n<li>Limited Horizon<ul>\n<li>a word’s tag only depends on <strong>previous</strong> tag(state只受前一個state影響)</li>\n</ul>\n</li>\n<li>Time Invariant<ul>\n<li>the dependency does not change over time(轉移矩陣不變)</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<p>Description   </p>\n<ul>\n<li>initial state π, state = Q, Observations = O, transition matrix = A, output(observation) matrix = B  </li>\n<li><img data-src=\"/img/NLP/hmm1.png\" alt=\"\"><ul>\n<li>$a_{ij}$ = probability of state $q_i$ transition to state $q_j$ </li>\n<li>$b_i(k)$ = probability of observe output symbol $O_k$ when state = $q_i$  </li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-problems-of-HMM\"><a href=\"#3-problems-of-HMM\" class=\"headerlink\" title=\"3 problems of HMM\"></a>3 problems of HMM</h3><p><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy41Mm5scC5jbi9obW0tbGVhcm4tYmVzdC1wcmFjdGljZXMtZm91ci1oaWRkZW4tbWFya292LW1vZGVscw==\">中文解說：隱馬可夫鏈<i class=\"fa fa-external-link-alt\"></i></span></p>\n<ol>\n<li>評估（Evaluation）：what is probability of the observation sequence given a model? (P(Observes|Model))<ul>\n<li>Used in model improvement</li>\n<li>Used in classification<ul>\n<li>Word spotting in speech recognition, language identification, speaker identification, author identification……</li>\n<li>Given an observation, compute P(O|model) for all models</li>\n</ul>\n</li>\n<li>Use <strong>Forward algorithm</strong> to solve it</li>\n</ul>\n</li>\n<li>解碼（Decoding）：Given an observation sequence and model, what is the <strong>most likely state sequence</strong>? (P(States|Observes, Model)) 下一個state是什麼<ul>\n<li>Used in tagging (tags=hidden states)</li>\n<li>Use <strong>Viterbi algorithm</strong> to solve it</li>\n</ul>\n</li>\n<li>學習（Learning）：Given an observation sequence, infer the best model parameters (argmax(Model) P(Model|Observes))<ul>\n<li>「fill in model parameters that make the observation sequence most likely」</li>\n<li>Used for building HMM Model from data</li>\n<li>Use <strong>EM(Baum-Welch, backward-forward algorithm)</strong> to solve it</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"Solutions-of-HMM-problem\"><a href=\"#Solutions-of-HMM-problem\" class=\"headerlink\" title=\"Solutions of HMM problem\"></a>Solutions of HMM problem</h3><h4 id=\"Forward\"><a href=\"#Forward\" class=\"headerlink\" title=\"Forward\"></a>Forward</h4><p><a href=\"http://www.52nlp.cn/hmm-learn-best-practices-five-forward-algorithm-1\" target=\"_blank\" rel=\"noopener external nofollow noreferrer\"></a></p>\n<ul>\n<li><img data-src=\"/img/NLP/fwformula.png\" alt=\"\"></li>\n<li>simply sum of the probability of each possible state sequence </li>\n<li>Direct evaluation<ul>\n<li>time complexity = $(2T+1) \\times N^{T+1}$ -&gt; too big <img data-src=\"/img/NLP/fw.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Use dynamic programming<ul>\n<li>record the probability of subpaths of the HMM</li>\n<li>The probability of longer subpaths can be calculated from shorter subpaths</li>\n<li>similar to Viterbi: viterbi use MAX() instead of SUM()</li>\n</ul>\n</li>\n</ul>\n<!-- Description:DP  \n- ![dp](/img/NLP/dp.png)\n- ![dp](/img/NLP/dptable.png)\n    - 選最高機率的路徑(將其他路徑的機率加入最高機率) \n    - 例：p(qqqq) = 0.01, p(qrrq) = 0.007 → P(qqqq) = 0.017\n-->\n\n<p>Forward Algorithm  </p>\n<ul>\n<li>$α_t(i)$ = probability of state = qi at time = t <img data-src=\"/img/NLP/forwardalgo.png\" alt=\"dp\"></li>\n<li>α的求法：將time = t-1 的 α 值，乘上在time = t時會在qi state的機率，並加總 <img data-src=\"/img/NLP/forwardfex.png\" alt=\"dp\"></li>\n<li>順向推出所有可能的state sequence會產生此observation的機率和, 即為此model會產生此observation的機率 <img data-src=\"/img/NLP/forwardexample.png\" alt=\"dp\"><ul>\n<li>Σ P($O_1, O_2, O_3$ | possible state sequence) = P($O_1, O_2, O_3$ | Model)</li>\n</ul>\n</li>\n<li><img data-src=\"/img/NLP/forwardpseudo.png\" alt=\"dp\"></li>\n</ul>\n<p>Example:Urn(甕)  </p>\n<ul>\n<li>genie has two urns filled with red and blue balls</li>\n<li>genie selects an urn and then draws a ball from it<ul>\n<li>The urns are hidden</li>\n<li>The balls are observed</li>\n</ul>\n</li>\n<li>After a lot of draws<ul>\n<li>know the distribution of colors of balls in each urn(B matrix) </li>\n<li>know the genie’s preferences in draw from one urn or the next(A matrix)</li>\n</ul>\n</li>\n<li>assume output (observation) is Blue Blue Red (BBR)<ul>\n<li>Forward: P(BBR|model) = 0.0792 (SUM of all possible states’ probability) <img data-src=\"/img/NLP/forward-urn.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>Viterbi</p>\n<ul>\n<li>compute <strong>the most possible path</strong></li>\n<li>$v_t(i)$ = <strong>most possible path probability</strong> from time = 0 to time = t, and state = qi at time = t <img data-src=\"/img/NLP/viterbi.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/viterbi-graph.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/viterbi-algo.png\" alt=\"\"></li>\n<li>Viterbi in Urn example <img data-src=\"/img/NLP/urn-cal.png\" alt=\"\"></li>\n</ul>\n<pre><code>def viterbi(obs, states, start_p, trans_p, emit_p):\n    V = [{}]\n    path = {}\n\n    # Initialize base cases (t == 0)\n    for y in states:\n        V[0][y] = start_p[y] * emit_p[y][obs[0]]\n        path[y] = [y]\n\n    # Run Viterbi for t &gt; 0\n    for t in range(1,len(obs)):\n        V.append({})\n        newpath = {}\n\n        for y in states:\n            (prob, state) = max([(V[t-1][y0] * trans_p[y0][y] * emit_p[y][obs[t]], y0) for y0 in states]) \n            # ↑ find the most possible state transitting to given state y at time=t\n            V[t][y] = prob\n            newpath[y] = path[state] + [y] \n\n        # newpath(at time t) can overwrite path(at time t-1) \n        path = newpath\n\n    (prob, state) = max([(V[len(obs) - 1][y], y) for y in states])\n    return (prob, path[state])</code></pre><h4 id=\"Backward\"><a href=\"#Backward\" class=\"headerlink\" title=\"Backward\"></a>Backward</h4><ul>\n<li>Useful for parameter estimation</li>\n</ul>\n<p>Description  </p>\n<ul>\n<li>Backward variables β, which are the total probability of seeing the rest of the observation sequence($O_t to O_T$) given state qi at time t <img data-src=\"/img/NLP/bw-procedure.png\" alt=\"\"><ul>\n<li><img data-src=\"/img/NLP/bw-f.png\" alt=\"\"></li>\n<li>初始化β：令t=T時刻所有狀態的β為1</li>\n</ul>\n</li>\n<li>由後往前計算 <img data-src=\"/img/NLP/bw-graph.png\" alt=\"\"></li>\n<li>如果要計算某observation的概率，只需將t=1的後向變量相加</li>\n</ul>\n<h4 id=\"Backward-Forward\"><a href=\"#Backward-Forward\" class=\"headerlink\" title=\"Backward-Forward\"></a>Backward-Forward</h4><ul>\n<li>We can locally maximize model parameter λ, by an iterative hill-climbing known as Baum-Welch algorithm(=Forward-Backward) (by EM Algorithm structure)</li>\n</ul>\n<p>Forward-Backward Algorithm    </p>\n<ul>\n<li>find which** state transitions(A matrix)** and <strong>symbol observaions(B matrix)</strong> were <strong>probably used the most</strong></li>\n<li>By <strong>increasing the probability of those</strong>, we can get a better model which gives a higher probability to the observation sequence</li>\n<li>transition probabilities and path probabilities are both require each other to calculate<ul>\n<li>use A matrix to calculate path probabilities</li>\n<li>need path probabilities to update A matrix</li>\n<li>use EM algorithm</li>\n</ul>\n</li>\n</ul>\n<p>EM algorithm (Expectation-Maximization)    </p>\n<ul>\n<li>迭代算法，它的最大優點是簡單和穩定，但容易陷入局部最優</li>\n<li>(隨機)選擇參數λ0，找出在λ0下最可能的狀態，計算每個訓練樣本的可能結果的概率，再<strong>重新估計新的參數λ</strong>。經過多次的迭代，直至某個收斂條件滿足為止</li>\n<li>Urn Example<ul>\n<li>update transition matrix A ($a_{12}, a_{11}$ … ) <img data-src=\"/img/NLP/newtrans.png\" alt=\"\"><ul>\n<li>P(1→2) = 0.0414 <img data-src=\"/img/NLP/1-2.png\" alt=\"1→2\"></li>\n<li>P(1→1) = 0.0537 <img data-src=\"/img/NLP/1-1.png\" alt=\"1→1\"></li>\n<li>normalize: P(1→2)+P(1→1) = 1, P(1→2) = 0.435 …</li>\n</ul>\n</li>\n<li>若state數目多的時候，計算量過大…<ul>\n<li>用backward, forward</li>\n<li>前面用forward, 後面用backward <img data-src=\"/img/NLP/bf-graph.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Combine forward and backward  </p>\n<ul>\n<li>Let ξt be the probability of being in state i at time t and state j at time t+1, <strong>given observation and model λ</strong><img data-src=\"/img/NLP/kesin.png\" alt=\"\"></li>\n<li>use not-quite-ξ to get ξ <img data-src=\"/img/NLP/nqkesin.png\" alt=\"\"> because <img data-src=\"/img/NLP/kesin-formula.png\" alt=\"\"><ul>\n<li>P(O|λ) → problem1 of HMM 的答案 → 用forward解</li>\n</ul>\n</li>\n<li>見上方backward, forward同時使用之圖 <img data-src=\"/img/NLP/nqkesin-formula.png\" alt=\"\"></li>\n<li>ξ可用來計算transition matrix <img data-src=\"/img/NLP/newtrans-final.png\" alt=\"\"></li>\n</ul>\n<p>Summary of Forward-Backward <img data-src=\"/img/NLP/fb-algo.png\" alt=\"\"> </p>\n<ol>\n<li>Initialize λ=(A,B)</li>\n<li>Compute α, β, ξ using observations</li>\n<li>Estimate new λ’=(A,B)</li>\n<li>Replace λ with λ’</li>\n<li>If not converged go to 2</li>\n</ol>\n<h2 id=\"Chap07-Part-of-Speech-Tagging\"><a href=\"#Chap07-Part-of-Speech-Tagging\" class=\"headerlink\" title=\"Chap07 Part-of-Speech Tagging\"></a>Chap07 Part-of-Speech Tagging</h2><p>alias: <strong>parts-of-speech</strong>, <strong>lexical categories</strong>, <strong>word classes</strong>, <strong>morphological classes</strong>, <strong>lexical tags</strong></p>\n<ul>\n<li>Noun, verb, adjective, preposition, adverb, article, interjection, pronoun, conjunction</li>\n<li>preposition(P)<ul>\n<li>of, by, to</li>\n</ul>\n</li>\n<li>pronoun(PRO)<ul>\n<li>I, me, mine</li>\n</ul>\n</li>\n<li>determiner(DET)<ul>\n<li>the, a, that, those</li>\n</ul>\n</li>\n</ul>\n<p>Usage  </p>\n<ul>\n<li><p>Speech synthesis</p>\n</li>\n<li><p>Tag before parsing</p>\n</li>\n<li><p>Information extraction</p>\n</li>\n<li><p>Finding names, relations, etc.</p>\n</li>\n<li><p>Machine Translation</p>\n</li>\n<li><p>Closed class</p>\n<ul>\n<li>the class that is hard to add new words</li>\n<li>Usually function words (short common words which play a role in grammar)<ul>\n<li>prepositions: on, under, over,…</li>\n<li>particles: up, down, on, off, …</li>\n<li>determiners: a, an, the, …</li>\n<li>pronouns: she, who, I, …</li>\n<li>conjunctions: and, but, or, …</li>\n<li>auxiliary verbs: can, may should, …</li>\n<li>numerals: one, two, three, third, …</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Open class</p>\n<ul>\n<li>new ones can be created all the time</li>\n<li>For English: Nouns, Verbs, Adjectives, Adverbs</li>\n</ul>\n</li>\n</ul>\n<p>Choosing Tagset: Ex. “Penn TreeBank tagset”, 45 tag<br><img data-src=\"/img/NLP/tagset.png\" alt=\"\"></p>\n<p>Methods for POS Tagging  </p>\n<ol>\n<li>Rule-based tagging<ul>\n<li>ENGTWOL: ENGlish TWO Level analysis</li>\n</ul>\n</li>\n<li>Stochastic: Probabilistic sequence models<ul>\n<li>HMM (Hidden Markov Model)</li>\n<li>MEMMs (Maximum Entropy Markov Models)</li>\n</ul>\n</li>\n<li>Transformation-Based Tagger (Brill)</li>\n</ol>\n<h3 id=\"Rule-Based-Tagging\"><a href=\"#Rule-Based-Tagging\" class=\"headerlink\" title=\"Rule-Based Tagging\"></a>Rule-Based Tagging</h3><ol>\n<li>Assign all possible tags to each word</li>\n<li>Remove tags according to set of rules<ol>\n<li>Typically more than 1000 hand-written rules</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"Hidden-Markov-Model-tagging\"><a href=\"#Hidden-Markov-Model-tagging\" class=\"headerlink\" title=\"Hidden Markov Model tagging\"></a>Hidden Markov Model tagging</h3><ul>\n<li>special case of Bayesian inference<ul>\n<li>Foundational work in computational linguistics</li>\n</ul>\n</li>\n<li>related to the “noisy channel” model that’s the basis for ASR, OCR and MT</li>\n<li>Decoding view  <ul>\n<li>Consider all possible sequences of tags</li>\n<li>choose the tag sequence which is most possible given the observation sequence of n words w1…wn</li>\n</ul>\n</li>\n<li>Generative view<ul>\n<li>This sequence of words must have resulted from some hidden process</li>\n<li>A sequence of tags (states), each of which emitted a word</li>\n</ul>\n</li>\n<li>$t^n_1$(t hat), which is the most possible tag <img data-src=\"/img/NLP/best-t.png\" alt=\"\"></li>\n<li>use viterbi to get tag <img data-src=\"/img/NLP/viterbi-ex.png\" alt=\"\"></li>\n</ul>\n<p>Evaluation  </p>\n<ul>\n<li>Overall error rate with respect to a gold-standard test set</li>\n<li>Error rates on particular tags/words</li>\n<li>Tag confusions, Unknown words…</li>\n<li>Typically accuracy reaches 96~97%</li>\n</ul>\n<p>Unknown Words</p>\n<ul>\n<li>Simplest model<ul>\n<li>Unknown words can be of any part of speech, or only in any open class</li>\n</ul>\n</li>\n<li>Morphological and other cues<ul>\n<li>~ed: past tense forms or past participles</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Maximum-entropy-Markov-model-MEMM\"><a href=\"#Maximum-entropy-Markov-model-MEMM\" class=\"headerlink\" title=\"Maximum entropy Markov model (MEMM)\"></a>Maximum entropy Markov model (MEMM)</h3><p>Maximum Entropy Model  </p>\n<ul>\n<li>MaxEnt: multinomial(多項式) logistic regression</li>\n<li>Used for sequence classification/sequence labeling</li>\n<li>Maximum entropy Markov model (MEMM)<ul>\n<li>a common MaxEnt classifier</li>\n</ul>\n</li>\n</ul>\n<!-- Classification\n- Task\n    - observation, Extract useful features, Classify the observation based on these features\n- Probabilistic classifier\n    - Given an observation, it gives a probability distribution over all classes\n- Non-sequential(連續的) Applications\n    - Text classification\n    - Sentiment analysis\n    - Sentence boundary detection\n-->\n\n\n<p>Exponential(log-linear) classifiers </p>\n<ul>\n<li>Combine features linearly</li>\n<li>Use the sum as an exponent <img data-src=\"/img/NLP/maxent.png\" alt=\"\"></li>\n<li>Example <img data-src=\"/img/NLP/maxent-ex.png\" alt=\"\"></li>\n</ul>\n<p>Maximum Entropy Markov Model       </p>\n<ul>\n<li>MaxEnt model<ul>\n<li>classifies <strong>a</strong> observation into <strong>one</strong> of discrete classes</li>\n</ul>\n</li>\n<li>MEMM<ul>\n<li>augmentation(增加) of the basic MaxEnt classifier</li>\n<li><strong>assign a class to each element in a sequence</strong></li>\n</ul>\n</li>\n</ul>\n<p>POS tagging from MaxExt to MEMM   </p>\n<ul>\n<li>include some source of knowledge into the tagging process</li>\n<li>The simplest approach<ul>\n<li>run the local classifier and <strong>feature is classifier from the previous word</strong></li>\n<li>Flaw<ul>\n<li>It makes a hard decision on each word before moving on the next word</li>\n<li>cannot use information from the later words</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>discriminative model</strong>(判別模型)   </p>\n<ul>\n<li>Compute the posterior P(Tag|Word) directly to decide tag <img data-src=\"/img/NLP/memm.png\" alt=\"\"></li>\n<li>求解條件機率分佈 P(y|x) 預測 y → 求P(tag|word)來取得tag </li>\n<li>不考慮聯合機率分佈 P(x, y)</li>\n<li>對於諸如分類和回歸問題，由於不考慮聯合機率分佈，採用判別模型可以取得更好的效果</li>\n</ul>\n<p>HMM and MEMM(順推和逆推的差別) <img data-src=\"/img/NLP/hmmandmemm.png\" alt=\"\">  </p>\n<ul>\n<li>Unlike HMM, MEMM can condition on any <strong>useful feature of observation</strong><ul>\n<li>HMM: state is the fiven value</li>\n<li>MEMM: observation is the given value</li>\n</ul>\n</li>\n<li>viterbi function for MEMM <img data-src=\"/img/NLP/viterbi-new.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/memm-ex.png\" alt=\"\"></li>\n</ul>\n<h3 id=\"Transformation-Based-Learning-of-Tags\"><a href=\"#Transformation-Based-Learning-of-Tags\" class=\"headerlink\" title=\"Transformation-Based Learning of Tags\"></a>Transformation-Based Learning of Tags</h3><ul>\n<li>Tag each word with its most frequent tag</li>\n<li>Construct a list of transformations that <strong>improve the initial tag</strong></li>\n<li>trigger environment: at the limited number of words before/after <img data-src=\"/img/NLP/transformed-learn.png\" alt=\"\"></li>\n<li><img data-src=\"/img/NLP/transformed-algo.png\" alt=\"\"></li>\n</ul>\n<ol>\n<li>Trigger by tags </li>\n<li>Trigger by word</li>\n<li>Trigger by morphology(詞法學)</li>\n</ol>\n<p>&lt;! – ====================分水嶺：尚未分類========================== –&gt;</p>\n<h3 id=\"Zipf’s-Law-long-tail-phenomenon\"><a href=\"#Zipf’s-Law-long-tail-phenomenon\" class=\"headerlink\" title=\"Zipf’s Law (long tail phenomenon)\"></a>Zipf’s Law (long tail phenomenon)</h3><p>a word’s frequency is approximately inversely proportional to its rank in the word distribution list<br>單詞出現的頻率與它在頻率表裡的排名成反比:<br>頻率最高的單詞出現的頻率大約是出現頻率第二位的單詞的2倍</p>\n<h4 id=\"Jelinek-Mercer-Smoothing\"><a href=\"#Jelinek-Mercer-Smoothing\" class=\"headerlink\" title=\"Jelinek-Mercer Smoothing\"></a>Jelinek-Mercer Smoothing</h4><p>interpolate(插值) between bigram and unigram<br>because if p(eat the) = 0 and p(eat thou) = 0<br>it still must consider that  p(eat the) &gt; p(eat thou)<br>because p(the) &gt; p(thou)<br>so p(eat the) = N * p(the | eat) + (1-N) * p(the | thou) </p>\n<h2 id=\"Language-Model-Applications\"><a href=\"#Language-Model-Applications\" class=\"headerlink\" title=\"Language Model: Applications\"></a>Language Model: Applications</h2><h3 id=\"Query-Likelihood-Model\"><a href=\"#Query-Likelihood-Model\" class=\"headerlink\" title=\"Query Likelihood Model\"></a>Query Likelihood Model</h3><p>given a query 𝑞, rank the probability 𝑝(𝑑|q)<br><img data-src=\"/img/NLP/cfKf6I3.png\" alt=\"\"><br>So the following arguments are equivalent:<br>1.𝑝𝑑𝑞: find the document 𝑑 that is most likely to be relevant to 𝑞<br>2.𝑝𝑞𝑑: find the document 𝑑 that is most likely to generate the query 𝑞</p>\n<p>Typically, unigram LMs are used in IR(information retrieval)<br>Retrieval does not depend that much on sentence structure</p>\n<h3 id=\"Dependence-Language-Model\"><a href=\"#Dependence-Language-Model\" class=\"headerlink\" title=\"Dependence Language Model\"></a>Dependence Language Model</h3><p>Relax the independence assumption of unigram LMs<br>Do not assume that the dependency only exist between <strong>adjacent</strong> words<br>Introduce a hidden variable: “linkage” 𝐿<br>Ex.<br><img data-src=\"/img/NLP/Z8ftSRP.png\" alt=\"\"></p>\n<p>skipped….</p>\n<h3 id=\"Proximity-Language-Model\"><a href=\"#Proximity-Language-Model\" class=\"headerlink\" title=\"Proximity Language Model\"></a>Proximity Language Model</h3><p>Proximity: how close the query terms appear in a document<br>the closer they are, the more likely they are describing the same topic or concept</p>\n<h3 id=\"Positional-Language-Model\"><a href=\"#Positional-Language-Model\" class=\"headerlink\" title=\"Positional Language Model\"></a>Positional Language Model</h3><p>Position: define a LM for each position of a document, instead of the entire document<br>Words closer to a position will contribute more to the language model of this position</p>\n<h3 id=\"Speech-Recognition\"><a href=\"#Speech-Recognition\" class=\"headerlink\" title=\"Speech Recognition\"></a>Speech Recognition</h3><ul>\n<li>The “origin” of language models</li>\n<li>used to restrict the search space of possible word sequences</li>\n<li>requires higher order models: knowing previous acoustic is important!</li>\n<li>Speed is important!</li>\n<li>N-gram LM with modified Kneser-Ney smoothing is extensively used</li>\n</ul>\n<h3 id=\"Machine-Translation-MT\"><a href=\"#Machine-Translation-MT\" class=\"headerlink\" title=\"Machine Translation (MT)\"></a>Machine Translation (MT)</h3><ul>\n<li>Decoding: given the probability model(s), find the best translation</li>\n<li>Similar role as in speech recognition: <strong>eliminate unlikely word sequences</strong></li>\n<li>Higher order Kneser-Ney smoothed n-gram LM is widely used</li>\n<li>NNLM-style models tend to outperform standard back-off LMs</li>\n<li>Also significantly speeded up in (Delvin et al, 2014)</li>\n</ul>\n<h2 id=\"參考資料\"><a href=\"#參考資料\" class=\"headerlink\" title=\"參考資料\"></a>參考資料</h2><ul>\n<li>HHChen 課堂講義</li>\n<li>SDLin 講義</li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9jbGFzcy5jb3Vyc2VyYS5vcmcvbmxwLw==\">Stanford NLP course<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><a href=\"www.52nlp.cn\">52nlp</a></li>\n</ul>\n",
            "tags": [
                "機器學習",
                "自然語言處理",
                "統計"
            ]
        },
        {
            "id": "http://gitqwerty777.github.io/MLtechnique/",
            "url": "http://gitqwerty777.github.io/MLtechnique/",
            "title": "機器學習技法",
            "date_published": "2014-11-21T13:40:00.000Z",
            "content_html": "<blockquote>\n<p>尚未寫完</p>\n</blockquote>\n<a id=\"more\"></a>\n<h2 id=\"Chap01-SVM\"><a href=\"#Chap01-SVM\" class=\"headerlink\" title=\"Chap01 SVM\"></a>Chap01 SVM</h2><p>All line is the same using PLA, but WHICH line is best? <img data-src=\"/img/ML/bestline.png\" alt=\"\"><br>→ 可以容忍的誤差愈大愈好(最近的點與分隔線的距離愈遠愈好) <img data-src=\"/img/ML/circle.png\" alt=\"\"><br>→ fat hyperplane (large <strong>margin</strong>)(分隔線可以多寬) <img data-src=\"/img/ML/fat.png\" alt=\"\"></p>\n<p>若只有兩個點，必通過兩點連線之中垂線 <img data-src=\"/img/ML/funtime1.png\" alt=\"\"> </p>\n<h3 id=\"Standard-large-margin-hyperplane-problem\"><a href=\"#Standard-large-margin-hyperplane-problem\" class=\"headerlink\" title=\"Standard large-margin hyperplane problem\"></a>Standard large-margin hyperplane problem</h3><p>max fatness(w) (max margin)<br>= min distance($x_n$, w) (n = 1 ~ N)<br>= min distance($x_n$, b, w) – (1)<br>因為 w0 不列入計算(w0 = 常數項參數 = 截距b, x0 必為 1)</p>\n<p><img data-src=\"/img/ML/wtxx.png\" alt=\"\"><br>$w^tx$ = 0 → x除去x0成為x’ → $w^tx’$ + b = 0<br>設x’, x’’都在$w^tx’$ + b = 0平面上，$w^tx’$ = -b, $w^tx’’$ = -b → $w^t(x’’-x’)$ = 0<br>$w^t$ 垂直於 $w^tx’$ + b = 0平面<br>distance(x, b, w) = x 到 平面的距離 <img data-src=\"/img/ML/distancexbw.png\" alt=\"\">   </p>\n<p>單一data的distance: 因 $y_n(w^t x_n + b) &gt; 0$<br>$distance(x_n, b, w) = (1/|w|) * y_n(w^t x_n + b)$ – (2)  </p>\n<p>specialize<br>令 $min y_n(w^t x_n + b) = 1$<br>→ $distance(x_n, b, w) = 1/|w|$<br>由式子(1),(2)可得 max margin = min distance(x_n, b, w) = 1/|w|<br>條件為 $min y_n(w^t x_n + b) = 1$</p>\n<p>放鬆條件<br>$y_n(w_t x_n + b) &gt;= 1$ is necessary constraints for $ min y_n(w_t x_n + b) = 1$<br>if all y_n(w_t x_n + b) &gt; p &gt; 1 -&gt; can generate more optimal answer (b/p, w/p) -&gt; distance 1/|w| is smaller<br>so y_n(w_t x_n + b) &gt;= 1 → y_n(w_t x_n + b) = 1  </p>\n<p>max 1/|w| -&gt; min 1/2 * w_t *  w</p>\n<p><img data-src=\"\" alt=\"standard min bw\"></p>\n<p>can solve w by solve N inequality</p>\n<h3 id=\"Support-Vector-Machine-SVM\"><a href=\"#Support-Vector-Machine-SVM\" class=\"headerlink\" title=\"Support Vector Machine(SVM)\"></a>Support Vector Machine(SVM)</h3><p>只須找最近的點即可算出w<br>support vector: bounary data(胖線的邊界點)</p>\n<p>gradient? : not easy with constraints<br>but we have:</p>\n<ul>\n<li>(convex) quadratic objective function(b, w)</li>\n<li>linear constraints (b, w)</li>\n</ul>\n<p>can use quadratic programming (QP, 二次規劃): easy </p>\n<p><img data-src=\"\" alt=\"bw\"><br><img data-src=\"\" alt=\"QP\"><br><img data-src=\"\" alt=\"QP value\"></p>\n<p>Linear Hard-Margin(all wxy &gt; 0) SVM<br><img data-src=\"\" alt=\"regular\"><br>the same as ‘weight-decay regularization’ within Ein = 0</p>\n<p>Restricts Dichotomies(堅持胖的線): if margin &lt; p, no answer<br>fewer dichotomies -&gt; small VC dim -&gt; better generalization</p>\n<p><img data-src=\"\" alt=\"circle\"><br>最大胖度有極限：在圓上，最大 根號3/2<br><img data-src=\"\" alt=\"dvc ap\"></p>\n<p><img data-src=\"\" alt=\"compare\"><br>can be good with transform<br>控制複雜度的方法！<br>控制複雜度的方法！</p>\n<h3 id=\"Chap02-dual-SVM\"><a href=\"#Chap02-dual-SVM\" class=\"headerlink\" title=\"Chap02 dual SVM\"></a>Chap02 dual SVM</h3><p>if d_trans is big, or infinite?(非常複雜的轉換)<br>find: SVM without dependence of d_trans<br>去除計算(b, w)與轉換函式複雜度的關係<br>(QP of d_trans+1 variables -&gt; N variables)    </p>\n<p>Use lagrange multiplier: Dual SVM: 將lambda視為變數來解<br><img data-src=\"\" alt=\"lagrange function\"><br><img data-src=\"\" alt=\"SVM=\"><br>若不符條件，則L()的sigma部分是正的，MaxL()為無限大<br>若符條件，則L()的sigma部分最大值為0 -&gt; MaxL() = 1/2w^tw<br>所以Min(MaxL()) = Min(1/2w^tw)</p>\n<p>交換max min的位置，可求得原問題的下限<br><img data-src=\"\" alt=\"\"></p>\n<p>可以得到strong duality(強對偶關係，=)？<br>若在二次規劃滿足constraint qualification  </p>\n<ol>\n<li>convex</li>\n<li>feasible(有解)</li>\n<li>linear constraints</li>\n</ol>\n<p>則存在 primal-dual 最佳解(對左右邊均是最佳解)</p>\n<p>Dual SVM 最佳解為？<br><img data-src=\"\" alt=\"微分b=0\"><br>b可以被消除(=0)<br><img data-src=\"\" alt=\"微分wi=0\"><br>w代入得<br><img data-src=\"\" alt=\"w=\"></p>\n<p>Karush-Kuhn-Tucker (KKT) conditions<br><img data-src=\"\" alt=\"KKT\"><br>primal-inner -&gt; an = 0 或 1-yn(wtzn+b) = 0(complementary slackness)<br>=&gt; at optimal all ‘Lagrange terms’ disappear</p>\n<p>用KKT求得a(原lambda)之後，即可得原來的(b, w)<br>w = sigma anynzn<br>b 僅有邊界(primal feasible)，但b = yn - wtzn when an &gt; 0 (primal-inner，代表必在SVM邊界上)</p>\n<p>重訂support vector的條件(a_n &gt;0)<br><img data-src=\"\" alt=\"sv &lt; sv\"><br>=&gt; b, w 都可以只用SV求到</p>\n<p>SVM:找到有用的點(SV)</p>\n<h4 id=\"Standard-hard-margin-SVM-dual\"><a href=\"#Standard-hard-margin-SVM-dual\" class=\"headerlink\" title=\"Standard hard-margin SVM dual\"></a>Standard hard-margin SVM dual</h4><p>經過整理<br><img data-src=\"\" alt=\"min 1/2\"><br>現在有N個a, 並有N+1個條件了</p>\n<p><img data-src=\"\" alt=\"QP\"><br>when N is big, qn,m is dense array and very big(N &gt;30000, use &gt;3G ram)<br>use special QP solver</p>\n<h4 id=\"SVM-和-PLA-比較\"><a href=\"#SVM-和-PLA-比較\" class=\"headerlink\" title=\"SVM 和 PLA 比較\"></a>SVM 和 PLA 比較</h4><p><img data-src=\"\" alt=\"pla\"><br>w = linear combination of data =&gt; w represented by data<br>SVM: represent w by only SV</p>\n<p><img data-src=\"\" alt=\"dual\"><br>Primal: 對(b,w)做特別縮放<br>Dual: 找到SV 和其 lagrange multiplier</p>\n<p>問題：q_n,m 需要做O(d_trans)的運算，如何避免？</p>\n<h3 id=\"Chap03-Kernel-SVM\"><a href=\"#Chap03-Kernel-SVM\" class=\"headerlink\" title=\"Chap03 Kernel SVM\"></a>Chap03 Kernel SVM</h3><p>(z_n^T)(z_m)如何算更快</p>\n<p>轉換+內積 -&gt; Kernel function<br><img data-src=\"\" alt=\"xxxxxxx\"><br>use O(d) instead of O(d^2)</p>\n<p><img data-src=\"\" alt=\"kernel\"><br><img data-src=\"\" alt=\"b=\"><br><img data-src=\"\" alt=\"gsvm =\"><br>用kernel簡化！(gsvm -&gt; 無w)</p>\n<h4 id=\"Kernel-Hard-Margin-SVM\"><a href=\"#Kernel-Hard-Margin-SVM\" class=\"headerlink\" title=\"Kernel Hard-Margin SVM\"></a>Kernel Hard-Margin SVM</h4><p><img data-src=\"\" alt=\"kernel svm algo\"></p>\n<h4 id=\"polynomial-Kernel\"><a href=\"#polynomial-Kernel\" class=\"headerlink\" title=\"polynomial Kernel\"></a>polynomial Kernel</h4><p>簡化的kernel: 對應到同等大小，不同幾何特性(如內積)的空間<br><img data-src=\"\" alt=\"kernel\"><br>r影響SV的選擇<br><img data-src=\"\" alt=\"SELECTED sv\"></p>\n<p><img data-src=\"\" alt=\"high dim\"><br>可以快速做高次轉換(和二次相同複雜度)</p>\n<p>特例: linear只需用 K1 = (0+1xtx)^1</p>\n<h4 id=\"infinite-Kernel\"><a href=\"#infinite-Kernel\" class=\"headerlink\" title=\"infinite Kernel\"></a>infinite Kernel</h4><p>taylor展開<br><img data-src=\"\" alt=\"k(x,x&#39;)\"></p>\n<p>無限維度的Gaussian Kernel (Radial Basis Funtion(RBF))<br><img data-src=\"\" alt=\"g\"></p>\n<p><img data-src=\"\" alt=\"support vector mechanism\"><br>large =&gt; sharp Gaussians =&gt; ‘overfit’?<br><img data-src=\"\" alt=\"overfit \"></p>\n<h4 id=\"Kernel選擇\"><a href=\"#Kernel選擇\" class=\"headerlink\" title=\"Kernel選擇\"></a>Kernel選擇</h4><p>linear kernel: 等於沒有轉換，linear first, 計算快<br>polynomial: 轉換過，限制小，strong physical control, 維度太大K會趨向極端值<br>-&gt; 平常只用不大的維度<br>infinite dimension:<br>most powerful<br>less numerical difficulty than poly(僅兩次式)<br>one parameter only<br>cons: mysterious – no w , and too powerful</p>\n<p>define new kernel is hard:<br>Mercer’s condition:<br><img data-src=\"\" alt=\"mercer\"></p>\n<h3 id=\"Chap04-Soft-Margin-SVM\"><a href=\"#Chap04-Soft-Margin-SVM\" class=\"headerlink\" title=\"Chap04 Soft-Margin SVM\"></a>Chap04 Soft-Margin SVM</h3><p>overfit reason: transform &amp; hard-margin(全分開)</p>\n<p>Soft-Margin – 容忍錯誤，有錯誤penalty，只有對的需要符合條件<br><img data-src=\"\" alt=\"soft1\"></p>\n<p>缺點：<br>No QP anymore<br>error大小:離fat boundary的距離 </p>\n<p>改良：求最小(犯錯的點與boundary的距離和)(linear constraint, can use QP)<br><img data-src=\"\" alt=\"soft2\"></p>\n<p>parameter C: large when want less violate margin<br>small when want large margin, tolerate some violation</p>\n<p>Soft-margin Dual: 將條件加入min中<br><img data-src=\"\" alt=\"dual\"><br>化簡後得到和dual svm相同的式子(不同條件)<br><img data-src=\"\" alt=\"化簡後\"><br>C is exactly the upper bound of an</p>\n<h3 id=\"Kernel-Soft-Margin-SVM\"><a href=\"#Kernel-Soft-Margin-SVM\" class=\"headerlink\" title=\"Kernel Soft Margin SVM\"></a>Kernel Soft Margin SVM</h3><p>more flexible: always solvable<br><img data-src=\"\" alt=\"algo\"></p>\n<p>(3)-&gt;solve b:<br>若as &lt; C(unbounded, free), 則b的求法和hard-margin一樣<br><img data-src=\"\" alt=\"compare b = \"></p>\n<p>但soft-margin還是會overfit…</p>\n<p>physical meaning<br><img data-src=\"\" alt=\"\"><br>not SV(an = 0): C-an != 0 -&gt; En = 0<br>unbounded SV(0 &lt; an &lt; C，口) -&gt; En = 0 -&gt; on fat boundary<br>bounded SV(an = C, △) -&gt; En &gt;= 0(有違反，不在boundary上)<br>-&gt; 只有bounded SV才可違反</p>\n<p>difficult to optimize(C, r)</p>\n<h4 id=\"SVM-validation\"><a href=\"#SVM-validation\" class=\"headerlink\" title=\"SVM validation\"></a>SVM validation</h4><p>leave-one-out error &lt;= #SV/N<br>若移除non-SV的點，則得出的g不變<br>-&gt; 可以靠此特性做參數選擇(不選#SV太大的)</p>\n<h3 id=\"Chap05-Kernel-Logistic-SVM\"><a href=\"#Chap05-Kernel-Logistic-SVM\" class=\"headerlink\" title=\"Chap05 Kernel Logistic SVM\"></a>Chap05 Kernel Logistic SVM</h3><p>實用library: linear:LIBLINEAR nonlinear:LIBSVM  </p>\n<p>將E替代 -&gt; 像是 L2 regularization<br><img data-src=\"\" alt=\"\"><br><img data-src=\"\" alt=\"\"></p>\n<p>缺點：不能QP, 不能微分(難解)</p>\n<p><img data-src=\"\" alt=\"compare\"><br>large margin &lt;=&gt; fewer choices &lt;=&gt; L2 regularization of short w<br>soft margin &lt;=&gt; special err<br>larger C(in soft-margin or in regularization) &lt;=&gt; smaller lagrange multiplier &lt;=&gt; less regularization  </p>\n<p>We can extend SVM to other learning models!</p>\n<p>look (wtzn + b) as linear score(f(x) in PLA)<br><img data-src=\"\" alt=\"red-blue\"><br>we can have Err_svm is upper bound of Err0/1<br>(hinge error measure)<br><img data-src=\"\" alt=\"three graph\"></p>\n<p><img data-src=\"\" alt=\"errwsce\"><br>Err_sce: 與svm相似的一個logistic regression<br><img data-src=\"\" alt=\"errbound\"></p>\n<p><img data-src=\"\" alt=\"three compare\"><br>L2 logistic regression is similar to SVM,<br>所以SVM可以用來approximate Logistic regression?<br>-&gt; SVM當作Log regression的起始點? 沒有比較快(SVM優點)<br>-&gt; 將SVM答案當作Log的近似解(return theta(wx + b))? 沒有log reg的意義(maximum likelyhood)<br>=&gt; 加兩個自由度，return theta(A*(wx+b) + B)<br>-&gt; often A &gt; 0(同方向), B~=0(無位移)<br><img data-src=\"\" alt=\"NEW LOGREG\"><br>將原本的SVM視為一種轉換</p>\n<p>Platt’s Model<br><img data-src=\"\" alt=\"PLATT\"><br>kernel SVM在Z空間的解 – 用Log Reg微調後 –&gt; 用來近似Log Reg在Z空間的解(並不是在z空間最好的解)</p>\n<p>solve LogReg to get(A, B)</p>\n<p>能使用kernel的關鍵：w為z的線性組合<br><img data-src=\"\" alt=\"svm pla logreg by sgd\"></p>\n<p>Representer Theorem: 若解L2-正規化問題，最佳w必為z的線性組合<br>將w分為(與z垂直)+(與z平行), 希望w_垂直 = 0<br>證：(原本的w) 和 (與z平行的w) 所得的err是一樣的(因為w_垂直 * z = 0)<br>且w平行比較短<br>所以min w 必(與z平行)<br><img data-src=\"\" alt=\"\"><br>結果：L2的linear model都可以用kernel解！</p>\n<p>將w = sum(B*z) = sum(B*Kernel)代入logistic regression<br>-&gt; 解B</p>\n<p>Kernel Logistic Regression(KLR)<br>= linear model of B<br><img data-src=\"\" alt=\"special regularizer\"><br>把 kernel當作轉換, kernel當作regularizer<br>= linear model of w<br>with embedded-in-kernel transform &amp; L2 regularizer<br>把 kernel內部(z)當作轉換(?), L2-regularizer</p>\n<p>警告：算出的B不會有很多零</p>\n<p>soft margin SVM ~= L2 LOG REG, special error measure:hinge<br>在z空間解log reg -&gt; 用representor theorem 轉換為一般log reg, 有代價</p>\n<h2 id=\"Chap-06-Support-Vector-Regression-SVR\"><a href=\"#Chap-06-Support-Vector-Regression-SVR\" class=\"headerlink\" title=\"Chap 06 Support Vector Regression(SVR)\"></a>Chap 06 Support Vector Regression(SVR)</h2><p>ridge regression : 有regularized的regression<br>如何加入kernel?</p>\n<p>Kernel Ridge Regression<br><img data-src=\"\" alt=\"solve ridge\"><br>用representor theorem代入後得到regularization term 和 regression term</p>\n<p><img data-src=\"\" alt=\"梯度\"><br><img data-src=\"\" alt=\"B=\"><br>因為kernal必為為psd，所以B必有解 O(N^3)</p>\n<p>g(x) = wz = sum(bz)z = sum(bk)</p>\n<p>與linear的比較：<br>kernel自由度高<br>linear為O(d^3+d^2N)<br>kernel和資料量有關，為O(N^3)，檔案大時不快</p>\n<p>LS(least-squares)SVM = kernel ridge regression:<br>和一般regression boundary差不多，但SV很多(B dense)<br>=&gt; 代表計算時間長<br>=&gt; 找一個sparse B?</p>\n<p>tube regression:<br><img data-src=\"\" alt=\"tube\"><br>insensitive error:容忍一小段的差距(在誤差內err = 0，若超過, err只算超過的部分)<br>error增加的速度變慢</p>\n<p>學SVM，解QP, 用DUAL, KKT-&gt;sparse<br><img data-src=\"\" alt=\"mimicking\"><br>regulizer 和 超過tube上界的值，超過tube下界的值</p>\n<p>參數：C(violation重視程度), tube範圍</p>\n<p>作dual: lagrange multiplier + KKT condition<br><img data-src=\"\" alt=\"dual --\"></p>\n<p>在tube裡面的點：B=0<br>=&gt; 只要tube夠寬，B為sparse</p>\n<h3 id=\"Linear-SVM-Summary\"><a href=\"#Linear-SVM-Summary\" class=\"headerlink\" title=\"Linear, SVM Summary\"></a>Linear, SVM Summary</h3><p><img data-src=\"\" alt=\"linear\"></p>\n<p><img data-src=\"\" alt=\"SVM\"><br>first row: less used due to worse performance<br>third row: less used due to dense B<br>fourth row: popular in LIBSVM</p>\n<h2 id=\"Chap07-Blending-and-Bagging\"><a href=\"#Chap07-Blending-and-Bagging\" class=\"headerlink\" title=\"Chap07 Blending and Bagging\"></a>Chap07 Blending and Bagging</h2><p>Selection: rely on only once hypothesis<br>Aggregation: mix or combine hypothesiss<br>select trust-worthy from their usual performance<br>=&gt; validation<br>mix the prediction =&gt; vote with different weight of ballot<br>combine predictions conditionally(when some situation, give more ballots to friend t)</p>\n<p><img data-src=\"\" alt=\"real function\"></p>\n<p>Aggregation可做到：</p>\n<ol>\n<li>feature transform(?), 將hypothesis變強</li>\n<li>regularization(?)<br>控制 油門 和 煞車<br><img data-src=\"\" alt=\"two lines\"></li>\n</ol>\n<p>uniform blending: 一種model一票，取平均<br>證明可以比原本的Eout小: <img data-src=\"\" alt=\"\"></p>\n<p>一個演算法A的表現，可以用其hypothesis set中的”共識”來表示，等於共識的表現，加上共識的變異數，uniform blending就是將某些在A的hypothesis取平均(變成新的演算法A’)來減少A’的變異數<br>expected performance of A = expected deviation to consensus + performance of consensus</p>\n<p>linear blending: 加權(線性)平均，權重&gt;0<br><img data-src=\"\" alt=\"linear bledning for regression\"><br>求類似linear regression的式子: 兩段式學習，先算出許多g，再做  linear regression -&gt; 得到答案G<br>限制：權重a&gt;0 -&gt; 將error rate大的model反過來用(error rate = 99%, 取其相反答案即可將error rate = 1%)   </p>\n<p>any blending(stacking): 可用non-linear model(???)</p>\n<pre><code>算出g1-, g2- ...   \nphi-1 = (g1-, g2-, ...)   \ntransform validation data to Z = (phi-1(x), y)   \ncompuate g = AnyModel(Z, Y)   \nreturn G = g(phi(x))\nphi = (g1, g2 ... )</code></pre><p>比較：linear blending</p>\n<pre><code>compuate a = AnyModel(Z, Y)   \nreturn G = a * phi(x)</code></pre><p>learning: 邊學邊合，</p>\n<p>bootstrapping: 從有限的資料模擬出新的資料<br>bootstrap data: 從原本資料選擇N筆資料(可重複)<br>Virtual aggregation<br>bootstrap aggregation(bagging): 由bootstrap data訓練g，而非原資料<br>-&gt; meta algorithm for [base algorithm(可使用不同演算法)]</p>\n<p><img data-src=\"\" alt=\"BAGGING pocket in action\"></p>\n<h2 id=\"Chap08-Adaptive-Boosting\"><a href=\"#Chap08-Adaptive-Boosting\" class=\"headerlink\" title=\"Chap08 Adaptive Boosting\"></a>Chap08 Adaptive Boosting</h2><p>教小學生辨認蘋果:<br>由一個演算法提供[會混淆的資料]<br>由其他hypothesis提出一個不同的小規則來區分</p>\n<p>給不同的data權重，會混淆的占較大比例，取min Ein = avg(Wn * err(xn, yn))，可用SVM, lin_reg, log_reg解Wn</p>\n<p>gt = argmin(sum(ut * err))<br>gt+1 = argmin(sum(ut+1 * err))</p>\n<p>找完gt後，gt+1應該要找和gt不相似的-&gt;找ut+1使gt的err rate接近0.5(隨機)。<br><img data-src=\"\" alt=\"construct to make gt random-like\"></p>\n<p>err rate = 錯誤資料權重和 / (錯誤資料權重和 + 正確資料權重和) = 1/2<br>=&gt; 希望 正確資料權重和 = 錯誤資料權重和<br>在gt中正確的資料, 權重要乘(err rate)<br>在gt中錯誤的資料, 權重要乘(1-err rate)<br>如此一來兩者之和將會相等</p>\n<p>若scale factor = S = sqrt((1-err rate) / err rate)<br>incorrect *= S<br>correct *= 1/S<br>若 S&gt;1:<br>→ err rate &lt;= 1/2<br>→ incorrect↑, correct↓, close to 1/2</p>\n<p><img data-src=\"\" alt=\"preliminary algorithm\"><br>u1 可設所有為1/N，得到min Ein<br>G 設uniform會使成績變差</p>\n<p>Adaptive Boosting(皮匠法)<br><img data-src=\"\" alt=\"ADA BOOST\"><br>邊做邊算at</p>\n<p>希望愈好的gt，at愈大<br>-&gt; 設at = ln(St) (S = scale)<br>if(err rate == 1/2) -&gt; St = 1 -&gt; at = 0<br>if(err rate == 0) -&gt; St = inf -&gt; at = inf</p>\n<p>只要err rate &lt; 1/2 , 就可以參與投票：群眾的力量</p>\n<p>adapative boosting 的 algorithm 選擇(不需強演算法):<br>decision stump: 三個參數：which feature, threshold(線), direction(ox)，可以使Ein &lt;= 1/2</p>\n<h2 id=\"Chap09-Decision-Tree\"><a href=\"#Chap09-Decision-Tree\" class=\"headerlink\" title=\"Chap09 Decision Tree\"></a>Chap09 Decision Tree</h2><p><img data-src=\"\" alt=\"\"></p>\n<p>Traditional learning model that realize conditional aggregation<br>模仿人類決策過程</p>\n<p>Path View:<br>G = sum(q * g)<br>q = condition (is x on this path?)<br>g = base hypothesis, only constant, leaf in tree</p>\n<p>Recursive View:<br>G(x) = sum([b(x) == c] * Gc(x))<br>G: full tree<br>b: branching criteria<br>Gc: sub-tree hypothesis</p>\n<p>advantage: human-explainable, simple, efficient, missing feature handle, categorical features easily, multiclass easily<br>disadvantage: heuristic, little theoretical<br>Ex. C&amp;RT, C4.5, J48…</p>\n<p><img data-src=\"\" alt=\"basic decision tree algo\"><br>four choices: number of branches, branching<br>criteria, termination criteria, &amp; base hypothesis</p>\n<p>C&amp;RT(Classification and Regression Tree):<br>Tree which is fully-grown with constant leaves<br>C = 2(binary tree)，可用decision stump<br>gt(x) = 在此分類下output最有可能(出現最多次的yn or yn平均)<br>-&gt; 分得愈純愈好(同一類的output皆相同)</p>\n<p><img data-src=\"\" alt=\"more simple choices - argmin\"><br>impurity = 變異數 or 出現最多次的yn的比率<br><img data-src=\"\" alt=\"for classification error\"><br>popular to use :<br>Gini for classification<br>regression error for regression</p>\n<p><img data-src=\"\" alt=\"basic C&amp;RT\"><br>terminate criteria:</p>\n<ol>\n<li>all yn is the same: impurity = 0</li>\n<li>all xn the same: cannot cut</li>\n</ol>\n<p>if all xn different: Ein = 0<br>low-level tree built with small D -&gt; overfit </p>\n<p>regularizer: number of leaves<br>argmin(Ein(G) + c * number of leaves(G))<br>實作：一次剪一片葉子，選最好的  </p>\n<p>相較數字的feature, 處理類型問題較簡單  </p>\n<p>Surrogate(代理) branch:<br>找一些與最好切法相近的，若data features missing, 則使用之</p>\n<p><img data-src=\"\" alt=\"圖\"><br>與adaboost相比：片段切割，只在自身subtree切</p>\n<h2 id=\"Chap10-Random-Forest\"><a href=\"#Chap10-Random-Forest\" class=\"headerlink\" title=\"Chap10 Random Forest\"></a>Chap10 Random Forest</h2><p>Random Forest = bagging + fully-grown random-subspace random-combination C&amp;RT decision tree</p>\n<p>highly parallel, 減少 decision tree的variance  </p>\n<h3 id=\"增加decision-tree-diversity\"><a href=\"#增加decision-tree-diversity\" class=\"headerlink\" title=\"增加decision tree diversity\"></a>增加decision tree diversity</h3><ol>\n<li>random sample features from x(random subspace of X)</li>\n</ol>\n<p>-&gt; efficient, can be used for any learning models<br>10000個features, 只用100個維度來learn</p>\n<ol start=\"2\">\n<li>將 x 作 低維度random projection -&gt; 產生新的feature(斜線切割), random combination</li>\n</ol>\n<h3 id=\"Out-of-bag\"><a href=\"#Out-of-bag\" class=\"headerlink\" title=\"Out-of-bag\"></a>Out-of-bag</h3><p>out-of-bag: not sampled after N drawings<br>N個data抽N次，沒被抽到機率 ~= 1/e<br>=&gt; 將沒抽到的DATA作g的validation(通常不做，因為g只為G的其中之一)<br>=&gt; 將沒抽到的DATA作G的validation，Eoob = sum(err(G-(xn))) (G-不包含用到xn的g)<br><img data-src=\"\" alt=\"Eoob(G)\"><br>Eoob: self-validation</p>\n<h3 id=\"Feature-Selection\"><a href=\"#Feature-Selection\" class=\"headerlink\" title=\"Feature Selection\"></a>Feature Selection</h3><p>want to remove redundant, irrelevant features…</p>\n<p><strong>learn a subset-transform</strong> for the final hypothesis</p>\n<p>advantage: interpretability, remove ‘feature noise’, efficient<br>disadvantage: total computation time increase, ‘select feature overfit’, mis-interpretability(過度解釋)</p>\n<p>decision tree: built-in feature selection</p>\n<p>idea: rate importance of every features<br>linear model: 看w的大小<br>non-linear model: not easy to estimate</p>\n<p>idea: random test<br>put some random value into feature, check performance↓，下降愈多代表愈重要</p>\n<p>random value </p>\n<ul>\n<li>by original P(X = x)</li>\n<li>bootstrap, <strong>permutation</strong></li>\n</ul>\n<p>performance: 算很久<br>importance(i) = Eoob(G, D) - Eoob(G, Dp) (Dp = data with permutation in xn_i)</p>\n<p><img data-src=\"\" alt=\"strength-correlation\"><br>strength-correlation decomposition<br>s = average voting margin(投票最多-投票第二多…) with G<br>p = gt之間的相似度<br>bias-variance decomposition</p>\n<h2 id=\"Chap11-Gradient-Boost-Decision-Tree\"><a href=\"#Chap11-Gradient-Boost-Decision-Tree\" class=\"headerlink\" title=\"Chap11 Gradient Boost Decision Tree\"></a>Chap11 Gradient Boost Decision Tree</h2><h2 id=\"Chap12-Neural-Network\"><a href=\"#Chap12-Neural-Network\" class=\"headerlink\" title=\"Chap12 Neural Network\"></a>Chap12 Neural Network</h2>",
            "tags": [
                "機器學習"
            ]
        },
        {
            "id": "http://gitqwerty777.github.io/MLfoundation2/",
            "url": "http://gitqwerty777.github.io/MLfoundation2/",
            "title": "機器學習基石(下)",
            "date_published": "2014-10-31T07:45:45.000Z",
            "content_html": "<h2 id=\"Chap09-Linear-Regression\"><a href=\"#Chap09-Linear-Regression\" class=\"headerlink\" title=\"Chap09 Linear Regression\"></a>Chap09 Linear Regression</h2><ul>\n<li>將w向量(參數)最佳化<ul>\n<li>直接用計算出的 Wx</li>\n<li>perceptron → output 只有 +1/-1</li>\n<li>regression → output 為數字</li>\n</ul>\n</li>\n<li>找最少誤差 <img data-src=\"/img/ML/einout2.png\" alt=\"\"><ul>\n<li>wx 和 y 的誤差平方</li>\n<li>Ein: 取平均, Eout: 取期望值 </li>\n<li><img data-src=\"/img/ML/residual.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<a id=\"more\"></a>\n\n<p>轉換成矩陣形式(最後一行的X, y, w都是矩陣)<br><img data-src=\"/img/ML/einmatrix.png\" alt=\"\"></p>\n<p>Ein(w)函數性質  </p>\n<ul>\n<li>continuous</li>\n<li>differeniable</li>\n<li>convex(凸)     </li>\n<li>可以找到最低點(使Ein微分 = 0的w) <img data-src=\"/img/ML/ein=0.png\" alt=\"\"><ul>\n<li>因為 A 為 $X^tX$ 的形式，必為symmetric matrix，可直接做微分 <img data-src=\"/img/ML/vectorwdiff.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>計算W <img data-src=\"/img/ML/invertiblesingular.png\" alt=\"\">   </p>\n<ul>\n<li>若A invertible，可直接求inverse</li>\n<li>若非，則用X十字架(<span class=\"exturl\" data-url=\"aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTW9vcmUlRTIlODAlOTNQZW5yb3NlX3BzZXVkb2ludmVyc2U=\">pseudo inverse<i class=\"fa fa-external-link-alt\"></i></span>)</li>\n</ul>\n<h3 id=\"linear-regression-algorithm-easy\"><a href=\"#linear-regression-algorithm-easy\" class=\"headerlink\" title=\"linear regression algorithm(easy)\"></a>linear regression algorithm(easy)</h3><p> <img data-src=\"/img/ML/regressionalgo.png\" alt=\"linear regression algorithm\"></p>\n<p>Is it “learning algorithm”?    </p>\n<ul>\n<li>No → 直接計算所得的解</li>\n<li><strong>Yes</strong> → good Ein and Eout(finite $d_vc$), pseudo-inverse<ul>\n<li>可視為迭代進行(用矩陣一次算出)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"“Simpler-than-VC”-Guarantee\"><a href=\"#“Simpler-than-VC”-Guarantee\" class=\"headerlink\" title=\"“Simpler-than-VC” Guarantee\"></a>“Simpler-than-VC” Guarantee</h3><p>可證Ein會小於????</p>\n<p><img data-src=\"/img/ML/9-3.png\" alt=\"\"><br>預測值 $\\hat{y} = Xw = XX^{-1}y = Hy$<br>定義 hat matrix $H = XX^{-1}$ </p>\n<h4 id=\"Hat-Matrix-in-Geometry\"><a href=\"#Hat-Matrix-in-Geometry\" class=\"headerlink\" title=\"Hat Matrix in Geometry\"></a>Hat Matrix in Geometry</h4><p><img data-src=\"/img/ML/9-1.png\" alt=\"\">  </p>\n<ul>\n<li>預測值 $\\hat{y}$ 被限制在X的span上 ($\\hat{y} = WX$)</li>\n<li>最小誤差會出現在y - $\\hat{y}$ 與 span of X 垂直時</li>\n<li>H 將向量投影至span of X上</li>\n<li>I-H : 投影至與 span of X 垂直的向量 (即為誤差: y - $\\hat{y}$)<ul>\n<li>可以發現 I-H 對角線上的值之和 trace(I - H) = N - (d + 1)<ul>\n<li>其物理意義為在N維空間投影至d+1維空間</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>with noise <img data-src=\"/img/ML/9-4.png\" alt=\"\"><ul>\n<li>f(x) + noise → y<ul>\n<li>f(x) 為正確的 y</li>\n<li>noise * (I-H) = y - $\\hat{y}$</li>\n</ul>\n</li>\n<li>可算出Ein和noise的關係: N變大時, Eout↓, Ein↑, noise level收斂在σ^2 <img data-src=\"/img/ML/9-5.png\" alt=\"\"> <img data-src=\"/img/ML/9-6.png\" alt=\"\"> <img data-src=\"/img/ML/9-7.png\" alt=\"\"> <ul>\n<li>Eout可用類似方法證明</li>\n<li>expected generalization error(= Eout - Ein): 2(d+1)/N </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Usage\"><a href=\"#Usage\" class=\"headerlink\" title=\"Usage\"></a>Usage</h3><p>Run Linear Regression Algo(efficient) and set initial w = $w^T_{lin}$ to speed up the perceptron learning     </p>\n<ul>\n<li>誤差大小(看面積): Square Error &gt; 0/1 Error <img data-src=\"/img/ML/9-8.png\" alt=\"\"><ul>\n<li>regression產生的 W 比 classification 的誤差更大，但計算時間較短 <img data-src=\"/img/ML/9-9.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap-10-Logistic-Regression\"><a href=\"#Chap-10-Logistic-Regression\" class=\"headerlink\" title=\"Chap 10 Logistic Regression\"></a>Chap 10 Logistic Regression</h2><p>Heart attack prediction<br>Not every people with bad condition will have heart attack<br>→ only P(Heart attack | x) probability<br>→ look it as noise</p>\n<p>若機率 P(+1|X) &gt; 1/2，則當成 +1，其他output當作noise <img data-src=\"/img/ML/10-1.png\" alt=\"\"></p>\n<p>‘soft’ binary classification: f(x) = P(+1|x)</p>\n<ul>\n<li>ideal data: probabilty ↔ we have only noisy data(+1 or -1)</li>\n<li><strong>the same data as perceptron, but different target function</strong></li>\n</ul>\n<h3 id=\"Logistic-Hypothesis\"><a href=\"#Logistic-Hypothesis\" class=\"headerlink\" title=\"Logistic Hypothesis\"></a>Logistic Hypothesis</h3><ul>\n<li>smooth, monotonic, sigmoid(S形) function <img data-src=\"/img/ML/10-3.png\" alt=\"\"> <img data-src=\"/img/ML/10-2.png\" alt=\"\"><ul>\n<li>0 &lt;= θ(x) &lt;= 1</li>\n<li>θ(x) + θ(-x) = 1 </li>\n<li>θ(-∞) = 0, θ(0) = 0.5, θ(∞) = 1</li>\n<li>令 $h(x) = θ(w^Tx)$ <img data-src=\"/img/ML/10-4.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>Likelihood<br>if h ~= f, [h 產生 y 的機率] 接近 [f 產生 y 的機率(其值通常大於產生其他output的機率)]<br>猜測：若能找到h, 其產生的output和f很像的話(也就是和data的output很像), 那h也會和f很像 → 成功學習<br>g = argmax_h likelihood(h) <img data-src=\"/img/ML/10-6.png\" alt=\"\"></p>\n<h4 id=\"Cross-Entropy-Error\"><a href=\"#Cross-Entropy-Error\" class=\"headerlink\" title=\"Cross-Entropy Error\"></a>Cross-Entropy Error</h4><ul>\n<li>因為 h(x) = θ(wTx)<ul>\n<li>h(x) = h(-x)</li>\n</ul>\n</li>\n<li>最高可能性 = max Π h(ynxn)<ul>\n<li>Π = 連乘 </li>\n</ul>\n</li>\n<li>= max ln Π θ(ynwxn)<ul>\n<li>轉換成θ，取ln </li>\n</ul>\n</li>\n<li>= max Σ ln θ(ynwxn)<ul>\n<li>ln Π = Σ ln </li>\n</ul>\n</li>\n<li>= min 1/N x Σ -lnθ(ynwxn) <img data-src=\"/img/ML/10-7.png\" alt=\"\"><ul>\n<li>乘1/N, 加上min和負號 </li>\n</ul>\n</li>\n<li>代入θ公式, Ein = <img data-src=\"/img/ML/10-8.png\" alt=\"\"><ul>\n<li>此函式即為 Cross-Entropy Error <img data-src=\"/img/ML/10-9.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Minimize-Cross-Entropy-Error\"><a href=\"#Minimize-Cross-Entropy-Error\" class=\"headerlink\" title=\"Minimize Cross-Entropy Error\"></a>Minimize Cross-Entropy Error</h4><p>find ∇Ein(w) = 0 to find min(Ein) <img data-src=\"/img/ML/10-11.png\" alt=\"\">  </p>\n<ol>\n<li>使所有θ項都為0<ul>\n<li>only if all ynwxn &gt;&gt; 0</li>\n<li>需要linear-seqerable data</li>\n</ul>\n</li>\n<li>Σ(-ynxn) = 0<ul>\n<li>non-linear equation of w</li>\n</ul>\n</li>\n</ol>\n<p>→ 都不好算 </p>\n<p>solve it by PLA  </p>\n<ul>\n<li>若有錯則更新，正確則不變($w^t = w^{t+1}$) <img data-src=\"/img/ML/10-12.png\" alt=\"\"></li>\n<li>加入參數η，為更新的幅度倍率(本來為1) <img data-src=\"/img/ML/10-13.png\" alt=\"\"><ul>\n<li>v 代表原本的式子</li>\n<li>用 (xn, yn) 更新的大小： $θ(-y_nw^Tx_n)$</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"PLA-smoothing\"><a href=\"#PLA-smoothing\" class=\"headerlink\" title=\"PLA smoothing\"></a>PLA smoothing</h3><p><img data-src=\"/img/ML/10-14.png\" alt=\"\">   </p>\n<ul>\n<li>更新時就是在往Ein較低的方向走<ul>\n<li>v為方向, η為幅度 </li>\n</ul>\n</li>\n<li>Greedy<ul>\n<li>每次更新時調整η, 使Ein最小 <img data-src=\"/img/ML/10-17.png\" alt=\"\"><ul>\n<li>η夠小的時候，可用泰勒展開式 <img data-src=\"/img/ML/10-15.png\" alt=\"\"></li>\n<li>估算出的greedy更新公式 <img data-src=\"/img/ML/10-16.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Gradient Descent <img data-src=\"/img/ML/10-18.png\" alt=\"\">   <ul>\n<li>最優的v是與梯度相反的方向，如果一條直線的斜率k&gt;0，說明向右是上升的方向，應該向左走 </li>\n<li>距離谷底較遠（位置較高）時，步幅(η)大些比較好；接近谷底時，步幅小些比較好<ul>\n<li>梯度的數值大小間接反映距離谷底的遠近 <img data-src=\"/img/ML/10-19.png\" alt=\"\"></li>\n<li>希望步幅與梯度大小成正比<ul>\n<li>wt+1 ← wt - η∇Ein(wt)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Update Time  </p>\n<ul>\n<li>decide wt+1 by all data → O(N) time</li>\n<li>Can logistic regression with O(1) time per iteration(like PLA)?<br>→ use one of data instead of all data<br>→ <strong>Stochastic Gradient Descent</strong></li>\n</ul>\n<h4 id=\"Stochastic-Gradient-Descent-SGD\"><a href=\"#Stochastic-Gradient-Descent-SGD\" class=\"headerlink\" title=\"Stochastic Gradient Descent (SGD)\"></a>Stochastic Gradient Descent (SGD)</h4><p><img data-src=\"/img/ML/11-7.png\" alt=\"stochastic gradient descent\">      </p>\n<ul>\n<li>隨機選其中一筆資料來對 w 進行更新<ul>\n<li>進行足夠多的更新後，平均的隨機梯度與平均的真實梯度近似相等</li>\n</ul>\n</li>\n<li>η often use 0.1</li>\n<li>compare PLA and SGD <img data-src=\"/img/ML/11-8.png\" alt=\"\"><ul>\n<li>SGD is more flexible than PLA</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap-11-Linear-Models\"><a href=\"#Chap-11-Linear-Models\" class=\"headerlink\" title=\"Chap 11 Linear Models\"></a>Chap 11 Linear Models</h2><p>can linear regression or logistic regression help linear classification? </p>\n<p><img data-src=\"/img/ML/11-1.png\" alt=\"\"><br><img data-src=\"/img/ML/11-2.png\" alt=\"\"> ys: classification correctness score</p>\n<p>Error functions <img data-src=\"/img/ML/11-3.png\" alt=\"\">  </p>\n<ul>\n<li>small 0/1 error $\\nRightarrow$ small square error</li>\n<li>small err SQR → small err 0/1</li>\n<li>small Err CE → small Err 0/1<ul>\n<li>** small cross entropy error implies small classification error **<img data-src=\"/img/ML/11-4.png\" alt=\"\"> <img data-src=\"/img/ML/11-5.png\" alt=\"\"></li>\n<li>small err SCE(scaled cross entropy) ↔ small err 0/1 </li>\n</ul>\n</li>\n</ul>\n<p><img data-src=\"/img/ML/11-6.png\" alt=\"\">   </p>\n<ol>\n<li>PLA<ol>\n<li>優點：在數據線性可分時高效且準確</li>\n<li>缺點：只有在數據線性可分時才可行，否則需要借助POCKET算法（沒有理論保證）</li>\n</ol>\n</li>\n<li>linear regression<ol>\n<li>優點：最簡單的優化（直接利用矩陣運算工具）</li>\n<li>缺點：ys 的值較大時，與 0/1 error 相差較大</li>\n<li>線性回歸得到的結果w可作為其他算法的初始值</li>\n</ol>\n</li>\n<li>logistic regression<ol>\n<li>優點：比較容易優化（梯度下降）</li>\n<li>缺點：ys 是非常小的負數時，與 0/1 error 相差較大</li>\n<li>實際中，logistic回歸用於分類的效果優於線性回歸的方法和POCKET算法</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"Multiclass-Classification-meta-algorithms\"><a href=\"#Multiclass-Classification-meta-algorithms\" class=\"headerlink\" title=\"Multiclass Classification - meta algorithms\"></a>Multiclass Classification - meta algorithms</h3><ol>\n<li>One-Versus-All (OVA) Decomposition<ol>\n<li>對每個分類做logistic regression(共N個)，選分數y最高的</li>\n<li>優點：prediction有效率，學習時可平行處理</li>\n<li>缺點：output種類很多時，數據往往非常不平衡(x 遠大於 o)，會嚴重影響訓練準確性</li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTXVsdGlub21pYWxfbG9naXN0aWNfcmVncmVzc2lvbg==\">multinomial logistic regression<i class=\"fa fa-external-link-alt\"></i></span> 考慮了這個問題</li>\n</ol>\n</li>\n<li>One versus One (OVO) <img data-src=\"/img/ML/11-9.png\" alt=\"\"><ol>\n<li>共有 N(N-1)/2 個 perceptron，投票決定</li>\n<li>優點： training有效率(每個perceptron較小), 可以使用 binary classification 的方法</li>\n<li>缺點： O(K^2) space</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"Chap-12-Nonlinear-Transformation\"><a href=\"#Chap-12-Nonlinear-Transformation\" class=\"headerlink\" title=\"Chap 12 Nonlinear Transformation\"></a>Chap 12 Nonlinear Transformation</h2><p>座標系轉換  </p>\n<ul>\n<li>將非線性的h(x)轉成線性 <ul>\n<li>circular separable in X → linear separable in Z <img data-src=\"/img/ML/12-2.png\" alt=\"\">  <img data-src=\"/img/ML/12-1.png\" alt=\"\"> </li>\n<li>可在X做出任何二次曲線的Z <img data-src=\"/img/ML/12-3.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>transform data X to Z to train easily <img data-src=\"/img/ML/12-4.png\" alt=\"\"><ul>\n<li>(xn, yn) → (zn = ϕ(xn), yn)</li>\n<li>train w by (z, y)</li>\n<li>g(x) = sign(ϕ(x)w) (= sign(wz))</li>\n</ul>\n</li>\n<li>代價<ul>\n<li>O(Q^d)<ul>\n<li>Q次方座標系, d個參數(x, y)</li>\n</ul>\n</li>\n<li>$d_{vc}$ 隨 Q 成長</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>力量愈強，代價愈大(可能會overfit)(見Chap13)</p>\n</blockquote>\n<p>有效學習的條件</p>\n<ol>\n<li>Ein(g) 約等於 Eout(g)</li>\n<li>Ein(g)足夠小</li>\n</ol>\n<p>當模型很簡單時（dvc 很小），我們更容易滿足1. 而不容易滿足2. ；反之，模型很複雜時（dvc很大），更容易滿足2. 而不容易滿足1.<br>→ 次方愈高，hypothesis set 包含愈多、愈複雜，Eout更偏離，也對數據擬合得更充分，Ein 更小 <img data-src=\"/img/ML/12_1.png\" alt=\"\"> </p>\n<p>安全的方法: 先算低次方, 若結果已足夠好就不用繼續尋找<br>實務上的機器學習，通常都不會使用太高維度的learning</p>\n<blockquote>\n<p>linear model first: simple, efficient, safe, and workable!</p>\n</blockquote>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL3poLndpa2lwZWRpYS5vcmcvemgtdHcvJUU1JThCJTkyJUU4JUFFJUE5JUU1JUJFJUI3JUU1JUE0JTlBJUU5JUExJUI5JUU1JUJDJThG\">Legendre Polynomials Transform<i class=\"fa fa-external-link-alt\"></i></span>: 互為正交的函式(?)，用來做transform效果較好</p>\n<h2 id=\"Chap-13-Overfitting\"><a href=\"#Chap-13-Overfitting\" class=\"headerlink\" title=\"Chap 13 Overfitting\"></a>Chap 13 Overfitting</h2><p>overfitting: *<em>lower Ein, higher Eout *</em><br><img data-src=\"/img/ML/13_01.png\" alt=\"\"><br>右側為overfitting, 左側為underfitting</p>\n<h3 id=\"Case-Study\"><a href=\"#Case-Study\" class=\"headerlink\" title=\"Case Study\"></a>Case Study</h3><p>target function <img data-src=\"/img/ML/12_4.png\" alt=\"\"><br>try 2nd order and 10th order function <img data-src=\"/img/ML/12_5.png\" alt=\"\">     </p>\n<ul>\n<li>左側： H10 performance is not good even if original function is 10th-order</li>\n<li>右側： even if no noise, there are still overfitting in H10<ul>\n<li>hypothesis complexity acts like noise</li>\n</ul>\n</li>\n<li>philosophy: 以退為進<ul>\n<li>絕聖棄智，其效百倍，絕巧棄利，error無有</li>\n</ul>\n</li>\n</ul>\n<p>雖然H10在N大的時候Eout較低，但在N小的時候Eout非常大 <img data-src=\"/img/ML/12_6.png\" alt=\"\"><br>→ 資料不夠多(N小)的時候，不能用太複雜的hypothesis</p>\n<p>實驗 noise 對 overfit 的影響  </p>\n<ul>\n<li>noise ε with variance σ^2<ul>\n<li>normal distributed iid</li>\n<li>red area has more overfit <img data-src=\"/img/ML/13_1.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>造成overfit的原因  </li>\n</ul>\n<ol>\n<li>deterministic noise<ol>\n<li>最好的hypothesis和target function的差異(depends on H)</li>\n<li>hypothesis complexity愈大，deterministic noise愈小</li>\n</ol>\n</li>\n<li>stochastic noise<ol>\n<li>N改變時，noise level對noise的影響  </li>\n<li>不是固定值、不能改善</li>\n<li>example: sample error</li>\n</ol>\n</li>\n</ol>\n<p>overfit的四個原因  </p>\n<ol>\n<li>data size ↓</li>\n<li>stochastic noise ↑</li>\n<li>deterministic noise ↑</li>\n<li>hypothesis set的power ↑</li>\n</ol>\n<p>類比成開車 <img data-src=\"/img/ML/13-2.png\" alt=\"\"></p>\n<p><strong>Overfit是很常發生的！</strong></p>\n<p>Data Cleaning/Pruning:<br>對於有些”奇怪”的data   </p>\n<ol>\n<li>改成自己認為是對的output (data cleaning)</li>\n<li>移除資料 (data pruning)</li>\n</ol>\n<p>Data Hinting:<br>用已有的知識處理原有的data，產生新的data(不是偷看！)，適合於資料量不足時(Ex. 手寫辨識，可稍微旋轉、平移)，要注意新data的比例是否符合現實情況</p>\n<h2 id=\"Chap14-Regularization\"><a href=\"#Chap14-Regularization\" class=\"headerlink\" title=\"Chap14 Regularization\"></a>Chap14 Regularization</h2><p>Regularization： 設條件(constraint) 降低 hypothesis set 的 complexity</p>\n<table>\n<thead>\n<tr>\n<th>對H10改動(= H2’)</th>\n<th>complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>w3~w10為0</td>\n<td>H2’ == H2</td>\n</tr>\n<tr>\n<td>w0~w10其中3個不為0</td>\n<td>H2 &lt; H2’ &lt; H10</td>\n</tr>\n<tr>\n<td>wi的和小於一固定值  H(C)=sum(w^2)&lt;=C</td>\n<td>soft and smooth structure, e.g. H(0) &lt; H(11.26) &lt; H(∞) = H10</td>\n</tr>\n</tbody></table>\n<p>限制參數大小： NP-hard to solve <img data-src=\"/img/ML/13-3.png\" alt=\"\"></p>\n<p>Lagrange Multiplier <img data-src=\"/img/ML/13-4.png\" alt=\"\">   </p>\n<ul>\n<li>min(Ein)：朝梯度的反方向<ul>\n<li>-▽Ein(W)為更新的向量</li>\n</ul>\n</li>\n<li>W為原點到指定點的向量</li>\n<li>只找出在紅色圓(限制)內的最佳解，即紅色圓上與$W_{lin}$最近的點<ul>\n<li>$W_{lin}$為linear regression的解</li>\n<li>W與-▽Ein(W)平行的時候 <img data-src=\"/img/ML/13-5.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>可以藉由設不同的 λ 來產生 W，此時 λ 和 H(C) 的 C 相似，用來限制參數    </p>\n<ul>\n<li>Ridge Regression (similar to linear regression) <img data-src=\"/img/ML/13-6.png\" alt=\"\"></li>\n<li>Augmented Error  <ul>\n<li>solve min(Eaug) (unconstrained) is easier than solve min(Ein)(constrained) </li>\n<li>積分後得到regularizer$w^Tw$ <img data-src=\"/img/ML/13-01.png\" alt=\"\"></li>\n<li>wREG = argmin(w) Eaug(w)</li>\n<li>weight-decay<ul>\n<li>Penalize large weights using penalties</li>\n<li>λ↑ → perfer shorter w → effective C↓</li>\n</ul>\n</li>\n<li>λ 對應到 C <img data-src=\"/img/ML/13-9.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>只需一點regularization就有效 <img data-src=\"/img/ML/13-8.png\" alt=\"\"> </p>\n<p>regularizer只限制單一hypothesis的complexity，不像VC bound整個hypothesis set都限制，所以Eaug比Ein更接近Eout <img data-src=\"/img/ML/14-1.png\" alt=\"\"></p>\n<p>Effective VC Dimension of Eaug  </p>\n<ul>\n<li>dVC(H) = d + 1 </li>\n<li>實際的 dvc 更小(被λ限制)，但不好證明 <img data-src=\"/img/ML/14-2.png\" alt=\"\"></li>\n</ul>\n<p>General Regularizers <img data-src=\"/img/ML/14-3.png\" alt=\"\">    </p>\n<ul>\n<li>target-dependent<ul>\n<li>用target function的性質來限制</li>\n</ul>\n</li>\n<li>plausible(合理的)<ul>\n<li>預期比較平滑、簡單的hypothesis，因為noise是較不平滑的</li>\n<li>L1(sparsity regularizer): regularizer = Σ|wq|</li>\n</ul>\n</li>\n<li>friendly(easy to use)<ul>\n<li>L2(weight-decay regularizer): regularizer = Σ$w_q^2$</li>\n</ul>\n</li>\n<li>comparison: error → user-dependent, plausible, friendly</li>\n<li>augmented error = error + regulizer</li>\n</ul>\n<p><img data-src=\"/img/ML/14-4.png\" alt=\"L1 and L2\"> L1 useful when need sparse solution(有許多零的w, 因w最終會落到正方形的頂點)，L1即表示限制函數為一次方 </p>\n<p>noise愈多，需要的regularization愈多 ↔ more bumpy road, putting brakes more <img data-src=\"/img/ML/14-5.png\" alt=\"\"></p>\n<p>Conclusion:<br>正規化用來減少hypothesis的complexity，避免overfit，用wTw作regulizer(L2)，以λ為參數調整正規化的程度(即L2圓的大小，L1正方形的大小)，通常λ不會太大</p>\n<h2 id=\"Chap15-Validation\"><a href=\"#Chap15-Validation\" class=\"headerlink\" title=\"Chap15 Validation\"></a>Chap15 Validation</h2><p>So Many Models can choose, so use validation to check which is good choice</p>\n<p>selecting by E_in is dangerous(can’t reflect Eout)<br>selecting by E_test is infeasible and cheating(not easy to get test data)</p>\n<p>$E_{val}$: legal cheating     </p>\n<ul>\n<li>將data分成train和validation二部分</li>\n<li>用train學習，用valid測試</li>\n</ul>\n<p>在許多切割(fold)之中，找$E_{val}$最小的hypothesis，並用這個hypothesis和<strong>全部的data</strong>算出g <img data-src=\"/img/ML/15-1.png\" alt=\"\">   </p>\n<ul>\n<li>$g_m^{-}$ 為 validation data 算出的g <img data-src=\"/img/ML/14-7.png\" alt=\"\"> </li>\n<li>find balance of validation data size <img data-src=\"/img/ML/15-2.png\" alt=\"\"><ul>\n<li>leave-one-out cross validation ($E_{loocv}$)<ul>\n<li>每次只用一個資料作validation(K = 1)</li>\n<li>often called ‘almost unbiased estimate of Eout’ <img data-src=\"/img/ML/15-3.png\" alt=\"\">  </li>\n<li>缺點：計算太多(一個model要train N次, N為資料個數)</li>\n<li>改善：切成n塊(通常5fold, 10fold)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>選擇 - 先選要測試的models，再用validation選出最好的     </p>\n<ol>\n<li>all training models: select among hypotheses(初賽)</li>\n<li>all validation schemes: select among finalists(複賽)</li>\n<li>all testing methods: just evaluate</li>\n<li>Still use <strong>test result(之前沒用過的data)</strong> for final benchmark, not best validation result</li>\n</ol>\n<h2 id=\"Chap16-Three-Learning-Principles\"><a href=\"#Chap16-Three-Learning-Principles\" class=\"headerlink\" title=\"Chap16 Three Learning Principles\"></a>Chap16 Three Learning Principles</h2><h3 id=\"Occam’s-Razor\"><a href=\"#Occam’s-Razor\" class=\"headerlink\" title=\"Occam’s Razor\"></a>Occam’s Razor</h3><p>An explanation of the data should be made as simple as possible, but no simpler<br>用最簡單且有效的方法解釋資料<br><img data-src=\"/img/ML/16-1.png\" alt=\"\"><br>因為愈簡單的H 愈難分資料 → 可以分開資料時，有顯著性(若是用複雜模型，分開是很容易的)<br>→ linear first, always ask whether overfitting</p>\n<h3 id=\"Sampling-Bias\"><a href=\"#Sampling-Bias\" class=\"headerlink\" title=\"Sampling Bias\"></a>Sampling Bias</h3><p>抽樣誤差：抽樣非真正隨機<br>Ex. 1948電話民調，但電話當時昂貴<br>movie recommend system: When data have time sequential, should emphasize later data, do not use random data</p>\n<h3 id=\"Data-Snooping\"><a href=\"#Data-Snooping\" class=\"headerlink\" title=\"Data Snooping\"></a>Data Snooping</h3><p>偷看資料(機器學習 → 人腦學習)，會包含大腦所花的complexity <img data-src=\"/img/ML/16-2.png\" alt=\"\"> <img data-src=\"/img/ML/16-3.png\" alt=\"\"> </p>\n<p>paper1: H1 works well on data D<br>paper2: find H2 and <strong>publish if better than H1 on D</strong><br>….<br>→ bad generalization, cause overfit (if you torture the data long enough, it will confess)<br>→ 解決方法：不要先看paper，先提出自己的方法，再和已發表的方法比較</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>Three Related Fields<br><img data-src=\"/img/ML/16-4.png\" alt=\"\"><br>Three Theoretical Bounds<br><img data-src=\"/img/ML/16-5.png\" alt=\"\"><br>Three Linear Models<br><img data-src=\"/img/ML/16-6.png\" alt=\"\"><br>Three Key Tools: Feature Transform, Regularization, Validation<br><img data-src=\"/img/ML/16-7.png\" alt=\"\"><br>Three Future Directions(in <a href=\"http://gitqwerty777.github.io/MLtechnique/\">ML techniques</a>)<br><img data-src=\"/img/ML/16-8.png\" alt=\"\">  </p>\n<p>End~~</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5kb3ViYW4uY29tL2RvdWxpc3QvMzM4MTg1My8=\">http://www.douban.com/doulist/3381853/<i class=\"fa fa-external-link-alt\"></i></span><br><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5jc2llLm50dS5lZHUudHcvfmh0bGluL2NvdXJzZS9tbDE0ZmFsbC8=\">HTLin講義<i class=\"fa fa-external-link-alt\"></i></span><br><span class=\"exturl\" data-url=\"aHR0cHM6Ly9jbGFzcy5jb3Vyc2VyYS5vcmcvbnR1bWxvbmUtMDAy\">Coursera<i class=\"fa fa-external-link-alt\"></i></span></p>\n",
            "tags": [
                "機器學習"
            ]
        },
        {
            "id": "http://gitqwerty777.github.io/computer-gaming/",
            "url": "http://gitqwerty777.github.io/computer-gaming/",
            "title": "電腦對局理論",
            "date_published": "2014-09-26T11:41:48.000Z",
            "content_html": "<!-- RENEW: -->\n\n<blockquote>\n<p>註：此為2014年版，且只寫到第八章(因為教授只考到這)</p>\n</blockquote>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><h3 id=\"學習電腦對局的用處\"><a href=\"#學習電腦對局的用處\" class=\"headerlink\" title=\"學習電腦對局的用處\"></a>學習電腦對局的用處</h3><ol>\n<li>電腦愈聰明，對人類愈有用</li>\n<li>電腦學得的技巧讓人學習</li>\n</ol>\n<h3 id=\"為何學棋局\"><a href=\"#為何學棋局\" class=\"headerlink\" title=\"為何學棋局\"></a>為何學棋局</h3><ol>\n<li>容易辨別輸贏</li>\n<li>規則簡單(先備知識少)</li>\n</ol>\n<a id=\"more\"></a>\n\n<h3 id=\"圖靈測試-Turing-test\"><a href=\"#圖靈測試-Turing-test\" class=\"headerlink\" title=\"圖靈測試(Turing test)\"></a>圖靈測試(Turing test)</h3><p>If a machine is intelligent, then it cannot be distinguished from a human</p>\n<ul>\n<li>反過來利用的例子 - CAPTCHA(驗證碼): Completely Automated Public Turing test to tell Computers and Humans Apart</li>\n<li>Wolfram Alpha<ul>\n<li>knowledge base of Siri</li>\n</ul>\n</li>\n</ul>\n<p>Problems  </p>\n<ul>\n<li>Are all human behaviors intelligent?</li>\n<li>Can human perform every possible intelligent behavior?<br>→ Human Intelligence 和 Intelligence 並不完全相同</li>\n</ul>\n<h3 id=\"改變目標\"><a href=\"#改變目標\" class=\"headerlink\" title=\"改變目標\"></a>改變目標</h3><ul>\n<li>From Artificial Intelligence to <strong>Machine Intelligence</strong><ul>\n<li>machine intelligence: the thing machine can do better than human do</li>\n</ul>\n</li>\n<li>From imitation of human behaviors to doing intelligent behaviors</li>\n<li>From general-purpose intelligence to <strong>domain-dependent</strong> Expert Systems</li>\n</ul>\n<h3 id=\"重大突破\"><a href=\"#重大突破\" class=\"headerlink\" title=\"重大突破\"></a>重大突破</h3><ul>\n<li>1912 - End-Game chess playing machine  </li>\n<li>~1970 - Brute Force    </li>\n<li>1975 - Alpha-Beta pruning(Knuth and Moore)   </li>\n<li>1993 - Monte Carlo  </li>\n</ul>\n<h3 id=\"無關：核心知識\"><a href=\"#無關：核心知識\" class=\"headerlink\" title=\"無關：核心知識\"></a>無關：核心知識</h3><p>用少部分的核心知識(要記得的事物)推得大多數的知識<br>Ex. 背九九乘法表推得所有多位數乘法<br>建構式數學(X)  </p>\n<h3 id=\"對局分類\"><a href=\"#對局分類\" class=\"headerlink\" title=\"對局分類\"></a>對局分類</h3><p><strong>研究遊戲之前的必要分析：分類</strong></p>\n<p>By number of players   </p>\n<ul>\n<li>Single-player games<ul>\n<li>puzzles</li>\n<li>Most of them are NP-complete<ul>\n<li>or the game will be not fun to play</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Two-player games<ul>\n<li>Most of them are either P-SPACE-complete(polynomial space usage) or exponential-time-complete<ul>\n<li>PSPACE-complete can be thought of as the hardest problems in PSPACE, solution of PSPACE-complete could easily be used to solve any other problem in PSPACE</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Multi-player games</li>\n</ul>\n<p>By state information obtained by each player(盤面資訊是否完全)  </p>\n<ul>\n<li>Perfect-information games<ul>\n<li>all players have all the information to make a correct decision</li>\n</ul>\n</li>\n<li>Imperfect-information games<ul>\n<li>some information is only available to selected players, for example you cannot see the opponent’s cards in Poker(不知對手的牌或棋子, Ex. 橋牌)</li>\n</ul>\n</li>\n</ul>\n<p>By rules of games known in advance(是否有特殊規則、是否知道對手的行動)  </p>\n<ul>\n<li>Complete-information games<ul>\n<li>rules of the game are fully known by all players in advance</li>\n</ul>\n</li>\n<li>Incomplete-information games<ul>\n<li>partial rules are not given in advance for some players(Ex. 囚犯困境賽局)</li>\n</ul>\n</li>\n</ul>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5lY29uLnVjc2IuZWR1L35nYXJyYXR0L0Vjb24xNzEvTGVjdDE0X1NsaWRlcy5wZGY=\">definition of perfect and complete information in game theory<i class=\"fa fa-external-link-alt\"></i></span></p>\n<p>By whether players can fully control the playing of the game(是否受隨機性影響)    </p>\n<ul>\n<li>Stochastic games<ul>\n<li>there is an element of chance such as dice rolls </li>\n</ul>\n</li>\n<li>Deterministic games<ul>\n<li>players have a full control over the games</li>\n</ul>\n</li>\n</ul>\n<p>Example(not fully sure):  </p>\n<ul>\n<li>perfect-information complete-information deterministic game: chinese chess, go    </li>\n<li>perfect-information complete-information stochastic game: dark chinese chess, 輪盤(Roulette)    </li>\n<li>perfect-information incomplete-information deterministic game: Prisoner’s Dilemma    </li>\n<li>perfect-information incomplete-information stochastic game: ?    </li>\n<li>inperfect-information complete-information deterministic game: ?    </li>\n<li>inperfect-information complete-information stochastic game: monopoly, bridge   </li>\n<li>inperfect-information incomplete-information deterministic game: battleship, bingo    </li>\n<li>inperfect-information incomplete-information stochastic game: most of the table/computer games</li>\n</ul>\n<h2 id=\"Chap02-Basic-Search-Algorithms\"><a href=\"#Chap02-Basic-Search-Algorithms\" class=\"headerlink\" title=\"Chap02 Basic Search Algorithms\"></a>Chap02 Basic Search Algorithms</h2><ul>\n<li>Brute force</li>\n<li>Systematic brute-force search  <ul>\n<li>Breadth-first search (BFS)  </li>\n<li>Depth-first search (DFS)  <ul>\n<li>Depth-first Iterative-deepening (DFID)  </li>\n</ul>\n</li>\n<li>Bi-directional search</li>\n</ul>\n</li>\n<li>Heuristic search: best-first search  <ul>\n<li>A*  <ul>\n<li>IDA*</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Symbol-Definition\"><a href=\"#Symbol-Definition\" class=\"headerlink\" title=\"Symbol Definition\"></a>Symbol Definition</h3><ul>\n<li>Node branching factor <code>b</code><ul>\n<li>degree</li>\n<li>number of neighbor vertexs of a node</li>\n</ul>\n</li>\n<li>Edge branching factor <code>e</code><ul>\n<li>number of connected edges of a node</li>\n</ul>\n</li>\n<li>Depth of a solution <code>d</code><ul>\n<li>最短深度, <code>D</code> 為最長深度</li>\n<li>Root深度為0</li>\n</ul>\n</li>\n<li>If <code>b</code> and <code>e</code> are average constant number, <code>e</code> &gt;= <code>b</code>(兩個點之間可能有多條線)</li>\n</ul>\n<h3 id=\"Brute-force-search\"><a href=\"#Brute-force-search\" class=\"headerlink\" title=\"Brute-force search\"></a>Brute-force search</h3><p>Used information  </p>\n<ul>\n<li>initial state</li>\n<li>method to find adjacent states</li>\n<li>goal-checking method(whether current state is goal)  </li>\n</ul>\n<p>Pure brute-force search program <img data-src=\"/img/TCG/54GbBxV.png\" alt=\"\">  </p>\n<ul>\n<li>隨機走旁邊的一個點</li>\n<li>不記憶走過的路<ul>\n<li>May take infinite time</li>\n</ul>\n</li>\n<li>Pure Random Algorithm 應用<ul>\n<li>驗證碼(e.g. 虛寶)</li>\n<li>純隨機數</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"BFS-Breadth-First-Search\"><a href=\"#BFS-Breadth-First-Search\" class=\"headerlink\" title=\"BFS(Breadth-First Search)\"></a>BFS(Breadth-First Search)</h3><p><img data-src=\"/img/TCG/mrf0Egx.png\" alt=\"\"><br>deeper(N): 回傳與N相鄰的點<br>record parent state and backtrace to Find the path </p>\n<ul>\n<li><p>Space complexity: $O(b^d)$ → Too big!</p>\n</li>\n<li><p>Time complexity: $O(b^{d-1} * e)$     </p>\n<ul>\n<li>→ costs O(e) to find deeper(N), at most check b^(d-1) times(deeper(leaf) do not return new node)</li>\n</ul>\n</li>\n<li><p>Open list: nodes that are in the queue(candidate nodes)</p>\n</li>\n<li><p>Closed list: nodes that have been explored(assure not answer, can skip)</p>\n<ul>\n<li>Need a good algorithm to check for states in deeper(N) are visited or not<ul>\n<li>Hash  </li>\n<li>Binary search</li>\n</ul>\n</li>\n<li>not need to have because it won’t guarantee to improve the performance</li>\n<li>if it is possible to have no solution, Need to store nodes that are already visited </li>\n</ul>\n</li>\n<li><p>node： open list → check is goal or not, explore(deeper) → closed list</p>\n</li>\n</ul>\n<p>Property    </p>\n<ul>\n<li>Always finds optimal solution</li>\n<li>Do not fall into loops if goal exists(always “deeper”) </li>\n</ul>\n<h4 id=\"Disk-based-algorithm\"><a href=\"#Disk-based-algorithm\" class=\"headerlink\" title=\"Disk based algorithm\"></a>Disk based algorithm</h4><p><img data-src=\"/img/TCG/i8bbMET.png\" alt=\"\"></p>\n<p>Solution for huge space complexity</p>\n<ul>\n<li>disk: store main data</li>\n<li>memory: store buffers</li>\n</ul>\n<ul>\n<li>Store open list(QUEUE) in disk<ul>\n<li><strong>Append</strong> buffered open list to disk when memory is full or QUEUE is empty</li>\n</ul>\n</li>\n<li>Store closed list in disk and maintain them as sorted<ul>\n<li><strong>Merge</strong> buffered closed list with disk closed list when memory is full   </li>\n<li>delay cheking: check node in the closed list or not before being taken from open list</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Disk-based-algorithms\"><a href=\"#Disk-based-algorithms\" class=\"headerlink\" title=\"Disk based algorithms\"></a>Disk based algorithms</h4><ul>\n<li>not too slow<ul>\n<li>read large file in sequence<ul>\n<li>queue(always retrieve at head and write at end)</li>\n</ul>\n</li>\n<li>sorting of data in disk<ul>\n<li>merge sort between disk list and buffer list</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>very slow<ul>\n<li>read file in random order(disk spinning)</li>\n</ul>\n</li>\n<li>系統為資源和效率(時間、空間、錢)的trade-off</li>\n</ul>\n<h3 id=\"DFS\"><a href=\"#DFS\" class=\"headerlink\" title=\"DFS\"></a>DFS</h3><p><img data-src=\"/img/TCG/65RmOgp.png\" alt=\"DFSalgo\">  </p>\n<ul>\n<li>performance mostly depends on <strong>move ordering</strong><ul>\n<li>If first choose the branch include the goal, find answer quick</li>\n<li>get out of long and wrong branches ASAP!</li>\n<li>implement <code>next(current, N)</code><ul>\n<li>作用：列舉出N的所有鄰居</li>\n<li>回傳下一個N的鄰居，目前列舉到current</li>\n<li>next(null, N) -&gt; return first neighbor of N</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>time complexity: $O(e^D)$<ul>\n<li>number of possible branches at depth D</li>\n</ul>\n</li>\n<li>space complexity: $O(D)$<ul>\n<li>Only need to store current path in the Stack</li>\n</ul>\n</li>\n</ul>\n<p>Property  </p>\n<ul>\n<li>need to store close list (BFS: do not need to)</li>\n<li>May not find an optimal solution</li>\n<li>Can’t properly implement on disk<ul>\n<li>very huge closed list<ul>\n<li>Use data compression or bit-operation techniques to store visited nodes</li>\n<li>Need a good heuristic to store the most frequently visited nodes to avoid swapping too often</li>\n</ul>\n</li>\n<li>need to check closed list instantly(BFS: can be delayed)</li>\n</ul>\n</li>\n<li>Can DFS be paralleled? Computer scientists fails to do so even after 30 years</li>\n<li>Most critical drawback: huge and unpredictable time complexity</li>\n</ul>\n<h3 id=\"General-skills-to-improve-searching-algorithm\"><a href=\"#General-skills-to-improve-searching-algorithm\" class=\"headerlink\" title=\"General skills to improve searching algorithm\"></a>General skills to improve searching algorithm</h3><h4 id=\"Iterative-Deepening-ID-逐層加深\"><a href=\"#Iterative-Deepening-ID-逐層加深\" class=\"headerlink\" title=\"Iterative-Deepening(ID) 逐層加深\"></a>Iterative-Deepening(ID) 逐層加深</h4><ul>\n<li>inspired from BFS(BFS = BFID)</li>\n<li>限制搜尋時的複雜度，若找不到再放寬限制</li>\n<li>prevent worse cases</li>\n</ul>\n<p>Deep First ID(DFID)     </p>\n<ul>\n<li>限制深度 <ul>\n<li>找到解立即return <img data-src=\"/img/TCG/9X2ZiRm.png\" alt=\"\"></li>\n<li><img data-src=\"/img/TCG/gmD51AT.png\" alt=\"\"></li>\n<li>time complexity using 二項式定理 <img data-src=\"/img/TCG/IfDEwFh.png\" alt=\"\"> <img data-src=\"/img/TCG/d0m27cU.png\" alt=\"\"><ul>\n<li>M(e, d) ~ $O(e^d)$ when e is sufficiently large</li>\n<li>→ no so much time penalty to use ID when e is big enough</li>\n</ul>\n</li>\n<li>關鍵：設定初始限制和限制放寬的大小</li>\n<li>always find optimal solution</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Bi-directional-search\"><a href=\"#Bi-directional-search\" class=\"headerlink\" title=\"Bi-directional search\"></a>Bi-directional search</h4><p><img data-src=\"/img/TCG/1-1.png\" alt=\"DFSdir\">  </p>\n<ul>\n<li><p><code>DFSdir(B, G, successor, i)</code>: DFS with starting states B, goal states G, successor function and <strong>depth limit i</strong>  </p>\n</li>\n<li><p><code>nextdir(current, successor, N)</code>: returns the state next to the state “current” in successor(N)</p>\n<ul>\n<li><code>deeper(current, N)</code> for forward searching<ul>\n<li>deeper(N) contains all next states of N</li>\n</ul>\n</li>\n<li><code>prev(current, N)</code> for backward searching<ul>\n<li>prev(N) contains all previous states of N<br><img data-src=\"/img/TCG/1-2.png\" alt=\"BDS\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Forward Search: store all states H</p>\n</li>\n<li><p>Backward Search: find the path from G(goal) to H at depth = limit or limit+1(for odd-lengthed solutions)  </p>\n</li>\n<li><p>also use the concept of iterative-deepening<br><img data-src=\"/img/TCG/7iBkfKB.png\" alt=\"\"></p>\n</li>\n<li><p>Time complexity: $O(e^{d/2})$</p>\n<ul>\n<li>the number of nodes visited is greatly reduced(compared with original $O(e^d)$)</li>\n</ul>\n</li>\n<li><p>Space complexity: $O(e^{d/2})$</p>\n<ul>\n<li>Pay the price of storing state depth(H)</li>\n</ul>\n</li>\n<li><p>restrict</p>\n<ul>\n<li>can’t assure to find optimal solution</li>\n<li>need to know what the goals are <ul>\n<li>bi-directional search is used when goal is known, only want to find path, like solving 15-puzzle</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Heuristic-啟發式-search\"><a href=\"#Heuristic-啟發式-search\" class=\"headerlink\" title=\"Heuristic(啟發式) search\"></a>Heuristic(啟發式) search</h3><p>Definition: criteria, methods, or principles for deciding which is the most effective to achieve some goal<br>→ By 經驗法則(so not always have optimal solution)  </p>\n<ul>\n<li>先走最有可能通往答案的state(good move ordering)<ul>\n<li>best-first algorithm : like greedy   </li>\n</ul>\n</li>\n<li>The unlikely path will be explored further(pruning)  </li>\n<li><strong>Key: how to pick the next state to explore</strong>   <ul>\n<li>need simple and effective <strong>estimate function</strong> to discriminate    </li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Heuristic-search-–-A\"><a href=\"#Heuristic-search-–-A\" class=\"headerlink\" title=\"Heuristic search – A*\"></a>Heuristic search – A*</h4><p><img data-src=\"/img/TCG/Vv8N3hj.png\" alt=\"A*\"><br>line 12: add all possible path that depth = depth + 1   </p>\n<ul>\n<li>Open list: a priorty queue(PQ) to store paths with costs</li>\n<li>Closed list: store all visited nodes with the smallest cost<ul>\n<li>Check for duplicated visits in the closed list only</li>\n<li>A node is inserted if <ul>\n<li>it has never been visited before</li>\n<li>being visited, but has smaller cost</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>Given a path P<ul>\n<li>g(P) = current cost of P</li>\n<li>h(P) = estimation of remaining path to goal(<strong>heuristic cost</strong> of P)</li>\n<li>f(P) = g(P) + h(P) is the cost function</li>\n</ul>\n</li>\n<li>Assume all costs are positive, so there is no need to check for falling into a loop  </li>\n<li>cost function所推測的cost不可超過實際的cost，否則不保證找到最佳解<ul>\n<li><strong>if h() never overestimates the actual cost to the goal</strong> (called admissible可容許), then <strong>A* always finds an optimal solution</strong></li>\n<li>證明？</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li>h(n)=0 : A* 等同 BFS</li>\n<li>h(n)&lt;目前節點到結束點的距離 : A* 演算法保證找到最短路徑, h(n)越小, 搜尋深度越深(代表花愈多時間)</li>\n<li>h(n)=目前節點到結束點的距離 : A* 演算法僅會尋找最佳路徑, 並且能快速找到結果(最理想情況)</li>\n<li>h(n)&gt;目前節點到結束點的距離 : 不保證能找到最短路徑, 但計算比較快</li>\n<li>h(n)與g(n)高度相關 : A* 演算法此時成為Best-First Search<br><span class=\"exturl\" data-url=\"aHR0cDovL2Jsb2cubWluc3RyZWwuaWR2LnR3LzIwMDQvMTIvc3Rhci1hbGdvcml0aG0uaHRtbA==\">http://blog.minstrel.idv.tw/2004/12/star-algorithm.html<i class=\"fa fa-external-link-alt\"></i></span></li>\n</ol>\n<p>Question:  </p>\n<ul>\n<li>What disk based techniques can be used?</li>\n<li>Why do we need a non-trivial h(P) that is admissible?</li>\n<li>How to design an admissible cost function?</li>\n</ul>\n<h3 id=\"DFS-with-threshold\"><a href=\"#DFS-with-threshold\" class=\"headerlink\" title=\"DFS with threshold\"></a>DFS with threshold</h3><ul>\n<li><code>DFScost(N, f, threshold)</code><ul>\n<li>starting state N </li>\n<li>cost function f</li>\n<li>cuts off a path if cost bigger than threshold </li>\n</ul>\n</li>\n</ul>\n<p><code>DFS1</code>: Use <code>next1(current,N)</code> find neighbors of N (in the order of low cost to high cost)<br><img data-src=\"/img/TCG/csd9mLf.png\" alt=\"dfs1\"><br><code>DFS2</code>: Use a priority queue instead of using a stack in <code>DFScost</code><br><img data-src=\"/img/TCG/jthjSm8.png\" alt=\"dfs2\"><br>It may be costly to maintain a priority queue</p>\n<h3 id=\"IDA-DFID-A\"><a href=\"#IDA-DFID-A\" class=\"headerlink\" title=\"IDA* = DFID + A*\"></a>IDA* = DFID + A*</h3><p>用A*的cost作為DFS的threshold<br><img data-src=\"/img/TCG/PJ2bPrX.png\" alt=\"\"> </p>\n<p>Ex. 15 puzzle<br>all posibilities: $16! \\leq 2.1 \\times 10^{13}$<br>g(P): the number of moves made so far<br>h(P): <strong>Manhattan distance</strong> between the current board and the goal<br>Manhattan distance from (i, j) to (i’, j’) is |i’ - i| + |j’ - j| (admissible)   </p>\n<h3 id=\"basic-thought-for-a-problem\"><a href=\"#basic-thought-for-a-problem\" class=\"headerlink\" title=\"basic thought for a problem\"></a>basic thought for a problem</h3><p><em>What you should think about before playing a game</em>：</p>\n<ul>\n<li>Needed to <ul>\n<li>Find an optimal solution?</li>\n<li>batch operations?</li>\n<li>disk based algorithms?</li>\n<li>Search in parallel?</li>\n</ul>\n</li>\n<li><strong>Balancing</strong> in resource usage:<ul>\n<li>memorize past results vs efforts to search again(time and space)</li>\n<li>The efforts to compute a better heuristic(time to think a heuristic?)</li>\n<li>The amount of resources spent in implementing a better heuristic and the amount of resources spent in searching(complexity of heuristic function)</li>\n</ul>\n</li>\n<li>For specific algorithm<ul>\n<li>heuristic : How to design a good and non-trivial heuristic function?</li>\n<li>DFS : How to get a better move ordering?</li>\n</ul>\n</li>\n</ul>\n<p>Can these techniques be applied to two-person game?</p>\n<h3 id=\"algorithm整理\"><a href=\"#algorithm整理\" class=\"headerlink\" title=\"algorithm整理\"></a>algorithm整理</h3><p>| Name      | Time Complexity | Space Complexity | OptimalSolution    | UseDisk | Description               |<br>| ——— | ————— | —————- | —————— | ——- |<br>| brute     | $∞$             | $O(1)$           | No                 | No      |<br>| BFS       | $O(b^d)$        | $O(b^{d-1} * e)$ | Yes                | Needed  |<br>| DFS       | $O(e^d)$        | $O(d)$           | No                 | NoNeed  |<br>| Heuristic | N\\A             | N\\A              | Yes, if admissible | –      | Ex. A*                    |<br>| BDS       | $O(e^{d/2})$    | $O(e^{d/2})$     | No                 | Needed  | DFS + bidiretional search |<br>| DFID      | $O(e^d)$        | $O(d)$           | Yes                | NoNeed  | DFS + ID                  |<br>| IDA*      | N\\A             | N\\A              | Yes                | N\\A     | DFID + A*                 |</p>\n<h2 id=\"Chap03-Heuristic-Search-with-Pre-Computed-Databases\"><a href=\"#Chap03-Heuristic-Search-with-Pre-Computed-Databases\" class=\"headerlink\" title=\"Chap03 Heuristic Search with Pre-Computed Databases\"></a>Chap03 Heuristic Search with Pre-Computed Databases</h2><p>new form of heuristic called <strong>pattern databases</strong></p>\n<ul>\n<li>If the subgoals can be divided<ul>\n<li>Can sget better admissible cost function by <strong>sum of costs of the subgoals</strong></li>\n</ul>\n</li>\n<li>Make use of the fact that computers can memorize lots of patterns<ul>\n<li>使用已經計算過的 pattern 來做出更好、更接近real cost的heuristic function </li>\n</ul>\n</li>\n</ul>\n<p>Using 15 puzzle as example <img data-src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/15-puzzle.svg/480px-15-puzzle.svg.png\" alt=\"\">  </p>\n<ul>\n<li>State space can be divided into two subsets: even and odd permutations</li>\n<li>$f_1$ is number of inversions in a permutation <code>X1X2...XN</code>  <ul>\n<li>inversion is a distinct pair Xi &gt; Xj such that i &lt; j(後面有幾個數比自己小) </li>\n<li>Example: <code>10,8,12,3,7,6,2,1,14,4,11,15,13,9,5</code> has 9+7+9+2+5+4+1+0+5+0+2+3+2+1 inversions</li>\n</ul>\n</li>\n<li>$f_2$ is the row number that empty cell is(空的那一格在哪一行)</li>\n<li>f = $f_1$ + $f_2$</li>\n<li>Slide a tile never change the parity    <ul>\n<li>Proof: skip(a lot of)</li>\n</ul>\n</li>\n</ul>\n<p>Solving Result</p>\n<ul>\n<li>1-MIPS machine</li>\n<li>30 CPU minutes in 1985 </li>\n<li>using IDA* with Manhattan distance heuristic</li>\n</ul>\n<h3 id=\"Non-additive-pattern-databases\"><a href=\"#Non-additive-pattern-databases\" class=\"headerlink\" title=\"Non-additive pattern databases\"></a>Non-additive pattern databases</h3><ul>\n<li>原本cost funtion為15片個別的distance之和，若能一次計算多片的distance？</li>\n<li>linear conflict: 靠很近不代表步數少(如[2, 1, 3, 4]交換至[1, 2, 3, 4]並不只兩步)<ul>\n<li>有可能移成pattern時，反而使其他片遠離</li>\n<li><img data-src=\"/img/TCG/4-1.png\" alt=\"linear conflict\"></li>\n</ul>\n</li>\n<li>Fringe(初級知識)<ul>\n<li>subset of selected tiles called <strong>pattern</strong><ul>\n<li>tiles not selected is “don’t-care tile”, all looked as the same</li>\n</ul>\n</li>\n<li>If there are 7 selected tiles, including empty cell  <ul>\n<li>16!/9! = 57657600 possible pattern size</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><img data-src=\"/img/TCG/4-2.png\" alt=\"prefrin\"><br>goal fringe: 選擇的方塊都和goal的位置一樣<br><img data-src=\"/img/TCG/4-3.png\" alt=\"goalfrin\"></p>\n<ul>\n<li>precompute the minimum number of moves(<strong>fringe number</strong>) to make goal fringe<ul>\n<li>goal fringe: 找給定的選擇方塊，在任何pattern中，最小需要移動成最終目標的步數</li>\n<li>We can solve it because the pattern size is relatively small</li>\n</ul>\n</li>\n<li>Pro’s<ul>\n<li>pattern size↑, fringe number↑, which means better estimation<ul>\n<li>because estimate number it is closer to the real answer    </li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Con’s    <ul>\n<li>Pattern with a larger size<ul>\n<li>consuming lots of memory and time</li>\n<li>limited by source</li>\n</ul>\n</li>\n<li>not optimal</li>\n</ul>\n</li>\n</ul>\n<p>Property   </p>\n<ol>\n<li>Divide and Conquer  </li>\n</ol>\n<ul>\n<li>Reduce a 15-puzzle problem into a 8-puzzle <img data-src=\"/img/TCG/4-4.png\" alt=\"15-8\"></li>\n<li>魔術方塊 – 分成六面</li>\n<li>Cannot easily combine<ul>\n<li>affect tiles that have reached the goal in the subproblem when solving the remains</li>\n</ul>\n</li>\n</ul>\n<ol start=\"2\">\n<li>Used as heuristic function(admissible)</li>\n</ol>\n<h3 id=\"More-than-one-patterns\"><a href=\"#More-than-one-patterns\" class=\"headerlink\" title=\"More than one patterns\"></a>More than one patterns</h3><ul>\n<li>How to Find better patterns for fringes?<ul>\n<li>→ Can we combine smaller patterns to form bigger patterns?</li>\n</ul>\n</li>\n</ul>\n<p>For different pattern databases P1, P2, P3 …  </p>\n<ul>\n<li>patterns may not be disjoint, may be overlapping</li>\n<li>The heuristic function we can use is<ul>\n<li>$h(P_1, P_2, P_3 … ) = max{h(P_1),h(P_2),h(P_3) …}$</li>\n</ul>\n</li>\n</ul>\n<p>How to make heuristics and the patterns disjoint?  </p>\n<ul>\n<li>patterns should be disjoint to add them together(see below)<ul>\n<li>Though patterns are disjoint, their costs are not disjoint<ul>\n<li>Some moves are counted more than once</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>f(P1) + f(P2) is admissible if  </p>\n<ol>\n<li>f() is disjoint with respect to P1 and P2</li>\n<li>both f(P1) and f(P2) are admissible</li>\n</ol>\n<p>For Manhattan distance heuristic  </p>\n<ol>\n<li>Each region is a tile<ul>\n<li><strong>Divide the board into several disjoint regions</strong></li>\n</ul>\n</li>\n<li>They are disjoint<ul>\n<li><strong>only count the number of moves made by each region</strong><ul>\n<li>doesn’t count cross-region moves</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p>Refinement<br>Partition the board into disjoint regions using the tiles in a region of the goal arrangement as a pattern<br><img data-src=\"/img/TCG/4-5.png\" alt=\"aabb\"><br><strong>只算每個region內的片所移動的步數和，作為新定義的fringe number</strong><br>如此一來，就可以將每個region的cost相加而保持admissible</p>\n<h3 id=\"Disjoint-pattern\"><a href=\"#Disjoint-pattern\" class=\"headerlink\" title=\"Disjoint pattern\"></a>Disjoint pattern</h3><p>A heuristic function f() is disjoint with respect to two patterns P1 and P2 if  </p>\n<ol>\n<li>P1 and P2 have no common cells</li>\n<li>The solutions corresponding to f(P1) and f(P2) do not interfere each other</li>\n</ol>\n<p>Revised fringe number f’(p): for each fringe arrangement F, the <strong>minimum</strong> number of <strong>fringe-only</strong> moves to make goal fringe</p>\n<h3 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h3><p>Solves the 15 puzzle problem using fringe that is more than <strong>2000</strong> times faster than the previous result by using the Manhattan distance  </p>\n<ul>\n<li>The average Manhattan distance is 76.078 moves in 24-puzzle    </li>\n<li>The average value for the disjoint database heuristic is 81.607 moves in 24-puzzle   </li>\n<li><strong>only small refinement on heuristic function would make performance far better</strong>  </li>\n</ul>\n<p>Other heuristics   </p>\n<ul>\n<li>pairwise distance<ul>\n<li>partition the board into many 2-tiles so that the sum of cost is <strong>maximized</strong><br>For an $n^2 - 1$ puzzle, we have $O(n^4)$ different combinations<br>using</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"What-else-can-be-done\"><a href=\"#What-else-can-be-done\" class=\"headerlink\" title=\"What else can be done?\"></a>What else can be done?</h3><ol start=\"2\">\n<li>Better way of partitioning</li>\n<li>Is it possible to generalize this result to other problem domains?</li>\n<li>Decide ratio of the time used in searching and the time used in retrieving pre-computed knowledge<ul>\n<li>memorize vs compute</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Chap-04-Two-Player-Perfect-Information-Games-Introductions\"><a href=\"#Chap-04-Two-Player-Perfect-Information-Games-Introductions\" class=\"headerlink\" title=\"Chap 04 Two-Player Perfect Information Games Introductions\"></a>Chap 04 Two-Player Perfect Information Games Introductions</h2><blockquote>\n<p>Conclusion: decision complexity is more important than state-space complexity   </p>\n</blockquote>\n<p>trade-off between <strong>knowledge-based</strong> methods and <strong>brute-force</strong> methods</p>\n<p>Domain: 2-person <strong>zero-sum games</strong> with perfect information<br>Zero-sum means one player’s loss is exactly the other player’s gain, and vice versa.</p>\n<h3 id=\"Definition\"><a href=\"#Definition\" class=\"headerlink\" title=\"Definition\"></a>Definition</h3><p>Game-theoretic value: the outcome of a game when all participants play optimally<br>Game-theoretic value for most games are unknown or are only known for some legal positions.</p>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ultra-weakly solved</td>\n<td>在初始盤面可知，遊戲中先行者或後行者誰有必勝、或必不敗之策略</td>\n</tr>\n<tr>\n<td>Weakly solved</td>\n<td>for the initial position a strategy has been determined to achieve the game-theoretic value(知道必不敗之策略為何)</td>\n</tr>\n<tr>\n<td>Strongly solved</td>\n<td>a strategy has been determined for all legal positions(任何合法情況都能知道最佳策略)</td>\n</tr>\n</tbody></table>\n<p>State-space complexity of a game: the <strong>number of the legal positions</strong> in a game(可能的盤面)<br>Game-tree complexity(decision complexity) of a game: the <strong>number of the leaf nodes</strong> in a solution search tree(可能的走法)  </p>\n<p>A fair game: the game-theoretic value is draw and both players have roughly equal probability on making a mistake.  </p>\n<ul>\n<li>Paper-scissor-stone</li>\n<li>Roll a dice and compare who gets a larger number</li>\n</ul>\n<p>Initiative(主動): the right to move first  </p>\n<ul>\n<li>A convergent game: the size of the state space decreases as the game progresses  <ul>\n<li>Example: Checkers  </li>\n</ul>\n</li>\n<li>A divergent game: the size of the state space increases as the game progresses  <ul>\n<li>Example: Connect-5 </li>\n</ul>\n</li>\n<li>A game may be convergent at one stage and then divergent at other stage.<ul>\n<li>Ex. Go, Tic-Tac-Toe</li>\n</ul>\n</li>\n</ul>\n<p>Threats are something like forced moved or moves you have little choices.<br>Threats are moves with predictable counter-moves</p>\n<h3 id=\"Classification\"><a href=\"#Classification\" class=\"headerlink\" title=\"Classification\"></a>Classification</h3><p><img data-src=\"/img/TCG/5-1.png\" alt=\"4\"></p>\n<p>Questions to be researched<br>Can perfect knowledge obtained from solved games be translated into rules and strategies which human beings can assimilate?<br>Are such rules generic, or do they constitute a multitude of ad hoc recipes?<br>Can methods be transferred between games?  </p>\n<p>Connection games<br>Connect-four (6 * 7)<br>Qubic (4 * 4 * 4)<br>Renju - Does not allow the First player to play certain moves, An asymmetric game.<br>mnk-Game: a game playing on a board of m rows and n columns with the goal of obtaining a straight line of length k.<br>Variations: First ply picks only one stone, the rest picks two stones in a ply. -&gt; Connect 6. </p>\n<p>Hex (10 * 10 or 11 * 11)<br>Exactly one of the players can win.<br>solved on a 6 * 6 board in 1994.</p>\n<p><img data-src=\"/img/TCG/5-2.png\" alt=\"Hex\"></p>\n<p>Proof on exactly one player win<br>Assume there is no winner<br><img data-src=\"/img/TCG/5-3.png\" alt=\"block\"><br>blue should totally block red at some place -&gt; blue will connect!  </p>\n<p>let R be the set of red cells that can be reached by chains from rightmost column<br>R does not contain a cell of the leftmost column; otherwise we have a contradiction<br>let N(R) be the blue cells that can be reached by chains originated from the rightmost column.<br>N(R) must contain a cell in the top and bottom row , Otherwise, R contains all cells in the First/bottom row, which is a contradiction.<br>N(R) must be connected. Otherwise, R can advance further. Hence N(R) is a blue winning chain.</p>\n<h3 id=\"Strategy-stealing-argument\"><a href=\"#Strategy-stealing-argument\" class=\"headerlink\" title=\"Strategy-stealing argument\"></a>Strategy-stealing argument</h3><p>made by John Nash in 1949<br>後手無一般化的必勝法<br>若後手有必勝法，則先手可以先隨機下一子(並無視之)，再照著後手的下法<br>後手必勝的下法包含了第一手，則再隨機下一子，將其視為第一子<br>限制：不能有和，下子不會有害，symmetric，history independent，</p>\n<p>Assume the initial board position is B0<br>f(B) has a value only when it is a legal position for the second player.<br>rev(x): interchange colors of pieces in a board or ply x.<br>always has exactly one winner  </p>\n<p>Not Solved<br>Chess DEEP BLUE beat the human World Champion in 1997<br>Chinese chess Professional 7-dan in 2007<br>Shogi<br>Claimed to be professional 2-dan in 2007<br>Defeat a 68-year old 1993 Meijin during 2011 and 2012</p>\n<p>Go<br>Recent success and breakthrough using Monte Carlo UCT based methods.<br>Amateur 1 dan in 2010.<br>Amateur 3 dan in 2011.<br>The program Zen beat a 9-dan professional master at March 17, 2012<br>  First game: Five stone handicap and won by 11 points<br>  Second game: four stones handicap and won by 20 points</p>\n<p><img data-src=\"/img/TCG/5-4.png\" alt=\"table of complexity\"></p>\n<p>possible to use heuristics to prune tremendously when the structure of the game is well studied</p>\n<p>Methods to solve games<br>Brute-force methods  </p>\n<ul>\n<li>Retrograde analysis(倒推)</li>\n<li>Enhanced transposition-table methods(?)<br>Knowledge-based methods  </li>\n<li>Threat-space search and lambda-search</li>\n<li>Proof-number search</li>\n<li>Depth-First proof-number search</li>\n<li>Pattern search<ul>\n<li>search threat patterns, which are collections of cells in a position</li>\n<li>A threat pattern can be thought of as representing the relevant area on the board<br>Recent advancements  </li>\n</ul>\n</li>\n<li>Monte Carlo UCT based game tree simulation<ul>\n<li>Monte Carlo method has a root from statistic</li>\n<li>Biased sampling</li>\n<li>Using methods from machine learning</li>\n<li>Combining domain knowledge with statistics</li>\n</ul>\n</li>\n<li>A majority vote algorithm</li>\n</ul>\n<p>low state-space complexity have mainly been solved with brute-force methods.<br>Nine Men’s Morris</p>\n<p>low game-tree-complexities have mainly been solved with knowledge-based methods.<br>by intelligent (heuristic) searching with help of databases<br>Go-Moku, Renju, and k-in-a-row games</p>\n<p>The First player has advantages.<br>Two kinds of positions<br>P-positions: the previous player can force a win.<br>N-positions: the next player can force a win.</p>\n<p>First player to have a forced win, just one of the moves that make P-position.<br>second player to have a forced win, all of the moves must lead to(造成) N-positions</p>\n<p>At small boards, the second player is able to draw or even to win for certain games.</p>\n<p>Try to obtain a small advantage by using the initiative.<br>The opponent must react adequately on the moves played by the other player.<br>Force the opponent to always play the moves you expected.</p>\n<p>Offsetting the initiative</p>\n<p>一子棋 by 張系國 棋王 -&gt; 先手優勢極大，隨著棋子增加，所需贏的步數就愈少。</p>\n<p>讓子<br>Ex. Go k = 7.5 in 2011</p>\n<p>Enforce rules so that the first player cannot win by selective patterns.<br>Ex. Renju</p>\n<p>The one-move-equalization rule: one player plays an opening move and the other player then has to decide which color to<br>play for the reminder of the game.<br>. Hex.<br>. Second-player will win.</p>\n<p>The First move plays one stone, the rest plays two stones each.<br>Can’t prove it is fair</p>\n<p>The first player uses less resource.<br>For example: using less time.<br>Ex. Chinese chess.</p>\n<p>1990’s prediction at 2000<br><img data-src=\"/img/TCG/5-5.png\" alt=\"\"><br>2000’s prediction at 2010<br><img data-src=\"/img/TCG/5-6.png\" alt=\"\"></p>\n<h2 id=\"Chap-05-Computer-chess-programming-by-Shannon\"><a href=\"#Chap-05-Computer-chess-programming-by-Shannon\" class=\"headerlink\" title=\"Chap 05 Computer chess programming by Shannon\"></a>Chap 05 Computer chess programming by Shannon</h2><p>C.E. Shannon</p>\n<ul>\n<li>1916 ~ 2001.</li>\n<li>The founding father of Information theory.</li>\n<li>The founding father of digital circuit design.</li>\n</ul>\n<p>Ground breaking paper for computer game playing: “Programming a Computer for Playing Chess”, 1950.<br>Presented many novel ideas that are still being used today.(太神啦！)  </p>\n<h3 id=\"Analysis\"><a href=\"#Analysis\" class=\"headerlink\" title=\"Analysis\"></a>Analysis</h3><ul>\n<li>typical 30 legal moves in one ply(下子)  </li>\n<li>typical game last about 40 moves  <ul>\n<li>will be 10^120 variations  </li>\n</ul>\n</li>\n<li>possible legal position(state space complexity) is roughly 10^43</li>\n<li>CPU speed in 1950 is 10^6 per second current CPU speed is 10^9 per second, still not fast enough to brute force it</li>\n</ul>\n<p>But it is possible to enumerate small endgames<br>3~6 piece endgame roughly 7.75*10^9 positions  </p>\n<h3 id=\"Three-phases-of-chess\"><a href=\"#Three-phases-of-chess\" class=\"headerlink\" title=\"Three phases of chess\"></a>Three phases of chess</h3><ul>\n<li>Opening <ul>\n<li>Development of pieces to good position</li>\n</ul>\n</li>\n<li>Middle<ul>\n<li>after opening until few pieces</li>\n<li>pawn structure </li>\n</ul>\n</li>\n<li>End game <ul>\n<li>concerning usage of pawns</li>\n</ul>\n</li>\n</ul>\n<p><strong>Different principles of play apply in the different phases</strong></p>\n<h3 id=\"Evaluating-Function\"><a href=\"#Evaluating-Function\" class=\"headerlink\" title=\"Evaluating Function\"></a>Evaluating Function</h3><p>position p, include board status, which side to move, history of moves<br>history -&gt; castling<br><img data-src=\"/img/TCG/6-1.png\" alt=\"\"></p>\n<p>Perfect evaluating function f(p):<br>f(p) = 1 for a won position.<br>f(p) = 0 for a drawn position.<br>f(p) = -1 for a lost position.<br>Perfect evaluating function is impossible for most games, and is <strong>not fun or educational</strong>.</p>\n<p>Factors considered in approximate evaluating functions:</p>\n<ul>\n<li>The relative values of differences in materials.<ul>\n<li>The values of queen, rook, bishop, knight and pawn are about 9, 5, 3, 3, and 1, respectively.</li>\n<li>How to determine good relative values? Static values verse dynamic values?</li>\n</ul>\n</li>\n<li>Position of pieces<ul>\n<li>Mobility: the freedom to move your pieces.</li>\n<li>at center , or at corner</li>\n<li>Doubled rooks</li>\n</ul>\n</li>\n<li>Pawn structure: the relative positions of the pawns.<ul>\n<li>Backward pawn: a pawn that is behind the pawn of the same color on an adjacent file that cannot advance without losing of itself.</li>\n<li>Isolated pawn: A pawn that has no friend pawn on the adjacent file.</li>\n<li>Doubled pawn: two pawns of the same color on the same file</li>\n<li>these three are all bad pawn</li>\n<li>Passed pawns: pawns that have no opposing pawns to prevent</li>\n<li>Pawns on opposite colour squares from bishop.</li>\n</ul>\n</li>\n<li>King safety.</li>\n<li>Threat and attack.<ul>\n<li>Attacks on pieces which give one player an option of exchanging</li>\n<li>Pins(小盯大) which mean here immobilizing pins where the pinned piece is of value not greater than the pinning piece</li>\n<li>Commitments -&gt; 需要保護其他子</li>\n</ul>\n</li>\n<li><img data-src=\"/img/TCG/6-2.png\" alt=\"three pawn\"></li>\n</ul>\n<p>Putting “right” coeffcients for diffferent factors<br>Dynamic setting in practical situations.</p>\n<p>evaluating function can be only applied in<br>relatively quiescent positions.</p>\n<p>not in the middle of material exchanging.<br>not being checked</p>\n<p>max-min strategy<br>In your move, you try to maximize your f(p).<br>In the opponent’s move, he tries to minimize f(p).</p>\n<p>A strategy in which all variations are considered out to a<br>definite number of moves and the move then determined from<br>a max-min formula is called type A strategy.</p>\n<p>Stalemate<br>Winning by making the opponent having no legal next move.<br>suicide move is not legal, and stalemate results in<br>a draw if it is not currently in check.</p>\n<p>Zugzwang(強制被動): In certain positions, a player is at a disadvantage if he is the next player to move.<br><img data-src=\"/img/TCG/6-3.png\" alt=\"\"></p>\n<p>Programming<br>    - Special rules of games<br>    - Methods of winning<br>    - Basic data structure for positions.<br>    - check for possible legal moves<br>    - Evaluating function.</p>\n<p>Forced variations(迫著)<br>one player has little or no choices in playing</p>\n<p>type B strategy<br>the machine must </p>\n<ol>\n<li><p>examine forceful variations out as far as possible and evaluate only at reasonable positions</p>\n</li>\n<li><p>select the variations to be explored by some process</p>\n<pre><code>| 1 if any piece is attacked by a piece of lower value,</code></pre><p>  g(P) =    /    or by more pieces then defences of if any check exists</p>\n<pre><code>\\    on a square controlled by opponent.\n | 0 otherwise.</code></pre><p>Using this function, variations could be explored until g(P)=0,</p>\n</li>\n</ol>\n<p><strong>effective branching factor</strong> is about 2 to 3.<br>Chinese chess has a larger real branching factor, but its average effective branching factor is also about 2 to 3.</p>\n<p>“style” of play by the machine can<br>be changed very easily by altering some of the coeffcients and<br>numerical factors involved in the evaluating function</p>\n<p>A chess master, on the other hand, has available knowledge of hundreds or perhaps thousands of standard situations, stock<br>combinations, and common manoeuvres based on pins, forks, discoveries, promotions, etc.<br>In a given position he recognizes some similarity to a familiar situation and this directs his mental calculations along the lines with greater probability of success.</p>\n<p>Need to re-think the goal of writing a computer program that<br>plays games.<br>To discover intelligence:<br>What is considered intelligence for computers may not be considered so for human.<br>To have fun:<br>A very strong program may not be a program that gives you the most pleasure.<br>To Find ways to make computers more helpful to human.<br>Techniques or (machine) intelligence discovered may be useful to computers performing other tasks</p>\n<h2 id=\"Chap-06-Alpha-Beta-Pruning\"><a href=\"#Chap-06-Alpha-Beta-Pruning\" class=\"headerlink\" title=\"Chap 06 Alpha-Beta Pruning\"></a>Chap 06 Alpha-Beta Pruning</h2><ul>\n<li>standard searching procedure for 2-person perfect-information zero sum games</li>\n<li>terminal position<ul>\n<li>a position whose (win/loss/draw) value can be know</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Dewey-decimal-system\"><a href=\"#Dewey-decimal-system\" class=\"headerlink\" title=\"Dewey decimal system\"></a>Dewey decimal system</h3><p>杜威分類法 <img data-src=\"/img/TCG/7-1.png\" alt=\"\"></p>\n<h3 id=\"Min-Max-method\"><a href=\"#Min-Max-method\" class=\"headerlink\" title=\"Min-Max method\"></a>Min-Max method</h3><p>假設持白子，數字為白子的evaluating function, 在下白子時，取分數最高(max)的，在下黑子時，取分數最低(min)的 <img data-src=\"/img/TCG/7-2.png\" alt=\"\"><br><img data-src=\"/img/TCG/7-3.png\" alt=\"max layer function F\"></p>\n<h3 id=\"Nega-max-method\"><a href=\"#Nega-max-method\" class=\"headerlink\" title=\"Nega-max method\"></a>Nega-max method</h3><p>將下黑子的分數取負號(即為黑子的分數，因為是零和遊戲)<br>這樣每一層都取最大分數即可<br><img data-src=\"/img/TCG/7-4.png\" alt=\"negamax algorithm\"></p>\n<p>優點是實作較快，程式碼簡潔 </p>\n<h3 id=\"Alpha-Beta-cut-off\"><a href=\"#Alpha-Beta-cut-off\" class=\"headerlink\" title=\"Alpha-Beta cut off\"></a>Alpha-Beta cut off</h3><ul>\n<li>current search window(score bound) = [α, β]</li>\n<li>If α &gt; β, no need to do further search in current branch </li>\n<li>initial alpha = -∞, beta = ∞</li>\n</ul>\n<p><img data-src=\"/img/TCG/7-5.png\" alt=\"Alpha Cut off\">  </p>\n<ul>\n<li>只要發現對手有一種反擊方式，使結果比其他手的結果還差，就砍掉這一手(branch)</li>\n<li>2.1 can cut off 2.x<ul>\n<li>before 2.1 , window = [15, ∞]</li>\n<li>after 2.1 , window = [15, 10]</li>\n</ul>\n</li>\n<li>We want to choose the biggest value at root for lower bound, so 2.x is all cut off</li>\n</ul>\n<p><img data-src=\"/img/TCG/7-6.png\" alt=\"Beta Cut off\">  </p>\n<ul>\n<li>只要對手發現自己有一種反擊方式，使結果比其他手的結果還差(α)，就砍掉這一手(branch)</li>\n<li>1.2.1 can cut off 1.2.x<ul>\n<li>beofre 1.2.1 , 1 bound is [-∞, 10]</li>\n<li>now 1.2 bound is [15, 10]</li>\n</ul>\n</li>\n<li>We want to choose smallest value at 1 for upper bound, 1.2.x is all cut off</li>\n</ul>\n<p>可以砍所有子孫 <img data-src=\"/img/TCG/7-7.png\" alt=\"Deep Cut off\">  </p>\n<ul>\n<li>2.1.1 is cut off   <ul>\n<li>root bound = [15, ∞]</li>\n<li>2.1.1 = [-∞, 7]</li>\n</ul>\n</li>\n</ul>\n<p><img data-src=\"/img/TCG/7-8.png\" alt=\"alpha-beta cut off Algorithm\"><br>f = white move, find max to be lower bound, do beta cut off<br>g = black move, find min to be upper bound, do alpha cut off<br><img data-src=\"/img/TCG/7-9.png\" alt=\"example\"></p>\n<p><img data-src=\"/img/TCG/7-10.png\" alt=\"F2\"><br>window變號，回傳的score也要變號<br>t = -F(pi, -beta, -m)</p>\n<h3 id=\"Analysis-for-AB-pruning\"><a href=\"#Analysis-for-AB-pruning\" class=\"headerlink\" title=\"Analysis for AB pruning\"></a>Analysis for AB pruning</h3><p><strong>different move orderings</strong> give very different cut branches<br>愈快找到最佳解，可以砍的branch愈多</p>\n<p>critical nodes 一定會搜到(cut off之前至少需搜完一個子branch) <img data-src=\"/img/TCG/7-11.png\" alt=\"Critical Node\"></p>\n<p>perfect-ordering tree: 每個branch的第一個child就是最佳解<br>Theorem: 若是perfect-ordering tree, AB pruning 會剛好走過所有 critical nodes<br>Proof:<br>Three Types of critial nodes  </p>\n<ul>\n<li>定義a_i = 第i層的node是第幾個child(杜威分類)</li>\n<li>a_j = 第一個「不是第一個child」的node(如果有的話)<ul>\n<li>a_j-1 = a_j+1 = 1<ul>\n<li>小於j的node都是1</li>\n<li>而且因為是critial node，所以a_j的child一定是1(其他會被砍掉)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>a_l = the last layer</li>\n</ul>\n<ol>\n<li>root and all node = 1(最左邊, 1, 1.1, 1.1.1 …)</li>\n<li>l-j = even<ol>\n<li>j = l (type1 的全部兒子(除了最左邊))  </li>\n<li>j &lt; l (type3 的全部兒子)</li>\n</ol>\n</li>\n<li>l-j = odd<ol>\n<li>j+1 = l (type2.1 的第一個兒子)</li>\n<li>j+1 &lt; l (type2.2的第一個兒子)</li>\n</ol>\n</li>\n</ol>\n<p><img data-src=\"/img/TCG/7-13.png\" alt=\"Three Types of critial nodes\"><br><img data-src=\"/img/TCG/7-14.png\" alt=\"Proof\"></p>\n<p>We can calculate the least number of nodes to be searched <img data-src=\"/img/TCG/7-15.png\" alt=\"\"> <img data-src=\"/img/TCG/7-16.png\" alt=\"\"></p>\n<p>when there’re some early terminate nodes <img data-src=\"/img/TCG/7-18.png\" alt=\"\"><br>l = even → x.1.x.1… = b0(q1b2)q3…<br>            1.x.1.x… = (q0b1)(q2b3)…(q0b1 = 第一個孩子的全child，若無child，則為(1-qi)*0)</p>\n<p>Perfect ordering is not always best when tree are not balanced <img data-src=\"/img/TCG/7-17.png\" alt=\"\"><br>→ When <strong>“relative” ordering of children</strong>(not perfect order!) are good enough, there are some cut-off  </p>\n<p>Theorem: 若知道所有的分數，就可以最佳化alpha-beta pruning(計算的點最少，cut最多)<br>→ 不過如果能算出來就不用search了…</p>\n<h3 id=\"Variations-of-alpha-beta-search\"><a href=\"#Variations-of-alpha-beta-search\" class=\"headerlink\" title=\"Variations of alpha-beta search\"></a>Variations of alpha-beta search</h3><ul>\n<li>Fail hard alpha-beta cut(Original) : F2 <img data-src=\"/img/TCG/7-19.png\" alt=\"\"> <ul>\n<li>returned value in [α, β] <img data-src=\"/img/TCG/7-20.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Fail soft alpha-beta cut(Variation): F3  <img data-src=\"/img/TCG/7-21.png\" alt=\"\"><ul>\n<li>Find “better” value when the value is out of the search window</li>\n<li>m is the value in this branch(not related to α)<ul>\n<li>use max(m, alpha) to get window </li>\n</ul>\n</li>\n<li>return original value m instead of α or β when cut off, which is more precise than fail-hard <img data-src=\"/img/TCG/7-22.png\" alt=\"\"></li>\n<li>Failed-high <ul>\n<li>return value &gt; β</li>\n</ul>\n</li>\n<li>Failed-low<ul>\n<li>return value &lt; α</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Comparison  </p>\n<ul>\n<li>fail-hard<ul>\n<li>return max{4000,200,v} <img data-src=\"/img/TCG/7-23.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>fail-soft<ul>\n<li>return max{200,v} <img data-src=\"/img/TCG/7-24.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>fail-soft provides more information when the true value is out of search window<ul>\n<li>can record better value to be used later when this position is revisited</li>\n<li>F3 saves about 7% of time than that of F2 when a transposition table is used to save and re-use searched results</li>\n<li>記錄F3傳回的值，可減少重複計算的時間，因為下一手的樹在下兩層，大部分node皆相同<ul>\n<li>if p1 is searched, p2 does not need to search again <img data-src=\"/img/TCG/7-25.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Questions\"><a href=\"#Questions\" class=\"headerlink\" title=\"Questions\"></a>Questions</h3><ul>\n<li>What move ordering is good?<ul>\n<li>search the best possible move first</li>\n<li>cut off a branch with more nodes first</li>\n</ul>\n</li>\n<li>What is the effect of using iterative-deepening alpha-beta cut off?</li>\n<li>How about searching game graph instead of game tree?</li>\n<li>Can some nodes be visited more than once?</li>\n</ul>\n<h3 id=\"Pruning-Techinique\"><a href=\"#Pruning-Techinique\" class=\"headerlink\" title=\"Pruning Techinique\"></a>Pruning Techinique</h3><ul>\n<li>Exact algorithms: by mathematical proof<ul>\n<li>Alpha-Beta pruning</li>\n<li>Scout(in Chap07)</li>\n</ul>\n</li>\n<li>Approximated heuristics: pruned branches with low probability to be solution<ul>\n<li>in very bad position(盤面太差)</li>\n<li>a little hope to gain back the advantage(無法逆轉)</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Chap07-Scout-and-Proof-Number-Search\"><a href=\"#Chap07-Scout-and-Proof-Number-Search\" class=\"headerlink\" title=\"Chap07 Scout and Proof Number Search\"></a>Chap07 Scout and Proof Number Search</h2><ul>\n<li>Suppose we get at least score s at the First branch<ul>\n<li>want to find whether second branch can get score over s or not</li>\n</ul>\n</li>\n</ul>\n<p><strong>Is there a way to search a tree approximately?</strong>  </p>\n<h3 id=\"SCOUT\"><a href=\"#SCOUT\" class=\"headerlink\" title=\"SCOUT\"></a>SCOUT</h3><ul>\n<li>Invented by Judea Pearl in 1980</li>\n<li>first time: search approximately<ul>\n<li>if there is better value, search again</li>\n<li>first search can provide useful information in the second search </li>\n</ul>\n</li>\n<li>TEST whether Tb can return score &gt; v <img data-src=\"/img/TCG/test-algo.png\" alt=\"\"><ul>\n<li>if p is max node → success with only one subbranch &gt; v</li>\n<li>if p is min node → success with all subbranches &gt; v</li>\n<li>If success, then search Tb. else, <strong>no need to search Tb</strong></li>\n</ul>\n</li>\n<li>algorithm <img data-src=\"/img/TCG/scout-algo.png\" alt=\"\"><ul>\n<li>scout first branch and test other branch<ul>\n<li>if test success, update the value by scout this branch</li>\n</ul>\n</li>\n<li>recursive procedure<ul>\n<li>Every ancestor of you may initiate a TEST to visit you<ul>\n<li>will be visited at most d times(= depth)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Time Complexity  </p>\n<ul>\n<li><strong>not guarantee</strong>(but most of the time) that the visited nodes number are less than alpha-beta<ul>\n<li>may search a branch two times</li>\n<li>may pay many visits to a node that is cut off by alpha-beta</li>\n</ul>\n</li>\n<li>TEST: Ω(b^(d/2))<ul>\n<li>but has small argument and will be very small at the best situation <img data-src=\"/img/TCG/nodes-visited.png\" alt=\"node visited\"><ul>\n<li>if the first subbranch has the best value, then TEST scans the tree fast</li>\n<li>move ordering is very important</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Comparison<ul>\n<li>alpha-beta<ul>\n<li>cut off comes from bounds of search windows(by ancestors)</li>\n</ul>\n</li>\n<li>scout<ul>\n<li>cut off from previous branches’ score(by brothers)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Performance  </p>\n<ul>\n<li>SCOUT favors “skinny” game trees<ul>\n<li>Show great improvements on depth &gt; 3 for games with <strong>small branching factors</strong></li>\n<li>On depth = 5, it saves over 40% of time</li>\n</ul>\n</li>\n<li>AB + scout gets average 10~20% improvement than only AB</li>\n</ul>\n<p>Null(Zero) window search    </p>\n<ul>\n<li>Using alpha-beta search with the window [m,m + 1]<ul>\n<li>result will be failed-high or failed-low</li>\n</ul>\n</li>\n<li>Failed-high means return value &gt; m + 1<ul>\n<li>Equivalent to TEST(p; m;&gt;) is true</li>\n</ul>\n</li>\n<li>Failed-low means return value &lt; m<ul>\n<li>Equivalent to TEST(p; m;&gt;) is false</li>\n</ul>\n</li>\n<li>Using searching window is better than using a single bound in SCOUT</li>\n</ul>\n<p><img data-src=\"/img/TCG/nega-scout.png\" alt=\"\">    </p>\n<ul>\n<li>depth &lt; 3 → no alpha-beta pruning → return value is exact value(no need to search again)</li>\n<li>first-time search → do null window search(scout)</li>\n<li>research → do normal window a-b pruning</li>\n</ul>\n<p>Refinements  </p>\n<ul>\n<li>Use information from previous search<ul>\n<li>When a subtree is re-searched, restart from the position that the value is returned in first search</li>\n</ul>\n</li>\n<li>Change move ordering<ul>\n<li>Reorder the moves by priority list</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Proof-Number-Search\"><a href=\"#Proof-Number-Search\" class=\"headerlink\" title=\"Proof Number Search\"></a>Proof Number Search</h3><p><img data-src=\"https://chessprogramming.wikispaces.com/Proof-number+search#Pseudo%20Code\" alt=\"參考資料: chessprogramming: proof-number search\"></p>\n<p>binary valued game tree    </p>\n<ul>\n<li><p>2-player game tree with either 0 or 1 on the leaves</p>\n<ul>\n<li>and-or tree: min → and, max → or</li>\n</ul>\n</li>\n<li><p>most proving node for node u</p>\n<ul>\n<li>node that if its value is 1, then the value of u is 1</li>\n</ul>\n</li>\n<li><p>most disproving node for node u</p>\n<ul>\n<li>node that if its value is 0, then the value of u is 0</li>\n</ul>\n</li>\n<li><p>proof(u): minimum number of nodes to visit to make u = 1</p>\n</li>\n<li><p>disproof(u): minimum number of nodes to visit to make u = 0</p>\n</li>\n</ul>\n<p>If value(u) is unknown, then proof(u) is the cost of evaluating u  </p>\n<ul>\n<li>If value(u) is 1, then proof(u) = 0</li>\n<li>If value(u) is 0, then proof(u) = ∞</li>\n<li>proof number can be calculate by search childrens <img data-src=\"/img/TCG/proof-number.png\" alt=\"\"><ul>\n<li>disproof number → reverse calculate method of proof number</li>\n</ul>\n</li>\n</ul>\n<p>Usage  </p>\n<ul>\n<li>find child u that have min{proof(root); disproof(root)}</li>\n<li>if we try to <strong>prove</strong> it<ul>\n<li>pick a child with the <strong>least proof number</strong> for a <strong>MAX node</strong></li>\n<li>pick <strong>any node that has a chance to be proved</strong> for a <strong>MIN node</strong></li>\n</ul>\n</li>\n<li>if we try to <strong>disprove</strong> it<ul>\n<li>pick a child with the <strong>least disproof number</strong> for a <strong>MIN node</strong></li>\n<li>pick <strong>any node that has a chance to be disproved</strong> for a <strong>MAX node</strong></li>\n</ul>\n</li>\n<li>used in open game tree or an endgame tree because some proof or disproof number is known<ul>\n<li>1 → proved to win, 0 → proved to lose </li>\n<li>or used to achieve sub-goal in games</li>\n</ul>\n</li>\n</ul>\n<!-- why smallest number because proof need all 1? -->\n<p>Proof-Number search algorithm <img data-src=\"/img/TCG/pn-algo.png\" alt=\"\">  </p>\n<ol>\n<li>keep update number by bottom-up<ol>\n<li>compare proof number and disproof number of root</li>\n</ol>\n</li>\n<li>find the leaf to prove or disprove</li>\n</ol>\n<p>Multi-value game tree  </p>\n<ul>\n<li>value in [0, 1]</li>\n<li>$proof_v(u)$: the minimum number of leaves needed to visited to make u &gt;= v<ul>\n<li>proof(u) = $proof_1(u)$</li>\n</ul>\n</li>\n<li>$disproof_v(u)$: the minimum number of leaves needed to visited to make u &lt; v<ul>\n<li>disproof(u) = $disproof_1(u)$</li>\n</ul>\n</li>\n<li>use binary search to set upper bound of the value <img data-src=\"/img/TCG/multivalue-pn-algo.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"Chap08-Monte-Carlo-Game-Tree-Search\"><a href=\"#Chap08-Monte-Carlo-Game-Tree-Search\" class=\"headerlink\" title=\"Chap08 Monte-Carlo Game Tree Search\"></a>Chap08 Monte-Carlo Game Tree Search</h2><h3 id=\"original-ideas\"><a href=\"#original-ideas\" class=\"headerlink\" title=\"original ideas\"></a>original ideas</h3><p>Algorithm $MCS_{pure}$ <img data-src=\"img/TCG/random-games.png\" alt=\"\">    </p>\n<ul>\n<li><p>For each possible next move</p>\n<ul>\n<li>play this move and then play a lot of random games(play every moves as random)</li>\n<li>calculate average score</li>\n</ul>\n</li>\n<li><p>Choose move with best score</p>\n</li>\n<li><p>Original version: GOBBLE in 1993  </p>\n<ul>\n<li>Performance is not good compared to other Go programs(alpha-beta)</li>\n</ul>\n</li>\n<li><p>Enhanced versions</p>\n<ul>\n<li>Adding the idea of minimax tree search</li>\n<li>Adding more domain knowledge</li>\n<li>Adding more searching techniques</li>\n<li>Building theoretical foundations from statistics, and on-line and off-line learning</li>\n<li>results<ul>\n<li>MoGo<ul>\n<li>Beat a professional human 8 dan(段) with a 8-stone handicap at January 2008</li>\n<li>Judged to be in a “professional level” for 9 x 9 Go in 2009</li>\n</ul>\n</li>\n<li>Zen<ul>\n<li>close to amateur 3-dan in 2011</li>\n<li>Beat a 9-dan professional master with handicaps at March 17, 2012<ul>\n<li>First game: Five stone handicap and won by 11 points</li>\n<li>Second game: four stones handicap and won by 20 points</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Disadvantage  </p>\n<ul>\n<li>average score search != minimax tree search<ul>\n<li>$MCS_{pure}$ prefer right branch, but it’s min value is low <img data-src=\"/img/TCG/minmax-and-avergae.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"First-Refinement-Monte-Carlo-based-tree-search\"><a href=\"#First-Refinement-Monte-Carlo-based-tree-search\" class=\"headerlink\" title=\"First Refinement: Monte-Carlo based tree search\"></a>First Refinement: Monte-Carlo based tree search</h3><p>Intuition   </p>\n<ul>\n<li>Best First tree growing<ul>\n<li>Expand one level of best leaf(which has largest score) <img data-src=\"/img/TCG/mct-ex2.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>if number of simulations is not enough, it can’t be a good simulation<ul>\n<li>on a MIN node, if not enough children are probed for enough number of times, you may miss a very bad branch</li>\n<li>take <strong>simulation count</strong> into consideration</li>\n</ul>\n</li>\n</ul>\n<p><img data-src=\"/img/TCG/MCT.png\" alt=\"\"><br><img data-src=\"/img/TCG/mct-ex1.png\" alt=\"\">  </p>\n<h3 id=\"Second-Refinement-UCT\"><a href=\"#Second-Refinement-UCT\" class=\"headerlink\" title=\"Second Refinement: UCT\"></a>Second Refinement: UCT</h3><ul>\n<li><p>Effcient sampling  </p>\n<ul>\n<li>Original: equally distributed among all legal moves</li>\n<li>Biased sampling: sample some moves more often than others</li>\n</ul>\n</li>\n<li><p>Observations</p>\n<ul>\n<li>Some moves are bad and do not need further exploring<ul>\n<li>Need to consider extremely bad luck sitiation<ul>\n<li>e.g. often “randomly” choose bad move and get bad score</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTXVsdGktYXJtZWRfYmFuZGl0\">K-arm bandit problem<i class=\"fa fa-external-link-alt\"></i></span>  </p>\n<ul>\n<li>Assume you have K slot machines each with a different payoff, i.e., expected value of returns ui, and an unknown distribution</li>\n<li>Assume you can bet on the machines N times, what is the best strategy to get the largest returns?</li>\n</ul>\n</li>\n<li><p>Ideas</p>\n<ul>\n<li>Try each machine a few, but enough, times and record their returns<ul>\n<li>For the machines that currently have the best returns, play more often later</li>\n<li>For the machines that currently return poorly, give them a chance sometimes to check their distributions are really bad or not</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>UCB: Upper Confidence Bound <img data-src=\"img/TCG/UCB.png\" alt=\"\">  </p>\n<ul>\n<li>Meaning<ul>\n<li>For a MAX node, Wi is the number of win’s for the MAX player</li>\n<li><strong>For a MIN node, Wi is the number of win’s for the MIN player</strong></li>\n<li>When N is approaching logN, then UCB is nothing but the current winning rate plus a constant</li>\n<li>When N getting larger, UCB will approachthe real winning rate</li>\n</ul>\n</li>\n<li>Expand for the move with the highest UCB value</li>\n<li><strong>only compare UCB scores among children of a node</strong><ul>\n<li>It is meaningless to compare scores of nodes that are not siblings</li>\n</ul>\n</li>\n<li>Using argument c to keep a balance between<ul>\n<li>Exploitation: exploring the best move so far</li>\n<li>Exploration: exploring other moves to see if they can be proved to be better <img data-src=\"/img/TCG/ucb-ex1.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>alternative<ul>\n<li>consider the variance of scores in each branch <img data-src=\"/img/TCG/UCB2.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<p>UCT: Upper Confidence Bound for Tree  </p>\n<ul>\n<li>Maintain the UCB value for each node in the game tree<ul>\n<li>Pick path such that each node in this path has a largest UCB score among all of its siblings</li>\n<li>Pick the leaf node in the path which has been visited more than a certain amount of times to expand</li>\n</ul>\n</li>\n</ul>\n<p>Usable when the “density of goals” is suffciently large  </p>\n<ul>\n<li>When there is only a unique goal, Monte-Carlo based simulation may not be useful</li>\n</ul>\n<p>new MCT algorithm(with UCT) <img data-src=\"/img/TCG/mct-uct.png\" alt=\"\"></p>\n<h4 id=\"Implementation-hints\"><a href=\"#Implementation-hints\" class=\"headerlink\" title=\"Implementation hints\"></a>Implementation hints</h4><p><img data-src=\"/img/TCG/uct-imp.png\" alt=\"\"><br><img data-src=\"/img/TCG/uct-imp2.png\" alt=\"\"><br><img data-src=\"/img/TCG/uct-imp3.png\" alt=\"\"></p>\n<h3 id=\"When-to-use-Monte-Carlo\"><a href=\"#When-to-use-Monte-Carlo\" class=\"headerlink\" title=\"When to use Monte-Carlo\"></a>When to use Monte-Carlo</h3><ul>\n<li>huge branching number </li>\n<li>cannot easily compute good evaluating function</li>\n<li>Mostly used in Go, Bridge(?)</li>\n</ul>\n<p>Rule of Go(圍棋)  </p>\n<ul>\n<li>Ko(打劫): 不能有重複盤面</li>\n<li>可以跳過，不能下自殺步</li>\n<li>Komi: 先手讓子</li>\n</ul>\n<p>Implementation  </p>\n<ul>\n<li>partition stones into strings(使用共同氣的子) by DFS</li>\n<li>check empty intersection is an eye or not(check neighbors and limits)</li>\n</ul>\n<h3 id=\"Domain-independent-refinements\"><a href=\"#Domain-independent-refinements\" class=\"headerlink\" title=\"Domain independent refinements\"></a>Domain independent refinements</h3><p>Main considerations   </p>\n<ul>\n<li>Avoid doing un-needed computations</li>\n<li>Increase the speed of convergence</li>\n<li>Avoid early mis-judgement</li>\n<li>Avoid extreme bad cases</li>\n</ul>\n<p>Refinements  </p>\n<ul>\n<li>Progressive pruning  <ul>\n<li>Cut hopeless nodes early</li>\n</ul>\n</li>\n<li>All moves at first(AMAF)<ul>\n<li>Increase the speed of convergence</li>\n</ul>\n</li>\n<li>Node expansion<ul>\n<li>Grow only nodes with a potential</li>\n</ul>\n</li>\n<li>Temperature<ul>\n<li>Introduce randomness</li>\n</ul>\n</li>\n<li>Depth-i enhancement<ul>\n<li>With regard to Line 1, the initial phase, exhaustively enumerate all possibilities</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Progressive-pruning\"><a href=\"#Progressive-pruning\" class=\"headerlink\" title=\"Progressive pruning\"></a>Progressive pruning</h4><p>Each move has a mean value m and a standard deviation σ  </p>\n<ul>\n<li><p>Left expected outcome ml = m - rd * σ</p>\n</li>\n<li><p>Right expected outcome mr = m + rd * σ</p>\n<ul>\n<li>rd is argument</li>\n</ul>\n</li>\n<li><p>A move M1 is <strong>statistically inferior</strong> to another move M2 if M1.mr &lt; M2.ml</p>\n</li>\n<li><p>Two moves M1 and M2 are <strong>statistically equal</strong> if M1.σ &lt; σe and M2.σ &lt; σe and no move is statistically inferior to the other</p>\n<ul>\n<li>σe is argument which called standard deviation for equality</li>\n</ul>\n</li>\n</ul>\n<p>Remarks  </p>\n<ul>\n<li>only compare nodes that are of the same parent</li>\n<li>compare their raw scores not their UCB values<ul>\n<li>If you use UCB scores, then the mean and standard deviation of a move are those calculated only from its un-pruned children</li>\n</ul>\n</li>\n<li>prune statistically inferior moves after enough number of times of simulation</li>\n</ul>\n<p>This process is stopped when  </p>\n<ul>\n<li>there is only one move left</li>\n<li>the moves left are statistically equal</li>\n<li>a maximal threshold(like 10000 multiplied by the number of legal moves) of iterations is reached</li>\n</ul>\n<p>Two different pruning rules  </p>\n<ul>\n<li>Hard: a pruned move cannot be a candidate later on</li>\n<li>Soft: a move pruned at a given time <strong>can be a candidate later on</strong> if its value is no longer statistically inferior to a currently active move<ul>\n<li>Periodically check whether to reactive it</li>\n</ul>\n</li>\n</ul>\n<p>Arguments  </p>\n<ul>\n<li><p>Selection of rd <img data-src=\"/img/TCG/uct-result2.png\" alt=\"\">   </p>\n<ul>\n<li>The greater rd is</li>\n<li>the less pruned the moves are</li>\n<li>the better the algorithm performs</li>\n<li>the slower at each play</li>\n</ul>\n</li>\n<li><p>Selection of σe <img data-src=\"/img/TCG/uct-result1.png\" alt=\"\"></p>\n<ul>\n<li>The smaller σe is</li>\n<li>the fewer equalities there are</li>\n<li>the better the algorithm performs</li>\n<li>the slower at each play</li>\n</ul>\n</li>\n<li><p>rd plays an important role in the move pruning process</p>\n</li>\n<li><p>σe is less sensitive</p>\n</li>\n<li><p>Another trick is progressive widening or progressive un-pruning</p>\n<ul>\n<li>A node is effective if enough simulations are done on it and its values are good</li>\n</ul>\n</li>\n<li><p>We can set threshold on whether to expand the best path, for exmaple</p>\n<ul>\n<li>enough simulations are done</li>\n<li>score is good enough</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"All-moves-at-first-AMAF\"><a href=\"#All-moves-at-first-AMAF\" class=\"headerlink\" title=\"All moves at first(AMAF)\"></a>All moves at first(AMAF)</h4><ul>\n<li>score is used for <strong>all moves the same player played in a random game</strong><ul>\n<li>in this example, after simulate r→v→y→u→w, w which  has parent v and u which has parent r will be updated, too <img data-src=\"/img/TCG/amaf.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Advantage<ul>\n<li>speeding up the experiments</li>\n</ul>\n</li>\n<li>Drawback<ul>\n<li>not the same move - move in early game is not equal to late game </li>\n<li>Recapturing<ul>\n<li>Order of moves is important for certain games(圍棋)</li>\n<li>Modification: if several moves are played at the same place because of captures, modify the statistics only for the player who played first </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Refinement: RAVE    </p>\n<ul>\n<li>Let v1(m) be the score of a move m without using AMAF</li>\n<li>Let v2(m) be the score of a move m with AMAF</li>\n<li>Observations<ul>\n<li>v1(m) is good when suffcient number of simulations are starting with m</li>\n<li>v2(m) is a <strong>good guess for the true score</strong> of the move m<ul>\n<li>when <strong>approaching the end of a game</strong></li>\n<li>when <strong>too few simulations starting with m</strong></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Rapid Action Value Estimate (RAVE)  </p>\n<ul>\n<li>revised score $v3(m) = a \\times v1(m) + (1-a) \\times v2(m)$</li>\n<li>can dynamically change a as the game goes<ul>\n<li>For example: a = min{1, Nm/10000}, where Nm is simulation times start from m<ul>\n<li>This means when Nm reaches 10000, then no RAVE is used</li>\n</ul>\n</li>\n<li>Works out better than setting a = 0(i.e. pure AMAF)</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Node-expansion\"><a href=\"#Node-expansion\" class=\"headerlink\" title=\"Node expansion\"></a>Node expansion</h4><ul>\n<li>May decide to expand potentially good nodes judging from the<br>current statistics</li>\n<li>All ends: expand all possible children of a newly added node</li>\n<li>Visit count: delay the expansion of a node until it is visited a certain number of times</li>\n<li>Transition probability: delay the expansion of a node until its \\score” or estimated visit count is high comparing to its siblings</li>\n<li>Use the current value, variance and parent’s value to derive a good estimation using statistical methods<br>Expansion policy with some transition probability is much better than the \\all ends” or \\pure visit count” policy</li>\n</ul>\n<!-- ##Chap09 Other way to increase performance -->\n\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5paXMuc2luaWNhLmVkdS50dy9+dHNoc3UvdGNnLw==\">TSHsu講義 2014年版<i class=\"fa fa-external-link-alt\"></i></span></p>\n",
            "tags": [
                "電腦對局理論",
                "機器學習",
                "人工智慧",
                "圍棋",
                "象棋",
                "蒙地卡羅",
                "Alpha-Beta搜尋",
                "強化學習"
            ]
        },
        {
            "id": "http://gitqwerty777.github.io/MLfoundation1/",
            "url": "http://gitqwerty777.github.io/MLfoundation1/",
            "title": "機器學習基石(上)",
            "date_published": "2014-09-16T11:41:48.000Z",
            "content_html": "<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2NvdXJzZS9udHVtbG9uZQ==\">原版<i class=\"fa fa-external-link-alt\"></i></span>的講義做得十分精美，可以很快了解</p>\n<h2 id=\"Chap01-Introduction\"><a href=\"#Chap01-Introduction\" class=\"headerlink\" title=\"Chap01 Introduction\"></a>Chap01 Introduction</h2><p>課堂討論：學習的定義    </p>\n<ol>\n<li>從不會到會 </li>\n<li>從會到更進步、熟練</li>\n</ol>\n<p><img data-src=\"/img/ML/0FPIeqh.png\" alt=\"\"></p>\n<p>課堂討論：學習的方法    </p>\n<ul>\n<li>以「樹的定義」為例  </li>\n<li>如何寫出「能判斷是否是樹」的程式？ <ol>\n<li>define trees and hand-program: difficult</li>\n<li>learn from data by observation and recognize: more easier(機器「自己」學習)<a id=\"more\"></a>\n<img data-src=\"/img/ML/BuqSVKs.png\" alt=\"\"></li>\n</ol>\n</li>\n</ul>\n<p>課堂討論：兩種學習方法  </p>\n<ul>\n<li>電腦: learn from data -&gt; get knowledge by observing  </li>\n<li>人腦: learn from teachers -&gt; get the essence of the knowledge(can computer do that?)</li>\n</ul>\n<h3 id=\"key-eassence-of-ML\"><a href=\"#key-eassence-of-ML\" class=\"headerlink\" title=\"key eassence of ML\"></a>key eassence of ML</h3><ol>\n<li>存在「<strong>潛藏模式</strong>」可以學習<ul>\n<li>若認為有「潛藏模式」，才需要學習  </li>\n</ul>\n</li>\n<li><strong>無法簡單定義</strong></li>\n<li>有可提供學習的<strong>資料</strong></li>\n</ol>\n<h3 id=\"ML使用時機\"><a href=\"#ML使用時機\" class=\"headerlink\" title=\"ML使用時機\"></a>ML使用時機</h3><ul>\n<li>人類無法操作<ul>\n<li>火星探索</li>\n</ul>\n</li>\n<li>難以定義的問題<ul>\n<li>視覺/聽覺辨識  </li>\n</ul>\n</li>\n<li>需要快速判斷<ul>\n<li>股票炒短線程式</li>\n</ul>\n</li>\n<li>大量資料<ul>\n<li>個人化使用者體驗</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"ML應用\"><a href=\"#ML應用\" class=\"headerlink\" title=\"ML應用\"></a>ML應用</h3><p>推薦系統<br>將物品分解成各個porperty factors，形成vector，並與自己的喜好vector比較  </p>\n<h3 id=\"formalize-the-learning-problem\"><a href=\"#formalize-the-learning-problem\" class=\"headerlink\" title=\"formalize the learning problem\"></a>formalize the learning problem</h3><ul>\n<li>target funcion <code>f</code><ul>\n<li>unknown pattern to be learned   </li>\n</ul>\n</li>\n<li>data <code>D</code><ul>\n<li>training examples</li>\n</ul>\n</li>\n<li>hypothesis set <code>h</code><ul>\n<li>candidate functions to be choosed</li>\n</ul>\n</li>\n<li>hypothesis <code>g</code> <ul>\n<li>best candidate function which is learned from data</li>\n</ul>\n</li>\n<li>use algorithm(A) with data(D) and hypothesis set(H) to get g <img data-src=\"/img/ML/c5XEqoy.png\" alt=\"\"></li>\n</ul>\n<blockquote>\n<p>Machine Learning:<br><br>use data to compute hypothesis <code>g</code> that approximates target <code>f</code></p>\n</blockquote>\n<h3 id=\"Differences\"><a href=\"#Differences\" class=\"headerlink\" title=\"Differences\"></a>Differences</h3><h4 id=\"Machine-Learning-amp-Data-Mining\"><a href=\"#Machine-Learning-amp-Data-Mining\" class=\"headerlink\" title=\"Machine Learning &amp; Data Mining\"></a>Machine Learning &amp; Data Mining</h4><p>ML: the same as above<br>DM: use <strong>huge</strong> data to <strong>find property</strong> that is interesting</p>\n<h4 id=\"Machine-Learning-amp-Artificial-Intelligence\"><a href=\"#Machine-Learning-amp-Artificial-Intelligence\" class=\"headerlink\" title=\"Machine Learning &amp; Artificial Intelligence\"></a>Machine Learning &amp; Artificial Intelligence</h4><p>AI -&gt; compute something that shows intelligent behavior</p>\n<p><strong>ML can realize AI</strong><br>traditional AI -&gt; game tree<br>ML -&gt; learning (techiniques) from board data</p>\n<h4 id=\"Machine-Learning-amp-Statistics\"><a href=\"#Machine-Learning-amp-Statistics\" class=\"headerlink\" title=\"Machine Learning &amp; Statistics\"></a>Machine Learning &amp; Statistics</h4><p>Statistics: use data to make inference about an unknown process<br>-&gt; many <strong>useful tools for ML</strong></p>\n<p>課堂討論：Big Data     </p>\n<ul>\n<li>As data getting bigger, the way to deal with data has to be changed.(such as distributed computation)</li>\n<li><strong>not</strong> a new topic</li>\n<li>marketing buzz word<br>課堂討論：Maching Learning &amp; Neural Network  </li>\n<li>A technique used in early AI and ML</li>\n</ul>\n<h2 id=\"Chap-02-Perceptron-感知器\"><a href=\"#Chap-02-Perceptron-感知器\" class=\"headerlink\" title=\"Chap 02 Perceptron(感知器)\"></a>Chap 02 Perceptron(感知器)</h2><h3 id=\"yes-no-question-by-grading\"><a href=\"#yes-no-question-by-grading\" class=\"headerlink\" title=\"yes/no question by grading\"></a>yes/no question by grading</h3><p>用feature(特質)來分隔兩種不同的結果    </p>\n<ul>\n<li>x: input</li>\n<li>w: hypothesis</li>\n<li>x是在d維度空間的點(d個features)，w為分隔此空間的線(平面)的法向量 <img data-src=\"/img/ML/pla-w.png\" alt=\"\"> </li>\n<li>以二維空間為例：w產生的線分隔兩邊 <img data-src=\"/img/ML/MOzf2UK.png\" alt=\"\"><ul>\n<li>也就是h(x)的正負，w所在的那一側為正 <img data-src=\"/img/ML/joxwtUt.png\" alt=\"\">   </li>\n</ul>\n</li>\n</ul>\n<h3 id=\"select-g-from-h\"><a href=\"#select-g-from-h\" class=\"headerlink\" title=\"select g from h\"></a>select g from h</h3><p>Difficult: h is infinite<br>Idea: 從某一條線開始，進行更改(local search)</p>\n<h3 id=\"Perception-Learning-Algorithm-PLA\"><a href=\"#Perception-Learning-Algorithm-PLA\" class=\"headerlink\" title=\"Perception Learning Algorithm(PLA)\"></a>Perception Learning Algorithm(PLA)</h3><p>A fault confessed is half redressed(知錯能改)</p>\n<ol>\n<li>find a mistake(which sign is wrong) <img data-src=\"/img/ML/u0KFPyS.png\" alt=\"\"></li>\n<li>correct the mistake <img data-src=\"/img/ML/Mow3SlT.png\" alt=\"\"><ul>\n<li>if real ans = +, new w = w + x(使w靠近正的點)</li>\n<li>if real ans = -, new w = w - x(使w遠離負的點) </li>\n</ul>\n</li>\n<li>keep doing until no mistake </li>\n</ol>\n<p>question<br>同乘$y_nx_n$ <img data-src=\"/img/ML/KKHE36Z.png\" alt=\"\"><br>可看出錯誤變少：正確的時候，$w_nx_n$和$y_n$同號，所以$w_nx_ny_n$是正的    </p>\n<h3 id=\"linear-seperability\"><a href=\"#linear-seperability\" class=\"headerlink\" title=\"linear seperability\"></a>linear seperability</h3><p><img data-src=\"/img/ML/5L1kwEZ.png\" alt=\"\">  </p>\n<ul>\n<li>linear seperable<ul>\n<li>exist perfect w makes $sign(y) = sign(w_nx_n)$, n = 0~N</li>\n<li>用直線(平面)必可分成無錯誤的兩塊  </li>\n</ul>\n</li>\n<li>if Data is linear seperable, then PLA can generate w to make no mistake </li>\n<li>每次改動使$w_f$(正解)和$w_t$的內積變大，也就是愈來愈接近 <img data-src=\"/img/ML/unBVfjt.png\" alt=\"\"><ul>\n<li>但成長速度有限 <img data-src=\"/img/ML/LHtRvcu.png\" alt=\"\">    <ul>\n<li>$|W_t| &lt;= sqrt(t) max(X_n)$</li>\n<li><img data-src=\"/img/ML/J66FCPC.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>question<br><img data-src=\"/img/ML/0szpVwP.png\" alt=\"\"></p>\n<h3 id=\"PLA-Guarantee\"><a href=\"#PLA-Guarantee\" class=\"headerlink\" title=\"PLA Guarantee\"></a>PLA Guarantee</h3><p><img data-src=\"/img/ML/9qQxERz.png\" alt=\"\"></p>\n<ul>\n<li>advantage<ul>\n<li>simple to implement</li>\n<li>fast</li>\n</ul>\n</li>\n<li>disadvantage<ul>\n<li>not fully sure how long it will take</li>\n<li>assume linear seperable<ul>\n<li>What if no linear seperate?(in reality)</li>\n<li>選出犯錯最少的</li>\n<li>這是個NP-HARD問題… <img data-src=\"/img/ML/oRWuGAO.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Pocket-Algorithm-a-little-modified-by-PLA\"><a href=\"#Pocket-Algorithm-a-little-modified-by-PLA\" class=\"headerlink\" title=\"Pocket Algorithm(a little modified by PLA)\"></a>Pocket Algorithm(a little modified by PLA)</h3><p><img data-src=\"/img/ML/XkWjmux.png\" alt=\"\"></p>\n<ul>\n<li>greedy <ul>\n<li>may not be the best answer: 可能是局部最佳解</li>\n</ul>\n</li>\n<li>slower than PLA(need to compare Wt+1 and Wt)  </li>\n</ul>\n<h2 id=\"Chap03-types-of-learning\"><a href=\"#Chap03-types-of-learning\" class=\"headerlink\" title=\"Chap03 types of learning\"></a>Chap03 types of learning</h2><h3 id=\"Different-Output-Space\"><a href=\"#Different-Output-Space\" class=\"headerlink\" title=\"Different Output Space\"></a>Different Output Space</h3><p>Binary Classification  </p>\n<ul>\n<li>yes/no</li>\n<li>core problem to build tools</li>\n</ul>\n<p>Multiclass Classification(N output class)    </p>\n<ul>\n<li>Regression(迴歸分析)<ul>\n<li>output 為一數字</li>\n<li>Ex. temperature, stock price</li>\n<li><strong>core problem to build statistic tools</strong> </li>\n</ul>\n</li>\n<li>Structured Learning<ul>\n<li>output $y$ = structures with <strong>implicit class definition</strong></li>\n<li>too many class → structure</li>\n<li>Ex. Speech parse tree, sequence tagging(標詞性), protein folding</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Different-Data-Label\"><a href=\"#Different-Data-Label\" class=\"headerlink\" title=\"Different Data Label\"></a>Different Data Label</h3><p><strong>Supervised</strong> Learning(監督式學習)  </p>\n<ul>\n<li>data with pairs of input and output</li>\n</ul>\n<p>Unsupervised Learning  </p>\n<ul>\n<li>doesn’t have output data(沒正確答案)</li>\n<li>clustering(分群問題)<ul>\n<li>density estimation(find traffic dangerous areas)</li>\n<li>unusual detection(find unusual data)</li>\n</ul>\n</li>\n<li>usually used in data mining <img data-src=\"/img/ML/Jz6fiwk.png\" alt=\"\"></li>\n</ul>\n<p>Semi-Supervised  </p>\n<ul>\n<li>given small amount of data with output, find output of other data<ul>\n<li>Ex. facebook face identifier</li>\n<li>leverage unlabeled data to avoid ‘expensive’ labeling</li>\n</ul>\n</li>\n</ul>\n<p>Reinforcement Learning(增強學習)  </p>\n<ul>\n<li>natural way of learning(行為學派)<ul>\n<li>learn with <strong>‘seqentially implicit output’</strong></li>\n<li>if output is good, give reinforcement<ul>\n<li>probability of this input increases</li>\n</ul>\n</li>\n<li>if output is bad, give pushnishment<ul>\n<li>probability of this input decreases</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Ex. <ul>\n<li>train a dog</li>\n<li>online ADs</li>\n<li>chess AI</li>\n</ul>\n</li>\n<li>和gene algorithm類似</li>\n</ul>\n<h3 id=\"Different-Protocol\"><a href=\"#Different-Protocol\" class=\"headerlink\" title=\"Different Protocol\"></a>Different Protocol</h3><p>Batch Learning    </p>\n<ul>\n<li>learn from known data<ul>\n<li>duck feeding(填鴨式)</li>\n</ul>\n</li>\n<li><strong>very common protocol</strong></li>\n</ul>\n<p>Online Learning  </p>\n<ul>\n<li>sequential, passive data(不斷的得到新資料)</li>\n<li>Every datum can improve <code>g</code></li>\n<li>PLA, reinforcement learning is often used with online learning</li>\n<li>Ex. spam filter</li>\n</ul>\n<p>Active Learning  </p>\n<ul>\n<li>strategically-observed data</li>\n<li>machine can ask question(take <strong>chosen</strong>(input, output)pair to learn)<ul>\n<li>關於自己不會(錯誤)的問題，拿相關的資料來學習</li>\n<li>比對有自信的答案(= 對答案)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Different-Input-Space\"><a href=\"#Different-Input-Space\" class=\"headerlink\" title=\"Different Input Space\"></a>Different Input Space</h3><p>Feature &lt;-&gt; Input</p>\n<p><strong>Concrete</strong> Features  </p>\n<ul>\n<li>each input class represents some ‘sophisticated physical meaning’</li>\n<li>input 和 output 有相關(經過人類分類過)</li>\n</ul>\n<p>Raw Features(未處理的資料)   </p>\n<ul>\n<li>‘simple physical meaing’ -&gt; difficult to learn</li>\n<li>Ex. Digit Recognition<ul>\n<li>concrete feature: symmtry, density</li>\n<li>raw feature: matrix of image bits</li>\n</ul>\n</li>\n</ul>\n<p>Abstract Features  </p>\n<ul>\n<li>‘no physical learning’ -&gt; the most difficult to learn</li>\n<li>need ‘feature conversion’</li>\n<li>Ex. Rating Prediction Problem<ul>\n<li>從歌曲評分抽出feature: 喜好, 歌的性質……  </li>\n</ul>\n</li>\n</ul>\n<p>In general machine learning, those three feature types will be used</p>\n<h2 id=\"Chap-04-Feasibility-of-Learning\"><a href=\"#Chap-04-Feasibility-of-Learning\" class=\"headerlink\" title=\"Chap 04 Feasibility of Learning\"></a>Chap 04 Feasibility of Learning</h2><ul>\n<li>learning will be stricted by limited data(no free lunch)</li>\n<li>learning from D (to infer something outside D) is doomed</li>\n</ul>\n<p>Statistics   </p>\n<ul>\n<li>Real environment -&gt; unknown</li>\n<li>Sample data -&gt; known<ul>\n<li>Can sample represent the real?</li>\n</ul>\n</li>\n<li>有極小可能無法代表real status</li>\n</ul>\n<h3 id=\"Hoeffding’s-Inequality\"><a href=\"#Hoeffding’s-Inequality\" class=\"headerlink\" title=\"Hoeffding’s Inequality\"></a>Hoeffding’s Inequality</h3><ul>\n<li>v and u are error rate of certain h in sample and real data <img data-src=\"/img/ML/PG3e7Jr.png\" alt=\"\"></li>\n<li>larger sample size N or looser gap(誤差)<ul>\n<li>higher probability to approximate real</li>\n</ul>\n</li>\n</ul>\n<p><strong>Error between hypothesis and target function</strong> can be inferred by data <img data-src=\"/img/ML/2I9ZSPn.png\" alt=\"\"> <img data-src=\"/img/ML/AC3KnSC.png\" alt=\"\"></p>\n<h3 id=\"Ein-and-Eout\"><a href=\"#Ein-and-Eout\" class=\"headerlink\" title=\"Ein and Eout\"></a>Ein and Eout</h3><p>in-sample error(Ein) and out-of-sample error(Eout)<br>Guarantee: for large N, Ein(h) ~= Eout(h) is probably approximately correct (PAC) <img data-src=\"/img/ML/colR3kh.png\" alt=\"\">  </p>\n<p>Q: if 150 people flips a coin 5 times, and one of them gets 5 heads.  A: Probability is &gt; 99% <img data-src=\"/img/ML/CCrtjgi.png\" alt=\"\"><br>→ 做愈多次，遇到的BAD sample(Eout 和 Ein 差很多; sample和實際差距過大)的機率愈大<br>→ Real learning: Algorithm choose the best <code>h</code> which has lowest Ein(h) among <code>H</code></p>\n<ul>\n<li>Bad Data for a <code>H</code>  <ul>\n<li>存在 <code>h</code> 使 Ein(h) 和 Eout(h) 相差很大 <img data-src=\"/img/ML/x6wkDZk.png\" alt=\"\"><ul>\n<li>由 hoeffding 知道抽到bad data的機率很小</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>hypothesis的個數愈多，抽到BAD data的機率愈高 <img data-src=\"/img/ML/IK9lYNY.png\" alt=\"\"><ul>\n<li>安全的data(在任何h都不是bad data)的比例 若很高，則學到的東西可能不好</li>\n</ul>\n</li>\n</ul>\n<p>若hypothesis set的大小是有限的話，只要N夠大，Eout ~= Ein<br>但perceptron不是finite(有無限多種分隔可選)</p>\n<h2 id=\"Chap05-Training-versus-Testing\"><a href=\"#Chap05-Training-versus-Testing\" class=\"headerlink\" title=\"Chap05 Training versus Testing\"></a>Chap05 Training versus Testing</h2><p>g is similar to f ↔ Eout(g) ~= Ein(g) ~= 0  </p>\n<p>But need train and test <img data-src=\"/img/ML/TXVWRpF.png\" alt=\"\">       </p>\n<ul>\n<li>Train: find hypothesis that can fit sample data   </li>\n<li>Test: take <strong>good sample data</strong> that is similar to exact data  </li>\n</ul>\n<p>How to decide the number of hypothesis set<br><img data-src=\"/img/ML/mrA45Zq.png\" alt=\"\"> <img data-src=\"/img/ML/hsyNq1P.png\" alt=\"\"><br>Cannot both satisfied!</p>\n<p>Todo: Find a finite value $m_H$ can replace infinite M<br><img data-src=\"/img/ML/LOwwaGm.png\" alt=\"\"><br>Idea: M is overestimated, we use classification:<br>how many lines =&gt; how many kinds of line(that makes different output)<br>This method is called Dichotomies(二分法): Mini-hypotheses<br><img data-src=\"/img/ML/8CcPNcS.png\" alt=\"\"></p>\n<table>\n<thead>\n<tr>\n<th>input</th>\n<th>types of lines</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4 (00, 01, 10, 11)</td>\n</tr>\n<tr>\n<td>3</td>\n<td>8</td>\n</tr>\n<tr>\n<td>4</td>\n<td>14 (2 lines that is not <br> linearly seperable)</td>\n</tr>\n<tr>\n<td>N</td>\n<td>effective(N) &lt;= $2^N$</td>\n</tr>\n</tbody></table>\n<p>Growth Function $m_H$ = <strong>max number of dichotomies(max number of different outputs)</strong><br><img data-src=\"/img/ML/xc50yGO.png\" alt=\"\">  </p>\n<h3 id=\"Types-of-Growth-Function\"><a href=\"#Types-of-Growth-Function\" class=\"headerlink\" title=\"Types of Growth Function\"></a>Types of Growth Function</h3><ul>\n<li>Positive Rays <img data-src=\"/img/ML/vmoIwfN.png\" alt=\"\"><ul>\n<li>$m_H(N)$ = N + 1</li>\n</ul>\n</li>\n<li>Positive Intervals <img data-src=\"/img/ML/FcLeNhZ.png\" alt=\"\"><ul>\n<li>$C^{N+1}_2 + 1$ <img data-src=\"/img/ML/D4mfUyr.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Convex Sets<ul>\n<li>worst case: every point make a circle <img data-src=\"/img/ML/tVqlZrK.png\" alt=\"\"></li>\n<li>$m_H(N) = 2^N$ -&gt; exists N inputs that can be <strong>shattered(所有output皆可產生)</strong></li>\n</ul>\n</li>\n</ul>\n<p><img data-src=\"/img/ML/eEXFWde.png\" alt=\"\"><br>Now $m_H(N)$ is finite, but exponential<br>Question:Can we find polynomial instead of exponential?</p>\n<h3 id=\"Break-Point-of-H\"><a href=\"#Break-Point-of-H\" class=\"headerlink\" title=\"Break Point of H\"></a>Break Point of H</h3><p>if all possible k inputs can’t be shattered by H<br>k = break point for H <img data-src=\"/img/ML/q3wjQSm.png\" alt=\"\"></p>\n<p>2D perceptrons: break point at 4<br>3 inputs: exist at least one input that can shatter <img data-src=\"/img/ML/perceptron-shatter.png\" alt=\"\"><br>4 inputs: for all inputs, no shatter  </p>\n<p>If there is no breakpoint, we can only find exponential($2^N$) increase<br>If there is a breakpoint, we can find polynomial($O(N^k)$)increase<br>breakpoint愈小，hypothesis set 成長的速度受到愈多限制(因為無法shatter，所以hypothesis數比exponential小)</p>\n<h2 id=\"Chap06-Theory-of-Generalization\"><a href=\"#Chap06-Theory-of-Generalization\" class=\"headerlink\" title=\"Chap06 Theory of Generalization\"></a>Chap06 Theory of Generalization</h2><p>Q: maximum possible $m_H(N)$ if input number(N) = 3 when breakpoint(k) = 2?<br>A: x1, x2 cannot shatter, and so does x2, x3 and x1, x3 <img data-src=\"/img/ML/KE3Xwxf.png\" alt=\"\"><br>→ When N &gt; breakpoint, break point restricts $m_H(N)$ a lot!</p>\n<p>idea: prove $m_H(N) \\leq$ poly(N) if N &gt; k </p>\n<h3 id=\"Bounding-function\"><a href=\"#Bounding-function\" class=\"headerlink\" title=\"Bounding function\"></a>Bounding function</h3><p>bounding function B(N, k): maximum possible $m_H(N)$ when break point = k</p>\n<p>Table of bounding function(incomplete) <img data-src=\"/img/ML/darN0tn.png\" alt=\"\"><br>B(N, k) = $m_H(N) = 2^N$ when N &lt; k(shatter)<br>B(N, k) &lt; $m_H(N) = 2^N - 1$ when N = k(至少比shatter少一種)<br>When N &gt; k :Using reduce, Ex. B(4,3) <img data-src=\"/img/ML/gDjeq7v.png\" alt=\"\"><br>α: dichotomies on (x1, x2, x3) with x4 paired<br>β: dichotomies on (x1, x2, x3) with x4 no paired</p>\n<p>Because B(4,3) can’t shatter any 3 inputs<br>→ α + β can’t shatter at (x1, x2, x3)<br>→ α + β $\\leq$ B(3,3)</p>\n<p>Because B(4,3) can’t shatter any 3 inputs and x4 is already paired<br>→ α can’t shatter any 2 inputs at (x1, x2, x3)<br>→ α $\\leq$ B(3,2)</p>\n<p>B(4,3) = 2α + β $\\leq$ B(3,3) + B(3,2)<br>Generalized: B(N,k) $\\leq$ B(N-1,k) + B(N-1,k-1) <img data-src=\"/img/ML/jbksHEC.png\" alt=\"\"><br>By calculation: $m_H(N) \\leq B(N,k) \\leq N^{k-1}$  </p>\n<p>Conclusion: $m_H(N)$ is polynomial if break point exists for N &gt;= 2 &amp; k &gt;= 3!!<br><img data-src=\"/img/ML/M8N4HsO.png\" alt=\"\"><br><img data-src=\"/img/ML/OqhVOS4.png\" alt=\"\"><br>‘&lt;=’ can be ‘=’ actually -&gt; not easy proof(skipped)</p>\n<h3 id=\"Vapnik-Chervonenkis-VC-bound\"><a href=\"#Vapnik-Chervonenkis-VC-bound\" class=\"headerlink\" title=\"Vapnik-Chervonenkis (VC) bound\"></a>Vapnik-Chervonenkis (VC) bound</h3><p>Proof: BAD Bound for General H   </p>\n<ol>\n<li>Now Ein(h) finite, but Eout(h) still infinite(Eout的點有無限個)<ol>\n<li>use ghost sample data Ein’ to replace(<strong>想像</strong>再sample一次會產生的Ein’，將這段資料作為eout)</li>\n<li>圖中Ein離Eout很遠，是bad data，只要Ein’在Eout附近，Ein’也會離Eout很遠 <img data-src=\"/img/ML/kK29SSC.png\" alt=\"\"></li>\n<li>Eout 乘1/2，使其成為不等式 <img data-src=\"/img/ML/jr6WUKW.png\" alt=\"\"></li>\n</ol>\n</li>\n<li>將bad data相似的hypothesis分在一起 <img data-src=\"\" alt=\"\"><ol>\n<li>總共有2N個data(Ein + Ein’) → $m_H(2N)$ <img data-src=\"/img/ML/MQ5v22d.png\" alt=\"\"></li>\n<li>因為有了$m_H()$函數，變成只考慮固定的hypothesis   </li>\n</ol>\n</li>\n<li>Use Hoeffding without Replacement<ol>\n<li>可視為2N個點取N個點，sample為Ein，剩下為Ein’(不放回去)</li>\n<li>使用 ‘Hoeffding without Replacement’： 公式和hoeffding 一樣 <img data-src=\"/img/ML/0ZC5xI3.png\" alt=\"\"></li>\n<li>Hoeffding只用於單一hypothesis，所以需要步驟2</li>\n</ol>\n</li>\n</ol>\n<p>Vapnik-Chervonenkis (VC) bound <img data-src=\"/img/ML/tjn5okQ.png\" alt=\"\"><br>→ proved that learning with <strong>2D perceptrons</strong> feasible!<br><img data-src=\"/img/ML/kyXVoYU.png\" alt=\"\"><br>You need to let everything good to learned well <img data-src=\"/img/ML/n8YPfWQ.png\" alt=\"\"></p>\n<h2 id=\"Chap-07-VC-Dimension\"><a href=\"#Chap-07-VC-Dimension\" class=\"headerlink\" title=\"Chap 07 VC Dimension\"></a>Chap 07 VC Dimension</h2><p>VC Dimension<br>= maximum non-break point = (minimum k) - 1<br>= largest N that can shatter </p>\n<p>2D perceptron review <img data-src=\"/img/ML/EOUT=0.png\" alt=\"\"><br>How does PLA in more than 2 dimension?  </p>\n<ul>\n<li>2D → 3</li>\n<li>d-dimension perceptron <ul>\n<li>d_VC = d+1 </li>\n</ul>\n</li>\n</ul>\n<p>Proof</p>\n<ol>\n<li>d_VC &gt; d+1 → d+1 can shatter<br>input matrix which is invertible <img data-src=\"/img/ML/specificmatrix.png\" alt=\"\"><br>for any y, we can find w such that sign(Xw) = y → $w = yX^{-1}$ → it can shatter </li>\n<li>d_VC &lt; d+1 → d+2 can’t shatter<br>linear dependence restricts dichotomy <img data-src=\"/img/ML/linearrely.png\" alt=\"\"><br>if row &gt; column, it would cause linear dependence <img data-src=\"/img/ML/xd+2=all.png\" alt=\"\"><br>for any input, we can find some $a_n$ that makes an output can’t happen → no shatter <img data-src=\"/img/ML/geneag0.png\" alt=\"\"></li>\n</ol>\n<h3 id=\"freedom\"><a href=\"#freedom\" class=\"headerlink\" title=\"freedom\"></a>freedom</h3><p>dimension, number of parameters, hypothesis quantity(M) → degrees of freedom<br>d_VC(H) = effitive binary degrees of freedom = powerfulness of H</p>\n<p>The more powerful it is (d_vc bigger), the more probability to get bad data <img data-src=\"/img/ML/dvcbigsmall.png\" alt=\"D_vc\"><br>question:<img data-src=\"/img/ML/Qhyperplane.png\" alt=\"\"><br>比perceptron少一個parameter → d</p>\n<p>penalty for model complexity <img data-src=\"/img/ML/smalle.png\" alt=\"\"><br>model愈強，Ein愈小，和Eout誤差愈大 <img data-src=\"/img/ML/modelcomplexity.png\" alt=\"\">  </p>\n<p>number of data(N) should be 10000 d_vc in theory; 10 d_vc is enough in practice, because VC bound is loose <img data-src=\"/img/ML/nanddvc.png\" alt=\"\"> <img data-src=\"/img/ML/hoffedingloose.png\" alt=\"\"></p>\n<p>question: <img data-src=\"/img/ML/q2.png\" alt=\"\"><br>all of above(increase power of model)</p>\n<h2 id=\"Chap08-Noise-and-Error\"><a href=\"#Chap08-Noise-and-Error\" class=\"headerlink\" title=\"Chap08 Noise and Error\"></a>Chap08 Noise and Error</h2><ul>\n<li>Noise in y<ul>\n<li>Example: good customer mislabeled as bad</li>\n</ul>\n</li>\n<li>Noise in x<ul>\n<li>Example: incorrect feature calculation </li>\n</ul>\n</li>\n<li>Would get probabilisic output y ≠ h(x) by given P(y|x)</li>\n</ul>\n<p>Does VC bound works in noise? Yes, if i.i.d.(Independent and identically distributed) <img data-src=\"/img/ML/iid.png\" alt=\"\"><br>→ we can view as ‘ideal mini-target’ + noise<br>→ learning goal is to <strong>predict ideal mini-target(which is Y that has high P(Y|X) given X) on often seen inputs(X with high P(X))</strong> </p>\n<p>Eout use expectation instead of Σ , $err$ means pointwise error(only consider a point x) <img data-src=\"/img/ML/einout.png\" alt=\"\">  </p>\n<h3 id=\"Error-Measure\"><a href=\"#Error-Measure\" class=\"headerlink\" title=\"Error Measure\"></a>Error Measure</h3><p><img data-src=\"/img/ML/01andsquare.png\" alt=\"\"></p>\n<ul>\n<li>classification(0/1 error)<ul>\n<li>minimum flipping noise(最少錯誤的output) </li>\n<li>NP-hard to optimize</li>\n</ul>\n</li>\n<li>regression use squared error<ul>\n<li>minimum gaussian noise(output和正確答案的平方差最小)</li>\n</ul>\n</li>\n</ul>\n<p>Error is *<em>application/user dependent *</em> </p>\n<ul>\n<li>CIA fingerprint login error<ul>\n<li>not allow predict 0  to 1 <img data-src=\"/img/ML/unbalancedata.png\" alt=\"\">     </li>\n</ul>\n</li>\n<li>Supermarket member login error<ul>\n<li>not want to predict 1 to 0 </li>\n</ul>\n</li>\n<li>error weight is not the same!</li>\n</ul>\n<p>Example: pocket  </p>\n<ul>\n<li>modify Ein to $E^w_{in}$(with weight)</li>\n<li>weight愈高的錯誤愈容易被選來修正</li>\n</ul>\n<p>權重可以套用在許多機器學習的演算法</p>\n<h3 id=\"algorithm-choosing\"><a href=\"#algorithm-choosing\" class=\"headerlink\" title=\"algorithm choosing\"></a>algorithm choosing</h3><p>Algorithmic Error Measures $\\hat{err}$   </p>\n<ul>\n<li>True<ul>\n<li>error cannot be ignored or created</li>\n</ul>\n</li>\n<li>plausible(可用性)<ul>\n<li>0/1 error</li>\n<li>squared error</li>\n</ul>\n</li>\n<li>friendly(較容易的演算法)    <ul>\n<li>close form solution(有公式解，如Chap09的linear regression)</li>\n<li>convex objective function(可以持續更新的，如PLA)</li>\n</ul>\n</li>\n<li>$\\hat{err}$ is key part of many algorithms</li>\n</ul>\n<p><img data-src=\"/img/ML/err-flow.png\" alt=\"\"></p>\n<h2 id=\"參考資料\"><a href=\"#參考資料\" class=\"headerlink\" title=\"參考資料\"></a>參考資料</h2><p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2NvdXJzZS9udHVtbG9uZQ==\">Coursera機器學習基石<i class=\"fa fa-external-link-alt\"></i></span><br>C老師上課講解</p>\n",
            "tags": [
                "機器學習",
                "perceptron"
            ]
        }
    ]
}