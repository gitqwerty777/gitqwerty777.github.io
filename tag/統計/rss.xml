<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>QWERTY • Posts by &#34;統計&#34; tag</title>
        <link>http://gitqwerty777.github.io</link>
        <description>Programming | Computer Science | Thought</description>
        <language>zh-TW</language>
        <pubDate>Fri, 01 May 2015 12:37:47 +0800</pubDate>
        <lastBuildDate>Fri, 01 May 2015 12:37:47 +0800</lastBuildDate>
        <category>C#</category>
        <category>CodingStyle</category>
        <category>Emacs</category>
        <category>編輯器</category>
        <category>CFR</category>
        <category>電腦對局理論</category>
        <category>指令</category>
        <category>機器學習</category>
        <category>perceptron</category>
        <category>readme</category>
        <category>文件</category>
        <category>github</category>
        <category>artificial intelligence</category>
        <category>search</category>
        <category>First-Order Logic</category>
        <category>大數</category>
        <category>程式</category>
        <category>C++</category>
        <category>Hexo</category>
        <category>網誌</category>
        <category>Markdown</category>
        <category>CleanCode</category>
        <category>重構</category>
        <category>TDD</category>
        <category>設計模式</category>
        <category>CMake</category>
        <category>Makefile</category>
        <category>Linux</category>
        <category>Todo</category>
        <category>註解</category>
        <category>經濟學</category>
        <category>策略</category>
        <category>競爭</category>
        <category>博弈論</category>
        <category>計算機結構</category>
        <category>人工智慧</category>
        <category>圍棋</category>
        <category>象棋</category>
        <category>蒙地卡羅</category>
        <category>Alpha-Beta搜尋</category>
        <category>強化學習</category>
        <category>計算機網路</category>
        <category>boost</category>
        <category>函式庫</category>
        <category>編譯</category>
        <category>gcc</category>
        <category>g++</category>
        <category>clang</category>
        <category>最佳化</category>
        <category>推薦系統</category>
        <category>FM</category>
        <category>FFM</category>
        <category>SVM</category>
        <category>Embedding</category>
        <category>自然語言處理</category>
        <category>外國用語</category>
        <category>萌典</category>
        <category>opencc</category>
        <category>PTT</category>
        <category>vuejs</category>
        <category>linux</category>
        <category>c</category>
        <category>compile</category>
        <category>gdb</category>
        <category>c語言</category>
        <category>cpp</category>
        <category>除錯</category>
        <category>git</category>
        <category>VMWare</category>
        <category>虛擬機</category>
        <category>IFTTT</category>
        <category>自動化</category>
        <category>備份</category>
        <category>webhook</category>
        <category>簡報</category>
        <category>軟體</category>
        <category>PowerPoint</category>
        <category>Latex</category>
        <category>JavaScript</category>
        <category>CSS</category>
        <category>Unity</category>
        <category>fcitx</category>
        <category>嘸蝦米</category>
        <category>輸入法</category>
        <category>硬碟</category>
        <category>記憶體</category>
        <category>效能</category>
        <category>錯誤</category>
        <category>makefile</category>
        <category>備忘錄</category>
        <category>存檔</category>
        <category>統計</category>
        <category>byobu</category>
        <category>screen</category>
        <category>tmux</category>
        <category>reactjs</category>
        <category>javascript</category>
        <category>WideAndDeep</category>
        <category>Google</category>
        <category>觀察者</category>
        <category>訂閱</category>
        <category>委託</category>
        <category>正規表示式(RegExp)</category>
        <category>上下文無關文法(CFG)</category>
        <category>hexo</category>
        <category>blog</category>
        <category>theme</category>
        <category>feature</category>
        <category>revealJS</category>
        <category>markdown</category>
        <category>rss</category>
        <category>facebook</category>
        <category>youtube</category>
        <category>ptt</category>
        <category>bilibili</category>
        <category>pixiv</category>
        <category>crawler</category>
        <category>SEO</category>
        <category>google</category>
        <category>html</category>
        <category>amazon</category>
        <category>webhost</category>
        <category>ssl</category>
        <category>漢字</category>
        <category>中文</category>
        <category>異體字</category>
        <category>unicode</category>
        <category>unity</category>
        <category>演算法</category>
        <category>隨機排序</category>
        <category>洗牌</category>
        <category>Fisher-Yates</category>
        <category>證明</category>
        <category>python</category>
        <item>
            <guid isPermalink="true">http://gitqwerty777.github.io/natural-language-processing2/</guid>
            <title>自然語言處理(下)</title>
            <link>http://gitqwerty777.github.io/natural-language-processing2/</link>
            <category>機器學習</category>
            <category>自然語言處理</category>
            <category>統計</category>
            <pubDate>Fri, 01 May 2015 12:37:47 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;Chap08-Syntax-and-Grammars&#34;&gt;&lt;a href=&#34;#Chap08-Syntax-and-Grammars&#34; class=&#34;headerlink&#34; title=&#34;Chap08 Syntax and Grammars&#34;&gt;&lt;/a&gt;Chap08 Syntax and Grammars&lt;/h2&gt;&lt;p&gt;Grammar    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;represent certain knowledges of what we know about a language&lt;/li&gt;
&lt;li&gt;General criteria&lt;ul&gt;
&lt;li&gt;linguistic naturalness&lt;/li&gt;
&lt;li&gt;mathematical power&lt;/li&gt;
&lt;li&gt;computational effectiveness&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Context-free grammars(CFG)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alias&lt;ul&gt;
&lt;li&gt;Phrase structure grammars&lt;/li&gt;
&lt;li&gt;Backus-Naur form&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;More powerful than finite state machine&lt;/li&gt;
&lt;li&gt;Rules &lt;ul&gt;
&lt;li&gt;Terminals &lt;ul&gt;
&lt;li&gt;words&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Non-terminals &lt;ul&gt;
&lt;li&gt;Noun phrase, Verb phrase …&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Generate strings in the language&lt;/li&gt;
&lt;li&gt;Reject strings not in the language  &lt;/li&gt;
&lt;li&gt;LHS → RHS&lt;ul&gt;
&lt;li&gt;LHS: Non-terminals &lt;/li&gt;
&lt;li&gt;RHS: Non-terminals or Terminals&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Context Free&lt;ul&gt;
&lt;li&gt;probability of a subtree does not depend on words not dominated by the subtree(subtree出現的機率和上下文無關)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;a id=&#34;more&#34;&gt;&lt;/a&gt;

&lt;p&gt;Evaluation  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Does it undergenerate?&lt;ul&gt;
&lt;li&gt;rules cannot completely explain language&lt;/li&gt;
&lt;li&gt;not a problem if the goal is to produce a language&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Does it overgenerate?&lt;ul&gt;
&lt;li&gt;rules overly explain the language&lt;/li&gt;
&lt;li&gt;not a problem if the goal is to recognize or understand well-formed(correct) language&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Does it assign appropriate structures to the strings that it generates?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Parsing  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;take a string and a grammar&lt;/li&gt;
&lt;li&gt;assigning trees that covers all and only words in input strings&lt;/li&gt;
&lt;li&gt;return parse tree for that string&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;English Grammar Fragment  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sentences&lt;/li&gt;
&lt;li&gt;Noun Phrases&lt;ul&gt;
&lt;li&gt;Ex. NP → det Nominal&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;head: central criticial noun in NP&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;important in statistical parsing&lt;/li&gt;
&lt;li&gt;after det(冠詞), before pp(介系詞片語) &lt;img data-src=&#34;/img/NLP/np-parse.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Agreement&lt;ul&gt;
&lt;li&gt;a part of overgenerate&lt;/li&gt;
&lt;li&gt;This flight(○)&lt;/li&gt;
&lt;li&gt;This flights(×)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Verb Phrases&lt;ul&gt;
&lt;li&gt;head verb with arguments&lt;/li&gt;
&lt;li&gt;Subcategorization(分類)&lt;ul&gt;
&lt;li&gt;categorize according to VP rules&lt;/li&gt;
&lt;li&gt;a part of overgenerate&lt;/li&gt;
&lt;li&gt;Prefer&lt;ul&gt;
&lt;li&gt;I prefer [to leave earlier]TO-VP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Told&lt;ul&gt;
&lt;li&gt;I was told [United has a flight]S&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overgenerate Solution&lt;br&gt;    - transform into multiple rules&lt;br&gt;        - NP → Single_Det Single_Nominal&lt;br&gt;        - NP → 複數_Det 複數_Nominal&lt;br&gt;        - Will generate a lot of rules!&lt;br&gt;    - out of CFG framework&lt;br&gt;        - Chomsky hierarchy of languages &lt;img data-src=&#34;/img/NLP/modelclass.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3poLndpa2lwZWRpYS5vcmcvd2lraS8lRTklOTklODQlRTYlQTAlODclRTglQUYlQUQlRTglQTglODA=&#34;&gt;Indexed Grammar&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Indexed grammars and languages problem &lt;img data-src=&#34;/img/NLP/index-example.png&#34; alt=&#34;&#34;&gt; &lt;/li&gt;
&lt;li&gt;recognized by nested stack automata &lt;img data-src=&#34;/img/NLP/index-grammar.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Treebanks-and-headfinding&#34;&gt;&lt;a href=&#34;#Treebanks-and-headfinding&#34; class=&#34;headerlink&#34; title=&#34;Treebanks and headfinding&#34;&gt;&lt;/a&gt;Treebanks and headfinding&lt;/h3&gt;&lt;p&gt;critical to the development of statistical parsers&lt;/p&gt;
&lt;p&gt;Treebanks  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;corpora with parse trees&lt;ul&gt;
&lt;li&gt;created by automatic parser and human annotators&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ex. Penn Treebank&lt;/li&gt;
&lt;li&gt;Grammar&lt;ul&gt;
&lt;li&gt;Treebanks implicitly define a grammar&lt;ul&gt;
&lt;li&gt;Simply make all subtrees fit the rules&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;parse tree tend to be very flat to avoid recursion&lt;ul&gt;
&lt;li&gt;Penn Treebank has ~4500 different rules for VPs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Head Finding  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use tree traversal rules specific to each non-terminal in the grammar&lt;/li&gt;
&lt;li&gt;先向右再向左 &lt;img data-src=&#34;/img/NLP/head-np.png&#34; alt=&#34;&#34;&gt;&lt;!--重要--&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Dependency-Grammars&#34;&gt;&lt;a href=&#34;#Dependency-Grammars&#34; class=&#34;headerlink&#34; title=&#34;Dependency Grammars&#34;&gt;&lt;/a&gt;Dependency Grammars&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;every possible parse is a tree &lt;img data-src=&#34;/img/NLP/dependency-parse.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;every node is a word &lt;/li&gt;
&lt;li&gt;every link is dependency relations between words &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advantage&lt;ul&gt;
&lt;li&gt;Deals well with long-distance word order languages &lt;ul&gt;
&lt;li&gt;structure is flexible&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parsing is much faster than CFG&lt;/li&gt;
&lt;li&gt;Tree can be used by later applications&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Approaches&lt;!--重要--&gt;&lt;ul&gt;
&lt;li&gt;Optimization-based approaches &lt;ul&gt;
&lt;li&gt;search for the tree that matches some criteria the best&lt;/li&gt;
&lt;li&gt;spanning tree algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Shift-reduce approaches&lt;ul&gt;
&lt;li&gt;greedily take actions based on the current word and state&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Summary  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Constituency(顧客, words that behave as a single unit) is a key phenomena easily captured with CFG rules&lt;ul&gt;
&lt;li&gt;But agreement and subcategorization make problems&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap09-Syntactic-Parsing&#34;&gt;&lt;a href=&#34;#Chap09-Syntactic-Parsing&#34; class=&#34;headerlink&#34; title=&#34;Chap09 Syntactic Parsing&#34;&gt;&lt;/a&gt;Chap09 Syntactic Parsing&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Top-Down Search &lt;img data-src=&#34;/img/NLP/top-down.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;Search trees among possible answers  &lt;!--- But suggests trees that are not consistent with words--&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bottom-Up Parsing&lt;ul&gt;
&lt;li&gt;Only forms trees that can fit the words &lt;!-- global tree may not form answer(S) --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mixed parsing strategy&lt;ul&gt;
&lt;li&gt;looks like Binomial Search&lt;/li&gt;
&lt;li&gt;The number of rules tried at each deicision of the analysis (branching factor)&lt;ul&gt;
&lt;li&gt;top-down parsing: categories of LHS(Left Hand Side) word&lt;/li&gt;
&lt;li&gt;bottom-up parsing: categories of left most RHS(Right Hand Side) word&lt;ul&gt;
&lt;li&gt;倒推：從最左邊可以倒推的字開始&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;backtracking methods are doomed because of two inter-related problems  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1)Structural and lexical ambiguity&lt;ul&gt;
&lt;li&gt;PP(介系詞片語) attachment&lt;ul&gt;
&lt;li&gt;PP can attach to [sentences, verb phrases, noun phrases, and adjectival phrases]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;coordination&lt;ul&gt;
&lt;li&gt;P and Q or R &lt;ul&gt;
&lt;li&gt;P and (Q or R)&lt;/li&gt;
&lt;li&gt;(P and Q) or R&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;noun-noun compounding&lt;ul&gt;
&lt;li&gt;town widget hammer&lt;ul&gt;
&lt;li&gt;((town widget) hammer)&lt;/li&gt;
&lt;li&gt;(town (widget hammer))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solution&lt;ul&gt;
&lt;li&gt;how to determine the intended structure?&lt;/li&gt;
&lt;li&gt;how to store the partial structures?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(2)Shared subproblems&lt;ul&gt;
&lt;li&gt;naïve backtracking will lead to duplicated work(不一定會對，所以會一直backtrack…)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dynamic Programming  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avoid doing repeated work&lt;/li&gt;
&lt;li&gt;Solve in polynomial time&lt;/li&gt;
&lt;li&gt;approaches&lt;ul&gt;
&lt;li&gt;CKY&lt;/li&gt;
&lt;li&gt;Earley&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;CKY-bottom-up&#34;&gt;&lt;a href=&#34;#CKY-bottom-up&#34; class=&#34;headerlink&#34; title=&#34;CKY(bottom-up)&#34;&gt;&lt;/a&gt;CKY(bottom-up)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;transform rules into Chomsky-Normal Form &lt;img data-src=&#34;/img/NLP/cnf-transform.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;build a table &lt;ul&gt;
&lt;li&gt;A spanning from i to j in the input is in [i,j]&lt;/li&gt;
&lt;li&gt;A → BC == [i,j] → [i,k] [k,j]&lt;/li&gt;
&lt;li&gt;entire string = [0, n] &lt;ul&gt;
&lt;li&gt;expected to be S&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iterate all possible k &lt;img data-src=&#34;/img/NLP/CKY-table2.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;[i,j] = [i,i+1]x[i+1, j], [i,i+2]x[i+2,j] ……&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;fill the table a column at a time, from left to right, bottom to top &lt;img data-src=&#34;/img/NLP/CKY-table3.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;Ex. [1,3] = [1,2]Det + [2,3] Nomimal, Noun = NP&lt;/li&gt;
&lt;li&gt;Ex. &lt;img data-src=&#34;/img/NLP/CKY-ex.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Algorithm &lt;img data-src=&#34;/img/NLP/CKY-algo.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Performance&lt;ul&gt;
&lt;li&gt;a lot of elements unrelated to the answer&lt;/li&gt;
&lt;li&gt;can use online search to fill table (from left to right)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Earley&#34;&gt;&lt;a href=&#34;#Earley&#34; class=&#34;headerlink&#34; title=&#34;Earley&#34;&gt;&lt;/a&gt;Earley&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;parser that exploits chart as data structure&lt;/li&gt;
&lt;li&gt;node = vertex&lt;/li&gt;
&lt;li&gt;arc = edge&lt;ul&gt;
&lt;li&gt;active edge: (a) and (b)&lt;/li&gt;
&lt;li&gt;inactive edge: (c)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;decorated grammar  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(a) “•” in the first &lt;img data-src=&#34;/img/NLP/dot-a.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;• NP VP&lt;/li&gt;
&lt;li&gt;A hypothesis has been made, but has not been verified yet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(b) “•” in the middle &lt;img data-src=&#34;/img/NLP/dot-b.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;NP • VP&lt;/li&gt;
&lt;li&gt;A hypothesis has been partially confirmed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(c) “•” in the last&lt;ul&gt;
&lt;li&gt;NP VP •&lt;/li&gt;
&lt;li&gt;A hypothesis has been wholly confirmed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;representation of edge &lt;img data-src=&#34;/img/NLP/chart-struct.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;initialization &lt;img data-src=&#34;/img/NLP/chart-initialize.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;for each rule A → W, if A is a category that can span a chart (typically S), add &amp;lt;0, 0, A → •W&amp;gt; &lt;img data-src=&#34;/img/NLP/chartchart-init.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;A implies •W from position 0 to 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Housekeeping&lt;ul&gt;
&lt;li&gt;prevent duplicate rules&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fundamental rule  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the chart contains &amp;lt;i, j, A → W1 •B W2&amp;gt; and &amp;lt;j, k, B → W3 •&amp;gt;, then add edge &amp;lt;i, k, A → W1 B •W2&amp;gt; to the chart &lt;img data-src=&#34;/img/NLP/chart-fund.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Notes&lt;ol&gt;
&lt;li&gt;New edge may be either active or inactive&lt;/li&gt;
&lt;li&gt;does not remove the active edge that has succeeded&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bottom-up rule  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if adding edge &amp;lt;i, j, C → W1 •&amp;gt; to the chart, then for every rule that has the form B → C W2, add &amp;lt;i, i, B → • C W2&amp;gt; &lt;img data-src=&#34;/img/NLP/chart-bottom.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Top-down rule   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If adding edge &amp;lt;i, j, C → W1 •B W2&amp;gt; to the chart, then for each rule B → W, add &amp;lt; j, j, B →•W&amp;gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Full-Syntactic-Parsing&#34;&gt;&lt;a href=&#34;#Full-Syntactic-Parsing&#34; class=&#34;headerlink&#34; title=&#34;Full Syntactic Parsing&#34;&gt;&lt;/a&gt;Full Syntactic Parsing&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;necessary for deep semantic analysis of texts&lt;/li&gt;
&lt;li&gt;not practical for many applications (given typical resources)&lt;ul&gt;
&lt;li&gt;O(n^3) for straight parsing&lt;/li&gt;
&lt;li&gt;O(n^5) for probabilistic versions&lt;/li&gt;
&lt;li&gt;Too slow for real time applications (search engines)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two Alternatives&lt;ul&gt;
&lt;li&gt;Dependency parsing&lt;ul&gt;
&lt;li&gt;Change the underlying grammar formalism&lt;/li&gt;
&lt;li&gt;can get a lot done with just binary relations among the words&lt;/li&gt;
&lt;li&gt;詳見Chap08 dependency grammar&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Partial parsing&lt;ul&gt;
&lt;li&gt;Approximate phrase-structure parsing with finite-state and statistical approaches&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Both of these approaches give up something (syntactic, structure) in return for more robust and efficient parsing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Partial parsing&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For many applications you don’t really need full parse&lt;/li&gt;
&lt;li&gt;For example, if you’re interested in locating all the people, places and organizations  &lt;ul&gt;
&lt;li&gt;base-NP chunking &lt;ul&gt;
&lt;li&gt;[NP The morning flight] from [NP Denvar] has arrived &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two approaches&lt;ul&gt;
&lt;li&gt;Rule-based (hierarchical) transduction(轉導) &lt;!--???--&gt;&lt;ul&gt;
&lt;li&gt;Restrict recursive rules (make the rules flat)&lt;ul&gt;
&lt;li&gt;like NP → NP VP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Group the rules so that RHS of the rules can refer to non-terminals introduced in earlier transducers, but not later ones&lt;/li&gt;
&lt;li&gt;Combine the rules in a group in the same way we did with the rules for spelling changes&lt;/li&gt;
&lt;li&gt;Combine the groups into a cascade&lt;ul&gt;
&lt;li&gt;can be used to find the sequence of flat chunks you’re interested in&lt;/li&gt;
&lt;li&gt;or approximate hierarchical trees you get from full parsing with a CFG&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Typical Architecture ![](/img/NLP/Cascaded Transducers.png)&lt;ul&gt;
&lt;li&gt;Phase 1: Part of speech tags&lt;/li&gt;
&lt;li&gt;Phase 2: Base syntactic phrases&lt;/li&gt;
&lt;li&gt;Phase 3: Larger verb and noun groups&lt;/li&gt;
&lt;li&gt;Phase 4: Sentential level rules&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Statistical sequence labeling&lt;ul&gt;
&lt;li&gt;HMMs&lt;/li&gt;
&lt;li&gt;MEMMs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap10-Statistical-Parsing&#34;&gt;&lt;a href=&#34;#Chap10-Statistical-Parsing&#34; class=&#34;headerlink&#34; title=&#34;Chap10 Statistical Parsing&#34;&gt;&lt;/a&gt;Chap10 Statistical Parsing&lt;/h2&gt;&lt;p&gt;Motivation  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N-gram models and HMM Tagging only allowed us to process sentences linearly&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probabilistic Context Free Grammars&lt;/strong&gt;(PCFG)&lt;ul&gt;
&lt;li&gt;alias: Stochastic context-free grammar(SCFG)&lt;/li&gt;
&lt;li&gt;simplest and most natural probabilistic model for tree structures&lt;/li&gt;
&lt;li&gt;closely related to those for HMMs&lt;/li&gt;
&lt;li&gt;為每一個CFG的規則標示其發生的可能性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Idea  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reduce “right” parse to “most probable parse”&lt;ul&gt;
&lt;li&gt;Argmax P(Parse|Sentence)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A PCFG consists of  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;set of terminals, {wk}&lt;/li&gt;
&lt;li&gt;set of nonterminals, {Ni}&lt;/li&gt;
&lt;li&gt;start symbol N1&lt;/li&gt;
&lt;li&gt;set of rules&lt;ul&gt;
&lt;li&gt;{Ni –&amp;gt; ξj}(ξj is a sequence of terminals and nonterminals)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;probabilities of rules&lt;ul&gt;
&lt;li&gt;total probability of imply Ni to other sequence ξj is 1 &lt;/li&gt;
&lt;li&gt;∀i Σj P(Ni → ξj) = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Probability of sentence according to grammar G &lt;ul&gt;
&lt;li&gt;P($w_{1m}$) = sum of P($w_{1m}$, t) for every possible tree t&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Nj dominates the words wa … wb&lt;ul&gt;
&lt;li&gt;Nj → wa … wb&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assumptions of the Model  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Place Invariance&lt;ul&gt;
&lt;li&gt;probability of a subtree does not depend on its position in the string&lt;/li&gt;
&lt;li&gt;similar to time invariance in HMMs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ancestor Free&lt;ul&gt;
&lt;li&gt;probability of a subtree does not depend on nodes in the derivation outside the subtree(subtree的機率只和subtree內的node有關)&lt;/li&gt;
&lt;li&gt;can simplify probability calculation &lt;img data-src=&#34;/img/NLP/after-assump.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Questions of PCFGs(similar to three questions of HMM)    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assign probabilities to parse trees&lt;ul&gt;
&lt;li&gt;What is the probability of a sentence $w_{1m}$ according to a grammar G&lt;ul&gt;
&lt;li&gt;P(w1m|G)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parsing with probabilities(Decoding)&lt;ul&gt;
&lt;li&gt;What is the most likely parse for a sentence&lt;ul&gt;
&lt;li&gt;argmax_t P(t|w1m,G) &lt;/li&gt;
&lt;li&gt;How to efficiently find the best (or N best) trees &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Training the model (Learning) &lt;ul&gt;
&lt;li&gt;How to set rule probabilities(parameter of grammar model) that maximize the probability of a sentence&lt;ul&gt;
&lt;li&gt;argmax_G P(w1m|G)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simple Probability Model  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;probability of a tree is the product of the probabilities of rules in derivation&lt;/li&gt;
&lt;li&gt;Rule Probabilities&lt;ul&gt;
&lt;li&gt;S → NP &lt;/li&gt;
&lt;li&gt;P(NP | S)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Training the Model&lt;ul&gt;
&lt;li&gt;estimate probability from data&lt;/li&gt;
&lt;li&gt;P(α → β | α) = Count(α→β) / Count(α) = Count(α→β) / Σγ Count(α→γ)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parsing (Decoding)&lt;ul&gt;
&lt;li&gt;trees with highest probability in the model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: Book the dinner flight&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/pm-ex.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/pm-ex2.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;too slow!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dynamic Programming again  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use CKY and Earley to &lt;strong&gt;parse&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Viterbi and HMMs to &lt;strong&gt;get the best parse&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Parameters of a PCFG in Chomsky Normal Form&lt;ul&gt;
&lt;li&gt;P(Nj→NrNs | G) , $n^3$ matrix of parameters&lt;/li&gt;
&lt;li&gt;P(Nj→wk | G), $nV$ parameters&lt;/li&gt;
&lt;li&gt;n is the number of nonterminals &lt;/li&gt;
&lt;li&gt;V is the number of terminals&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Σr,s P(Nj→NrNs) + ΣkP(Nj→wk) = 1&lt;ul&gt;
&lt;li&gt;所有由Nj導出的rule，機率總和必為1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Probabilistic Regular Grammars (PRG)    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;start state N1 &lt;/li&gt;
&lt;li&gt;rules&lt;ul&gt;
&lt;li&gt;Ni → wjNk&lt;/li&gt;
&lt;li&gt;Ni → wj&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PRG is a HMM with [start state] and [finish(sink) state] &lt;img data-src=&#34;/img/NLP/prg-sink.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inside and Outside probability &lt;img data-src=&#34;/img/NLP/prg-graph.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/prg-bf.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/prg-bf2.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Forward(Outside) probability&lt;ul&gt;
&lt;li&gt;$ α&lt;em&gt;i(t) = P(w&lt;/em&gt;{1(t-1)}, X_t = i)$&lt;/li&gt;
&lt;li&gt;everything above a certain node(include the node)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Backward(Inside) probability&lt;ul&gt;
&lt;li&gt;$ β&lt;em&gt;i(t, T) = P(w&lt;/em&gt;{tT} | X_t = i)$&lt;/li&gt;
&lt;li&gt;everything below a certain node&lt;/li&gt;
&lt;li&gt;total probability of generating words $w_t \cdots w_T$, given the root nonterminal $N^i$ and a grammar G&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inside Algorithm (bottom-up)      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P(w_{1m} | G) = P(N_1 → w_{1m} | G) = P(w_{1m} | N^1_{1m}, G) = B_1(1,m)$&lt;ul&gt;
&lt;li&gt;$B_1(1,m)$ is Inside probability&lt;ul&gt;
&lt;li&gt;P(w1~wm are below N1(start symbol))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;base rule&lt;ul&gt;
&lt;li&gt;$ B_j(k, k) = P(w_k | N^j_{kk}, G) = P(N^j → w_k | G)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$ B_j(p, q) = P(w_{pq} | N^j_{pq}, G) = $ &lt;img data-src=&#34;/img/NLP/inside-induction.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;try every possible rules to split Nj, product of *&lt;em&gt;rule probabilty and segments’ inside probabilities *&lt;/em&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;use grid to solve again&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/inside-grid.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;X軸代表起始座標，Y軸代表長度&lt;ul&gt;
&lt;li&gt;(2,3) → flies like ants&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Outside Algorithm (top-down)     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ P(w_{1m} | G) = Σ_j α_j(k, k)P(N^j → w_k$ &lt;img data-src=&#34;/img/NLP/outside-graph.png&#34; alt=&#34;&#34;&gt; &lt;!--為何是sum...--&gt;&lt;ul&gt;
&lt;li&gt;outside probability of wk x (inside) probability of wk  of every Nj&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;basecase &lt;ul&gt;
&lt;li&gt;$ α_1(1, m) = 1, α_j(1,m) = $&lt;/li&gt;
&lt;li&gt;P(N1) = 1, P(Nj outside w1 to wm) = 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;自己的outside probability 等於 &lt;ul&gt;
&lt;li&gt;爸爸的outside probability 乘以 爸爸的inside probability 除以 自己的inside probability&lt;ul&gt;
&lt;li&gt;inside x outside 是固定值？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;爸爸的inside probabiliity 除以 自己的inside probability 就是其兄弟的inside probability&lt;/li&gt;
&lt;li&gt;使用此公式計算 &lt;img data-src=&#34;/img/NLP/inout.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$ α&lt;em&gt;j(p, q)β_j(p, q) = P(w&lt;/em&gt;{1m}, N^j_{pq} | G) $&lt;ul&gt;
&lt;li&gt;某個點的inside 乘 outside = 在某grammar中，出現此句子，且包含此點的機率 &lt;/li&gt;
&lt;li&gt;所有點的總和：在某grammar下，某parse tree(包含所有node)的機率 &lt;img data-src=&#34;/img/NLP/parse-probability.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Outside example: 這些數字理論上算起來會一樣… &lt;img data-src=&#34;/img/NLP/outside-forward.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finding the Most Likely Parse for a Sentence     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;δi(p,q)= the highest inside probability parse of a subtree $N_{pq}^i$&lt;/li&gt;
&lt;li&gt;Initialization &lt;ul&gt;
&lt;li&gt;δi(p,p)= P(Ni → wp)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Induction and Store backtrace&lt;ul&gt;
&lt;li&gt;δi(p,q)= $argmax(j,k,r)P(Ni→NjNk)δj(p,r)δk(r+1,q)$&lt;/li&gt;
&lt;li&gt;找所有可能的切法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Termination&lt;ul&gt;
&lt;li&gt;answer = δ1(1,m)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Training a PCFG&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;find the optimal probabilities among grammar rules&lt;/li&gt;
&lt;li&gt;use EM Training Algorithm to seek the grammar that maximizes the likelihood of the training data&lt;ul&gt;
&lt;li&gt;Inside-Outside Algorithm &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/inoutagain.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;將產生句子的機率視為π，為Nj產生pq的機率 &lt;img data-src=&#34;/img/NLP/pi.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Nj被使用的機率 &lt;img data-src=&#34;/img/NLP/pi2.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Nj被使用，且Nj→NrNs的機率 &lt;img data-src=&#34;/img/NLP/pi3.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Nj→NrNs這條rule被使用的機率=前兩式相除 &lt;img data-src=&#34;/img/NLP/pi4.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Nj→wk &lt;img data-src=&#34;/img/NLP/pi5.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;僅分子差異 &lt;img data-src=&#34;/img/NLP/pi6.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Problems with the Inside-Outside Algorithm    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extremely Slow&lt;ul&gt;
&lt;li&gt;For each sentence, each iteration of training is $O(m^3n^3)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Local Maxima&lt;/li&gt;
&lt;li&gt;Satisfactory learning requires many more nonterminals than are theoretically needed to describe the language&lt;/li&gt;
&lt;li&gt;There is no guarantee that the learned nonterminals will be linguistically motivated&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap11-Dependency-Parsing&#34;&gt;&lt;a href=&#34;#Chap11-Dependency-Parsing&#34; class=&#34;headerlink&#34; title=&#34;Chap11 Dependency Parsing&#34;&gt;&lt;/a&gt;Chap11 Dependency Parsing&lt;/h2&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3N0cC5saW5nZmlsLnV1LnNlL35uaXZyZS9kb2NzL0FDTHNsaWRlcy5wZGY=&#34;&gt;COLING-ACL 2006, Dependency Parsing, by Joachim Nivre and Sandra Kuebler&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL25hYWNsaGx0MjAxMC5pc2kuZWR1L3R1dG9yaWFscy90Ny1zbGlkZXMucGRm&#34;&gt;NAACL 2010, Recent Advances in Dependency Parsing, by Qin Iris. Wang and YueZhang&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3NpdGUvemhlbmdodWFubHAvcHVibGljYXRpb25zL0lKQ05MUDIwMTMtdHV0b3JpYWwtRFAucGRmP2F0dHJlZGlyZWN0cz0wJmQ9MQ==&#34;&gt;IJCNLP 2013, Dependency Parsing: Past, Present, and Future, by Zhenghua Li, Wenliang Chen, Min Zhang&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Dependency Structure vs. Constituency Structure &lt;img data-src=&#34;/img/NLP/parse.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Parsing is one way to deal with the ambiguity problem in&lt;br&gt;natural language&lt;br&gt;dependency syntax is syntactic relations (dependencies) &lt;/p&gt;
&lt;p&gt;Constraint: between word pairs  &lt;img data-src=&#34;/img/NLP/depend.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;    Projective: No crossing links(a word and its dependents form a contiguous substring of the sentence)&lt;br&gt;    An arc (wi , r ,wj ) ∈ A is projective iff wi →∗ wk for all:&lt;br&gt;    i &amp;lt; k &amp;lt; j when i &amp;lt; j&lt;br&gt;    j &amp;lt; k &amp;lt; i when j &amp;lt; i&lt;br&gt;    射出去的那一方也可以射到兩個字中間的任何一字&lt;br&gt;&lt;img data-src=&#34;/img/NLP/depend-ex.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Non-projective Dependency Trees  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Long-distance dependencies  &lt;/li&gt;
&lt;li&gt;With crossing links&lt;/li&gt;
&lt;li&gt;Not so frequent in English&lt;ul&gt;
&lt;li&gt;All the dependency trees from Penn Treebank are projective&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Common in other languages with free word order&lt;ul&gt;
&lt;li&gt;Prague(23%) and Czech, German and Dutch&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data Driven Dependency Parsing  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data-driven parsing&lt;ul&gt;
&lt;li&gt;No grammar / rules needed&lt;/li&gt;
&lt;li&gt;Parsing decisions are made based on learned models&lt;/li&gt;
&lt;li&gt;deal with ambiguities well&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/data-driven.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Three approaches&lt;ul&gt;
&lt;li&gt;Graph-based models&lt;/li&gt;
&lt;li&gt;Transition-based models(good in practice)&lt;ul&gt;
&lt;li&gt;Define a transition system for &lt;strong&gt;mapping a sentence to its dependency tree&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Predefine some transition actions&lt;/li&gt;
&lt;li&gt;Learning: predicting the next state transition, by transition history&lt;/li&gt;
&lt;li&gt;Parsing: construct the optimal transition sequence&lt;/li&gt;
&lt;li&gt;Greedy search / beam search&lt;/li&gt;
&lt;li&gt;Features are defined over a richer parsing history&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hybrid models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Comparison   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Graph-based models&lt;ul&gt;
&lt;li&gt;Find the optimal tree from all the possible ones&lt;/li&gt;
&lt;li&gt;Global, exhaustive&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Transition-based models&lt;ul&gt;
&lt;li&gt;Predefine some actions (shift and reduce)&lt;/li&gt;
&lt;li&gt;use stack to hold partially built parses&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Find the optimal action sequence&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Local, Greedy or beam search&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The two models produce different types of errors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hybrid Models  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Three integration methods&lt;ul&gt;
&lt;li&gt;Ensemble approach: parsing time integration (Sagae &amp;amp; Lavie 2006)&lt;/li&gt;
&lt;li&gt;Feature-based integration (Nivre &amp;amp; Mcdonald 2008)&lt;/li&gt;
&lt;li&gt;Single model combination (Zhang &amp;amp; Clark 2008)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gain benefits from both models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/NLP/parse-algo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;Graph-based-dependency-parsing-models&#34;&gt;&lt;a href=&#34;#Graph-based-dependency-parsing-models&#34; class=&#34;headerlink&#34; title=&#34;Graph-based dependency parsing models&#34;&gt;&lt;/a&gt;Graph-based dependency parsing models&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Search for a tree with the highest score&lt;/li&gt;
&lt;li&gt;Define search space&lt;ul&gt;
&lt;li&gt;Exhaustive search&lt;/li&gt;
&lt;li&gt;Features are defined over a limited parsing history&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The score is linear combination of features &lt;ul&gt;
&lt;li&gt;What features we can use? (later)&lt;/li&gt;
&lt;li&gt;What learning approaches can lead us to find the best tree with the highest score (later)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Applicable to both probabilistic and nonprobabilistic models &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Features  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dynamic features&lt;ul&gt;
&lt;li&gt;Take into account the link labels of the surrounding word-pairs when predicting the label of current pair&lt;/li&gt;
&lt;li&gt;Commonly used in sequential labeling&lt;/li&gt;
&lt;li&gt;A word’s children are generated first(先生child, 再找parent), before it modifies another word&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learning Approaches   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local learning approaches&lt;ul&gt;
&lt;li&gt;Learn a local link classifier given of features defined on training data&lt;/li&gt;
&lt;li&gt;example &lt;img data-src=&#34;/img/NLP/local-feature-example.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;3-class classification: No link, left link or right link&lt;/li&gt;
&lt;li&gt;Efficient O(n) local training&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;local training and parsing &lt;img data-src=&#34;/img/NLP/local-train-with-parse.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Learn the weights of features&lt;ul&gt;
&lt;li&gt;Maximum entropy models (Ratnaparkhi 99, Charniak 00)&lt;/li&gt;
&lt;li&gt;Support vector machines (Yamada &amp;amp; Matsumoto 03)&lt;/li&gt;
&lt;li&gt;Use a richer feature set!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Global learning approaches&lt;/li&gt;
&lt;li&gt;Unsupervised/Semi-supervised learning approaches&lt;ul&gt;
&lt;li&gt;Use both annotated training data and un-annotated raw text&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Transition-based-model&#34;&gt;&lt;a href=&#34;#Transition-based-model&#34; class=&#34;headerlink&#34; title=&#34;Transition-based model&#34;&gt;&lt;/a&gt;Transition-based model&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Stack holds partially built parses&lt;/li&gt;
&lt;li&gt;Queue holds unprocessed words&lt;/li&gt;
&lt;li&gt;Actions&lt;ul&gt;
&lt;li&gt;use input words to build output parse&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;parsing-processes&#34;&gt;&lt;a href=&#34;#parsing-processes&#34; class=&#34;headerlink&#34; title=&#34;parsing processes&#34;&gt;&lt;/a&gt;parsing processes&lt;/h4&gt;&lt;p&gt;Arc-eager parser  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 tranition actions&lt;ul&gt;
&lt;li&gt;SHIFT: push stack&lt;/li&gt;
&lt;li&gt;REDUCE: pop stack&lt;/li&gt;
&lt;li&gt;ARC-LEFT: pop stack and add link&lt;/li&gt;
&lt;li&gt;ARC-RIGHT: push stack and add link&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/arc-eager-example.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Time complexity: linear&lt;ul&gt;
&lt;li&gt;every word will be pushed once and popped once(except root)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;parse&lt;ul&gt;
&lt;li&gt;by actions: arcleft → arclect subject, noun, …&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Arc-standard parser  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 actions&lt;ul&gt;
&lt;li&gt;SHIFT: push&lt;/li&gt;
&lt;li&gt;LEFT: pop leftmost stack element and add&lt;/li&gt;
&lt;li&gt;RIGHT: pop rightmost stack element and add&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Also linear time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Non-projectivity  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;neither of parser can solve it&lt;ul&gt;
&lt;li&gt;online reorder&lt;ul&gt;
&lt;li&gt;add extra action: swap&lt;/li&gt;
&lt;li&gt;not linear: $N^2$, but expect to belinear&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Decoding-algorithms&#34;&gt;&lt;a href=&#34;#Decoding-algorithms&#34; class=&#34;headerlink&#34; title=&#34;Decoding algorithms&#34;&gt;&lt;/a&gt;Decoding algorithms&lt;/h4&gt;&lt;p&gt;search action sequence to build the parse&lt;br&gt;scoring action given context&lt;br&gt;Candidate item &amp;lt;S, G, Q&amp;gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;greedy local search&lt;ul&gt;
&lt;li&gt;initialize: Q = input&lt;/li&gt;
&lt;li&gt;goal: S=[root], G=tree, Q=[]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;problem: one error leads to incorrect parse&lt;ul&gt;
&lt;li&gt;Beam search: keep N highest partial states&lt;ul&gt;
&lt;li&gt;use total score of all actions to rank a parse&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Score-Models&#34;&gt;&lt;a href=&#34;#Score-Models&#34; class=&#34;headerlink&#34; title=&#34;Score Models&#34;&gt;&lt;/a&gt;Score Models&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;linear model&lt;/li&gt;
&lt;li&gt;SVM&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap12-Semantic-Representation-and-Computational-Semantics&#34;&gt;&lt;a href=&#34;#Chap12-Semantic-Representation-and-Computational-Semantics&#34; class=&#34;headerlink&#34; title=&#34;Chap12 Semantic Representation and Computational Semantics&#34;&gt;&lt;/a&gt;Chap12 Semantic Representation and Computational Semantics&lt;/h2&gt;&lt;p&gt;Semantic aren’t primarily descriptions of inputs&lt;/p&gt;
&lt;p&gt;Semantic Processing  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reason about the truth&lt;/li&gt;
&lt;li&gt;answer questions based on content&lt;ul&gt;
&lt;li&gt;Touchstone application is often question answering&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;inference to determine the truth that isn’t actually know&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Method    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;principled, theoretically motivated approach&lt;ul&gt;
&lt;li&gt;Computational/Compositional Semantics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;limited, practical approaches that have some hope of being useful&lt;ul&gt;
&lt;li&gt;Information extraction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Information-Extraction&#34;&gt;&lt;a href=&#34;#Information-Extraction&#34; class=&#34;headerlink&#34; title=&#34;Information Extraction&#34;&gt;&lt;/a&gt;Information Extraction&lt;/h3&gt;&lt;p&gt;Information Extraction = segmentation + classification +  association + clustering &lt;img data-src=&#34;/img/NLP/IE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;superficial analysis &lt;ul&gt;
&lt;li&gt;pulls out only the entities, relations and roles related to consuming application&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Similar to chunking&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Compositional-Semantics&#34;&gt;&lt;a href=&#34;#Compositional-Semantics&#34; class=&#34;headerlink&#34; title=&#34;Compositional Semantics&#34;&gt;&lt;/a&gt;Compositional Semantics&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Use First-Order Logic(FOL) representation that accounts for all the entities, roles and relations present in a sentence&lt;/li&gt;
&lt;li&gt;Similar to our approach to full parsing&lt;/li&gt;
&lt;li&gt;Compositional: The meaning of a whole is derived from the meanings of the parts(syntatic) &lt;img data-src=&#34;/img/NLP/syntax-semantic.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Syntax-Driven Semantic Analysis&lt;ul&gt;
&lt;li&gt;The composition of meaning representations is guided by the &lt;strong&gt;syntactic&lt;/strong&gt; components and relations provided by the  grammars&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;FOL&#34;&gt;&lt;a href=&#34;#FOL&#34; class=&#34;headerlink&#34; title=&#34;FOL&#34;&gt;&lt;/a&gt;FOL&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;allow to answer yes/no questions&lt;/li&gt;
&lt;li&gt;allow variable&lt;/li&gt;
&lt;li&gt;allow inference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Events, actions and relationships can be captured with representations that consist of predicates with arguments  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predicates&lt;ul&gt;
&lt;li&gt;Primarily Verbs, VPs, Sentences&lt;/li&gt;
&lt;li&gt;Verbs introduce/refer to events and processes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Arguments &lt;ul&gt;
&lt;li&gt;Primarily Nouns, Nominals, NPs, PPs&lt;/li&gt;
&lt;li&gt;Nouns introduce the things that play roles in those events&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: Mary gave a list to John &lt;ul&gt;
&lt;li&gt;Giving(Mary, John, List)&lt;/li&gt;
&lt;li&gt;Gave: Predicate&lt;/li&gt;
&lt;li&gt;Mary, John, List: Argument&lt;/li&gt;
&lt;li&gt;better representation &lt;img data-src=&#34;/img/NLP/FOL-better.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lambda Forms&lt;ul&gt;
&lt;li&gt;Allow variables to be bound&lt;/li&gt;
&lt;li&gt;λxP(x)(Sally) = P(Sally)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ambiguation  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mismatch between syntax and semantics&lt;ul&gt;
&lt;li&gt;displaced arguments&lt;/li&gt;
&lt;li&gt;complex NPs with quantifiers&lt;ul&gt;
&lt;li&gt;A menu&lt;/li&gt;
&lt;li&gt;Every restaurant &lt;img data-src=&#34;/img/NLP/complicate-NP.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Not every waiter&lt;/li&gt;
&lt;li&gt;Most restaurants&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/complicate-NP-induction.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;still preserving strict compositionality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two (syntax) rules to revise&lt;ul&gt;
&lt;li&gt;The S rule&lt;ul&gt;
&lt;li&gt;S → NP VP, NP.Sem(VP.Sem)&lt;/li&gt;
&lt;li&gt;NP and VP swapped, because S is NP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simple NP’s like proper nouns&lt;ul&gt;
&lt;li&gt;λx.Franco(x)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Store and Retrieve  &lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/ambiguity-of-same-POS.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Retrieving the quantifiers one at a time and placing them in front&lt;/li&gt;
&lt;li&gt;The order determines the meaning &lt;img data-src=&#34;/img/NLP/store.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;retrieve &lt;img data-src=&#34;/img/NLP/retrieve.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Set-Based-Models&#34;&gt;&lt;a href=&#34;#Set-Based-Models&#34; class=&#34;headerlink&#34; title=&#34;Set-Based Models&#34;&gt;&lt;/a&gt;Set-Based Models&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;domain: the set of elements&lt;/li&gt;
&lt;li&gt;entity: elements of domain&lt;/li&gt;
&lt;li&gt;Properties of the elements: sets of elements from the domain&lt;/li&gt;
&lt;li&gt;Relations: sets of tuples of elements from the domain&lt;/li&gt;
&lt;li&gt;FOL&lt;ul&gt;
&lt;li&gt;FOL Terms → elements of the domain&lt;ul&gt;
&lt;li&gt;Med -&amp;gt; “f”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FOL atomic formula → sets, or sets of tuples&lt;ul&gt;
&lt;li&gt;Noisy(Med) is true if “f is in the set of elements that corresponds to the noisy relation&lt;/li&gt;
&lt;li&gt;Near(Med, Rio) is true if “the tuple &amp;lt;f,g&amp;gt; is in the set of tuples that corresponds to “Near” in the interpretation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: Everyone likes a noisy restaurant &lt;img data-src=&#34;/img/NLP/set-based-model.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;There is a particular restaurant out there; it’s a noisy place; everybody likes it 有一家吵雜的餐廳大家都喜歡&lt;/li&gt;
&lt;li&gt;Everybody has at least one noisy restaurant that they like 大家都喜歡一家吵雜的餐廳&lt;/li&gt;
&lt;li&gt;Everybody likes noisy restaurants (i.e., there is no noisy restaurant out there that is disliked by anyone) 大家都喜歡吵雜的餐廳&lt;/li&gt;
&lt;li&gt;Using predicates to create &lt;strong&gt;categories&lt;/strong&gt; of concepts &lt;ul&gt;
&lt;li&gt;people and restaurants&lt;/li&gt;
&lt;li&gt;basis for OWL (Web Ontology Language)網絡本體語言&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;before &lt;img data-src=&#34;/img/NLP/uncategories.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;after &lt;img data-src=&#34;/img/NLP/categories.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap13-Lexical-Semantics&#34;&gt;&lt;a href=&#34;#Chap13-Lexical-Semantics&#34; class=&#34;headerlink&#34; title=&#34;Chap13 Lexical Semantics&#34;&gt;&lt;/a&gt;Chap13 Lexical Semantics&lt;/h2&gt;&lt;p&gt;we didn’t do word meaning in compositional semantics&lt;/p&gt;
&lt;p&gt;WordNet  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;meaning and relationship about words&lt;ul&gt;
&lt;li&gt;hypernym(上位詞)&lt;ul&gt;
&lt;li&gt;breakfast → meal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hierarchies &lt;img data-src=&#34;/img/NLP/wordnet-hierarchy.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our semantics examples, we used various FOL predicates to capture various aspects of events, including the notion of roles&lt;br&gt;Havers, takers, givers, servers, etc.&lt;/p&gt;
&lt;p&gt;Thematic roles(語義關係) &lt;img data-src=&#34;/img/NLP/thematic-roles.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;semantic generalizations over the specific roles that occur with specific verbs&lt;ul&gt;
&lt;li&gt;provide a shallow level of semantic analysis&lt;/li&gt;
&lt;li&gt;tied to syntactic analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;i.e. Takers, givers, eaters, makers, doers, killers&lt;ul&gt;
&lt;li&gt;They’re all the agents of the actions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AGENTS are often subjects&lt;/li&gt;
&lt;li&gt;In a VP-&amp;gt;V NP rule, the NP is often a THEME&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2 major English resources using thematic data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PropBank&lt;ul&gt;
&lt;li&gt;Layered on the Penn TreeBank&lt;/li&gt;
&lt;li&gt;Small number (25ish) labels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FrameNet&lt;ul&gt;
&lt;li&gt;Based on frame semantics&lt;/li&gt;
&lt;li&gt;Large number of frame-specific labels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[McAdams and crew] covered [the floors] with [checked linoleum].格子花紋油毯&lt;ul&gt;
&lt;li&gt;Arg0 (agent: the causer of the smearing)&lt;/li&gt;
&lt;li&gt;Arg1 (theme: “thing covered”)&lt;/li&gt;
&lt;li&gt;Arg2 (covering: “stuff being smeared”)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;including agent and theme, remaining args are verb specific&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Logical Statements  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example: EAT – Eating(e) ^Agent(e,x)^ Theme(e,y)^Food(y)&lt;ul&gt;
&lt;li&gt;(adding in all the right quantifiers and lambdas)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use WordNet to encode the selection restrictions&lt;/li&gt;
&lt;li&gt;Unfortunately, language is creative&lt;ul&gt;
&lt;li&gt;… ate glass on an empty stomach accompanied only by water and tea&lt;/li&gt;
&lt;li&gt;you &lt;strong&gt;can’t eat gold&lt;/strong&gt; for lunch if you’re hungry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;can we discover a verb’s restrictions by using a corpus and WordNet?    &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Parse sentences and find heads&lt;/li&gt;
&lt;li&gt;Label the thematic roles&lt;/li&gt;
&lt;li&gt;Collect statistics on the co-occurrence of particular headwords with particular thematic roles&lt;/li&gt;
&lt;li&gt;Use the WordNet hypernym structure to &lt;strong&gt;find the most meaningful level to use as a restriction&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;WSD&#34;&gt;&lt;a href=&#34;#WSD&#34; class=&#34;headerlink&#34; title=&#34;WSD&#34;&gt;&lt;/a&gt;WSD&lt;/h3&gt;&lt;p&gt;Word sense disambiguation  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;select right sense for a word &lt;/li&gt;
&lt;li&gt;Semantic selection restrictions can be used to disambiguate&lt;ul&gt;
&lt;li&gt;Ambiguous arguments to unambiguous predicates&lt;/li&gt;
&lt;li&gt;Ambiguous predicates with unambiguous arguments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ambiguous arguments&lt;ul&gt;
&lt;li&gt;Prepare a dish(菜餚)&lt;/li&gt;
&lt;li&gt;Wash a dish(盤子)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ambiguous predicates&lt;ul&gt;
&lt;li&gt;Serve (任職/服務) Denver&lt;/li&gt;
&lt;li&gt;Serve (供應) breakfast&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Methodology   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Supervised Disambiguation&lt;ul&gt;
&lt;li&gt;based on a labeled training set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dictionary-Based Disambiguation&lt;ul&gt;
&lt;li&gt;based on lexical resource like dictionaries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unsupervised Disambiguation&lt;ul&gt;
&lt;li&gt;label training data is expensive &lt;/li&gt;
&lt;li&gt;based on unlabeled corpora&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Upper(human) and Lower(simple model) Bounds&lt;/li&gt;
&lt;li&gt;Pseudoword&lt;ul&gt;
&lt;li&gt;Generate artificial evaluation data for comparison and improvement of text processing algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Supervised ML Approaches  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What’s a tag?&lt;ul&gt;
&lt;li&gt;In WordNet, “bass” in a text has 8 possible tags or labels (bass1 through bass8)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;require very simple representation for training data&lt;ul&gt;
&lt;li&gt;Vectors of sets of feature/value pairs&lt;/li&gt;
&lt;li&gt;need to extract training data by characterization of text surrounding the target&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If you decide to use features that require more analysis (say parse trees) then the ML part may be doing less work (relatively) if these features are truly informative&lt;/li&gt;
&lt;li&gt;Classification&lt;ul&gt;
&lt;li&gt;Naïve Bayes (the right thing to try first)&lt;/li&gt;
&lt;li&gt;Decision lists&lt;/li&gt;
&lt;li&gt;Decision trees&lt;/li&gt;
&lt;li&gt;MaxEnt&lt;/li&gt;
&lt;li&gt;Support vector machines&lt;/li&gt;
&lt;li&gt;Nearest neighbor methods…&lt;/li&gt;
&lt;li&gt;choice of technique depends on features that have been used&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bootstrapping&lt;ul&gt;
&lt;li&gt;Use when don’t have enough data to train a system…&lt;/li&gt;
&lt;li&gt;集中有放回的均勻抽樣&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Naive-Bayes&#34;&gt;&lt;a href=&#34;#Naive-Bayes&#34; class=&#34;headerlink&#34; title=&#34;Naive Bayes&#34;&gt;&lt;/a&gt;Naive Bayes&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Argmax P(sense|feature vector) &lt;img data-src=&#34;/img/NLP/bayesian-decision.png&#34; alt=&#34;&#34;&gt; &lt;/li&gt;
&lt;li&gt;find maximum probabilty of words given possible sk &lt;img data-src=&#34;/img/NLP/bayesian-decision2.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/bayesian-classifier.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;assumption&lt;ul&gt;
&lt;li&gt;bag of words model&lt;ul&gt;
&lt;li&gt;structure and order of words is ignored&lt;/li&gt;
&lt;li&gt;each pair of words in the bag is independent&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;73% correct&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Dictionary-Based-Disambiguation&#34;&gt;&lt;a href=&#34;#Dictionary-Based-Disambiguation&#34; class=&#34;headerlink&#34; title=&#34;Dictionary-Based Disambiguation&#34;&gt;&lt;/a&gt;Dictionary-Based Disambiguation&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;Disambiguation based on sense definitions&lt;/li&gt;
&lt;li&gt;Thesaurus-Based Disambiguation&lt;/li&gt;
&lt;li&gt;Disambiguation based on translations in a second-language corpus&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;sense definition&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;find keywords in definition of a word&lt;ul&gt;
&lt;li&gt;cone&lt;ul&gt;
&lt;li&gt;… pollen-bearing scales or bracts in &lt;strong&gt;trees&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;shape for holding &lt;strong&gt;ice cream&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;50%~70% accuracies&lt;/li&gt;
&lt;li&gt;Alternatives&lt;ul&gt;
&lt;li&gt;Several iterations to determine correct sense&lt;/li&gt;
&lt;li&gt;Combine the dictionary-based and thesaurus-based disambiguation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU3JUI0JUEyJUU1JUJDJTk1JUU1JTg1JUI4&#34;&gt;Thesaurus-Based(索引典)&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt; Disambiguation    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Category can determine which word senses are used&lt;/li&gt;
&lt;li&gt;Each word is assigned one or more subject codes which correspond to its different meanings&lt;ul&gt;
&lt;li&gt;select the most often subject code&lt;/li&gt;
&lt;li&gt;考慮w的context，有多少words的senses與w相同&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Walker’s Algorithm&lt;ul&gt;
&lt;li&gt;50% accuracy for “interest, point, power, state, and terms”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Problems&lt;ul&gt;
&lt;li&gt;general topic categorization, e.g., mouse in computer&lt;/li&gt;
&lt;li&gt;coverage, e.g., Navratilova&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Yarowsky’s Algorithm &lt;img data-src=&#34;/img/NLP/yarowsky-algo.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/yarowsky-algo2.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/yarowsky-algo3.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;categorize sentences&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;categorize words&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;disambiguate by decision rule for Naïve Bayes&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;result &lt;img data-src=&#34;/img/NLP/yarowsky-result.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disambiguation based on translations in a second-language corpus  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the word “interest” has two translations in German&lt;ul&gt;
&lt;li&gt;“Beteiligung” (legal share–50% a interest in the company)&lt;/li&gt;
&lt;li&gt;“Interesse” (attention, concern–her interest in Mathematics)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: … showed interest …&lt;ul&gt;
&lt;li&gt;Look up English-German dictionary, show → zeigen&lt;/li&gt;
&lt;li&gt;Compute R(Interesse, zeigen) and R(Beteiligung, zeigen)&lt;/li&gt;
&lt;li&gt;R(Interesse, zeigen) &amp;gt; R(Beteiligung, zeigen)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Unsupervised-Disambiguation&#34;&gt;&lt;a href=&#34;#Unsupervised-Disambiguation&#34; class=&#34;headerlink&#34; title=&#34;Unsupervised Disambiguation&#34;&gt;&lt;/a&gt;Unsupervised Disambiguation&lt;/h4&gt;&lt;p&gt;P(vj|sk) are estimated using the EM algorithm  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Random initialization of P(vj|sk)(word)&lt;/li&gt;
&lt;li&gt;For each context ci of w, compute P(ci|sk)(sentence)&lt;/li&gt;
&lt;li&gt;Use P(ci|sk) as training data&lt;/li&gt;
&lt;li&gt;Reestimate P(vj|sk)(word)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Surface Representations(features)   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Collocational&lt;ul&gt;
&lt;li&gt;words that appear in specific positions to the right and left of the target word&lt;/li&gt;
&lt;li&gt;limited to the words themselves as well as part of speech&lt;/li&gt;
&lt;li&gt;Example: guitar and bassplayer stand&lt;ul&gt;
&lt;li&gt;[guitar, NN, and, CJC, player, NN, stand, VVB]&lt;/li&gt;
&lt;li&gt;In other words, a vector consisting of [position n word, position n part-of-speech…]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Co-occurrence&lt;ul&gt;
&lt;li&gt;words that occur regardless of position&lt;/li&gt;
&lt;li&gt;Typically limited to frequency counts&lt;/li&gt;
&lt;li&gt;Assume we’ve settled on a possible vocabulary of 12 words that includes guitarand playerbut not andand stand&lt;/li&gt;
&lt;li&gt;Example: guitar and bassplayer stand&lt;ul&gt;
&lt;li&gt;Assume a 12-word sentence includes guitar and player but not “and” and stand&lt;/li&gt;
&lt;li&gt;[0,0,0,1,0,0,0,0,0,1,0,0]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Applications  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tagging&lt;ul&gt;
&lt;li&gt;translation&lt;/li&gt;
&lt;li&gt;information retrieval&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;different label  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generic thematic roles (aka case roles)&lt;ul&gt;
&lt;li&gt;Agent, instrument, source, goal, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Propbank labels&lt;ul&gt;
&lt;li&gt;Common set of labels ARG0-ARG4, ARGM&lt;/li&gt;
&lt;li&gt;specific to verb semantics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FrameNet frame elements&lt;ul&gt;
&lt;li&gt;Conceptual and frame-specific &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example: [Ochocinco] bought [Burke] [a diamond ring]&lt;ul&gt;
&lt;li&gt;generic: Agent, Goal, Theme&lt;/li&gt;
&lt;li&gt;propbank: ARG0, ARG2, ARG1&lt;/li&gt;
&lt;li&gt;framenet: Customer, Recipe, Goods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Semantic Role Labeling  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;automatically identify and label thematic roles&lt;ul&gt;
&lt;li&gt;For each verb in a sentence&lt;ul&gt;
&lt;li&gt;For each constituent&lt;ul&gt;
&lt;li&gt;Decide if it is an argument to that verb&lt;/li&gt;
&lt;li&gt;if it is an argument, determine what kind&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;feature&lt;ul&gt;
&lt;li&gt;from parse and lexical item&lt;/li&gt;
&lt;li&gt;“path” &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Lexical-Acquisition&#34;&gt;&lt;a href=&#34;#Lexical-Acquisition&#34; class=&#34;headerlink&#34; title=&#34;Lexical Acquisition&#34;&gt;&lt;/a&gt;Lexical Acquisition&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Verb Subcategorization&lt;ul&gt;
&lt;li&gt;the syntactic means by which verbs express their arguments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Attachment Ambiguity&lt;ul&gt;
&lt;li&gt;The children ate the cake with their hands&lt;/li&gt;
&lt;li&gt;The children ate the cake with blue icing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SelectionalPreferences&lt;ul&gt;
&lt;li&gt;The semantic categorization of a verb’s arguments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semantic Similarity (refer to IR course)&lt;ul&gt;
&lt;li&gt;Semantic similarity between words&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Verb-Subcategorization&#34;&gt;&lt;a href=&#34;#Verb-Subcategorization&#34; class=&#34;headerlink&#34; title=&#34;Verb Subcategorization&#34;&gt;&lt;/a&gt;Verb Subcategorization&lt;/h4&gt;&lt;p&gt;a particular set of syntactic categories that a verb can appear with is called a &lt;strong&gt;subcategorization frame&lt;/strong&gt; &lt;img data-src=&#34;/img/NLP/subcategorization.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Brent’s subcategorization frame learner  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cues: Define a regular pattern of words and syntactic categories&lt;ol&gt;
&lt;li&gt;ε: error rate of assigning frame f to verb v based on cue cj&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Hypothesis Testing: Define null hypothesis H0: “the frame is not appropriate for the verb” &lt;ol&gt;
&lt;li&gt;Reject this hypothesis if the cue cj indicates with high probability that our H0 is wrong&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example&lt;br&gt;Cues  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;regular pattern for subcategorization frame “NP NP”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(OBJ | SUBJ_OBJ | CAP) (PUNC |CC)&lt;br&gt;Null hypothesis testing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Verb vi occurs a total of n times in the corpus and there are m &amp;lt; n occurrences with a cue for frame fj&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reject the null hypothesis H0 that vi does not accept fj with the following probability of error &lt;img data-src=&#34;/img/NLP/brent-null-hypothesis.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Brent’s system does well at precision, but not well at recall&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Manning’s system&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;solve this problem by using a tagger and running the cue detection on the output of the tagger&lt;/li&gt;
&lt;li&gt;learn a lot of subcategorization frames, even those it is low-reliability&lt;/li&gt;
&lt;li&gt;still low performance &lt;/li&gt;
&lt;li&gt;improve : use prior knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PCFG prefers to parse common construction  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(A|prep, verb, np1, np2, w) ~= P(A|prep, verb, np1, np2)&lt;ul&gt;
&lt;li&gt;Do not count the word outside of frame&lt;/li&gt;
&lt;li&gt;w: words outside of “verb np1(prep np2)”&lt;/li&gt;
&lt;li&gt;A: random variable representing attachment decision&lt;/li&gt;
&lt;li&gt;V(A): verb or np1&lt;/li&gt;
&lt;li&gt;Counter example&lt;ul&gt;
&lt;li&gt;Fred saw a movie with Arnold Schwarzenegger&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;P(A|prep, verb, np1, np2, noun1, noun2) ~= P(A|prep, verb, noun1, noun2)&lt;ul&gt;
&lt;li&gt;noun1 = head of np1, noun2 = head of np2&lt;/li&gt;
&lt;li&gt;total parameters: $10^{13}$ = #(prep) x #(verb) x #(noun) x #(noun) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;P(A= noun | prep, verb, noun1) vs. P(A= verb | prep, verb, noun1)&lt;ul&gt;
&lt;li&gt;compare probability to be verb and probability to be noun&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Technique: Alternative to reduce parameters   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Condition probabilities on fewer things&lt;/li&gt;
&lt;li&gt;Condition probabilities on more general things&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model asks the following questions  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VAp: Is there a PP headed by p and following the verb v which attaches to v(VAp=1) or not (VAp=0)?&lt;/li&gt;
&lt;li&gt;NAp: Is there a PP headed by p and following the noun n which attaches to n (NAp=1) or not (NAp=0)?&lt;/li&gt;
&lt;li&gt;(1) Determine the attachment of a PP that is immediately following an object noun, i.e. compute the probability of NAp=1&lt;/li&gt;
&lt;li&gt;In order for the first PP headed by the preposition p to attach to the verb, both VAp=1 and NAp=0&lt;ul&gt;
&lt;li&gt;calculate likelihood ratio between V and N &lt;img data-src=&#34;/img/NLP/likelihood-ratio-vn.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;maximum estimation&lt;ul&gt;
&lt;li&gt;P(VA = 1 | v) = C(v, p) / C(v)&lt;/li&gt;
&lt;li&gt;P(NA = 1 | n) = C(n, p) / C(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Estimation of PP attachment counts&lt;ul&gt;
&lt;li&gt;Sure Noun Attach&lt;ul&gt;
&lt;li&gt;If a noun is followed by a PP but no preceding verb, increment C(prep attached to noun) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sure Verb Attach&lt;ul&gt;
&lt;li&gt;if a passive verb is followed by a PP other than a “by” phrase, increment C(prep attached to verb) &lt;/li&gt;
&lt;li&gt;if a PP follows both a noun phrase and a verb but the noun phrase is a pronoun, increment C(prep attached to verb)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ambiguous Attach&lt;ul&gt;
&lt;li&gt;if a PP follows both a noun and a verb, see if the probabilities based on the attachment decided by previous way&lt;/li&gt;
&lt;li&gt;otherwise increment both attachment counters by 0.5&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/attach-example.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Sparse data is a major cause of the difference between the human and program performance(attachment indeterminacy不確定性)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using Semantic Information  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;condition on semantic tags of verb &amp;amp; noun&lt;ul&gt;
&lt;li&gt;Sue bought a plant with Jane(human)&lt;/li&gt;
&lt;li&gt;Sue bought a plant with yellow leaves(object)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assumption&lt;br&gt;The noun phrase serves as the subject of the relative clause&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;collect “ subject-verb” and “verb-object” pairs.(training part)  &lt;/li&gt;
&lt;li&gt;compute t-score (testing part) &lt;ul&gt;
&lt;li&gt;t-score &amp;gt; 0.10 (significant)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;P (relative clause attaches to x | main verb of clause =v) &amp;gt; P (relative clause attaches to y | main verb of clause=v)&lt;br&gt;↔ P (x= subject/object | v) &amp;gt; P (y= subject/ object|v)&lt;/p&gt;
&lt;p&gt;Selectional Preferences  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most verbs prefer particular type of arguments&lt;ul&gt;
&lt;li&gt;eat → object (food item)&lt;/li&gt;
&lt;li&gt;think → subject (people)&lt;/li&gt;
&lt;li&gt;bark → subject (dog)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Aspects of meaning of a word can be inferred&lt;ul&gt;
&lt;li&gt;Susan had never eaten a fresh &lt;strong&gt;durian&lt;/strong&gt; before (food item)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Selectional preferences can be used to rank different parses of a sentence&lt;/li&gt;
&lt;li&gt;Selectional preference strength&lt;ul&gt;
&lt;li&gt;how strongly the verb constrains its direct object&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/selection-strength.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;KL divergence between the prior distribution of direct objects of general verb and the distribution of direct objects of specific verb&lt;/li&gt;
&lt;li&gt;2 assumptions&lt;ul&gt;
&lt;li&gt;only the head noun of the object is considered&lt;/li&gt;
&lt;li&gt;rather than dealing with individual nouns, we look at classes of nouns&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Selectional association&lt;ul&gt;
&lt;li&gt;Selectional Association between a verb and a class is this class’s contribution to S(v) / the overall preference strength S(v) &lt;img data-src=&#34;/img/NLP/selectional-association.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;There is also a rule for assigning association strengths to nouns instead of noun classes&lt;ul&gt;
&lt;li&gt;If noun belongs to several classes, then its choose the highest association strength among all classes &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;estimating the probability that a direct object in noun class c occurs given a verb v&lt;ul&gt;
&lt;li&gt;A(interrupt, chair) = max(A(interrupt, people), A(interrupt, furniture)) = A(interrupt, people)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example &lt;img data-src=&#34;/img/NLP/selectional-example.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;eat prefers fooditem &lt;ul&gt;
&lt;li&gt;A(eat, food)=1.08 → very specific&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;seehas a uniform distribution&lt;ul&gt;
&lt;li&gt;A(see, people)=A(see, furniture)=A(see, food)=A(see, action)=0 → no selectional preference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;find disprefers action item&lt;ul&gt;
&lt;li&gt;A(find, action)=-0.13 → less specific&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Semantic Similarity  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assessing semantic similarity between a new word and other already known words&lt;/li&gt;
&lt;li&gt;Vector Space vs Probabilistic&lt;/li&gt;
&lt;li&gt;Vector Space&lt;ul&gt;
&lt;li&gt;Words can be expressed in different spaces: document space, word spaceand modifier space&lt;/li&gt;
&lt;li&gt;Similarity measures for binary vectors: matching coefficient, Dice coefficient, Jaccard(or Tanimoto) coefficient, Overlap coefficientand cosine&lt;/li&gt;
&lt;li&gt;Similarity measures for the real-valued vector space: cosine, Euclidean Distance, normalized correlation coefficient&lt;ul&gt;
&lt;li&gt;cosine assumes a Euclidean space which is not well-motivated when dealing with word counts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/similarity-measure.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Probabilistic Measures&lt;ul&gt;
&lt;li&gt;viewing word counts by representing them as probability distributions&lt;/li&gt;
&lt;li&gt;compare two probability distributions using&lt;ul&gt;
&lt;li&gt;KL Divergence&lt;/li&gt;
&lt;li&gt;Information Radius(Irad)&lt;/li&gt;
&lt;li&gt;L1Norm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap14-Computational-Discourse&#34;&gt;&lt;a href=&#34;#Chap14-Computational-Discourse&#34; class=&#34;headerlink&#34; title=&#34;Chap14 Computational Discourse&#34;&gt;&lt;/a&gt;Chap14 Computational Discourse&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Level&lt;/th&gt;
&lt;th&gt;Well-formedness constraints&lt;/th&gt;
&lt;th&gt;Types of ambiguity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Lexical&lt;/td&gt;
&lt;td&gt;Rules of inflection and derivation&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;structural, morpheme boundaries, morpheme identity&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Syntactic&lt;/td&gt;
&lt;td&gt;Grammar rules&lt;/td&gt;
&lt;td&gt;structural, POS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Semantic&lt;/td&gt;
&lt;td&gt;Selection restrictions&lt;/td&gt;
&lt;td&gt;word sense, quantifier scope&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU4JUFGJUFEJUU3JTk0JUE4JUU1JUFEJUE2&#34;&gt;Pragmatic&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;conversation principles&lt;/td&gt;
&lt;td&gt;pragmatic function&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Computational Discourse  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discourse(語篇)&lt;ul&gt;
&lt;li&gt;A group of sentences with the same coherence relation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Coherence relation&lt;ul&gt;
&lt;li&gt;the 2nd sentence offers the reader an explaination or cause for the 1st sentence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Entity-based Coherence&lt;ul&gt;
&lt;li&gt;relationships with the entities, introducing them and following them in a focused way&lt;/li&gt;
&lt;li&gt;Discourse Segmentation&lt;ul&gt;
&lt;li&gt;Divide a document into a linear sequence of multiparagraph passages&lt;/li&gt;
&lt;li&gt;Academic article&lt;ul&gt;
&lt;li&gt;Abstract&lt;/li&gt;
&lt;li&gt;Introduction&lt;/li&gt;
&lt;li&gt;Methodology&lt;/li&gt;
&lt;li&gt;Results&lt;/li&gt;
&lt;li&gt;Conclusion&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;http://www.wannabehacks.co.uk/images/Inverted_pyramid_in_comprehensive_form.jpg&#34; alt=&#34;Inverted Pyramid&#34;&gt;&lt;/li&gt;
&lt;li&gt;Applications&lt;ul&gt;
&lt;li&gt;News&lt;/li&gt;
&lt;li&gt;Summarize different segments of a document&lt;/li&gt;
&lt;li&gt;Extract information from inside a single discourse segment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TextTiling (Hearst,1997)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tokenization&lt;ul&gt;
&lt;li&gt;Each space-delimited word in the input is converted to lower-case&lt;/li&gt;
&lt;li&gt;Words in a stop list of function words are thrown out&lt;/li&gt;
&lt;li&gt;The remaining words are morphologically stemmed&lt;/li&gt;
&lt;li&gt;The stemmed words are grouped into pseudo-sentencesof length w = 20&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lexical score determination&lt;ul&gt;
&lt;li&gt;compute a lexical cohesion(結合) score between pseudo-sentences&lt;ul&gt;
&lt;li&gt;score: average similarity of words in the pseudo-sentences before gap to pseudo-sentences after the gap(??)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Boundary identification    &lt;ul&gt;
&lt;li&gt;Compute a depth score for each gap&lt;/li&gt;
&lt;li&gt;Boundaries are assigned at any valley which is deeper than a cutoff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Coherence Relations  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Result&lt;ul&gt;
&lt;li&gt;The Tin Woodman was caught in the rain. His joints rusted&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Explanation&lt;ul&gt;
&lt;li&gt;John hid Bill’s car keys. He was drunk&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parallel&lt;ul&gt;
&lt;li&gt;The Scarecrow wanted some brains&lt;/li&gt;
&lt;li&gt;The Tin Woodman wanted a heart&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Elaboration(詳細論述)&lt;ul&gt;
&lt;li&gt;Dorothy was from Kansas&lt;/li&gt;
&lt;li&gt;She lived in the midst of the great Kansas prairies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Occasion(起因)&lt;ul&gt;
&lt;li&gt;Dorothy picked up the oil-can&lt;/li&gt;
&lt;li&gt;She oiled the Tin Woodman’s joints&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Coherence Relation Assignment  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discourse parsing&lt;/li&gt;
&lt;li&gt;Open problems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cue-Phrase-Based Algorithm  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Using cue phrases&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Segment the text into discourse segments&lt;/li&gt;
&lt;li&gt;Classify the relationship between each consecutive discourse&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cue phrase&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;connectives, which are often conjunctions or adverbs &lt;ul&gt;
&lt;li&gt;because, although, but, for example, yet, with, and&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;discourse uses vs. sentential uses&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;With&lt;/strong&gt; its distant orbit, Mars exhibits frigid weather conditions. (因為長距離的運行軌道，火星天氣酷寒)&lt;/li&gt;
&lt;li&gt;We can see Mars &lt;strong&gt;with&lt;/strong&gt; an ordinary telescope&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;img data-src=&#34;/img/NLP/discourse-relation.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Temporal Relation  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ordered in time (Asynchronous)&lt;ul&gt;
&lt;li&gt;before, after …&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;overlapped (Synchronous)&lt;ul&gt;
&lt;li&gt;at the same time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Contingency Relation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因果關係，附帶條件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Comparison Relation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;difference between two arguments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Expansion Relation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;expands the information for one argument in the other one or continues the narrative flow&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implicit Relation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discourse marker is absent&lt;/li&gt;
&lt;li&gt;颱風來襲，學校停止上課&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Chinese Relation Words &lt;img data-src=&#34;/img/NLP/chinese-coherence-relation.png&#34; alt=&#34;&#34;&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ambiguous Discourse Markers &lt;ul&gt;
&lt;li&gt;而：而且, 然而, 因而&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Reference-Resolution&#34;&gt;&lt;a href=&#34;#Reference-Resolution&#34; class=&#34;headerlink&#34; title=&#34;Reference Resolution&#34;&gt;&lt;/a&gt;Reference Resolution&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/NLP/reference-resolution.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evoke&lt;ul&gt;
&lt;li&gt;When a referent is first mentioned in a discourse, we say that a representation for it is &lt;strong&gt;evoked into&lt;/strong&gt; the model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Access&lt;ul&gt;
&lt;li&gt;Upon subsequent mention, this representation is &lt;strong&gt;accessed from&lt;/strong&gt; the model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Five Types of Referring Expressions  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Indefinite Noun Phrases(不定名詞)&lt;ul&gt;
&lt;li&gt;marked with the determiner a, some, this …&lt;/li&gt;
&lt;li&gt;Create a new internal symbol and add to the current world model&lt;ul&gt;
&lt;li&gt;Mayumi has bought a new automobile&lt;/li&gt;
&lt;li&gt;automobile(g123)&lt;/li&gt;
&lt;li&gt;new(g123)&lt;/li&gt;
&lt;li&gt;owns(mayumi, g123)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;non-specific sense to describe an object&lt;ul&gt;
&lt;li&gt;Mayumi wantsto buy a new XJE&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;whole classes of objects&lt;ul&gt;
&lt;li&gt;A new automobiletypically requires repair twice in the first 12 months&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;collect one or more properties&lt;ul&gt;
&lt;li&gt;The Macho GTE XL is a new automobile&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Question and commands&lt;ul&gt;
&lt;li&gt;Is her automobile in a parking placenear the exit?&lt;/li&gt;
&lt;li&gt;Put her automobile into a parking placenear the exit!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Definite Noun Phrases(定名詞)&lt;ul&gt;
&lt;li&gt;simple referential and generic uses(the same as indefinite)&lt;/li&gt;
&lt;li&gt;indicate an individual by description that they satisfy&lt;ul&gt;
&lt;li&gt;The manufacturer &lt;strong&gt;of this automobile&lt;/strong&gt; should be indicted&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pronouns(代名詞)&lt;ul&gt;
&lt;li&gt;reference backs to entities that have been introduced by previous nounphrases in a discourse&lt;/li&gt;
&lt;li&gt;non-referential noun phrase&lt;ul&gt;
&lt;li&gt;non-exist object&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;logical variable&lt;ul&gt;
&lt;li&gt;No male driveradmits that heis incompetent &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;something that is available from the context of utterance, but has not been explicitly mentioned before&lt;ul&gt;
&lt;li&gt;Here they come, late again!&lt;/li&gt;
&lt;li&gt;Can’t easily know who are “they”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Anaphora&lt;ul&gt;
&lt;li&gt;Number Agreements&lt;ul&gt;
&lt;li&gt;John has a Ford Falcon. It is red&lt;/li&gt;
&lt;li&gt;John has three Ford Falcons. They are red&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Person Agreement(人稱)&lt;/li&gt;
&lt;li&gt;Gender Agreement&lt;/li&gt;
&lt;li&gt;Selection Restrictions&lt;ul&gt;
&lt;li&gt;verb and its arguments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Demonstratives (指示詞)&lt;ul&gt;
&lt;li&gt;this, that&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Names&lt;ul&gt;
&lt;li&gt;Full name &amp;gt; long definite description &amp;gt; short definite description &amp;gt; last name&amp;gt; first name &amp;gt; distal demonstrative &amp;gt; proximate demonstrative &amp;gt; NP &amp;gt; stressed pronoun &amp;gt; unstressed pronoun&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Information Status  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Referential forms used to provide new or old information&lt;/li&gt;
&lt;li&gt;givenness hierarchy &lt;img data-src=&#34;/img/NLP/givenness-hierarchy.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Definite-indefinite is a clue to given-new status&lt;ul&gt;
&lt;li&gt;The sales managere(given) employed a foreign distributor(new)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If there are ambiguous noun phrases in a sentence, then it extracts the presuppositions to provide extra constraints&lt;/li&gt;
&lt;li&gt;When some new information is added to knowledge base, check if it is consistent with what we already know&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Active model of understanding  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given a text, build up predictions or expectations about new information and actively compare these with successive input to resolve ambiguities&lt;/li&gt;
&lt;li&gt;Construct a proof of the information provided in a sentence from the existing world knowledge and plausible inference rules illustrated&lt;/li&gt;
&lt;li&gt;the inference are not sensitive to the order&lt;ul&gt;
&lt;li&gt;if the proposition that the disc is heavy is inferred, then it is not changed after the discourse has finished&lt;/li&gt;
&lt;li&gt;Solution: describe the propositions in temporal order&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Script: encapsulate a sequence of actions that belong together into a script&lt;figure class=&#34;highlight dns&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;automobile_buying:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&amp;lt;&amp;#123;customer(C), automobile(&lt;span class=&#34;keyword&#34;&gt;A&lt;/span&gt;), dealer(D), garage(G)&amp;#125;,&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;	&amp;lt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;		goes(C, G),&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;		test_drives(C, &lt;span class=&#34;keyword&#34;&gt;A&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;		orders(C, &lt;span class=&#34;keyword&#34;&gt;A&lt;/span&gt;, D),&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;		delivers(D, &lt;span class=&#34;keyword&#34;&gt;A&lt;/span&gt;, C),&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;		drives(C, &lt;span class=&#34;keyword&#34;&gt;A&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;	&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;參考資料&#34;&gt;&lt;a href=&#34;#參考資料&#34; class=&#34;headerlink&#34; title=&#34;參考資料&#34;&gt;&lt;/a&gt;參考資料&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;HHChen 課堂講義&lt;/li&gt;
&lt;/ul&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://gitqwerty777.github.io/natural-language-processing/</guid>
            <title>自然語言處理(上)</title>
            <link>http://gitqwerty777.github.io/natural-language-processing/</link>
            <category>機器學習</category>
            <category>自然語言處理</category>
            <category>統計</category>
            <pubDate>Sat, 07 Mar 2015 11:00:47 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;Chap01-Introduction&#34;&gt;&lt;a href=&#34;#Chap01-Introduction&#34; class=&#34;headerlink&#34; title=&#34;Chap01 Introduction&#34;&gt;&lt;/a&gt;Chap01 Introduction&lt;/h2&gt;&lt;h3 id=&#34;Applications-of-NLP&#34;&gt;&lt;a href=&#34;#Applications-of-NLP&#34; class=&#34;headerlink&#34; title=&#34;Applications of NLP&#34;&gt;&lt;/a&gt;Applications of NLP&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Machine translation&lt;ul&gt;
&lt;li&gt;google translate&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Speech recognition&lt;ul&gt;
&lt;li&gt;Siri&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Smart input method&lt;ul&gt;
&lt;li&gt;ㄐㄅㄈㄏ → 加倍奉還&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sentiment(情感) analysis&lt;/li&gt;
&lt;li&gt;Information retrieval&lt;/li&gt;
&lt;li&gt;Question Anwering&lt;ul&gt;
&lt;li&gt;Turing Test&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optical character recognition (OCR)&lt;a id=&#34;more&#34;&gt;&lt;/a&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Critical-Problems-in-NLP&#34;&gt;&lt;a href=&#34;#Critical-Problems-in-NLP&#34; class=&#34;headerlink&#34; title=&#34;Critical Problems in NLP&#34;&gt;&lt;/a&gt;Critical Problems in NLP&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Ambiguity(不明確性)&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The most important thing in NLP&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Lexical(字辭)&lt;ul&gt;
&lt;li&gt;&lt;code&gt;current&lt;/code&gt;: noun or adjective&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bank&lt;/code&gt; (noun): money or river&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Syntactic(語法)&lt;ul&gt;
&lt;li&gt;&lt;code&gt;[saw [the boy] [in the park]]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[saw [the boy in the park]]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semantic(語義)&lt;ul&gt;
&lt;li&gt;“John kissed his wife, and so did Sam”. (Sam kissed John’s wife or his own?)&lt;/li&gt;
&lt;li&gt;agent(施事) vs. patient(受事)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ill-form(bad form)&lt;ul&gt;
&lt;li&gt;typo&lt;/li&gt;
&lt;li&gt;grammatical errors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Robustness&lt;ul&gt;
&lt;li&gt;various domain&lt;/li&gt;
&lt;li&gt;網路語言：取材於方言俗語、各門外語、縮略語、諧音、甚至以符號合併以達至象形效果等等&lt;ul&gt;
&lt;li&gt;emoticon(表情符號)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Main-Topics-in-Large-Scale-NLP-Design&#34;&gt;&lt;a href=&#34;#Main-Topics-in-Large-Scale-NLP-Design&#34; class=&#34;headerlink&#34; title=&#34;Main Topics in Large-Scale NLP Design&#34;&gt;&lt;/a&gt;Main Topics in Large-Scale NLP Design&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Knowledge representation&lt;ul&gt;
&lt;li&gt;organize and describe linguistic knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge strategies&lt;ul&gt;
&lt;li&gt;use knowledge for efficient parsing, ambiguity resolution, ill-formed recovery&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge acquisition&lt;ul&gt;
&lt;li&gt;setup and maintain knowledge base systematically and cost-effectively&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge integration&lt;ul&gt;
&lt;li&gt;consider various knowledge sources effectively&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Models&#34;&gt;&lt;a href=&#34;#Models&#34; class=&#34;headerlink&#34; title=&#34;Models&#34;&gt;&lt;/a&gt;Models&lt;/h3&gt;&lt;p&gt;用演算法來轉換文字結構，以產生最後結果   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;State machines&lt;/li&gt;
&lt;li&gt;Rule-based approaches&lt;/li&gt;
&lt;li&gt;Logical formalisms&lt;/li&gt;
&lt;li&gt;Probabilistic models&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Approaches&#34;&gt;&lt;a href=&#34;#Approaches&#34; class=&#34;headerlink&#34; title=&#34;Approaches&#34;&gt;&lt;/a&gt;Approaches&lt;/h3&gt;&lt;p&gt;NLP start from 1960, &lt;strong&gt;statictics method&lt;/strong&gt; wins after 1995&lt;/p&gt;
&lt;p&gt;Rule-based approach&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advantages&lt;ul&gt;
&lt;li&gt;No need database&lt;/li&gt;
&lt;li&gt;Easy to incorporate with knowledge&lt;/li&gt;
&lt;li&gt;Better generalization to a unseen domain&lt;/li&gt;
&lt;li&gt;Explainable and traceable&lt;ul&gt;
&lt;li&gt;easy to understand&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Disadvantages&lt;ul&gt;
&lt;li&gt;Hard to maintain consistency (at different situation)&lt;/li&gt;
&lt;li&gt;Hard to handle uncertain knowledge (define uncertainty factor)&lt;ul&gt;
&lt;li&gt;irregular information&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Not easy to avoid redundancy&lt;/li&gt;
&lt;li&gt;Knowledge acquisition is time consuming&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Corpus(語料庫)-based approach  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advantages&lt;ul&gt;
&lt;li&gt;Knowledge acquisition can be automatically achieved by the computer&lt;/li&gt;
&lt;li&gt;Uncertain knowledge can be objectively quantified(知識可被量化)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consistency and completeness&lt;/strong&gt; are easy to obtain&lt;/li&gt;
&lt;li&gt;Well established statistical theories and technique are available&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Disadvantages&lt;ul&gt;
&lt;li&gt;Generalization is poor for small-size database&lt;/li&gt;
&lt;li&gt;Unable to reasoning&lt;/li&gt;
&lt;li&gt;Hard to identify the effect of each parameter&lt;/li&gt;
&lt;li&gt;Build database is time consuming&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Corpus&lt;ul&gt;
&lt;li&gt;Brown Corpus (1M words),Birmingham Corpus (7.5M words), LOB Corpus (1M words), etc&lt;/li&gt;
&lt;li&gt;Corpora(語料庫(複數)) of special domains or style&lt;ul&gt;
&lt;li&gt;Newspaper, Bible, etc&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Information in Corpora&lt;ul&gt;
&lt;li&gt;pure-text corpus&lt;ul&gt;
&lt;li&gt;language usage of real world, word distribution, co-occurrence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;tagged corpus&lt;ul&gt;
&lt;li&gt;parts of speech, structures, features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hybrid approach   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use rule-based approach when &lt;ul&gt;
&lt;li&gt;there are rules that have good coverage&lt;ul&gt;
&lt;li&gt;it can be governed by a small number of rules&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;extensional knowledge is important to the system&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use corpus-based approach when&lt;ul&gt;
&lt;li&gt;Knowledge needed to solve the problem is huge and intricate&lt;/li&gt;
&lt;li&gt;A good model or formulation exists&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Implementation&#34;&gt;&lt;a href=&#34;#Implementation&#34; class=&#34;headerlink&#34; title=&#34;Implementation&#34;&gt;&lt;/a&gt;Implementation&lt;/h3&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3d3dy5ubHRrLm9yZy8=&#34;&gt;Natural Language Toolkit(NLTK)&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;: Open source Python modules, linguistic data and documentation for research and development in natural language processing&lt;/p&gt;
&lt;p&gt;features  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Corpus readers&lt;/li&gt;
&lt;li&gt;Tokenizers&lt;ul&gt;
&lt;li&gt;whitespace, newline, blankline, word, treebank, sexpr, regexp, Punkt sentence segmenter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stemmers&lt;ul&gt;
&lt;li&gt;Porter, Lancaster, regexp&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Taggers&lt;ul&gt;
&lt;li&gt;regexp, n-gram, backoff, Brill, HMM, TnT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Chunkers&lt;ul&gt;
&lt;li&gt;regexp, n-gram, named-entity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Metrics&lt;ul&gt;
&lt;li&gt;accuracy, precision, recall, windowdiff, distance metrics, inter-annotator agreement coefficients, word association measures, rank correlation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Estimation&lt;ul&gt;
&lt;li&gt;uniform, maximum likelihood, Lidstone, Laplace, expected likelihood, heldout, cross-validation, Good-Turing, Witten-Bell&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Miscellaneous&lt;ul&gt;
&lt;li&gt;unification, chatbots, many utilities&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap02-Overall-Pictures&#34;&gt;&lt;a href=&#34;#Chap02-Overall-Pictures&#34; class=&#34;headerlink&#34; title=&#34;Chap02 Overall Pictures&#34;&gt;&lt;/a&gt;Chap02 Overall Pictures&lt;/h2&gt;&lt;p&gt;&lt;img data-src=&#34;/img/NLP/overview.png&#34; alt=&#34;overview&#34;&gt;&lt;/p&gt;
&lt;p&gt;Knowledge Categories     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Phonology(聲音，資料來源)&lt;/li&gt;
&lt;li&gt;Morphology(詞性)&lt;/li&gt;
&lt;li&gt;Syntax(句構)&lt;/li&gt;
&lt;li&gt;Semantics(語義)&lt;/li&gt;
&lt;li&gt;Pragmatics(句子關聯，語用學)&lt;/li&gt;
&lt;li&gt;Discourse(篇章分析，話語)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Morphology-Structure-of-words&#34;&gt;&lt;a href=&#34;#Morphology-Structure-of-words&#34; class=&#34;headerlink&#34; title=&#34;Morphology(Structure of words)&#34;&gt;&lt;/a&gt;Morphology(Structure of words)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;part-of-speech(POS) tagging(詞性標註, lexical category)&lt;/li&gt;
&lt;li&gt;find the roots of words&lt;ul&gt;
&lt;li&gt;e.g., going → go, cats → cat&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Syntax-structure-of-sentences&#34;&gt;&lt;a href=&#34;#Syntax-structure-of-sentences&#34; class=&#34;headerlink&#34; title=&#34;Syntax(structure of sentences)&#34;&gt;&lt;/a&gt;Syntax(structure of sentences)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Context-Free Grammars(CFG) &lt;img data-src=&#34;/img/NLP/cfg.png&#34; alt=&#34;parse tree&#34;&gt;&lt;/li&gt;
&lt;li&gt;Chomsky Normal Form(CNF)&lt;ul&gt;
&lt;li&gt;can only use following two rules &lt;ol&gt;
&lt;li&gt;&lt;code&gt;non-terminal → terminal&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;non-terminal → non-terminal non-terminal&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;dependency&lt;ul&gt;
&lt;li&gt;local dependency&lt;ul&gt;
&lt;li&gt;words near together would probably have the same syntax rule&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;long-distance dependency &lt;ul&gt;
&lt;li&gt;wh-movement(疑問詞移位)&lt;ul&gt;
&lt;li&gt;What did Jennifer buy? → 什麼 (助動詞) 珍妮佛 買了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分裂句 Right-node raising&lt;ul&gt;
&lt;li&gt;[[she would have bought] and [he might sell]] shares&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Argument-cluster coordination&lt;ul&gt;
&lt;li&gt;I give [[you an apple] and [him a pear]]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;challenge for some statistical NLP approaches (like n-grams)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Semantics-meaning-of-individual-sentences&#34;&gt;&lt;a href=&#34;#Semantics-meaning-of-individual-sentences&#34; class=&#34;headerlink&#34; title=&#34;Semantics(meaning of individual sentences)&#34;&gt;&lt;/a&gt;Semantics(meaning of individual sentences)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;semantic roles  &lt;ul&gt;
&lt;li&gt;agent(主詞)&lt;/li&gt;
&lt;li&gt;patient(受詞)&lt;/li&gt;
&lt;li&gt;instrument(工具)&lt;/li&gt;
&lt;li&gt;goal(目標)&lt;/li&gt;
&lt;li&gt;Beneficiary(受益)&lt;/li&gt;
&lt;li&gt;He threw the book(patient) at me(goal)&lt;/li&gt;
&lt;li&gt;John sold the car for a friend(beneficiary)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Subcategorizations(次分類)&lt;ul&gt;
&lt;li&gt;及物、不及物動詞&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semantics can be divided into two parts&lt;ul&gt;
&lt;li&gt;Lexical Semantics&lt;ul&gt;
&lt;li&gt;上下位，同義(反義)，部分-整體&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Composition Semantics&lt;ul&gt;
&lt;li&gt;合起來的意義與單一字意義不同&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Implementation&lt;ul&gt;
&lt;li&gt;WordNet®(large lexical database of English)&lt;/li&gt;
&lt;li&gt;Thesaurus(索引典)&lt;/li&gt;
&lt;li&gt;同義詞詞林&lt;/li&gt;
&lt;li&gt;廣義知網中文詞知識庫(E-HowNet)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9mcmFtZW5ldC5pY3NpLmJlcmtlbGV5LmVkdS9mbmRydXBhbC9hYm91dA==&#34;&gt;FrameNet&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;FrameNet&#34;&gt;&lt;a href=&#34;#FrameNet&#34; class=&#34;headerlink&#34; title=&#34;FrameNet&#34;&gt;&lt;/a&gt;FrameNet&lt;/h4&gt;&lt;p&gt;A dictionary of more than 10,000 word senses, 170,000 manually annotated sentences&lt;/p&gt;
&lt;p&gt;Frame Semantics(Charles J. Fillmore)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the meanings of most words can be more understood by semantic frame&lt;/li&gt;
&lt;li&gt;Including description of a type of event, relation, or entity and the participants in it&lt;/li&gt;
&lt;li&gt;Example: &lt;code&gt;apply_heat&lt;/code&gt; frame&lt;ul&gt;
&lt;li&gt;When one of these words appear, this frame will be applied&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Fry(炸)&lt;/code&gt;, &lt;code&gt;bake(烘)&lt;/code&gt;, &lt;code&gt;boil(煮)&lt;/code&gt;, a`nd broil(烤)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Frame elements: Cook, Food, Heating_instrument and Container&lt;ul&gt;
&lt;li&gt;a person doing the cooking (Cook)&lt;/li&gt;
&lt;li&gt;the food that is to be cooked (Food)&lt;/li&gt;
&lt;li&gt;something to hold the food while cooking (Container)&lt;/li&gt;
&lt;li&gt;a source of heat (Heating_instrument)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;[&lt;code&gt;Cook&lt;/code&gt; the boys] … GRILL [&lt;code&gt;Food&lt;/code&gt; fish] [&lt;code&gt;Heating_instrument&lt;/code&gt; on an open fire]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Pragmatics-how-sentences-relate-to-each-other&#34;&gt;&lt;a href=&#34;#Pragmatics-how-sentences-relate-to-each-other&#34; class=&#34;headerlink&#34; title=&#34;Pragmatics(how sentences relate to each other)&#34;&gt;&lt;/a&gt;Pragmatics(how sentences relate to each other)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;explain what the speaker really expressed&lt;/li&gt;
&lt;li&gt;Understand the scope of &lt;ul&gt;
&lt;li&gt;quantifiers&lt;/li&gt;
&lt;li&gt;speech acts&lt;/li&gt;
&lt;li&gt;discourse analysis&lt;/li&gt;
&lt;li&gt;anaphoric relations(首語重複)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Anaphora(首語重複) and Coreference(指代)&lt;ul&gt;
&lt;li&gt;張三是老師,他教學很認真,同時,他也是一個好爸爸。&lt;/li&gt;
&lt;li&gt;Type/Instance: “老師”/“張三”, “一個好爸爸”/“張三”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;crucial to &lt;strong&gt;information extraction&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Dialogue Tagging &lt;img data-src=&#34;/img/NLP/dialogue_tag.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Discourse-Analysis&#34;&gt;&lt;a href=&#34;#Discourse-Analysis&#34; class=&#34;headerlink&#34; title=&#34;Discourse Analysis&#34;&gt;&lt;/a&gt;Discourse Analysis&lt;/h3&gt;&lt;p&gt;Example  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1a: 佛羅倫斯哪個博物館在1993年的爆炸事件中受到破壞？&lt;/li&gt;
&lt;li&gt;1b: 這個事件哪一天發生？&lt;ul&gt;
&lt;li&gt;問句1b「這個事件」，指的是問句1a「1993年的爆炸事件」&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Summary&#34;&gt;&lt;a href=&#34;#Summary&#34; class=&#34;headerlink&#34; title=&#34;Summary&#34;&gt;&lt;/a&gt;Summary&lt;/h3&gt;&lt;p&gt;From &lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3d3dy5paXMuc2luaWNhLmVkdS50dy9wYWdlL2V2ZW50cy9GSUxFLzEyMDMxMzEwMTA3U2xpZGVzLnBkZg==&#34;&gt;The Three (and a Half) Futures of NLP&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NLP is &lt;strong&gt;Notation Transformation&lt;/strong&gt;(e.g. English → Chinese), with some information(POS, syntatic, senmatic…) added&lt;/li&gt;
&lt;li&gt;Much NLP is engineering&lt;ul&gt;
&lt;li&gt;select and tuning learning performance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge is crucial in language-related research areas, but providing a large scaleknowledge base is difficult and costly&lt;ul&gt;
&lt;li&gt;Knowledge Base&lt;ul&gt;
&lt;li&gt;WordNet&lt;/li&gt;
&lt;li&gt;FrameNet&lt;/li&gt;
&lt;li&gt;Wikipedia&lt;/li&gt;
&lt;li&gt;Dbpedia&lt;/li&gt;
&lt;li&gt;Freebase&lt;/li&gt;
&lt;li&gt;Siri&lt;/li&gt;
&lt;li&gt;Google Knowledge Graph&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hierarchy of transformations(由深至淺)&lt;ul&gt;
&lt;li&gt;pragmatics, writing style&lt;ul&gt;
&lt;li&gt;deeper semantics, discourse&lt;ul&gt;
&lt;li&gt;shallow semantics, co-reference&lt;ul&gt;
&lt;li&gt;syntax, POS(part-of-speech)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分析時由淺至深&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Analysis&#34;&gt;&lt;a href=&#34;#Analysis&#34; class=&#34;headerlink&#34; title=&#34;Analysis&#34;&gt;&lt;/a&gt;Analysis&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;/img/NLP/layer.png&#34; alt=&#34;Layer&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/l1.png&#34; alt=&#34;L1&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/l2.png&#34; alt=&#34;L2&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/l3.png&#34; alt=&#34;L3&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/l4.png&#34; alt=&#34;L4&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;NLP-progress-by-now&#34;&gt;&lt;a href=&#34;#NLP-progress-by-now&#34; class=&#34;headerlink&#34; title=&#34;NLP progress by now&#34;&gt;&lt;/a&gt;NLP progress by now&lt;/h4&gt;&lt;p&gt;&lt;img data-src=&#34;/img/NLP/sub.png&#34; alt=&#34;NLP subclass&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/dowell.png&#34; alt=&#34;NLP do today&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/NLP/cantdo.png&#34; alt=&#34;NLP can&amp;#39;t do today&#34;&gt;  &lt;/p&gt;
&lt;h2 id=&#34;Chap03-Collocations-搭配詞&#34;&gt;&lt;a href=&#34;#Chap03-Collocations-搭配詞&#34; class=&#34;headerlink&#34; title=&#34;Chap03 Collocations(搭配詞)&#34;&gt;&lt;/a&gt;Chap03 Collocations(搭配詞)&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;多個單字組合成一個有意義的語詞，其意義無法從各個單字中推得&lt;ul&gt;
&lt;li&gt;e.g. black market&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Subclasses of Collocations&lt;ul&gt;
&lt;li&gt;compound nouns&lt;ul&gt;
&lt;li&gt;telephone box and post office&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;idioms&lt;ul&gt;
&lt;li&gt;kick the bucket(氣絕)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Light verbs(輕動詞)&lt;ul&gt;
&lt;li&gt;動詞失去其意義，需要和其他有實質意義的詞作搭配&lt;/li&gt;
&lt;li&gt;e.g. The man took a walk(walk, not take) vs The man took a radio(take)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Verb particle constructions(語助詞) or Phrasal Verbs(詞組動詞, 短語動詞, V + 介系詞)&lt;ul&gt;
&lt;li&gt;take in = deceive, look sth. up&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;proper names&lt;ul&gt;
&lt;li&gt;San Francisco&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Terminology(專有名詞)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Classification&lt;ul&gt;
&lt;li&gt;Fixed expressions&lt;ul&gt;
&lt;li&gt;in short (O)&lt;/li&gt;
&lt;li&gt;in shorter or in very short(X)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semi-fixed expressions(可用變化形)&lt;ul&gt;
&lt;li&gt;non-decomposable idioms&lt;ul&gt;
&lt;li&gt;kick the bucket (O)&lt;/li&gt;
&lt;li&gt;he kicks the bucket(O)&lt;/li&gt;
&lt;li&gt;the bucket was kicked (X)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;compound nominals&lt;ul&gt;
&lt;li&gt;car park, car parks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Proper names&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Syntactically-Flexible Expressions&lt;ul&gt;
&lt;li&gt;decomposable idioms&lt;ul&gt;
&lt;li&gt;let the cat out of the bag&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;verb-particle constructions&lt;/li&gt;
&lt;li&gt;light verbs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Institutionalized Phrases (習慣用法)&lt;ul&gt;
&lt;li&gt;salt and pepper(○) pepper and salt(×)&lt;/li&gt;
&lt;li&gt;traffic light&lt;/li&gt;
&lt;li&gt;kindle excitement(點燃激情)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Collocation-detection&#34;&gt;&lt;a href=&#34;#Collocation-detection&#34; class=&#34;headerlink&#34; title=&#34;Collocation detection&#34;&gt;&lt;/a&gt;Collocation detection&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;by Frequency&lt;/li&gt;
&lt;li&gt;by Mean and Variance of the distance between focal word (焦點詞) and collocating word(搭配詞)&lt;/li&gt;
&lt;li&gt;Hypothesis Testing&lt;/li&gt;
&lt;li&gt;Mutual Information&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;By-Frequency&#34;&gt;&lt;a href=&#34;#By-Frequency&#34; class=&#34;headerlink&#34; title=&#34;By Frequency&#34;&gt;&lt;/a&gt;By Frequency&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;找出現機率大的bigrams&lt;ul&gt;
&lt;li&gt;not always significant&lt;/li&gt;
&lt;li&gt;篩選可能為組合詞的詞性組合(Ex. adj+N) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The collocations found &lt;img data-src=&#34;/img/NLP/freandtag.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;What if a word have two possible collocations?(strong force, powerful force) &lt;img data-src=&#34;/img/NLP/frecomp.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;By-Mean-and-Variance-of-the-distance-between-focal-word-焦點詞-and-collocating-word-搭配詞&#34;&gt;&lt;a href=&#34;#By-Mean-and-Variance-of-the-distance-between-focal-word-焦點詞-and-collocating-word-搭配詞&#34; class=&#34;headerlink&#34; title=&#34;By Mean and Variance of the distance between focal word(焦點詞) and collocating word(搭配詞)&#34;&gt;&lt;/a&gt;By Mean and Variance of the distance between focal word(焦點詞) and collocating word(搭配詞)&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;many collocations consist of more flexible relationships&lt;ul&gt;
&lt;li&gt;frequency is not suitable&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Define a collocational window&lt;ol&gt;
&lt;li&gt;e.g., 3-4 words before/after&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;assemble every word pair as a bigram&lt;ol&gt;
&lt;li&gt;e.g., A B C D → AB, AC, AD, BC, BD, CD&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;computes the mean and variance of the offset between the two words&lt;ol&gt;
&lt;li&gt;變異數愈低，代表兩個字之間的位置關聯愈固定 &lt;img data-src=&#34;/img/NLP/meanvar.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;z-score $z = {freq - \mu \over \sigma}$: the strength of a word pair&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Hypothesis-Testing-假設檢定&#34;&gt;&lt;a href=&#34;#Hypothesis-Testing-假設檢定&#34; class=&#34;headerlink&#34; title=&#34;Hypothesis Testing(假設檢定)&#34;&gt;&lt;/a&gt;Hypothesis Testing(假設檢定)&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Even high frequency and low variance can be accidental&lt;/li&gt;
&lt;li&gt;null hypothesis(虛無假設, H0) &lt;ul&gt;
&lt;li&gt;設 w1 and w2 is completely independent → w1 and w2 不是搭配詞&lt;ul&gt;
&lt;li&gt;P(w1w2) = P(w1)P(w2) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;假設H0為真，計算這兩個字符合H0的機率P&lt;ul&gt;
&lt;li&gt;若P太低則否決H0(→ 是搭配詞)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two issues&lt;ul&gt;
&lt;li&gt;Look for particular patterns in the data&lt;/li&gt;
&lt;li&gt;How much data we have seen&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;種類包括：t檢驗，Z檢驗，卡方檢驗，F檢驗      &lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;t-test&#34;&gt;&lt;a href=&#34;#t-test&#34; class=&#34;headerlink&#34; title=&#34;t-test&#34;&gt;&lt;/a&gt;t-test&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Test whether &lt;strong&gt;distributions of two groups&lt;/strong&gt; are &lt;strong&gt;statistically different&lt;/strong&gt; or not&lt;ul&gt;
&lt;li&gt;H0 → (w1, w2) has no differnece with normal distribution&lt;/li&gt;
&lt;li&gt;considering &lt;strong&gt;variance&lt;/strong&gt; of the data &lt;img data-src=&#34;/img/NLP/ttest.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;formula &lt;img data-src=&#34;/img/NLP/ttest2.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/ttest3.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Calculate t by alpha level and degree of freedom&lt;ul&gt;
&lt;li&gt;alpha level &lt;code&gt;α&lt;/code&gt;: confidence&lt;ul&gt;
&lt;li&gt;in normal distribution，α = 95%落在mean±1.96std之間, α = 99%落在mean±2.576std之間&lt;/li&gt;
&lt;li&gt;If t-value is larger than 2.576, we say the two groups &lt;strong&gt;are different&lt;/strong&gt; with 99% confidence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;degree of freedom: number of sample-1&lt;ul&gt;
&lt;li&gt;total = number of two groups-2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;t↑ → more difference → more possible to reject null hypothesis → more possible to be collocation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Example: “new” occurs 15,828 times, “companies” 4,675 times, “new companies” 8 times, total 14,307,668 tokens&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Null hypothesis: the occurrences of new and companies are independent(not collocation)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;H0 mean = P(new, companies) = P(new) x P(companies) = $\frac{15828 \times 4678}{14307668^2} = 3.615 \times 10^{-7}$&lt;/li&gt;
&lt;li&gt;H0 var = p(1-p) ~= p when p is small&lt;/li&gt;
&lt;li&gt;tvalue = $\frac{5.591 \times 10^7 - 3.615\times 10^7}{\sqrt{\frac{5.591 \times 10^7}{14307668}}} = 0.999932$&lt;/li&gt;
&lt;li&gt;0.999932 &amp;lt; 2.576, we cannot reject the null hypothesis&lt;ul&gt;
&lt;li&gt;new company are not collocation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the above words are possible collocations &lt;img data-src=&#34;/img/NLP/ttest4.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!--???????--&gt;
&lt;p&gt;Hypothesis testing of differences    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;useful for lexicography &lt;ul&gt;
&lt;li&gt;which word(strong, powerful) is suitable to modify “computer”?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;T-test can be used for &lt;strong&gt;comparison of the means of two normal populations&lt;/strong&gt; &lt;ul&gt;
&lt;li&gt;H0 is that the average difference is 0 (u = 0)&lt;/li&gt;
&lt;li&gt;v1 and v2 are the words we are comparing (e.g., powerful and strong), and w is the collocate of interest(e.g., computers)&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/ttest5.png&#34; alt=&#34;&#34;&gt;)&lt;img data-src=&#34;/img/NLP/ttest6.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;Chi-Square-test&#34;&gt;&lt;a href=&#34;#Chi-Square-test&#34; class=&#34;headerlink&#34; title=&#34;Chi-Square test&#34;&gt;&lt;/a&gt;Chi-Square test&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;T-test assumes that probabilities are normally distributed&lt;ul&gt;
&lt;li&gt;not really&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Chi-Square: compare &lt;strong&gt;observed frequencies&lt;/strong&gt; with &lt;strong&gt;expected frequencies&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;If &lt;strong&gt;difference between observed and expected frequencies&lt;/strong&gt; is large, we can reject H0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example&lt;ul&gt;
&lt;li&gt;expected frequency of “new companies”: $\frac{8+4667}{14307668} \times \frac{8+15820}{14307668} \times 14307668$ = 5.2 &lt;img data-src=&#34;/img/NLP/chi1.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;chi-square value = χ^2 &lt;img data-src=&#34;/img/NLP/chi3.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/NLP/chi2.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;When α=0.05, χ^2=3.841&lt;/li&gt;
&lt;li&gt;Because 1.55&amp;lt;3.841, we cannot reject the null hypothesis. new companies is not a good candidate for a collocation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Comparison with T-test&lt;ul&gt;
&lt;li&gt;The 20 bigrams with the highest t scores in the test corpus are also the 20 bigrams with the highest χ^2 scores&lt;/li&gt;
&lt;li&gt;χ^2 is appropriate for large probabilities(t-test is not because of normality assumption)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Application: Translation&lt;ul&gt;
&lt;li&gt;find similarity of word pairs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Likelihood-Ratios&#34;&gt;&lt;a href=&#34;#Likelihood-Ratios&#34; class=&#34;headerlink&#34; title=&#34;Likelihood Ratios&#34;&gt;&lt;/a&gt;Likelihood Ratios&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Advantage compared with Chi-Square test  &lt;ul&gt;
&lt;li&gt;more appropriate for sparse data&lt;/li&gt;
&lt;li&gt;easier to interpret&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Likelihood Ratios within single corpus  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;examine two hypothesis&lt;ul&gt;
&lt;li&gt;H1: occurrence of w2 is independent of the previous occurrence of w1(null hypothesis)&lt;/li&gt;
&lt;li&gt;H2: occurrence of w2 is dependent of the previous occurrence of w1 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;maximum likelihood estimate &lt;img data-src=&#34;/img/NLP/like1.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;using binomial distribution&lt;ul&gt;
&lt;li&gt;$b(k;n, x) = \binom nk x^k \times (1-x)^{n-k}$&lt;/li&gt;
&lt;li&gt;only different at probability&lt;ul&gt;
&lt;li&gt;$L(H_1) = b(c_{12};c_1, p)b(c_2-c_{12}; N-c_1, p)$&lt;/li&gt;
&lt;li&gt;$L(H_2) = b(c_{12};c_1, p_1)b(c_2-c_{12}; N-c_1, p_2)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/likew.png&#34; alt=&#34;likely probability&#34;&gt;&lt;/li&gt;
&lt;li&gt;log of likelihood ratio λ &lt;img data-src=&#34;/img/NLP/like2.png&#34; alt=&#34;log likelihood ratio&#34;&gt;&lt;/li&gt;
&lt;li&gt;use D = -2logλ to examine the significance of two words, which can asymptotically(漸近) chi-square distributed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Likelihood Ratios between two or more corpora   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;useful for the discovery of &lt;strong&gt;subject-specific collocations&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Mutual-Information&#34;&gt;&lt;a href=&#34;#Mutual-Information&#34; class=&#34;headerlink&#34; title=&#34;Mutual Information&#34;&gt;&lt;/a&gt;Mutual Information&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;measure of &lt;strong&gt;how much one word tells us about the other&lt;/strong&gt;(information theory)   &lt;/li&gt;
&lt;li&gt;pointwise mutual information(PMI) &lt;img data-src=&#34;/img/NLP/mutual.png&#34; alt=&#34;PMI formula&#34;&gt;&lt;ul&gt;
&lt;li&gt;MI是在獲得一個隨機變數的資訊之後，觀察另一個隨機變數所獲得的「資訊量」（單位通常為位元）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;mutual information = Expection(PMI) &lt;img data-src=&#34;/img/NLP/newMI.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;works bad in sparse environments&lt;ul&gt;
&lt;li&gt;As the perfectly dependent bigrams get rarer, their mutual information increases → &lt;strong&gt;bad measure of dependence&lt;/strong&gt; &lt;img data-src=&#34;/img/NLP/pmi-depend.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;good measure of independence&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;when perfect independence, I(x, y) = 0&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/pmi-independ.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;New formula: $C(w1w2)I(w1w2)$&lt;ul&gt;
&lt;li&gt;With MI, bigrams composed of low-frequency words will receive a higher score than bigrams composed of high-frequency words&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chain rule for entropy   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H(X,Y) = H(Y|X) + H(X) = H(X|Y) + H(Y)$&lt;/li&gt;
&lt;li&gt;Conditional entropy $H(Y|X)$ expresses how much &lt;strong&gt;extra information&lt;/strong&gt; you still need to supply on average to communicate Y when X is known &lt;img data-src=&#34;/img/NLP/conditional.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;$H(X)-H(X|Y) = H(Y)-H(Y|X)$&lt;ul&gt;
&lt;li&gt;This difference is called the &lt;strong&gt;mutual information between X and Y&lt;/strong&gt;(X, Y共同擁有的information)&lt;/li&gt;
&lt;li&gt;MI is not similar to chi-square &lt;img data-src=&#34;/img/NLP/wrongMI.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Entropy  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entropy: uncertainty of a variable &lt;img data-src=&#34;/img/NLP/entropy1.png&#34; alt=&#34;&#34;&gt;  &lt;/li&gt;
&lt;li&gt;Incorrect model’s cross entropy is larger than correct model’s &lt;img data-src=&#34;/img/NLP/entropy2.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;正確model和猜測model的差別：P(X)logP(X) ↔ P(X)logPM(X)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Entropy Rate: Per-word entropy(= sentence entropy / N) &lt;img data-src=&#34;/img/NLP/entropy_rate.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Cross Entropy: &lt;strong&gt;average informaton&lt;/strong&gt; needed to &lt;strong&gt;identify an event drawn from the set&lt;/strong&gt; between two probability distributions&lt;ul&gt;
&lt;li&gt;交叉熵的意義是用該模型對文本識別的難度，或者從壓縮的角度來看，每個詞平均要用幾個位來編碼&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/entropy_cross.png&#34; alt=&#34;&#34;&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Joint entropy H(X, Y): average information needed to &lt;strong&gt;specify both values&lt;/strong&gt; &lt;img data-src=&#34;/img/NLP/joint.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Case-Study&#34;&gt;&lt;a href=&#34;#Case-Study&#34; class=&#34;headerlink&#34; title=&#34;Case Study&#34;&gt;&lt;/a&gt;Case Study&lt;/h3&gt;&lt;p&gt;Emotion Analysis  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non-verbal Emotional Expressions&lt;/li&gt;
&lt;li&gt;text (raw) and emoticons(表情符號) (tag) form collection&lt;/li&gt;
&lt;li&gt;appearance of an emoticon is a good emotion indicator to sentences&lt;/li&gt;
&lt;li&gt;check the dependency of each word in sentences&lt;/li&gt;
&lt;li&gt;Evaluation&lt;ul&gt;
&lt;li&gt;Use top 200 lexiconentries as features&lt;/li&gt;
&lt;li&gt;Tag={Positive, Negative}&lt;/li&gt;
&lt;li&gt;LIBSVM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap04-N-gram-Model&#34;&gt;&lt;a href=&#34;#Chap04-N-gram-Model&#34; class=&#34;headerlink&#34; title=&#34;Chap04 N-gram Model&#34;&gt;&lt;/a&gt;Chap04 N-gram Model&lt;/h2&gt;&lt;p&gt;N-grams are token sequences of length N&lt;/p&gt;
&lt;p&gt;applications   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Automatic speech recognition&lt;/li&gt;
&lt;li&gt;Author Identification&lt;/li&gt;
&lt;li&gt;Spelling correction&lt;/li&gt;
&lt;li&gt;Grammatical Error Diagnosis&lt;/li&gt;
&lt;li&gt;Machine translation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Counting&#34;&gt;&lt;a href=&#34;#Counting&#34; class=&#34;headerlink&#34; title=&#34;Counting&#34;&gt;&lt;/a&gt;Counting&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Example: &lt;em&gt;I do uh main-mainly business data processing&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;Should we count “uh”(pause) as tokens?&lt;/li&gt;
&lt;li&gt;What about the repetition of “mainly”? Should such do-overs count twice or just once?(重複)&lt;/li&gt;
&lt;li&gt;The answers depend on the application&lt;ul&gt;
&lt;li&gt;“uh” is not needed for query &lt;/li&gt;
&lt;li&gt;“uh” is very useful in dialog management&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Corpora: Google Web Crawl&lt;ul&gt;
&lt;li&gt;1,024,908,267,229 English tokens&lt;/li&gt;
&lt;li&gt;13,588,391 wordform types&lt;/li&gt;
&lt;li&gt;even large dictionaries of English have only around 500k types. Why so many here?&lt;ul&gt;
&lt;li&gt;Numbers&lt;/li&gt;
&lt;li&gt;Misspellings&lt;/li&gt;
&lt;li&gt;Names&lt;/li&gt;
&lt;li&gt;Acronyms(縮寫)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Language-model&#34;&gt;&lt;a href=&#34;#Language-model&#34; class=&#34;headerlink&#34; title=&#34;Language model&#34;&gt;&lt;/a&gt;Language model&lt;/h3&gt;&lt;p&gt;Language models assign a probability to a word sequence&lt;br&gt;Ex. &lt;code&gt;P(the mythical unicorn) = P(the) * P(mythical | the) * P(unicorn | the mythical)&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Markov assumption: the probability of a word depends only on &lt;strong&gt;limited previous words&lt;/strong&gt;     &lt;ul&gt;
&lt;li&gt;Generalization: n previous words, like bigram, trigrams, 4-grams……&lt;/li&gt;
&lt;li&gt;As we increase the value of N, the accuracy of model increases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;N-Gram probabilities come from a training corpus&lt;br&gt;overly narrow corpus: probabilities don’t generalize&lt;br&gt;overly general corpus: probabilities don’t reflect task or domain  &lt;/p&gt;
&lt;p&gt;maximum likelihood estimate  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;maximizes the probability of the training set T given the model M&lt;/li&gt;
&lt;li&gt;Suppose the word “Chinese” occurs 400 times in a corpus&lt;ul&gt;
&lt;li&gt;MLE estimate is 400/1000000 = .004&lt;/li&gt;
&lt;li&gt;makes it most likely that “Chinese” will occur 400 times in a million word corpus&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;P([s] I want englishfood [s]) = P(I|[s]) x P(want|I) x P(english|want) x P(food|english) x P([s]|food) = 0.000031$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Usage&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;capture some knowledge about language&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;World Knowledge&lt;ul&gt;
&lt;li&gt;P(english food|want) = .0011&lt;/li&gt;
&lt;li&gt;P(chinese food|want) = .0065&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;syntax&lt;ul&gt;
&lt;li&gt;P(to|want) = .66&lt;/li&gt;
&lt;li&gt;P(eat| to) = .28&lt;/li&gt;
&lt;li&gt;P(food| to) = 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;discourse&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(i|&amp;lt;s&amp;gt;)&lt;/code&gt; = .25&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Shannon’s Method: use language model to generate random sentences&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shakespeare as a Corpus  &lt;ul&gt;
&lt;li&gt;99.96% of the possible bigrams were never seen (have zero entries in the table)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This is the biggest problem in language modeling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Evaluating-N-Gram-Models&#34;&gt;&lt;a href=&#34;#Evaluating-N-Gram-Models&#34; class=&#34;headerlink&#34; title=&#34;Evaluating N-Gram Models&#34;&gt;&lt;/a&gt;Evaluating N-Gram Models&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Extrinsic(外在的) evaluation&lt;ul&gt;
&lt;li&gt;Compare performance of the application within two models&lt;/li&gt;
&lt;li&gt;time-consuming&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Intrinsic evaluation&lt;ul&gt;
&lt;li&gt;perplexity&lt;ul&gt;
&lt;li&gt;But get poor approximation unless the test data looks just like the training data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;not sufficient to publish&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Standard Method&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train → Test &lt;/li&gt;
&lt;li&gt;A dataset which is different from our training set, but both drawn from the same source&lt;/li&gt;
&lt;li&gt;use evaluation metric(Ex. perplexity)&lt;/li&gt;
&lt;li&gt;Example &lt;ul&gt;
&lt;li&gt;Create a fixed lexicon L, of size V&lt;ul&gt;
&lt;li&gt;At text normalization phase, &lt;strong&gt;any training word not in L changed to UNK&lt;/strong&gt;(unknown word token)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;count UNK like a normal word&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;When testing, also use UNK counts for any word not in training&lt;/li&gt;
&lt;li&gt;The best language model is one that best predicts an unseen test set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;perplexity-複雜度&#34;&gt;&lt;a href=&#34;#perplexity-複雜度&#34; class=&#34;headerlink&#34; title=&#34;perplexity(複雜度)&#34;&gt;&lt;/a&gt;perplexity(複雜度)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Definition  &lt;ul&gt;
&lt;li&gt;notion of surprise&lt;ul&gt;
&lt;li&gt;The more surprised the model is, the lower probability it assigned to the test set&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Minimizing perplexity is the same as maximizing probability&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;probability of a test set, as normalized by the number of words &lt;img data-src=&#34;/img/NLP/perplexity.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;物理意義是單詞的編碼大小&lt;ul&gt;
&lt;li&gt;如果在某個測試語句上，語言模型的perplexity值為2^190，說明該句子的編碼需要190bits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;relate to entropy&lt;ul&gt;
&lt;li&gt;Perplexity(p, q) = $2^{H(p,q)}$   &lt;/li&gt;
&lt;li&gt;p is the test sample distribution, and q is the distribution of language model&lt;/li&gt;
&lt;li&gt;do everything in log space to avoid underflow and calculate faster&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;word-entropy&#34;&gt;&lt;a href=&#34;#word-entropy&#34; class=&#34;headerlink&#34; title=&#34;word entropy&#34;&gt;&lt;/a&gt;word entropy&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;word entropy for English&lt;ul&gt;
&lt;li&gt;11.82 bits per word [Shannon, 1951]&lt;/li&gt;
&lt;li&gt;9.8 bits per word [Grignetti, 1964]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;word entropy in medical language&lt;ul&gt;
&lt;li&gt;11.15 bits per word&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap05-Statistical-Inference&#34;&gt;&lt;a href=&#34;#Chap05-Statistical-Inference&#34; class=&#34;headerlink&#34; title=&#34;Chap05 Statistical Inference&#34;&gt;&lt;/a&gt;Chap05 Statistical Inference&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Statistical Inference：&lt;strong&gt;taking some data&lt;/strong&gt; (generated by unknown distribution) and then &lt;strong&gt;making some inferences(推理，推測)&lt;/strong&gt; about this distribution&lt;/li&gt;
&lt;li&gt;three issues&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dividing the training data into equivalence classes&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finding a good statistical estimator for each equivalence class&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Combining multiple estimators&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Form-Equivalence-Class&#34;&gt;&lt;a href=&#34;#Form-Equivalence-Class&#34; class=&#34;headerlink&#34; title=&#34;Form Equivalence Class&#34;&gt;&lt;/a&gt;Form Equivalence Class&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Classification Problem&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;predict target feature&lt;/strong&gt; based on various &lt;strong&gt;classificatory features&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;reliability v.s. discrimination&lt;ul&gt;
&lt;li&gt;The more classes, the more discrimination, but estimation feature is not reliable&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Independent assumption&lt;ul&gt;
&lt;li&gt;assume data is nearly independent&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Statistical Language Modeling&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/smodel.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Language Model: P(W)&lt;/li&gt;
&lt;li&gt;LM does not depend on acoustics&lt;ul&gt;
&lt;li&gt;the acoutstics probability is constant(calculated by data)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;n-gram model&lt;ul&gt;
&lt;li&gt;assume equivalence classes are previous n-1 words&lt;/li&gt;
&lt;li&gt;Markov Assumption: Only the prior n-1 local context affects the next entry&lt;ul&gt;
&lt;li&gt;(n-1)th Markov Model or n-gram&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Building n-grams&lt;/strong&gt;&lt;ol&gt;
&lt;li&gt;Remove punctuation(標點) and normalize text&lt;/li&gt;
&lt;li&gt;Map out-of-vocabulary words to unknown symbol(UNK)&lt;/li&gt;
&lt;li&gt;Estimate conditional probabilities by joint probabilities&lt;ul&gt;
&lt;li&gt;P(n | n-2, n-1) = P(n-2, n-1, n) / P(n-2, n-1)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Finding-statistical-estimator&#34;&gt;&lt;a href=&#34;#Finding-statistical-estimator&#34; class=&#34;headerlink&#34; title=&#34;Finding statistical estimator&#34;&gt;&lt;/a&gt;Finding statistical estimator&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Goal: derive &lt;strong&gt;probability estimate of target feature&lt;/strong&gt; based on observed data&lt;/li&gt;
&lt;li&gt;Running Example&lt;ul&gt;
&lt;li&gt;From n-gram data P(w1,..,wn), predict P(wn|w1,..,wn-1)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solutions&lt;ul&gt;
&lt;li&gt;Maximum Likelihood Estimation&lt;/li&gt;
&lt;li&gt;Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws&lt;/li&gt;
&lt;li&gt;Held Out Estimation&lt;/li&gt;
&lt;li&gt;Cross-Validation&lt;/li&gt;
&lt;li&gt;Good-Turing Estimation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model combination&lt;ul&gt;
&lt;li&gt;Combine models (unigram, bigram, trigram, …) to use the most precise model available&lt;/li&gt;
&lt;li&gt;interpolation(內插) and back-off(後退)&lt;/li&gt;
&lt;li&gt;use higher order models when model has enough data&lt;/li&gt;
&lt;li&gt;back off to lower order models when there isn’t enough data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Terminology  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ex. &lt;code&gt;[s] a b a b a&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;N = 5 (&lt;code&gt;[s]a,ab,ba,ab,ba&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;B = 3 (&lt;code&gt;[s]a,ab,ba&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;C(w1, w2…) = 某N-gram(Ex. ab)出現次數&lt;/li&gt;
&lt;li&gt;r =  某N-gram出現頻率&lt;/li&gt;
&lt;li&gt;Nr = 有幾個「出現r次的N-gram」&lt;/li&gt;
&lt;li&gt;Tr = 出現r次的N-gram，在test data出現的總次數&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;1-Maximum-Likelihood-Estimation&#34;&gt;&lt;a href=&#34;#1-Maximum-Likelihood-Estimation&#34; class=&#34;headerlink&#34; title=&#34;(1) Maximum Likelihood Estimation&#34;&gt;&lt;/a&gt;(1) Maximum Likelihood Estimation&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;usually unsuitable for NLP &lt;ul&gt;
&lt;li&gt;sparseness of the data(a lot of word sequences with zero probabilities)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use Discounting or Smoothing technique to improve&lt;ul&gt;
&lt;li&gt;Smoothing&lt;ul&gt;
&lt;li&gt;Smoothing is like Robin Hood: Steal from the rich and give to the poor&lt;/li&gt;
&lt;li&gt;no word sequences has 0 probability &lt;img data-src=&#34;/img/NLP/fBBrh6P.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Discounting&lt;ul&gt;
&lt;li&gt;assign some probability to unseen events&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-Laplace’s-Lidstone’s-and-Jeffreys-Perks’-Laws&#34;&gt;&lt;a href=&#34;#2-Laplace’s-Lidstone’s-and-Jeffreys-Perks’-Laws&#34; class=&#34;headerlink&#34; title=&#34;(2) Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws&#34;&gt;&lt;/a&gt;(2) Laplace’s, Lidstone’s and Jeffreys-Perks’ Laws&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Laplace: add 1 to every count &lt;ul&gt;
&lt;li&gt;gives far too much probabilities to unseen events&lt;/li&gt;
&lt;li&gt;Usage: In domains where the number of zeros isn’t so huge&lt;ul&gt;
&lt;li&gt;pilot studies&lt;/li&gt;
&lt;li&gt;document classification&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lidstone and Jeffreys-Perks: add a smaller value λ &amp;lt; 1&lt;ul&gt;
&lt;li&gt;B:number of bins &lt;img data-src=&#34;/img/NLP/lidstone.png&#34; alt=&#34;lid&#34;&gt;&lt;/li&gt;
&lt;li&gt;Expected Likelihood Estimation (ELE)(Jeffreys-Perks Law)&lt;ul&gt;
&lt;li&gt;λ=1/2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3-Held-Out-Estimation&#34;&gt;&lt;a href=&#34;#3-Held-Out-Estimation&#34; class=&#34;headerlink&#34; title=&#34;(3) Held Out Estimation&#34;&gt;&lt;/a&gt;(3) Held Out Estimation&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;compute frequencies in training data and held out data&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/heldout.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;Tr / Nr = Average frequency of training frequency r N-grams&lt;ul&gt;
&lt;li&gt;estimate frequency(value for validation)&lt;/li&gt;
&lt;li&gt;計算出現在training corpus r次的bigrams，在held-out corpus出現的次數稱為Tr。 因為這種bigrams有Nr個，因此平均為Tr / Nr&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Validation&lt;ul&gt;
&lt;li&gt;if the probabilities estimated on training data are close to those on held-out data, it’s a good language model&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://gitqwerty777.github.io/MLfoundation2/#chap15-validation&#34;&gt;參考資料–validation in machine learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Prevent Overtraining(overfit)&lt;ul&gt;
&lt;li&gt;test on different data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Training portion and testing portion (5-10% of total data)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Held out data (validation data)&lt;ul&gt;
&lt;li&gt;available training data: real training data(90%) + held out data(10%)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Instead of presenting a single performance figure, testing result on each smaller sample&lt;ul&gt;
&lt;li&gt;Using t-test to reject the possibility of an accidental difference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-Cross-Validation&#34;&gt;&lt;a href=&#34;#4-Cross-Validation&#34; class=&#34;headerlink&#34; title=&#34;(4) Cross-Validation&#34;&gt;&lt;/a&gt;(4) Cross-Validation&lt;/h4&gt;&lt;p&gt;If data is not enough, use each part of the data both as training data and held out data  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deleted Estimation&lt;ul&gt;
&lt;li&gt;$N_r^a$ = number of n-grams occurring r times in the &lt;strong&gt;a th part&lt;/strong&gt; of the training data&lt;/li&gt;
&lt;li&gt;$T_r^{ab}$ = number of occurs in part b of 「bigrams occurs r times in part a」&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/deleted_estimate.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Split the training data into K sections&lt;/li&gt;
&lt;li&gt;For each section k: hold-out section k and compute counts from remaining K-1 sections; compute Tr(k) &lt;/li&gt;
&lt;li&gt;Estimate probabilities by averaging over all sections&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;estimate frequency of deleted estimation &lt;img data-src=&#34;/img/NLP/del-estimate.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;5-Good-Turing-Estimation&#34;&gt;&lt;a href=&#34;#5-Good-Turing-Estimation&#34; class=&#34;headerlink&#34; title=&#34;(5) Good-Turing Estimation&#34;&gt;&lt;/a&gt;(5) Good-Turing Estimation&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;用出現一次的來預測沒出現過的&lt;/li&gt;
&lt;li&gt;若出現次數&amp;gt;k，不變，否則套用公式&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/goodturing.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;renormalize to sum = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simple Good-Turing&lt;ul&gt;
&lt;li&gt;replace any zeros in the sequence by linear regression: &lt;code&gt;log(Nc) = a+blog(c)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;after good-turing &lt;img data-src=&#34;/img/NLP/gttable.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;explaination from stanford NLP course   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;when use leave-one-out validation, the possibilities of unseen validation data is $\frac{N_1}{N}$(when thing-saw-once is the validation data), the possibilities of validation data have been seen K times is $\frac{(k+1)N_{k+1}}{N}$ &lt;/li&gt;
&lt;li&gt;Josh Goodman’s intuition: assume You are fishing, and caught 10 carp,3 perch,2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish&lt;ul&gt;
&lt;li&gt;P(unseen) = N1/N0 = N1/N = 3/18&lt;/li&gt;
&lt;li&gt;C(trout) = $2 \times N_2/N_1$ = $2 \times (1/3)$ = 2/3&lt;ul&gt;
&lt;li&gt;P(trout) = 2/3 / 18 = 1/27&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;for large k, often get zero estimate, so do not change the count&lt;ul&gt;
&lt;li&gt;C(the) = 200000, C(a) = 190000, $C*(the) = (200001)N_{200001} / N_{200000} = 0 (because N_{200001} = 0)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;6-Absolute-Discounting&#34;&gt;&lt;a href=&#34;#6-Absolute-Discounting&#34; class=&#34;headerlink&#34; title=&#34;(6) Absolute Discounting&#34;&gt;&lt;/a&gt;(6) Absolute Discounting&lt;/h4&gt;&lt;p&gt;從所有非零N-gram中拿出λ，平均分配給所有未出現過的N-gram  &lt;/p&gt;
&lt;h3 id=&#34;Combining-Estimator&#34;&gt;&lt;a href=&#34;#Combining-Estimator&#34; class=&#34;headerlink&#34; title=&#34;Combining Estimator&#34;&gt;&lt;/a&gt;Combining Estimator&lt;/h3&gt;&lt;p&gt;Combination Methods   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple Linear Interpolation(內插)(finite mixture models)&lt;ul&gt;
&lt;li&gt;Ex. trigram, bigram and unigram &lt;img data-src=&#34;/img/NLP/linearde.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;More generally, λ can be a function of (wn-2, wn-1, wn)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;use &lt;a href=&#34;#backward-forward&#34;&gt;Expectation-Maximization (EM) algorithm&lt;/a&gt; to get weights&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;General Linear Interpolation&lt;ul&gt;
&lt;li&gt;general form for a linear interpolation model&lt;/li&gt;
&lt;li&gt;weights are a function of the history &lt;img data-src=&#34;/img/NLP/gli.png&#34; alt=&#34;&#34;&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Katz’s Backing-Off&lt;ul&gt;
&lt;li&gt;choose proper order to train model (base on training data)&lt;ul&gt;
&lt;li&gt;If the n-gram appeared more than k times&lt;ul&gt;
&lt;li&gt;use MLE estimate and discount it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If the n-gram appeared k times or less&lt;ul&gt;
&lt;li&gt;use an estimate from &lt;strong&gt;lower-order n-gram&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;back-off probability &lt;img data-src=&#34;/img/NLP/pbo.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;$P_{Dis}(w_n|w_{n-2},w_{n-1})$ is specific discounted estimate. e.g., Good-Turing or Absolute Discounting &lt;/li&gt;
&lt;li&gt;unseen trigram is estimated by bigram and β &lt;img data-src=&#34;/img/NLP/bosmooth.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;β(wn-2, wn-1)&lt;/strong&gt; and &lt;strong&gt;α&lt;/strong&gt; are chosen so that sum of probabilities = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;more genereal form &lt;img data-src=&#34;/img/NLP/botable.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Most usual approach in large speech recognition: trigram language model, Good-Turing discounting, back-off combination&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;Chap06-Hidden-Markov-Models-HMM&#34;&gt;&lt;a href=&#34;#Chap06-Hidden-Markov-Models-HMM&#34; class=&#34;headerlink&#34; title=&#34;Chap06 Hidden Markov Models(HMM)&#34;&gt;&lt;/a&gt;Chap06 Hidden Markov Models(HMM)&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;statistical tools that are useful for NLP&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;part-of-speech-tagging&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt;We construct “Visible” Markov Models in training, but treat them as Hidden Markov Models when tagging new corpora  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;model a &lt;strong&gt;state sequence&lt;/strong&gt; (perhaps through time) &lt;strong&gt;of random variables&lt;/strong&gt; that have dependencies&lt;ul&gt;
&lt;li&gt;狀態(state)並不是直接可見的，但受狀態影響的某些變量(output symbol)則是可見的&lt;/li&gt;
&lt;li&gt;known value&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;output symbols(words)&lt;/strong&gt; 字詞&lt;/li&gt;
&lt;li&gt;probabilistic function of state relation 和state相關的機率函式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;unknown value&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;state(part-of-speech tags)&lt;/strong&gt; 目前的state，即POS tag&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;rely on 2 assumptions&lt;ul&gt;
&lt;li&gt;Let X=(X1, …, XT) be a sequence of random variables, X is markov chain if&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Limited Horizon&lt;ul&gt;
&lt;li&gt;a word’s tag only depends on &lt;strong&gt;previous&lt;/strong&gt; tag(state只受前一個state影響)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Time Invariant&lt;ul&gt;
&lt;li&gt;the dependency does not change over time(轉移矩陣不變)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Description   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initial state π, state = Q, Observations = O, transition matrix = A, output(observation) matrix = B  &lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/hmm1.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;$a_{ij}$ = probability of state $q_i$ transition to state $q_j$ &lt;/li&gt;
&lt;li&gt;$b_i(k)$ = probability of observe output symbol $O_k$ when state = $q_i$  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-problems-of-HMM&#34;&gt;&lt;a href=&#34;#3-problems-of-HMM&#34; class=&#34;headerlink&#34; title=&#34;3 problems of HMM&#34;&gt;&lt;/a&gt;3 problems of HMM&lt;/h3&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3d3dy41Mm5scC5jbi9obW0tbGVhcm4tYmVzdC1wcmFjdGljZXMtZm91ci1oaWRkZW4tbWFya292LW1vZGVscw==&#34;&gt;中文解說：隱馬可夫鏈&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;評估（Evaluation）：what is probability of the observation sequence given a model? (P(Observes|Model))&lt;ul&gt;
&lt;li&gt;Used in model improvement&lt;/li&gt;
&lt;li&gt;Used in classification&lt;ul&gt;
&lt;li&gt;Word spotting in speech recognition, language identification, speaker identification, author identification……&lt;/li&gt;
&lt;li&gt;Given an observation, compute P(O|model) for all models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Forward algorithm&lt;/strong&gt; to solve it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;解碼（Decoding）：Given an observation sequence and model, what is the &lt;strong&gt;most likely state sequence&lt;/strong&gt;? (P(States|Observes, Model)) 下一個state是什麼&lt;ul&gt;
&lt;li&gt;Used in tagging (tags=hidden states)&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Viterbi algorithm&lt;/strong&gt; to solve it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;學習（Learning）：Given an observation sequence, infer the best model parameters (argmax(Model) P(Model|Observes))&lt;ul&gt;
&lt;li&gt;「fill in model parameters that make the observation sequence most likely」&lt;/li&gt;
&lt;li&gt;Used for building HMM Model from data&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;EM(Baum-Welch, backward-forward algorithm)&lt;/strong&gt; to solve it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;Solutions-of-HMM-problem&#34;&gt;&lt;a href=&#34;#Solutions-of-HMM-problem&#34; class=&#34;headerlink&#34; title=&#34;Solutions of HMM problem&#34;&gt;&lt;/a&gt;Solutions of HMM problem&lt;/h3&gt;&lt;h4 id=&#34;Forward&#34;&gt;&lt;a href=&#34;#Forward&#34; class=&#34;headerlink&#34; title=&#34;Forward&#34;&gt;&lt;/a&gt;Forward&lt;/h4&gt;&lt;p&gt;&lt;a href=&#34;http://www.52nlp.cn/hmm-learn-best-practices-five-forward-algorithm-1&#34; target=&#34;_blank&#34; rel=&#34;noopener external nofollow noreferrer&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/fwformula.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;simply sum of the probability of each possible state sequence &lt;/li&gt;
&lt;li&gt;Direct evaluation&lt;ul&gt;
&lt;li&gt;time complexity = $(2T+1) \times N^{T+1}$ -&amp;gt; too big &lt;img data-src=&#34;/img/NLP/fw.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use dynamic programming&lt;ul&gt;
&lt;li&gt;record the probability of subpaths of the HMM&lt;/li&gt;
&lt;li&gt;The probability of longer subpaths can be calculated from shorter subpaths&lt;/li&gt;
&lt;li&gt;similar to Viterbi: viterbi use MAX() instead of SUM()&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Description:DP  
- ![dp](/img/NLP/dp.png)
- ![dp](/img/NLP/dptable.png)
    - 選最高機率的路徑(將其他路徑的機率加入最高機率) 
    - 例：p(qqqq) = 0.01, p(qrrq) = 0.007 → P(qqqq) = 0.017
--&gt;

&lt;p&gt;Forward Algorithm  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$α_t(i)$ = probability of state = qi at time = t &lt;img data-src=&#34;/img/NLP/forwardalgo.png&#34; alt=&#34;dp&#34;&gt;&lt;/li&gt;
&lt;li&gt;α的求法：將time = t-1 的 α 值，乘上在time = t時會在qi state的機率，並加總 &lt;img data-src=&#34;/img/NLP/forwardfex.png&#34; alt=&#34;dp&#34;&gt;&lt;/li&gt;
&lt;li&gt;順向推出所有可能的state sequence會產生此observation的機率和, 即為此model會產生此observation的機率 &lt;img data-src=&#34;/img/NLP/forwardexample.png&#34; alt=&#34;dp&#34;&gt;&lt;ul&gt;
&lt;li&gt;Σ P($O_1, O_2, O_3$ | possible state sequence) = P($O_1, O_2, O_3$ | Model)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/forwardpseudo.png&#34; alt=&#34;dp&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example:Urn(甕)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;genie has two urns filled with red and blue balls&lt;/li&gt;
&lt;li&gt;genie selects an urn and then draws a ball from it&lt;ul&gt;
&lt;li&gt;The urns are hidden&lt;/li&gt;
&lt;li&gt;The balls are observed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;After a lot of draws&lt;ul&gt;
&lt;li&gt;know the distribution of colors of balls in each urn(B matrix) &lt;/li&gt;
&lt;li&gt;know the genie’s preferences in draw from one urn or the next(A matrix)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;assume output (observation) is Blue Blue Red (BBR)&lt;ul&gt;
&lt;li&gt;Forward: P(BBR|model) = 0.0792 (SUM of all possible states’ probability) &lt;img data-src=&#34;/img/NLP/forward-urn.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Viterbi&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compute &lt;strong&gt;the most possible path&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$v_t(i)$ = &lt;strong&gt;most possible path probability&lt;/strong&gt; from time = 0 to time = t, and state = qi at time = t &lt;img data-src=&#34;/img/NLP/viterbi.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/viterbi-graph.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/viterbi-algo.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Viterbi in Urn example &lt;img data-src=&#34;/img/NLP/urn-cal.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;def viterbi(obs, states, start_p, trans_p, emit_p):
    V = [{}]
    path = {}

    # Initialize base cases (t == 0)
    for y in states:
        V[0][y] = start_p[y] * emit_p[y][obs[0]]
        path[y] = [y]

    # Run Viterbi for t &amp;gt; 0
    for t in range(1,len(obs)):
        V.append({})
        newpath = {}

        for y in states:
            (prob, state) = max([(V[t-1][y0] * trans_p[y0][y] * emit_p[y][obs[t]], y0) for y0 in states]) 
            # ↑ find the most possible state transitting to given state y at time=t
            V[t][y] = prob
            newpath[y] = path[state] + [y] 

        # newpath(at time t) can overwrite path(at time t-1) 
        path = newpath

    (prob, state) = max([(V[len(obs) - 1][y], y) for y in states])
    return (prob, path[state])&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;Backward&#34;&gt;&lt;a href=&#34;#Backward&#34; class=&#34;headerlink&#34; title=&#34;Backward&#34;&gt;&lt;/a&gt;Backward&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Useful for parameter estimation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Description  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Backward variables β, which are the total probability of seeing the rest of the observation sequence($O_t to O_T$) given state qi at time t &lt;img data-src=&#34;/img/NLP/bw-procedure.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/bw-f.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;初始化β：令t=T時刻所有狀態的β為1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;由後往前計算 &lt;img data-src=&#34;/img/NLP/bw-graph.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;如果要計算某observation的概率，只需將t=1的後向變量相加&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;Backward-Forward&#34;&gt;&lt;a href=&#34;#Backward-Forward&#34; class=&#34;headerlink&#34; title=&#34;Backward-Forward&#34;&gt;&lt;/a&gt;Backward-Forward&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;We can locally maximize model parameter λ, by an iterative hill-climbing known as Baum-Welch algorithm(=Forward-Backward) (by EM Algorithm structure)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Forward-Backward Algorithm    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;find which** state transitions(A matrix)** and &lt;strong&gt;symbol observaions(B matrix)&lt;/strong&gt; were &lt;strong&gt;probably used the most&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;By &lt;strong&gt;increasing the probability of those&lt;/strong&gt;, we can get a better model which gives a higher probability to the observation sequence&lt;/li&gt;
&lt;li&gt;transition probabilities and path probabilities are both require each other to calculate&lt;ul&gt;
&lt;li&gt;use A matrix to calculate path probabilities&lt;/li&gt;
&lt;li&gt;need path probabilities to update A matrix&lt;/li&gt;
&lt;li&gt;use EM algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;EM algorithm (Expectation-Maximization)    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;迭代算法，它的最大優點是簡單和穩定，但容易陷入局部最優&lt;/li&gt;
&lt;li&gt;(隨機)選擇參數λ0，找出在λ0下最可能的狀態，計算每個訓練樣本的可能結果的概率，再&lt;strong&gt;重新估計新的參數λ&lt;/strong&gt;。經過多次的迭代，直至某個收斂條件滿足為止&lt;/li&gt;
&lt;li&gt;Urn Example&lt;ul&gt;
&lt;li&gt;update transition matrix A ($a_{12}, a_{11}$ … ) &lt;img data-src=&#34;/img/NLP/newtrans.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;P(1→2) = 0.0414 &lt;img data-src=&#34;/img/NLP/1-2.png&#34; alt=&#34;1→2&#34;&gt;&lt;/li&gt;
&lt;li&gt;P(1→1) = 0.0537 &lt;img data-src=&#34;/img/NLP/1-1.png&#34; alt=&#34;1→1&#34;&gt;&lt;/li&gt;
&lt;li&gt;normalize: P(1→2)+P(1→1) = 1, P(1→2) = 0.435 …&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;若state數目多的時候，計算量過大…&lt;ul&gt;
&lt;li&gt;用backward, forward&lt;/li&gt;
&lt;li&gt;前面用forward, 後面用backward &lt;img data-src=&#34;/img/NLP/bf-graph.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Combine forward and backward  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let ξt be the probability of being in state i at time t and state j at time t+1, &lt;strong&gt;given observation and model λ&lt;/strong&gt;&lt;img data-src=&#34;/img/NLP/kesin.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;use not-quite-ξ to get ξ &lt;img data-src=&#34;/img/NLP/nqkesin.png&#34; alt=&#34;&#34;&gt; because &lt;img data-src=&#34;/img/NLP/kesin-formula.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;P(O|λ) → problem1 of HMM 的答案 → 用forward解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;見上方backward, forward同時使用之圖 &lt;img data-src=&#34;/img/NLP/nqkesin-formula.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;ξ可用來計算transition matrix &lt;img data-src=&#34;/img/NLP/newtrans-final.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Summary of Forward-Backward &lt;img data-src=&#34;/img/NLP/fb-algo.png&#34; alt=&#34;&#34;&gt; &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize λ=(A,B)&lt;/li&gt;
&lt;li&gt;Compute α, β, ξ using observations&lt;/li&gt;
&lt;li&gt;Estimate new λ’=(A,B)&lt;/li&gt;
&lt;li&gt;Replace λ with λ’&lt;/li&gt;
&lt;li&gt;If not converged go to 2&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;Chap07-Part-of-Speech-Tagging&#34;&gt;&lt;a href=&#34;#Chap07-Part-of-Speech-Tagging&#34; class=&#34;headerlink&#34; title=&#34;Chap07 Part-of-Speech Tagging&#34;&gt;&lt;/a&gt;Chap07 Part-of-Speech Tagging&lt;/h2&gt;&lt;p&gt;alias: &lt;strong&gt;parts-of-speech&lt;/strong&gt;, &lt;strong&gt;lexical categories&lt;/strong&gt;, &lt;strong&gt;word classes&lt;/strong&gt;, &lt;strong&gt;morphological classes&lt;/strong&gt;, &lt;strong&gt;lexical tags&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Noun, verb, adjective, preposition, adverb, article, interjection, pronoun, conjunction&lt;/li&gt;
&lt;li&gt;preposition(P)&lt;ul&gt;
&lt;li&gt;of, by, to&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;pronoun(PRO)&lt;ul&gt;
&lt;li&gt;I, me, mine&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;determiner(DET)&lt;ul&gt;
&lt;li&gt;the, a, that, those&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Usage  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Speech synthesis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tag before parsing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Information extraction&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finding names, relations, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Machine Translation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Closed class&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the class that is hard to add new words&lt;/li&gt;
&lt;li&gt;Usually function words (short common words which play a role in grammar)&lt;ul&gt;
&lt;li&gt;prepositions: on, under, over,…&lt;/li&gt;
&lt;li&gt;particles: up, down, on, off, …&lt;/li&gt;
&lt;li&gt;determiners: a, an, the, …&lt;/li&gt;
&lt;li&gt;pronouns: she, who, I, …&lt;/li&gt;
&lt;li&gt;conjunctions: and, but, or, …&lt;/li&gt;
&lt;li&gt;auxiliary verbs: can, may should, …&lt;/li&gt;
&lt;li&gt;numerals: one, two, three, third, …&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open class&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;new ones can be created all the time&lt;/li&gt;
&lt;li&gt;For English: Nouns, Verbs, Adjectives, Adverbs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Choosing Tagset: Ex. “Penn TreeBank tagset”, 45 tag&lt;br&gt;&lt;img data-src=&#34;/img/NLP/tagset.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Methods for POS Tagging  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Rule-based tagging&lt;ul&gt;
&lt;li&gt;ENGTWOL: ENGlish TWO Level analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stochastic: Probabilistic sequence models&lt;ul&gt;
&lt;li&gt;HMM (Hidden Markov Model)&lt;/li&gt;
&lt;li&gt;MEMMs (Maximum Entropy Markov Models)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Transformation-Based Tagger (Brill)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;Rule-Based-Tagging&#34;&gt;&lt;a href=&#34;#Rule-Based-Tagging&#34; class=&#34;headerlink&#34; title=&#34;Rule-Based Tagging&#34;&gt;&lt;/a&gt;Rule-Based Tagging&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Assign all possible tags to each word&lt;/li&gt;
&lt;li&gt;Remove tags according to set of rules&lt;ol&gt;
&lt;li&gt;Typically more than 1000 hand-written rules&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;Hidden-Markov-Model-tagging&#34;&gt;&lt;a href=&#34;#Hidden-Markov-Model-tagging&#34; class=&#34;headerlink&#34; title=&#34;Hidden Markov Model tagging&#34;&gt;&lt;/a&gt;Hidden Markov Model tagging&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;special case of Bayesian inference&lt;ul&gt;
&lt;li&gt;Foundational work in computational linguistics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;related to the “noisy channel” model that’s the basis for ASR, OCR and MT&lt;/li&gt;
&lt;li&gt;Decoding view  &lt;ul&gt;
&lt;li&gt;Consider all possible sequences of tags&lt;/li&gt;
&lt;li&gt;choose the tag sequence which is most possible given the observation sequence of n words w1…wn&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Generative view&lt;ul&gt;
&lt;li&gt;This sequence of words must have resulted from some hidden process&lt;/li&gt;
&lt;li&gt;A sequence of tags (states), each of which emitted a word&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$t^n_1$(t hat), which is the most possible tag &lt;img data-src=&#34;/img/NLP/best-t.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;use viterbi to get tag &lt;img data-src=&#34;/img/NLP/viterbi-ex.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Evaluation  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overall error rate with respect to a gold-standard test set&lt;/li&gt;
&lt;li&gt;Error rates on particular tags/words&lt;/li&gt;
&lt;li&gt;Tag confusions, Unknown words…&lt;/li&gt;
&lt;li&gt;Typically accuracy reaches 96~97%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unknown Words&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simplest model&lt;ul&gt;
&lt;li&gt;Unknown words can be of any part of speech, or only in any open class&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Morphological and other cues&lt;ul&gt;
&lt;li&gt;~ed: past tense forms or past participles&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Maximum-entropy-Markov-model-MEMM&#34;&gt;&lt;a href=&#34;#Maximum-entropy-Markov-model-MEMM&#34; class=&#34;headerlink&#34; title=&#34;Maximum entropy Markov model (MEMM)&#34;&gt;&lt;/a&gt;Maximum entropy Markov model (MEMM)&lt;/h3&gt;&lt;p&gt;Maximum Entropy Model  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MaxEnt: multinomial(多項式) logistic regression&lt;/li&gt;
&lt;li&gt;Used for sequence classification/sequence labeling&lt;/li&gt;
&lt;li&gt;Maximum entropy Markov model (MEMM)&lt;ul&gt;
&lt;li&gt;a common MaxEnt classifier&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Classification
- Task
    - observation, Extract useful features, Classify the observation based on these features
- Probabilistic classifier
    - Given an observation, it gives a probability distribution over all classes
- Non-sequential(連續的) Applications
    - Text classification
    - Sentiment analysis
    - Sentence boundary detection
--&gt;


&lt;p&gt;Exponential(log-linear) classifiers &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Combine features linearly&lt;/li&gt;
&lt;li&gt;Use the sum as an exponent &lt;img data-src=&#34;/img/NLP/maxent.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Example &lt;img data-src=&#34;/img/NLP/maxent-ex.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Maximum Entropy Markov Model       &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MaxEnt model&lt;ul&gt;
&lt;li&gt;classifies &lt;strong&gt;a&lt;/strong&gt; observation into &lt;strong&gt;one&lt;/strong&gt; of discrete classes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MEMM&lt;ul&gt;
&lt;li&gt;augmentation(增加) of the basic MaxEnt classifier&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;assign a class to each element in a sequence&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;POS tagging from MaxExt to MEMM   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;include some source of knowledge into the tagging process&lt;/li&gt;
&lt;li&gt;The simplest approach&lt;ul&gt;
&lt;li&gt;run the local classifier and &lt;strong&gt;feature is classifier from the previous word&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Flaw&lt;ul&gt;
&lt;li&gt;It makes a hard decision on each word before moving on the next word&lt;/li&gt;
&lt;li&gt;cannot use information from the later words&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;discriminative model&lt;/strong&gt;(判別模型)   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute the posterior P(Tag|Word) directly to decide tag &lt;img data-src=&#34;/img/NLP/memm.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;求解條件機率分佈 P(y|x) 預測 y → 求P(tag|word)來取得tag &lt;/li&gt;
&lt;li&gt;不考慮聯合機率分佈 P(x, y)&lt;/li&gt;
&lt;li&gt;對於諸如分類和回歸問題，由於不考慮聯合機率分佈，採用判別模型可以取得更好的效果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;HMM and MEMM(順推和逆推的差別) &lt;img data-src=&#34;/img/NLP/hmmandmemm.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlike HMM, MEMM can condition on any &lt;strong&gt;useful feature of observation&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;HMM: state is the fiven value&lt;/li&gt;
&lt;li&gt;MEMM: observation is the given value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;viterbi function for MEMM &lt;img data-src=&#34;/img/NLP/viterbi-new.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/memm-ex.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Transformation-Based-Learning-of-Tags&#34;&gt;&lt;a href=&#34;#Transformation-Based-Learning-of-Tags&#34; class=&#34;headerlink&#34; title=&#34;Transformation-Based Learning of Tags&#34;&gt;&lt;/a&gt;Transformation-Based Learning of Tags&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Tag each word with its most frequent tag&lt;/li&gt;
&lt;li&gt;Construct a list of transformations that &lt;strong&gt;improve the initial tag&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;trigger environment: at the limited number of words before/after &lt;img data-src=&#34;/img/NLP/transformed-learn.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/NLP/transformed-algo.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Trigger by tags &lt;/li&gt;
&lt;li&gt;Trigger by word&lt;/li&gt;
&lt;li&gt;Trigger by morphology(詞法學)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&amp;lt;! – ====================分水嶺：尚未分類========================== –&amp;gt;&lt;/p&gt;
&lt;h3 id=&#34;Zipf’s-Law-long-tail-phenomenon&#34;&gt;&lt;a href=&#34;#Zipf’s-Law-long-tail-phenomenon&#34; class=&#34;headerlink&#34; title=&#34;Zipf’s Law (long tail phenomenon)&#34;&gt;&lt;/a&gt;Zipf’s Law (long tail phenomenon)&lt;/h3&gt;&lt;p&gt;a word’s frequency is approximately inversely proportional to its rank in the word distribution list&lt;br&gt;單詞出現的頻率與它在頻率表裡的排名成反比:&lt;br&gt;頻率最高的單詞出現的頻率大約是出現頻率第二位的單詞的2倍&lt;/p&gt;
&lt;h4 id=&#34;Jelinek-Mercer-Smoothing&#34;&gt;&lt;a href=&#34;#Jelinek-Mercer-Smoothing&#34; class=&#34;headerlink&#34; title=&#34;Jelinek-Mercer Smoothing&#34;&gt;&lt;/a&gt;Jelinek-Mercer Smoothing&lt;/h4&gt;&lt;p&gt;interpolate(插值) between bigram and unigram&lt;br&gt;because if p(eat the) = 0 and p(eat thou) = 0&lt;br&gt;it still must consider that  p(eat the) &amp;gt; p(eat thou)&lt;br&gt;because p(the) &amp;gt; p(thou)&lt;br&gt;so p(eat the) = N * p(the | eat) + (1-N) * p(the | thou) &lt;/p&gt;
&lt;h2 id=&#34;Language-Model-Applications&#34;&gt;&lt;a href=&#34;#Language-Model-Applications&#34; class=&#34;headerlink&#34; title=&#34;Language Model: Applications&#34;&gt;&lt;/a&gt;Language Model: Applications&lt;/h2&gt;&lt;h3 id=&#34;Query-Likelihood-Model&#34;&gt;&lt;a href=&#34;#Query-Likelihood-Model&#34; class=&#34;headerlink&#34; title=&#34;Query Likelihood Model&#34;&gt;&lt;/a&gt;Query Likelihood Model&lt;/h3&gt;&lt;p&gt;given a query 𝑞, rank the probability 𝑝(𝑑|q)&lt;br&gt;&lt;img data-src=&#34;/img/NLP/cfKf6I3.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;So the following arguments are equivalent:&lt;br&gt;1.𝑝𝑑𝑞: find the document 𝑑 that is most likely to be relevant to 𝑞&lt;br&gt;2.𝑝𝑞𝑑: find the document 𝑑 that is most likely to generate the query 𝑞&lt;/p&gt;
&lt;p&gt;Typically, unigram LMs are used in IR(information retrieval)&lt;br&gt;Retrieval does not depend that much on sentence structure&lt;/p&gt;
&lt;h3 id=&#34;Dependence-Language-Model&#34;&gt;&lt;a href=&#34;#Dependence-Language-Model&#34; class=&#34;headerlink&#34; title=&#34;Dependence Language Model&#34;&gt;&lt;/a&gt;Dependence Language Model&lt;/h3&gt;&lt;p&gt;Relax the independence assumption of unigram LMs&lt;br&gt;Do not assume that the dependency only exist between &lt;strong&gt;adjacent&lt;/strong&gt; words&lt;br&gt;Introduce a hidden variable: “linkage” 𝐿&lt;br&gt;Ex.&lt;br&gt;&lt;img data-src=&#34;/img/NLP/Z8ftSRP.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;skipped….&lt;/p&gt;
&lt;h3 id=&#34;Proximity-Language-Model&#34;&gt;&lt;a href=&#34;#Proximity-Language-Model&#34; class=&#34;headerlink&#34; title=&#34;Proximity Language Model&#34;&gt;&lt;/a&gt;Proximity Language Model&lt;/h3&gt;&lt;p&gt;Proximity: how close the query terms appear in a document&lt;br&gt;the closer they are, the more likely they are describing the same topic or concept&lt;/p&gt;
&lt;h3 id=&#34;Positional-Language-Model&#34;&gt;&lt;a href=&#34;#Positional-Language-Model&#34; class=&#34;headerlink&#34; title=&#34;Positional Language Model&#34;&gt;&lt;/a&gt;Positional Language Model&lt;/h3&gt;&lt;p&gt;Position: define a LM for each position of a document, instead of the entire document&lt;br&gt;Words closer to a position will contribute more to the language model of this position&lt;/p&gt;
&lt;h3 id=&#34;Speech-Recognition&#34;&gt;&lt;a href=&#34;#Speech-Recognition&#34; class=&#34;headerlink&#34; title=&#34;Speech Recognition&#34;&gt;&lt;/a&gt;Speech Recognition&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;The “origin” of language models&lt;/li&gt;
&lt;li&gt;used to restrict the search space of possible word sequences&lt;/li&gt;
&lt;li&gt;requires higher order models: knowing previous acoustic is important!&lt;/li&gt;
&lt;li&gt;Speed is important!&lt;/li&gt;
&lt;li&gt;N-gram LM with modified Kneser-Ney smoothing is extensively used&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Machine-Translation-MT&#34;&gt;&lt;a href=&#34;#Machine-Translation-MT&#34; class=&#34;headerlink&#34; title=&#34;Machine Translation (MT)&#34;&gt;&lt;/a&gt;Machine Translation (MT)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Decoding: given the probability model(s), find the best translation&lt;/li&gt;
&lt;li&gt;Similar role as in speech recognition: &lt;strong&gt;eliminate unlikely word sequences&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Higher order Kneser-Ney smoothed n-gram LM is widely used&lt;/li&gt;
&lt;li&gt;NNLM-style models tend to outperform standard back-off LMs&lt;/li&gt;
&lt;li&gt;Also significantly speeded up in (Delvin et al, 2014)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;參考資料&#34;&gt;&lt;a href=&#34;#參考資料&#34; class=&#34;headerlink&#34; title=&#34;參考資料&#34;&gt;&lt;/a&gt;參考資料&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;HHChen 課堂講義&lt;/li&gt;
&lt;li&gt;SDLin 講義&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9jbGFzcy5jb3Vyc2VyYS5vcmcvbmxwLw==&#34;&gt;Stanford NLP course&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;www.52nlp.cn&#34;&gt;52nlp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
 ]]></description>
        </item>
    </channel>
</rss>
