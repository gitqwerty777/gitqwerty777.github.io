<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>QWERTY • Posts by &#34;perceptron&#34; tag</title>
        <link>http://gitqwerty777.github.io</link>
        <description>Programming | Computer Science | Thought</description>
        <language>zh-TW</language>
        <pubDate>Tue, 16 Sep 2014 19:41:48 +0800</pubDate>
        <lastBuildDate>Tue, 16 Sep 2014 19:41:48 +0800</lastBuildDate>
        <category>C#</category>
        <category>CodingStyle</category>
        <category>Emacs</category>
        <category>編輯器</category>
        <category>CFR</category>
        <category>電腦對局理論</category>
        <category>指令</category>
        <category>機器學習</category>
        <category>perceptron</category>
        <category>readme</category>
        <category>文件</category>
        <category>github</category>
        <category>artificial intelligence</category>
        <category>search</category>
        <category>First-Order Logic</category>
        <category>大數</category>
        <category>程式</category>
        <category>C++</category>
        <category>Hexo</category>
        <category>網誌</category>
        <category>Markdown</category>
        <category>CleanCode</category>
        <category>重構</category>
        <category>TDD</category>
        <category>設計模式</category>
        <category>CMake</category>
        <category>Makefile</category>
        <category>Linux</category>
        <category>Todo</category>
        <category>註解</category>
        <category>經濟學</category>
        <category>策略</category>
        <category>競爭</category>
        <category>博弈論</category>
        <category>計算機結構</category>
        <category>人工智慧</category>
        <category>圍棋</category>
        <category>象棋</category>
        <category>蒙地卡羅</category>
        <category>Alpha-Beta搜尋</category>
        <category>強化學習</category>
        <category>計算機網路</category>
        <category>boost</category>
        <category>函式庫</category>
        <category>編譯</category>
        <category>gcc</category>
        <category>g++</category>
        <category>clang</category>
        <category>最佳化</category>
        <category>推薦系統</category>
        <category>FM</category>
        <category>FFM</category>
        <category>SVM</category>
        <category>Embedding</category>
        <category>自然語言處理</category>
        <category>外國用語</category>
        <category>萌典</category>
        <category>opencc</category>
        <category>PTT</category>
        <category>vuejs</category>
        <category>linux</category>
        <category>c</category>
        <category>compile</category>
        <category>gdb</category>
        <category>c語言</category>
        <category>cpp</category>
        <category>除錯</category>
        <category>git</category>
        <category>VMWare</category>
        <category>虛擬機</category>
        <category>IFTTT</category>
        <category>自動化</category>
        <category>備份</category>
        <category>webhook</category>
        <category>簡報</category>
        <category>軟體</category>
        <category>PowerPoint</category>
        <category>Latex</category>
        <category>JavaScript</category>
        <category>CSS</category>
        <category>Unity</category>
        <category>fcitx</category>
        <category>嘸蝦米</category>
        <category>輸入法</category>
        <category>硬碟</category>
        <category>記憶體</category>
        <category>效能</category>
        <category>錯誤</category>
        <category>makefile</category>
        <category>備忘錄</category>
        <category>存檔</category>
        <category>統計</category>
        <category>byobu</category>
        <category>screen</category>
        <category>tmux</category>
        <category>reactjs</category>
        <category>javascript</category>
        <category>WideAndDeep</category>
        <category>Google</category>
        <category>觀察者</category>
        <category>訂閱</category>
        <category>委託</category>
        <category>正規表示式(RegExp)</category>
        <category>上下文無關文法(CFG)</category>
        <category>hexo</category>
        <category>blog</category>
        <category>theme</category>
        <category>feature</category>
        <category>revealJS</category>
        <category>markdown</category>
        <category>rss</category>
        <category>facebook</category>
        <category>youtube</category>
        <category>ptt</category>
        <category>bilibili</category>
        <category>pixiv</category>
        <category>crawler</category>
        <category>SEO</category>
        <category>google</category>
        <category>html</category>
        <category>amazon</category>
        <category>webhost</category>
        <category>ssl</category>
        <category>漢字</category>
        <category>中文</category>
        <category>異體字</category>
        <category>unicode</category>
        <category>unity</category>
        <category>演算法</category>
        <category>隨機排序</category>
        <category>洗牌</category>
        <category>Fisher-Yates</category>
        <category>證明</category>
        <category>python</category>
        <item>
            <guid isPermalink="true">http://gitqwerty777.github.io/MLfoundation1/</guid>
            <title>機器學習基石(上)</title>
            <link>http://gitqwerty777.github.io/MLfoundation1/</link>
            <category>機器學習</category>
            <category>perceptron</category>
            <pubDate>Tue, 16 Sep 2014 19:41:48 +0800</pubDate>
            <description><![CDATA[ &lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2NvdXJzZS9udHVtbG9uZQ==&#34;&gt;原版&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;的講義做得十分精美，可以很快了解&lt;/p&gt;
&lt;h2 id=&#34;Chap01-Introduction&#34;&gt;&lt;a href=&#34;#Chap01-Introduction&#34; class=&#34;headerlink&#34; title=&#34;Chap01 Introduction&#34;&gt;&lt;/a&gt;Chap01 Introduction&lt;/h2&gt;&lt;p&gt;課堂討論：學習的定義    &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;從不會到會 &lt;/li&gt;
&lt;li&gt;從會到更進步、熟練&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/ML/0FPIeqh.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;課堂討論：學習的方法    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;以「樹的定義」為例  &lt;/li&gt;
&lt;li&gt;如何寫出「能判斷是否是樹」的程式？ &lt;ol&gt;
&lt;li&gt;define trees and hand-program: difficult&lt;/li&gt;
&lt;li&gt;learn from data by observation and recognize: more easier(機器「自己」學習)&lt;a id=&#34;more&#34;&gt;&lt;/a&gt;
&lt;img data-src=&#34;/img/ML/BuqSVKs.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;課堂討論：兩種學習方法  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;電腦: learn from data -&amp;gt; get knowledge by observing  &lt;/li&gt;
&lt;li&gt;人腦: learn from teachers -&amp;gt; get the essence of the knowledge(can computer do that?)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-eassence-of-ML&#34;&gt;&lt;a href=&#34;#key-eassence-of-ML&#34; class=&#34;headerlink&#34; title=&#34;key eassence of ML&#34;&gt;&lt;/a&gt;key eassence of ML&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;存在「&lt;strong&gt;潛藏模式&lt;/strong&gt;」可以學習&lt;ul&gt;
&lt;li&gt;若認為有「潛藏模式」，才需要學習  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;無法簡單定義&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;有可提供學習的&lt;strong&gt;資料&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;ML使用時機&#34;&gt;&lt;a href=&#34;#ML使用時機&#34; class=&#34;headerlink&#34; title=&#34;ML使用時機&#34;&gt;&lt;/a&gt;ML使用時機&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;人類無法操作&lt;ul&gt;
&lt;li&gt;火星探索&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;難以定義的問題&lt;ul&gt;
&lt;li&gt;視覺/聽覺辨識  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;需要快速判斷&lt;ul&gt;
&lt;li&gt;股票炒短線程式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;大量資料&lt;ul&gt;
&lt;li&gt;個人化使用者體驗&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ML應用&#34;&gt;&lt;a href=&#34;#ML應用&#34; class=&#34;headerlink&#34; title=&#34;ML應用&#34;&gt;&lt;/a&gt;ML應用&lt;/h3&gt;&lt;p&gt;推薦系統&lt;br&gt;將物品分解成各個porperty factors，形成vector，並與自己的喜好vector比較  &lt;/p&gt;
&lt;h3 id=&#34;formalize-the-learning-problem&#34;&gt;&lt;a href=&#34;#formalize-the-learning-problem&#34; class=&#34;headerlink&#34; title=&#34;formalize the learning problem&#34;&gt;&lt;/a&gt;formalize the learning problem&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;target funcion &lt;code&gt;f&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;unknown pattern to be learned   &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;data &lt;code&gt;D&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;training examples&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hypothesis set &lt;code&gt;h&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;candidate functions to be choosed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hypothesis &lt;code&gt;g&lt;/code&gt; &lt;ul&gt;
&lt;li&gt;best candidate function which is learned from data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;use algorithm(A) with data(D) and hypothesis set(H) to get g &lt;img data-src=&#34;/img/ML/c5XEqoy.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Machine Learning:&lt;br&gt;&lt;br&gt;use data to compute hypothesis &lt;code&gt;g&lt;/code&gt; that approximates target &lt;code&gt;f&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;Differences&#34;&gt;&lt;a href=&#34;#Differences&#34; class=&#34;headerlink&#34; title=&#34;Differences&#34;&gt;&lt;/a&gt;Differences&lt;/h3&gt;&lt;h4 id=&#34;Machine-Learning-amp-Data-Mining&#34;&gt;&lt;a href=&#34;#Machine-Learning-amp-Data-Mining&#34; class=&#34;headerlink&#34; title=&#34;Machine Learning &amp;amp; Data Mining&#34;&gt;&lt;/a&gt;Machine Learning &amp;amp; Data Mining&lt;/h4&gt;&lt;p&gt;ML: the same as above&lt;br&gt;DM: use &lt;strong&gt;huge&lt;/strong&gt; data to &lt;strong&gt;find property&lt;/strong&gt; that is interesting&lt;/p&gt;
&lt;h4 id=&#34;Machine-Learning-amp-Artificial-Intelligence&#34;&gt;&lt;a href=&#34;#Machine-Learning-amp-Artificial-Intelligence&#34; class=&#34;headerlink&#34; title=&#34;Machine Learning &amp;amp; Artificial Intelligence&#34;&gt;&lt;/a&gt;Machine Learning &amp;amp; Artificial Intelligence&lt;/h4&gt;&lt;p&gt;AI -&amp;gt; compute something that shows intelligent behavior&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ML can realize AI&lt;/strong&gt;&lt;br&gt;traditional AI -&amp;gt; game tree&lt;br&gt;ML -&amp;gt; learning (techiniques) from board data&lt;/p&gt;
&lt;h4 id=&#34;Machine-Learning-amp-Statistics&#34;&gt;&lt;a href=&#34;#Machine-Learning-amp-Statistics&#34; class=&#34;headerlink&#34; title=&#34;Machine Learning &amp;amp; Statistics&#34;&gt;&lt;/a&gt;Machine Learning &amp;amp; Statistics&lt;/h4&gt;&lt;p&gt;Statistics: use data to make inference about an unknown process&lt;br&gt;-&amp;gt; many &lt;strong&gt;useful tools for ML&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;課堂討論：Big Data     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As data getting bigger, the way to deal with data has to be changed.(such as distributed computation)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;not&lt;/strong&gt; a new topic&lt;/li&gt;
&lt;li&gt;marketing buzz word&lt;br&gt;課堂討論：Maching Learning &amp;amp; Neural Network  &lt;/li&gt;
&lt;li&gt;A technique used in early AI and ML&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap-02-Perceptron-感知器&#34;&gt;&lt;a href=&#34;#Chap-02-Perceptron-感知器&#34; class=&#34;headerlink&#34; title=&#34;Chap 02 Perceptron(感知器)&#34;&gt;&lt;/a&gt;Chap 02 Perceptron(感知器)&lt;/h2&gt;&lt;h3 id=&#34;yes-no-question-by-grading&#34;&gt;&lt;a href=&#34;#yes-no-question-by-grading&#34; class=&#34;headerlink&#34; title=&#34;yes/no question by grading&#34;&gt;&lt;/a&gt;yes/no question by grading&lt;/h3&gt;&lt;p&gt;用feature(特質)來分隔兩種不同的結果    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;x: input&lt;/li&gt;
&lt;li&gt;w: hypothesis&lt;/li&gt;
&lt;li&gt;x是在d維度空間的點(d個features)，w為分隔此空間的線(平面)的法向量 &lt;img data-src=&#34;/img/ML/pla-w.png&#34; alt=&#34;&#34;&gt; &lt;/li&gt;
&lt;li&gt;以二維空間為例：w產生的線分隔兩邊 &lt;img data-src=&#34;/img/ML/MOzf2UK.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;也就是h(x)的正負，w所在的那一側為正 &lt;img data-src=&#34;/img/ML/joxwtUt.png&#34; alt=&#34;&#34;&gt;   &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;select-g-from-h&#34;&gt;&lt;a href=&#34;#select-g-from-h&#34; class=&#34;headerlink&#34; title=&#34;select g from h&#34;&gt;&lt;/a&gt;select g from h&lt;/h3&gt;&lt;p&gt;Difficult: h is infinite&lt;br&gt;Idea: 從某一條線開始，進行更改(local search)&lt;/p&gt;
&lt;h3 id=&#34;Perception-Learning-Algorithm-PLA&#34;&gt;&lt;a href=&#34;#Perception-Learning-Algorithm-PLA&#34; class=&#34;headerlink&#34; title=&#34;Perception Learning Algorithm(PLA)&#34;&gt;&lt;/a&gt;Perception Learning Algorithm(PLA)&lt;/h3&gt;&lt;p&gt;A fault confessed is half redressed(知錯能改)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;find a mistake(which sign is wrong) &lt;img data-src=&#34;/img/ML/u0KFPyS.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;correct the mistake &lt;img data-src=&#34;/img/ML/Mow3SlT.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;if real ans = +, new w = w + x(使w靠近正的點)&lt;/li&gt;
&lt;li&gt;if real ans = -, new w = w - x(使w遠離負的點) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;keep doing until no mistake &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;question&lt;br&gt;同乘$y_nx_n$ &lt;img data-src=&#34;/img/ML/KKHE36Z.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;可看出錯誤變少：正確的時候，$w_nx_n$和$y_n$同號，所以$w_nx_ny_n$是正的    &lt;/p&gt;
&lt;h3 id=&#34;linear-seperability&#34;&gt;&lt;a href=&#34;#linear-seperability&#34; class=&#34;headerlink&#34; title=&#34;linear seperability&#34;&gt;&lt;/a&gt;linear seperability&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/ML/5L1kwEZ.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;linear seperable&lt;ul&gt;
&lt;li&gt;exist perfect w makes $sign(y) = sign(w_nx_n)$, n = 0~N&lt;/li&gt;
&lt;li&gt;用直線(平面)必可分成無錯誤的兩塊  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;if Data is linear seperable, then PLA can generate w to make no mistake &lt;/li&gt;
&lt;li&gt;每次改動使$w_f$(正解)和$w_t$的內積變大，也就是愈來愈接近 &lt;img data-src=&#34;/img/ML/unBVfjt.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;但成長速度有限 &lt;img data-src=&#34;/img/ML/LHtRvcu.png&#34; alt=&#34;&#34;&gt;    &lt;ul&gt;
&lt;li&gt;$|W_t| &amp;lt;= sqrt(t) max(X_n)$&lt;/li&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/ML/J66FCPC.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;question&lt;br&gt;&lt;img data-src=&#34;/img/ML/0szpVwP.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;PLA-Guarantee&#34;&gt;&lt;a href=&#34;#PLA-Guarantee&#34; class=&#34;headerlink&#34; title=&#34;PLA Guarantee&#34;&gt;&lt;/a&gt;PLA Guarantee&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/ML/9qQxERz.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;advantage&lt;ul&gt;
&lt;li&gt;simple to implement&lt;/li&gt;
&lt;li&gt;fast&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;disadvantage&lt;ul&gt;
&lt;li&gt;not fully sure how long it will take&lt;/li&gt;
&lt;li&gt;assume linear seperable&lt;ul&gt;
&lt;li&gt;What if no linear seperate?(in reality)&lt;/li&gt;
&lt;li&gt;選出犯錯最少的&lt;/li&gt;
&lt;li&gt;這是個NP-HARD問題… &lt;img data-src=&#34;/img/ML/oRWuGAO.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Pocket-Algorithm-a-little-modified-by-PLA&#34;&gt;&lt;a href=&#34;#Pocket-Algorithm-a-little-modified-by-PLA&#34; class=&#34;headerlink&#34; title=&#34;Pocket Algorithm(a little modified by PLA)&#34;&gt;&lt;/a&gt;Pocket Algorithm(a little modified by PLA)&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/ML/XkWjmux.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;greedy &lt;ul&gt;
&lt;li&gt;may not be the best answer: 可能是局部最佳解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;slower than PLA(need to compare Wt+1 and Wt)  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Chap03-types-of-learning&#34;&gt;&lt;a href=&#34;#Chap03-types-of-learning&#34; class=&#34;headerlink&#34; title=&#34;Chap03 types of learning&#34;&gt;&lt;/a&gt;Chap03 types of learning&lt;/h2&gt;&lt;h3 id=&#34;Different-Output-Space&#34;&gt;&lt;a href=&#34;#Different-Output-Space&#34; class=&#34;headerlink&#34; title=&#34;Different Output Space&#34;&gt;&lt;/a&gt;Different Output Space&lt;/h3&gt;&lt;p&gt;Binary Classification  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;yes/no&lt;/li&gt;
&lt;li&gt;core problem to build tools&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Multiclass Classification(N output class)    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression(迴歸分析)&lt;ul&gt;
&lt;li&gt;output 為一數字&lt;/li&gt;
&lt;li&gt;Ex. temperature, stock price&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;core problem to build statistic tools&lt;/strong&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Structured Learning&lt;ul&gt;
&lt;li&gt;output $y$ = structures with &lt;strong&gt;implicit class definition&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;too many class → structure&lt;/li&gt;
&lt;li&gt;Ex. Speech parse tree, sequence tagging(標詞性), protein folding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Different-Data-Label&#34;&gt;&lt;a href=&#34;#Different-Data-Label&#34; class=&#34;headerlink&#34; title=&#34;Different Data Label&#34;&gt;&lt;/a&gt;Different Data Label&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Supervised&lt;/strong&gt; Learning(監督式學習)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data with pairs of input and output&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unsupervised Learning  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;doesn’t have output data(沒正確答案)&lt;/li&gt;
&lt;li&gt;clustering(分群問題)&lt;ul&gt;
&lt;li&gt;density estimation(find traffic dangerous areas)&lt;/li&gt;
&lt;li&gt;unusual detection(find unusual data)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;usually used in data mining &lt;img data-src=&#34;/img/ML/Jz6fiwk.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Semi-Supervised  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;given small amount of data with output, find output of other data&lt;ul&gt;
&lt;li&gt;Ex. facebook face identifier&lt;/li&gt;
&lt;li&gt;leverage unlabeled data to avoid ‘expensive’ labeling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reinforcement Learning(增強學習)  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;natural way of learning(行為學派)&lt;ul&gt;
&lt;li&gt;learn with &lt;strong&gt;‘seqentially implicit output’&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;if output is good, give reinforcement&lt;ul&gt;
&lt;li&gt;probability of this input increases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;if output is bad, give pushnishment&lt;ul&gt;
&lt;li&gt;probability of this input decreases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ex. &lt;ul&gt;
&lt;li&gt;train a dog&lt;/li&gt;
&lt;li&gt;online ADs&lt;/li&gt;
&lt;li&gt;chess AI&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;和gene algorithm類似&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Different-Protocol&#34;&gt;&lt;a href=&#34;#Different-Protocol&#34; class=&#34;headerlink&#34; title=&#34;Different Protocol&#34;&gt;&lt;/a&gt;Different Protocol&lt;/h3&gt;&lt;p&gt;Batch Learning    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;learn from known data&lt;ul&gt;
&lt;li&gt;duck feeding(填鴨式)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;very common protocol&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Online Learning  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sequential, passive data(不斷的得到新資料)&lt;/li&gt;
&lt;li&gt;Every datum can improve &lt;code&gt;g&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;PLA, reinforcement learning is often used with online learning&lt;/li&gt;
&lt;li&gt;Ex. spam filter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Active Learning  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;strategically-observed data&lt;/li&gt;
&lt;li&gt;machine can ask question(take &lt;strong&gt;chosen&lt;/strong&gt;(input, output)pair to learn)&lt;ul&gt;
&lt;li&gt;關於自己不會(錯誤)的問題，拿相關的資料來學習&lt;/li&gt;
&lt;li&gt;比對有自信的答案(= 對答案)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Different-Input-Space&#34;&gt;&lt;a href=&#34;#Different-Input-Space&#34; class=&#34;headerlink&#34; title=&#34;Different Input Space&#34;&gt;&lt;/a&gt;Different Input Space&lt;/h3&gt;&lt;p&gt;Feature &amp;lt;-&amp;gt; Input&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Concrete&lt;/strong&gt; Features  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;each input class represents some ‘sophisticated physical meaning’&lt;/li&gt;
&lt;li&gt;input 和 output 有相關(經過人類分類過)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Raw Features(未處理的資料)   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‘simple physical meaing’ -&amp;gt; difficult to learn&lt;/li&gt;
&lt;li&gt;Ex. Digit Recognition&lt;ul&gt;
&lt;li&gt;concrete feature: symmtry, density&lt;/li&gt;
&lt;li&gt;raw feature: matrix of image bits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abstract Features  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‘no physical learning’ -&amp;gt; the most difficult to learn&lt;/li&gt;
&lt;li&gt;need ‘feature conversion’&lt;/li&gt;
&lt;li&gt;Ex. Rating Prediction Problem&lt;ul&gt;
&lt;li&gt;從歌曲評分抽出feature: 喜好, 歌的性質……  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general machine learning, those three feature types will be used&lt;/p&gt;
&lt;h2 id=&#34;Chap-04-Feasibility-of-Learning&#34;&gt;&lt;a href=&#34;#Chap-04-Feasibility-of-Learning&#34; class=&#34;headerlink&#34; title=&#34;Chap 04 Feasibility of Learning&#34;&gt;&lt;/a&gt;Chap 04 Feasibility of Learning&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;learning will be stricted by limited data(no free lunch)&lt;/li&gt;
&lt;li&gt;learning from D (to infer something outside D) is doomed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Statistics   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Real environment -&amp;gt; unknown&lt;/li&gt;
&lt;li&gt;Sample data -&amp;gt; known&lt;ul&gt;
&lt;li&gt;Can sample represent the real?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;有極小可能無法代表real status&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Hoeffding’s-Inequality&#34;&gt;&lt;a href=&#34;#Hoeffding’s-Inequality&#34; class=&#34;headerlink&#34; title=&#34;Hoeffding’s Inequality&#34;&gt;&lt;/a&gt;Hoeffding’s Inequality&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;v and u are error rate of certain h in sample and real data &lt;img data-src=&#34;/img/ML/PG3e7Jr.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;larger sample size N or looser gap(誤差)&lt;ul&gt;
&lt;li&gt;higher probability to approximate real&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Error between hypothesis and target function&lt;/strong&gt; can be inferred by data &lt;img data-src=&#34;/img/ML/2I9ZSPn.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/ML/AC3KnSC.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;Ein-and-Eout&#34;&gt;&lt;a href=&#34;#Ein-and-Eout&#34; class=&#34;headerlink&#34; title=&#34;Ein and Eout&#34;&gt;&lt;/a&gt;Ein and Eout&lt;/h3&gt;&lt;p&gt;in-sample error(Ein) and out-of-sample error(Eout)&lt;br&gt;Guarantee: for large N, Ein(h) ~= Eout(h) is probably approximately correct (PAC) &lt;img data-src=&#34;/img/ML/colR3kh.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;p&gt;Q: if 150 people flips a coin 5 times, and one of them gets 5 heads.  A: Probability is &amp;gt; 99% &lt;img data-src=&#34;/img/ML/CCrtjgi.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ 做愈多次，遇到的BAD sample(Eout 和 Ein 差很多; sample和實際差距過大)的機率愈大&lt;br&gt;→ Real learning: Algorithm choose the best &lt;code&gt;h&lt;/code&gt; which has lowest Ein(h) among &lt;code&gt;H&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bad Data for a &lt;code&gt;H&lt;/code&gt;  &lt;ul&gt;
&lt;li&gt;存在 &lt;code&gt;h&lt;/code&gt; 使 Ein(h) 和 Eout(h) 相差很大 &lt;img data-src=&#34;/img/ML/x6wkDZk.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;由 hoeffding 知道抽到bad data的機率很小&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hypothesis的個數愈多，抽到BAD data的機率愈高 &lt;img data-src=&#34;/img/ML/IK9lYNY.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;安全的data(在任何h都不是bad data)的比例 若很高，則學到的東西可能不好&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若hypothesis set的大小是有限的話，只要N夠大，Eout ~= Ein&lt;br&gt;但perceptron不是finite(有無限多種分隔可選)&lt;/p&gt;
&lt;h2 id=&#34;Chap05-Training-versus-Testing&#34;&gt;&lt;a href=&#34;#Chap05-Training-versus-Testing&#34; class=&#34;headerlink&#34; title=&#34;Chap05 Training versus Testing&#34;&gt;&lt;/a&gt;Chap05 Training versus Testing&lt;/h2&gt;&lt;p&gt;g is similar to f ↔ Eout(g) ~= Ein(g) ~= 0  &lt;/p&gt;
&lt;p&gt;But need train and test &lt;img data-src=&#34;/img/ML/TXVWRpF.png&#34; alt=&#34;&#34;&gt;       &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train: find hypothesis that can fit sample data   &lt;/li&gt;
&lt;li&gt;Test: take &lt;strong&gt;good sample data&lt;/strong&gt; that is similar to exact data  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How to decide the number of hypothesis set&lt;br&gt;&lt;img data-src=&#34;/img/ML/mrA45Zq.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/ML/hsyNq1P.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Cannot both satisfied!&lt;/p&gt;
&lt;p&gt;Todo: Find a finite value $m_H$ can replace infinite M&lt;br&gt;&lt;img data-src=&#34;/img/ML/LOwwaGm.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Idea: M is overestimated, we use classification:&lt;br&gt;how many lines =&amp;gt; how many kinds of line(that makes different output)&lt;br&gt;This method is called Dichotomies(二分法): Mini-hypotheses&lt;br&gt;&lt;img data-src=&#34;/img/ML/8CcPNcS.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;input&lt;/th&gt;
&lt;th&gt;types of lines&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4 (00, 01, 10, 11)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;14 (2 lines that is not &lt;br&gt; linearly seperable)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;N&lt;/td&gt;
&lt;td&gt;effective(N) &amp;lt;= $2^N$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Growth Function $m_H$ = &lt;strong&gt;max number of dichotomies(max number of different outputs)&lt;/strong&gt;&lt;br&gt;&lt;img data-src=&#34;/img/ML/xc50yGO.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;h3 id=&#34;Types-of-Growth-Function&#34;&gt;&lt;a href=&#34;#Types-of-Growth-Function&#34; class=&#34;headerlink&#34; title=&#34;Types of Growth Function&#34;&gt;&lt;/a&gt;Types of Growth Function&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Positive Rays &lt;img data-src=&#34;/img/ML/vmoIwfN.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;$m_H(N)$ = N + 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Positive Intervals &lt;img data-src=&#34;/img/ML/FcLeNhZ.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;$C^{N+1}_2 + 1$ &lt;img data-src=&#34;/img/ML/D4mfUyr.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Convex Sets&lt;ul&gt;
&lt;li&gt;worst case: every point make a circle &lt;img data-src=&#34;/img/ML/tVqlZrK.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;$m_H(N) = 2^N$ -&amp;gt; exists N inputs that can be &lt;strong&gt;shattered(所有output皆可產生)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/ML/eEXFWde.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;Now $m_H(N)$ is finite, but exponential&lt;br&gt;Question:Can we find polynomial instead of exponential?&lt;/p&gt;
&lt;h3 id=&#34;Break-Point-of-H&#34;&gt;&lt;a href=&#34;#Break-Point-of-H&#34; class=&#34;headerlink&#34; title=&#34;Break Point of H&#34;&gt;&lt;/a&gt;Break Point of H&lt;/h3&gt;&lt;p&gt;if all possible k inputs can’t be shattered by H&lt;br&gt;k = break point for H &lt;img data-src=&#34;/img/ML/q3wjQSm.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;2D perceptrons: break point at 4&lt;br&gt;3 inputs: exist at least one input that can shatter &lt;img data-src=&#34;/img/ML/perceptron-shatter.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;4 inputs: for all inputs, no shatter  &lt;/p&gt;
&lt;p&gt;If there is no breakpoint, we can only find exponential($2^N$) increase&lt;br&gt;If there is a breakpoint, we can find polynomial($O(N^k)$)increase&lt;br&gt;breakpoint愈小，hypothesis set 成長的速度受到愈多限制(因為無法shatter，所以hypothesis數比exponential小)&lt;/p&gt;
&lt;h2 id=&#34;Chap06-Theory-of-Generalization&#34;&gt;&lt;a href=&#34;#Chap06-Theory-of-Generalization&#34; class=&#34;headerlink&#34; title=&#34;Chap06 Theory of Generalization&#34;&gt;&lt;/a&gt;Chap06 Theory of Generalization&lt;/h2&gt;&lt;p&gt;Q: maximum possible $m_H(N)$ if input number(N) = 3 when breakpoint(k) = 2?&lt;br&gt;A: x1, x2 cannot shatter, and so does x2, x3 and x1, x3 &lt;img data-src=&#34;/img/ML/KE3Xwxf.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ When N &amp;gt; breakpoint, break point restricts $m_H(N)$ a lot!&lt;/p&gt;
&lt;p&gt;idea: prove $m_H(N) \leq$ poly(N) if N &amp;gt; k &lt;/p&gt;
&lt;h3 id=&#34;Bounding-function&#34;&gt;&lt;a href=&#34;#Bounding-function&#34; class=&#34;headerlink&#34; title=&#34;Bounding function&#34;&gt;&lt;/a&gt;Bounding function&lt;/h3&gt;&lt;p&gt;bounding function B(N, k): maximum possible $m_H(N)$ when break point = k&lt;/p&gt;
&lt;p&gt;Table of bounding function(incomplete) &lt;img data-src=&#34;/img/ML/darN0tn.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;B(N, k) = $m_H(N) = 2^N$ when N &amp;lt; k(shatter)&lt;br&gt;B(N, k) &amp;lt; $m_H(N) = 2^N - 1$ when N = k(至少比shatter少一種)&lt;br&gt;When N &amp;gt; k :Using reduce, Ex. B(4,3) &lt;img data-src=&#34;/img/ML/gDjeq7v.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;α: dichotomies on (x1, x2, x3) with x4 paired&lt;br&gt;β: dichotomies on (x1, x2, x3) with x4 no paired&lt;/p&gt;
&lt;p&gt;Because B(4,3) can’t shatter any 3 inputs&lt;br&gt;→ α + β can’t shatter at (x1, x2, x3)&lt;br&gt;→ α + β $\leq$ B(3,3)&lt;/p&gt;
&lt;p&gt;Because B(4,3) can’t shatter any 3 inputs and x4 is already paired&lt;br&gt;→ α can’t shatter any 2 inputs at (x1, x2, x3)&lt;br&gt;→ α $\leq$ B(3,2)&lt;/p&gt;
&lt;p&gt;B(4,3) = 2α + β $\leq$ B(3,3) + B(3,2)&lt;br&gt;Generalized: B(N,k) $\leq$ B(N-1,k) + B(N-1,k-1) &lt;img data-src=&#34;/img/ML/jbksHEC.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;By calculation: $m_H(N) \leq B(N,k) \leq N^{k-1}$  &lt;/p&gt;
&lt;p&gt;Conclusion: $m_H(N)$ is polynomial if break point exists for N &amp;gt;= 2 &amp;amp; k &amp;gt;= 3!!&lt;br&gt;&lt;img data-src=&#34;/img/ML/M8N4HsO.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;&lt;img data-src=&#34;/img/ML/OqhVOS4.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;‘&amp;lt;=’ can be ‘=’ actually -&amp;gt; not easy proof(skipped)&lt;/p&gt;
&lt;h3 id=&#34;Vapnik-Chervonenkis-VC-bound&#34;&gt;&lt;a href=&#34;#Vapnik-Chervonenkis-VC-bound&#34; class=&#34;headerlink&#34; title=&#34;Vapnik-Chervonenkis (VC) bound&#34;&gt;&lt;/a&gt;Vapnik-Chervonenkis (VC) bound&lt;/h3&gt;&lt;p&gt;Proof: BAD Bound for General H   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Now Ein(h) finite, but Eout(h) still infinite(Eout的點有無限個)&lt;ol&gt;
&lt;li&gt;use ghost sample data Ein’ to replace(&lt;strong&gt;想像&lt;/strong&gt;再sample一次會產生的Ein’，將這段資料作為eout)&lt;/li&gt;
&lt;li&gt;圖中Ein離Eout很遠，是bad data，只要Ein’在Eout附近，Ein’也會離Eout很遠 &lt;img data-src=&#34;/img/ML/kK29SSC.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Eout 乘1/2，使其成為不等式 &lt;img data-src=&#34;/img/ML/jr6WUKW.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;將bad data相似的hypothesis分在一起 &lt;img data-src=&#34;&#34; alt=&#34;&#34;&gt;&lt;ol&gt;
&lt;li&gt;總共有2N個data(Ein + Ein’) → $m_H(2N)$ &lt;img data-src=&#34;/img/ML/MQ5v22d.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;因為有了$m_H()$函數，變成只考慮固定的hypothesis   &lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Use Hoeffding without Replacement&lt;ol&gt;
&lt;li&gt;可視為2N個點取N個點，sample為Ein，剩下為Ein’(不放回去)&lt;/li&gt;
&lt;li&gt;使用 ‘Hoeffding without Replacement’： 公式和hoeffding 一樣 &lt;img data-src=&#34;/img/ML/0ZC5xI3.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;Hoeffding只用於單一hypothesis，所以需要步驟2&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Vapnik-Chervonenkis (VC) bound &lt;img data-src=&#34;/img/ML/tjn5okQ.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ proved that learning with &lt;strong&gt;2D perceptrons&lt;/strong&gt; feasible!&lt;br&gt;&lt;img data-src=&#34;/img/ML/kyXVoYU.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;You need to let everything good to learned well &lt;img data-src=&#34;/img/ML/n8YPfWQ.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;Chap-07-VC-Dimension&#34;&gt;&lt;a href=&#34;#Chap-07-VC-Dimension&#34; class=&#34;headerlink&#34; title=&#34;Chap 07 VC Dimension&#34;&gt;&lt;/a&gt;Chap 07 VC Dimension&lt;/h2&gt;&lt;p&gt;VC Dimension&lt;br&gt;= maximum non-break point = (minimum k) - 1&lt;br&gt;= largest N that can shatter &lt;/p&gt;
&lt;p&gt;2D perceptron review &lt;img data-src=&#34;/img/ML/EOUT=0.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;How does PLA in more than 2 dimension?  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D → 3&lt;/li&gt;
&lt;li&gt;d-dimension perceptron &lt;ul&gt;
&lt;li&gt;d_VC = d+1 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Proof&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;d_VC &amp;gt; d+1 → d+1 can shatter&lt;br&gt;input matrix which is invertible &lt;img data-src=&#34;/img/ML/specificmatrix.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;for any y, we can find w such that sign(Xw) = y → $w = yX^{-1}$ → it can shatter &lt;/li&gt;
&lt;li&gt;d_VC &amp;lt; d+1 → d+2 can’t shatter&lt;br&gt;linear dependence restricts dichotomy &lt;img data-src=&#34;/img/ML/linearrely.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;if row &amp;gt; column, it would cause linear dependence &lt;img data-src=&#34;/img/ML/xd+2=all.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;for any input, we can find some $a_n$ that makes an output can’t happen → no shatter &lt;img data-src=&#34;/img/ML/geneag0.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;freedom&#34;&gt;&lt;a href=&#34;#freedom&#34; class=&#34;headerlink&#34; title=&#34;freedom&#34;&gt;&lt;/a&gt;freedom&lt;/h3&gt;&lt;p&gt;dimension, number of parameters, hypothesis quantity(M) → degrees of freedom&lt;br&gt;d_VC(H) = effitive binary degrees of freedom = powerfulness of H&lt;/p&gt;
&lt;p&gt;The more powerful it is (d_vc bigger), the more probability to get bad data &lt;img data-src=&#34;/img/ML/dvcbigsmall.png&#34; alt=&#34;D_vc&#34;&gt;&lt;br&gt;question:&lt;img data-src=&#34;/img/ML/Qhyperplane.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;比perceptron少一個parameter → d&lt;/p&gt;
&lt;p&gt;penalty for model complexity &lt;img data-src=&#34;/img/ML/smalle.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;model愈強，Ein愈小，和Eout誤差愈大 &lt;img data-src=&#34;/img/ML/modelcomplexity.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;p&gt;number of data(N) should be 10000 d_vc in theory; 10 d_vc is enough in practice, because VC bound is loose &lt;img data-src=&#34;/img/ML/nanddvc.png&#34; alt=&#34;&#34;&gt; &lt;img data-src=&#34;/img/ML/hoffedingloose.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;question: &lt;img data-src=&#34;/img/ML/q2.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;all of above(increase power of model)&lt;/p&gt;
&lt;h2 id=&#34;Chap08-Noise-and-Error&#34;&gt;&lt;a href=&#34;#Chap08-Noise-and-Error&#34; class=&#34;headerlink&#34; title=&#34;Chap08 Noise and Error&#34;&gt;&lt;/a&gt;Chap08 Noise and Error&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Noise in y&lt;ul&gt;
&lt;li&gt;Example: good customer mislabeled as bad&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Noise in x&lt;ul&gt;
&lt;li&gt;Example: incorrect feature calculation &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Would get probabilisic output y ≠ h(x) by given P(y|x)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Does VC bound works in noise? Yes, if i.i.d.(Independent and identically distributed) &lt;img data-src=&#34;/img/ML/iid.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;→ we can view as ‘ideal mini-target’ + noise&lt;br&gt;→ learning goal is to &lt;strong&gt;predict ideal mini-target(which is Y that has high P(Y|X) given X) on often seen inputs(X with high P(X))&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;Eout use expectation instead of Σ , $err$ means pointwise error(only consider a point x) &lt;img data-src=&#34;/img/ML/einout.png&#34; alt=&#34;&#34;&gt;  &lt;/p&gt;
&lt;h3 id=&#34;Error-Measure&#34;&gt;&lt;a href=&#34;#Error-Measure&#34; class=&#34;headerlink&#34; title=&#34;Error Measure&#34;&gt;&lt;/a&gt;Error Measure&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/ML/01andsquare.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;classification(0/1 error)&lt;ul&gt;
&lt;li&gt;minimum flipping noise(最少錯誤的output) &lt;/li&gt;
&lt;li&gt;NP-hard to optimize&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;regression use squared error&lt;ul&gt;
&lt;li&gt;minimum gaussian noise(output和正確答案的平方差最小)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Error is *&lt;em&gt;application/user dependent *&lt;/em&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CIA fingerprint login error&lt;ul&gt;
&lt;li&gt;not allow predict 0  to 1 &lt;img data-src=&#34;/img/ML/unbalancedata.png&#34; alt=&#34;&#34;&gt;     &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Supermarket member login error&lt;ul&gt;
&lt;li&gt;not want to predict 1 to 0 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error weight is not the same!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example: pocket  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;modify Ein to $E^w_{in}$(with weight)&lt;/li&gt;
&lt;li&gt;weight愈高的錯誤愈容易被選來修正&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;權重可以套用在許多機器學習的演算法&lt;/p&gt;
&lt;h3 id=&#34;algorithm-choosing&#34;&gt;&lt;a href=&#34;#algorithm-choosing&#34; class=&#34;headerlink&#34; title=&#34;algorithm choosing&#34;&gt;&lt;/a&gt;algorithm choosing&lt;/h3&gt;&lt;p&gt;Algorithmic Error Measures $\hat{err}$   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True&lt;ul&gt;
&lt;li&gt;error cannot be ignored or created&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;plausible(可用性)&lt;ul&gt;
&lt;li&gt;0/1 error&lt;/li&gt;
&lt;li&gt;squared error&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;friendly(較容易的演算法)    &lt;ul&gt;
&lt;li&gt;close form solution(有公式解，如Chap09的linear regression)&lt;/li&gt;
&lt;li&gt;convex objective function(可以持續更新的，如PLA)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\hat{err}$ is key part of many algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/ML/err-flow.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;參考資料&#34;&gt;&lt;a href=&#34;#參考資料&#34; class=&#34;headerlink&#34; title=&#34;參考資料&#34;&gt;&lt;/a&gt;參考資料&lt;/h2&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2NvdXJzZS9udHVtbG9uZQ==&#34;&gt;Coursera機器學習基石&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;br&gt;C老師上課講解&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>
