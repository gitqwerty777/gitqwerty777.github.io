{
    "version": "https://jsonfeed.org/version/1",
    "title": "QWERTY • All posts by \"perceptron\" tag",
    "description": "Programming | Computer Science | Thought",
    "home_page_url": "http://gitqwerty777.github.io",
    "items": [
        {
            "id": "http://gitqwerty777.github.io/MLfoundation1/",
            "url": "http://gitqwerty777.github.io/MLfoundation1/",
            "title": "機器學習基石(上)",
            "date_published": "2014-09-16T11:41:48.000Z",
            "content_html": "<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2NvdXJzZS9udHVtbG9uZQ==\">原版<i class=\"fa fa-external-link-alt\"></i></span>的講義做得十分精美，可以很快了解</p>\n<h2 id=\"Chap01-Introduction\"><a href=\"#Chap01-Introduction\" class=\"headerlink\" title=\"Chap01 Introduction\"></a>Chap01 Introduction</h2><p>課堂討論：學習的定義    </p>\n<ol>\n<li>從不會到會 </li>\n<li>從會到更進步、熟練</li>\n</ol>\n<p><img data-src=\"/img/ML/0FPIeqh.png\" alt=\"\"></p>\n<p>課堂討論：學習的方法    </p>\n<ul>\n<li>以「樹的定義」為例  </li>\n<li>如何寫出「能判斷是否是樹」的程式？ <ol>\n<li>define trees and hand-program: difficult</li>\n<li>learn from data by observation and recognize: more easier(機器「自己」學習)<a id=\"more\"></a>\n<img data-src=\"/img/ML/BuqSVKs.png\" alt=\"\"></li>\n</ol>\n</li>\n</ul>\n<p>課堂討論：兩種學習方法  </p>\n<ul>\n<li>電腦: learn from data -&gt; get knowledge by observing  </li>\n<li>人腦: learn from teachers -&gt; get the essence of the knowledge(can computer do that?)</li>\n</ul>\n<h3 id=\"key-eassence-of-ML\"><a href=\"#key-eassence-of-ML\" class=\"headerlink\" title=\"key eassence of ML\"></a>key eassence of ML</h3><ol>\n<li>存在「<strong>潛藏模式</strong>」可以學習<ul>\n<li>若認為有「潛藏模式」，才需要學習  </li>\n</ul>\n</li>\n<li><strong>無法簡單定義</strong></li>\n<li>有可提供學習的<strong>資料</strong></li>\n</ol>\n<h3 id=\"ML使用時機\"><a href=\"#ML使用時機\" class=\"headerlink\" title=\"ML使用時機\"></a>ML使用時機</h3><ul>\n<li>人類無法操作<ul>\n<li>火星探索</li>\n</ul>\n</li>\n<li>難以定義的問題<ul>\n<li>視覺/聽覺辨識  </li>\n</ul>\n</li>\n<li>需要快速判斷<ul>\n<li>股票炒短線程式</li>\n</ul>\n</li>\n<li>大量資料<ul>\n<li>個人化使用者體驗</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"ML應用\"><a href=\"#ML應用\" class=\"headerlink\" title=\"ML應用\"></a>ML應用</h3><p>推薦系統<br>將物品分解成各個porperty factors，形成vector，並與自己的喜好vector比較  </p>\n<h3 id=\"formalize-the-learning-problem\"><a href=\"#formalize-the-learning-problem\" class=\"headerlink\" title=\"formalize the learning problem\"></a>formalize the learning problem</h3><ul>\n<li>target funcion <code>f</code><ul>\n<li>unknown pattern to be learned   </li>\n</ul>\n</li>\n<li>data <code>D</code><ul>\n<li>training examples</li>\n</ul>\n</li>\n<li>hypothesis set <code>h</code><ul>\n<li>candidate functions to be choosed</li>\n</ul>\n</li>\n<li>hypothesis <code>g</code> <ul>\n<li>best candidate function which is learned from data</li>\n</ul>\n</li>\n<li>use algorithm(A) with data(D) and hypothesis set(H) to get g <img data-src=\"/img/ML/c5XEqoy.png\" alt=\"\"></li>\n</ul>\n<blockquote>\n<p>Machine Learning:<br><br>use data to compute hypothesis <code>g</code> that approximates target <code>f</code></p>\n</blockquote>\n<h3 id=\"Differences\"><a href=\"#Differences\" class=\"headerlink\" title=\"Differences\"></a>Differences</h3><h4 id=\"Machine-Learning-amp-Data-Mining\"><a href=\"#Machine-Learning-amp-Data-Mining\" class=\"headerlink\" title=\"Machine Learning &amp; Data Mining\"></a>Machine Learning &amp; Data Mining</h4><p>ML: the same as above<br>DM: use <strong>huge</strong> data to <strong>find property</strong> that is interesting</p>\n<h4 id=\"Machine-Learning-amp-Artificial-Intelligence\"><a href=\"#Machine-Learning-amp-Artificial-Intelligence\" class=\"headerlink\" title=\"Machine Learning &amp; Artificial Intelligence\"></a>Machine Learning &amp; Artificial Intelligence</h4><p>AI -&gt; compute something that shows intelligent behavior</p>\n<p><strong>ML can realize AI</strong><br>traditional AI -&gt; game tree<br>ML -&gt; learning (techiniques) from board data</p>\n<h4 id=\"Machine-Learning-amp-Statistics\"><a href=\"#Machine-Learning-amp-Statistics\" class=\"headerlink\" title=\"Machine Learning &amp; Statistics\"></a>Machine Learning &amp; Statistics</h4><p>Statistics: use data to make inference about an unknown process<br>-&gt; many <strong>useful tools for ML</strong></p>\n<p>課堂討論：Big Data     </p>\n<ul>\n<li>As data getting bigger, the way to deal with data has to be changed.(such as distributed computation)</li>\n<li><strong>not</strong> a new topic</li>\n<li>marketing buzz word<br>課堂討論：Maching Learning &amp; Neural Network  </li>\n<li>A technique used in early AI and ML</li>\n</ul>\n<h2 id=\"Chap-02-Perceptron-感知器\"><a href=\"#Chap-02-Perceptron-感知器\" class=\"headerlink\" title=\"Chap 02 Perceptron(感知器)\"></a>Chap 02 Perceptron(感知器)</h2><h3 id=\"yes-no-question-by-grading\"><a href=\"#yes-no-question-by-grading\" class=\"headerlink\" title=\"yes/no question by grading\"></a>yes/no question by grading</h3><p>用feature(特質)來分隔兩種不同的結果    </p>\n<ul>\n<li>x: input</li>\n<li>w: hypothesis</li>\n<li>x是在d維度空間的點(d個features)，w為分隔此空間的線(平面)的法向量 <img data-src=\"/img/ML/pla-w.png\" alt=\"\"> </li>\n<li>以二維空間為例：w產生的線分隔兩邊 <img data-src=\"/img/ML/MOzf2UK.png\" alt=\"\"><ul>\n<li>也就是h(x)的正負，w所在的那一側為正 <img data-src=\"/img/ML/joxwtUt.png\" alt=\"\">   </li>\n</ul>\n</li>\n</ul>\n<h3 id=\"select-g-from-h\"><a href=\"#select-g-from-h\" class=\"headerlink\" title=\"select g from h\"></a>select g from h</h3><p>Difficult: h is infinite<br>Idea: 從某一條線開始，進行更改(local search)</p>\n<h3 id=\"Perception-Learning-Algorithm-PLA\"><a href=\"#Perception-Learning-Algorithm-PLA\" class=\"headerlink\" title=\"Perception Learning Algorithm(PLA)\"></a>Perception Learning Algorithm(PLA)</h3><p>A fault confessed is half redressed(知錯能改)</p>\n<ol>\n<li>find a mistake(which sign is wrong) <img data-src=\"/img/ML/u0KFPyS.png\" alt=\"\"></li>\n<li>correct the mistake <img data-src=\"/img/ML/Mow3SlT.png\" alt=\"\"><ul>\n<li>if real ans = +, new w = w + x(使w靠近正的點)</li>\n<li>if real ans = -, new w = w - x(使w遠離負的點) </li>\n</ul>\n</li>\n<li>keep doing until no mistake </li>\n</ol>\n<p>question<br>同乘$y_nx_n$ <img data-src=\"/img/ML/KKHE36Z.png\" alt=\"\"><br>可看出錯誤變少：正確的時候，$w_nx_n$和$y_n$同號，所以$w_nx_ny_n$是正的    </p>\n<h3 id=\"linear-seperability\"><a href=\"#linear-seperability\" class=\"headerlink\" title=\"linear seperability\"></a>linear seperability</h3><p><img data-src=\"/img/ML/5L1kwEZ.png\" alt=\"\">  </p>\n<ul>\n<li>linear seperable<ul>\n<li>exist perfect w makes $sign(y) = sign(w_nx_n)$, n = 0~N</li>\n<li>用直線(平面)必可分成無錯誤的兩塊  </li>\n</ul>\n</li>\n<li>if Data is linear seperable, then PLA can generate w to make no mistake </li>\n<li>每次改動使$w_f$(正解)和$w_t$的內積變大，也就是愈來愈接近 <img data-src=\"/img/ML/unBVfjt.png\" alt=\"\"><ul>\n<li>但成長速度有限 <img data-src=\"/img/ML/LHtRvcu.png\" alt=\"\">    <ul>\n<li>$|W_t| &lt;= sqrt(t) max(X_n)$</li>\n<li><img data-src=\"/img/ML/J66FCPC.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>question<br><img data-src=\"/img/ML/0szpVwP.png\" alt=\"\"></p>\n<h3 id=\"PLA-Guarantee\"><a href=\"#PLA-Guarantee\" class=\"headerlink\" title=\"PLA Guarantee\"></a>PLA Guarantee</h3><p><img data-src=\"/img/ML/9qQxERz.png\" alt=\"\"></p>\n<ul>\n<li>advantage<ul>\n<li>simple to implement</li>\n<li>fast</li>\n</ul>\n</li>\n<li>disadvantage<ul>\n<li>not fully sure how long it will take</li>\n<li>assume linear seperable<ul>\n<li>What if no linear seperate?(in reality)</li>\n<li>選出犯錯最少的</li>\n<li>這是個NP-HARD問題… <img data-src=\"/img/ML/oRWuGAO.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Pocket-Algorithm-a-little-modified-by-PLA\"><a href=\"#Pocket-Algorithm-a-little-modified-by-PLA\" class=\"headerlink\" title=\"Pocket Algorithm(a little modified by PLA)\"></a>Pocket Algorithm(a little modified by PLA)</h3><p><img data-src=\"/img/ML/XkWjmux.png\" alt=\"\"></p>\n<ul>\n<li>greedy <ul>\n<li>may not be the best answer: 可能是局部最佳解</li>\n</ul>\n</li>\n<li>slower than PLA(need to compare Wt+1 and Wt)  </li>\n</ul>\n<h2 id=\"Chap03-types-of-learning\"><a href=\"#Chap03-types-of-learning\" class=\"headerlink\" title=\"Chap03 types of learning\"></a>Chap03 types of learning</h2><h3 id=\"Different-Output-Space\"><a href=\"#Different-Output-Space\" class=\"headerlink\" title=\"Different Output Space\"></a>Different Output Space</h3><p>Binary Classification  </p>\n<ul>\n<li>yes/no</li>\n<li>core problem to build tools</li>\n</ul>\n<p>Multiclass Classification(N output class)    </p>\n<ul>\n<li>Regression(迴歸分析)<ul>\n<li>output 為一數字</li>\n<li>Ex. temperature, stock price</li>\n<li><strong>core problem to build statistic tools</strong> </li>\n</ul>\n</li>\n<li>Structured Learning<ul>\n<li>output $y$ = structures with <strong>implicit class definition</strong></li>\n<li>too many class → structure</li>\n<li>Ex. Speech parse tree, sequence tagging(標詞性), protein folding</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Different-Data-Label\"><a href=\"#Different-Data-Label\" class=\"headerlink\" title=\"Different Data Label\"></a>Different Data Label</h3><p><strong>Supervised</strong> Learning(監督式學習)  </p>\n<ul>\n<li>data with pairs of input and output</li>\n</ul>\n<p>Unsupervised Learning  </p>\n<ul>\n<li>doesn’t have output data(沒正確答案)</li>\n<li>clustering(分群問題)<ul>\n<li>density estimation(find traffic dangerous areas)</li>\n<li>unusual detection(find unusual data)</li>\n</ul>\n</li>\n<li>usually used in data mining <img data-src=\"/img/ML/Jz6fiwk.png\" alt=\"\"></li>\n</ul>\n<p>Semi-Supervised  </p>\n<ul>\n<li>given small amount of data with output, find output of other data<ul>\n<li>Ex. facebook face identifier</li>\n<li>leverage unlabeled data to avoid ‘expensive’ labeling</li>\n</ul>\n</li>\n</ul>\n<p>Reinforcement Learning(增強學習)  </p>\n<ul>\n<li>natural way of learning(行為學派)<ul>\n<li>learn with <strong>‘seqentially implicit output’</strong></li>\n<li>if output is good, give reinforcement<ul>\n<li>probability of this input increases</li>\n</ul>\n</li>\n<li>if output is bad, give pushnishment<ul>\n<li>probability of this input decreases</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Ex. <ul>\n<li>train a dog</li>\n<li>online ADs</li>\n<li>chess AI</li>\n</ul>\n</li>\n<li>和gene algorithm類似</li>\n</ul>\n<h3 id=\"Different-Protocol\"><a href=\"#Different-Protocol\" class=\"headerlink\" title=\"Different Protocol\"></a>Different Protocol</h3><p>Batch Learning    </p>\n<ul>\n<li>learn from known data<ul>\n<li>duck feeding(填鴨式)</li>\n</ul>\n</li>\n<li><strong>very common protocol</strong></li>\n</ul>\n<p>Online Learning  </p>\n<ul>\n<li>sequential, passive data(不斷的得到新資料)</li>\n<li>Every datum can improve <code>g</code></li>\n<li>PLA, reinforcement learning is often used with online learning</li>\n<li>Ex. spam filter</li>\n</ul>\n<p>Active Learning  </p>\n<ul>\n<li>strategically-observed data</li>\n<li>machine can ask question(take <strong>chosen</strong>(input, output)pair to learn)<ul>\n<li>關於自己不會(錯誤)的問題，拿相關的資料來學習</li>\n<li>比對有自信的答案(= 對答案)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Different-Input-Space\"><a href=\"#Different-Input-Space\" class=\"headerlink\" title=\"Different Input Space\"></a>Different Input Space</h3><p>Feature &lt;-&gt; Input</p>\n<p><strong>Concrete</strong> Features  </p>\n<ul>\n<li>each input class represents some ‘sophisticated physical meaning’</li>\n<li>input 和 output 有相關(經過人類分類過)</li>\n</ul>\n<p>Raw Features(未處理的資料)   </p>\n<ul>\n<li>‘simple physical meaing’ -&gt; difficult to learn</li>\n<li>Ex. Digit Recognition<ul>\n<li>concrete feature: symmtry, density</li>\n<li>raw feature: matrix of image bits</li>\n</ul>\n</li>\n</ul>\n<p>Abstract Features  </p>\n<ul>\n<li>‘no physical learning’ -&gt; the most difficult to learn</li>\n<li>need ‘feature conversion’</li>\n<li>Ex. Rating Prediction Problem<ul>\n<li>從歌曲評分抽出feature: 喜好, 歌的性質……  </li>\n</ul>\n</li>\n</ul>\n<p>In general machine learning, those three feature types will be used</p>\n<h2 id=\"Chap-04-Feasibility-of-Learning\"><a href=\"#Chap-04-Feasibility-of-Learning\" class=\"headerlink\" title=\"Chap 04 Feasibility of Learning\"></a>Chap 04 Feasibility of Learning</h2><ul>\n<li>learning will be stricted by limited data(no free lunch)</li>\n<li>learning from D (to infer something outside D) is doomed</li>\n</ul>\n<p>Statistics   </p>\n<ul>\n<li>Real environment -&gt; unknown</li>\n<li>Sample data -&gt; known<ul>\n<li>Can sample represent the real?</li>\n</ul>\n</li>\n<li>有極小可能無法代表real status</li>\n</ul>\n<h3 id=\"Hoeffding’s-Inequality\"><a href=\"#Hoeffding’s-Inequality\" class=\"headerlink\" title=\"Hoeffding’s Inequality\"></a>Hoeffding’s Inequality</h3><ul>\n<li>v and u are error rate of certain h in sample and real data <img data-src=\"/img/ML/PG3e7Jr.png\" alt=\"\"></li>\n<li>larger sample size N or looser gap(誤差)<ul>\n<li>higher probability to approximate real</li>\n</ul>\n</li>\n</ul>\n<p><strong>Error between hypothesis and target function</strong> can be inferred by data <img data-src=\"/img/ML/2I9ZSPn.png\" alt=\"\"> <img data-src=\"/img/ML/AC3KnSC.png\" alt=\"\"></p>\n<h3 id=\"Ein-and-Eout\"><a href=\"#Ein-and-Eout\" class=\"headerlink\" title=\"Ein and Eout\"></a>Ein and Eout</h3><p>in-sample error(Ein) and out-of-sample error(Eout)<br>Guarantee: for large N, Ein(h) ~= Eout(h) is probably approximately correct (PAC) <img data-src=\"/img/ML/colR3kh.png\" alt=\"\">  </p>\n<p>Q: if 150 people flips a coin 5 times, and one of them gets 5 heads.  A: Probability is &gt; 99% <img data-src=\"/img/ML/CCrtjgi.png\" alt=\"\"><br>→ 做愈多次，遇到的BAD sample(Eout 和 Ein 差很多; sample和實際差距過大)的機率愈大<br>→ Real learning: Algorithm choose the best <code>h</code> which has lowest Ein(h) among <code>H</code></p>\n<ul>\n<li>Bad Data for a <code>H</code>  <ul>\n<li>存在 <code>h</code> 使 Ein(h) 和 Eout(h) 相差很大 <img data-src=\"/img/ML/x6wkDZk.png\" alt=\"\"><ul>\n<li>由 hoeffding 知道抽到bad data的機率很小</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>hypothesis的個數愈多，抽到BAD data的機率愈高 <img data-src=\"/img/ML/IK9lYNY.png\" alt=\"\"><ul>\n<li>安全的data(在任何h都不是bad data)的比例 若很高，則學到的東西可能不好</li>\n</ul>\n</li>\n</ul>\n<p>若hypothesis set的大小是有限的話，只要N夠大，Eout ~= Ein<br>但perceptron不是finite(有無限多種分隔可選)</p>\n<h2 id=\"Chap05-Training-versus-Testing\"><a href=\"#Chap05-Training-versus-Testing\" class=\"headerlink\" title=\"Chap05 Training versus Testing\"></a>Chap05 Training versus Testing</h2><p>g is similar to f ↔ Eout(g) ~= Ein(g) ~= 0  </p>\n<p>But need train and test <img data-src=\"/img/ML/TXVWRpF.png\" alt=\"\">       </p>\n<ul>\n<li>Train: find hypothesis that can fit sample data   </li>\n<li>Test: take <strong>good sample data</strong> that is similar to exact data  </li>\n</ul>\n<p>How to decide the number of hypothesis set<br><img data-src=\"/img/ML/mrA45Zq.png\" alt=\"\"> <img data-src=\"/img/ML/hsyNq1P.png\" alt=\"\"><br>Cannot both satisfied!</p>\n<p>Todo: Find a finite value $m_H$ can replace infinite M<br><img data-src=\"/img/ML/LOwwaGm.png\" alt=\"\"><br>Idea: M is overestimated, we use classification:<br>how many lines =&gt; how many kinds of line(that makes different output)<br>This method is called Dichotomies(二分法): Mini-hypotheses<br><img data-src=\"/img/ML/8CcPNcS.png\" alt=\"\"></p>\n<table>\n<thead>\n<tr>\n<th>input</th>\n<th>types of lines</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4 (00, 01, 10, 11)</td>\n</tr>\n<tr>\n<td>3</td>\n<td>8</td>\n</tr>\n<tr>\n<td>4</td>\n<td>14 (2 lines that is not <br> linearly seperable)</td>\n</tr>\n<tr>\n<td>N</td>\n<td>effective(N) &lt;= $2^N$</td>\n</tr>\n</tbody></table>\n<p>Growth Function $m_H$ = <strong>max number of dichotomies(max number of different outputs)</strong><br><img data-src=\"/img/ML/xc50yGO.png\" alt=\"\">  </p>\n<h3 id=\"Types-of-Growth-Function\"><a href=\"#Types-of-Growth-Function\" class=\"headerlink\" title=\"Types of Growth Function\"></a>Types of Growth Function</h3><ul>\n<li>Positive Rays <img data-src=\"/img/ML/vmoIwfN.png\" alt=\"\"><ul>\n<li>$m_H(N)$ = N + 1</li>\n</ul>\n</li>\n<li>Positive Intervals <img data-src=\"/img/ML/FcLeNhZ.png\" alt=\"\"><ul>\n<li>$C^{N+1}_2 + 1$ <img data-src=\"/img/ML/D4mfUyr.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Convex Sets<ul>\n<li>worst case: every point make a circle <img data-src=\"/img/ML/tVqlZrK.png\" alt=\"\"></li>\n<li>$m_H(N) = 2^N$ -&gt; exists N inputs that can be <strong>shattered(所有output皆可產生)</strong></li>\n</ul>\n</li>\n</ul>\n<p><img data-src=\"/img/ML/eEXFWde.png\" alt=\"\"><br>Now $m_H(N)$ is finite, but exponential<br>Question:Can we find polynomial instead of exponential?</p>\n<h3 id=\"Break-Point-of-H\"><a href=\"#Break-Point-of-H\" class=\"headerlink\" title=\"Break Point of H\"></a>Break Point of H</h3><p>if all possible k inputs can’t be shattered by H<br>k = break point for H <img data-src=\"/img/ML/q3wjQSm.png\" alt=\"\"></p>\n<p>2D perceptrons: break point at 4<br>3 inputs: exist at least one input that can shatter <img data-src=\"/img/ML/perceptron-shatter.png\" alt=\"\"><br>4 inputs: for all inputs, no shatter  </p>\n<p>If there is no breakpoint, we can only find exponential($2^N$) increase<br>If there is a breakpoint, we can find polynomial($O(N^k)$)increase<br>breakpoint愈小，hypothesis set 成長的速度受到愈多限制(因為無法shatter，所以hypothesis數比exponential小)</p>\n<h2 id=\"Chap06-Theory-of-Generalization\"><a href=\"#Chap06-Theory-of-Generalization\" class=\"headerlink\" title=\"Chap06 Theory of Generalization\"></a>Chap06 Theory of Generalization</h2><p>Q: maximum possible $m_H(N)$ if input number(N) = 3 when breakpoint(k) = 2?<br>A: x1, x2 cannot shatter, and so does x2, x3 and x1, x3 <img data-src=\"/img/ML/KE3Xwxf.png\" alt=\"\"><br>→ When N &gt; breakpoint, break point restricts $m_H(N)$ a lot!</p>\n<p>idea: prove $m_H(N) \\leq$ poly(N) if N &gt; k </p>\n<h3 id=\"Bounding-function\"><a href=\"#Bounding-function\" class=\"headerlink\" title=\"Bounding function\"></a>Bounding function</h3><p>bounding function B(N, k): maximum possible $m_H(N)$ when break point = k</p>\n<p>Table of bounding function(incomplete) <img data-src=\"/img/ML/darN0tn.png\" alt=\"\"><br>B(N, k) = $m_H(N) = 2^N$ when N &lt; k(shatter)<br>B(N, k) &lt; $m_H(N) = 2^N - 1$ when N = k(至少比shatter少一種)<br>When N &gt; k :Using reduce, Ex. B(4,3) <img data-src=\"/img/ML/gDjeq7v.png\" alt=\"\"><br>α: dichotomies on (x1, x2, x3) with x4 paired<br>β: dichotomies on (x1, x2, x3) with x4 no paired</p>\n<p>Because B(4,3) can’t shatter any 3 inputs<br>→ α + β can’t shatter at (x1, x2, x3)<br>→ α + β $\\leq$ B(3,3)</p>\n<p>Because B(4,3) can’t shatter any 3 inputs and x4 is already paired<br>→ α can’t shatter any 2 inputs at (x1, x2, x3)<br>→ α $\\leq$ B(3,2)</p>\n<p>B(4,3) = 2α + β $\\leq$ B(3,3) + B(3,2)<br>Generalized: B(N,k) $\\leq$ B(N-1,k) + B(N-1,k-1) <img data-src=\"/img/ML/jbksHEC.png\" alt=\"\"><br>By calculation: $m_H(N) \\leq B(N,k) \\leq N^{k-1}$  </p>\n<p>Conclusion: $m_H(N)$ is polynomial if break point exists for N &gt;= 2 &amp; k &gt;= 3!!<br><img data-src=\"/img/ML/M8N4HsO.png\" alt=\"\"><br><img data-src=\"/img/ML/OqhVOS4.png\" alt=\"\"><br>‘&lt;=’ can be ‘=’ actually -&gt; not easy proof(skipped)</p>\n<h3 id=\"Vapnik-Chervonenkis-VC-bound\"><a href=\"#Vapnik-Chervonenkis-VC-bound\" class=\"headerlink\" title=\"Vapnik-Chervonenkis (VC) bound\"></a>Vapnik-Chervonenkis (VC) bound</h3><p>Proof: BAD Bound for General H   </p>\n<ol>\n<li>Now Ein(h) finite, but Eout(h) still infinite(Eout的點有無限個)<ol>\n<li>use ghost sample data Ein’ to replace(<strong>想像</strong>再sample一次會產生的Ein’，將這段資料作為eout)</li>\n<li>圖中Ein離Eout很遠，是bad data，只要Ein’在Eout附近，Ein’也會離Eout很遠 <img data-src=\"/img/ML/kK29SSC.png\" alt=\"\"></li>\n<li>Eout 乘1/2，使其成為不等式 <img data-src=\"/img/ML/jr6WUKW.png\" alt=\"\"></li>\n</ol>\n</li>\n<li>將bad data相似的hypothesis分在一起 <img data-src=\"\" alt=\"\"><ol>\n<li>總共有2N個data(Ein + Ein’) → $m_H(2N)$ <img data-src=\"/img/ML/MQ5v22d.png\" alt=\"\"></li>\n<li>因為有了$m_H()$函數，變成只考慮固定的hypothesis   </li>\n</ol>\n</li>\n<li>Use Hoeffding without Replacement<ol>\n<li>可視為2N個點取N個點，sample為Ein，剩下為Ein’(不放回去)</li>\n<li>使用 ‘Hoeffding without Replacement’： 公式和hoeffding 一樣 <img data-src=\"/img/ML/0ZC5xI3.png\" alt=\"\"></li>\n<li>Hoeffding只用於單一hypothesis，所以需要步驟2</li>\n</ol>\n</li>\n</ol>\n<p>Vapnik-Chervonenkis (VC) bound <img data-src=\"/img/ML/tjn5okQ.png\" alt=\"\"><br>→ proved that learning with <strong>2D perceptrons</strong> feasible!<br><img data-src=\"/img/ML/kyXVoYU.png\" alt=\"\"><br>You need to let everything good to learned well <img data-src=\"/img/ML/n8YPfWQ.png\" alt=\"\"></p>\n<h2 id=\"Chap-07-VC-Dimension\"><a href=\"#Chap-07-VC-Dimension\" class=\"headerlink\" title=\"Chap 07 VC Dimension\"></a>Chap 07 VC Dimension</h2><p>VC Dimension<br>= maximum non-break point = (minimum k) - 1<br>= largest N that can shatter </p>\n<p>2D perceptron review <img data-src=\"/img/ML/EOUT=0.png\" alt=\"\"><br>How does PLA in more than 2 dimension?  </p>\n<ul>\n<li>2D → 3</li>\n<li>d-dimension perceptron <ul>\n<li>d_VC = d+1 </li>\n</ul>\n</li>\n</ul>\n<p>Proof</p>\n<ol>\n<li>d_VC &gt; d+1 → d+1 can shatter<br>input matrix which is invertible <img data-src=\"/img/ML/specificmatrix.png\" alt=\"\"><br>for any y, we can find w such that sign(Xw) = y → $w = yX^{-1}$ → it can shatter </li>\n<li>d_VC &lt; d+1 → d+2 can’t shatter<br>linear dependence restricts dichotomy <img data-src=\"/img/ML/linearrely.png\" alt=\"\"><br>if row &gt; column, it would cause linear dependence <img data-src=\"/img/ML/xd+2=all.png\" alt=\"\"><br>for any input, we can find some $a_n$ that makes an output can’t happen → no shatter <img data-src=\"/img/ML/geneag0.png\" alt=\"\"></li>\n</ol>\n<h3 id=\"freedom\"><a href=\"#freedom\" class=\"headerlink\" title=\"freedom\"></a>freedom</h3><p>dimension, number of parameters, hypothesis quantity(M) → degrees of freedom<br>d_VC(H) = effitive binary degrees of freedom = powerfulness of H</p>\n<p>The more powerful it is (d_vc bigger), the more probability to get bad data <img data-src=\"/img/ML/dvcbigsmall.png\" alt=\"D_vc\"><br>question:<img data-src=\"/img/ML/Qhyperplane.png\" alt=\"\"><br>比perceptron少一個parameter → d</p>\n<p>penalty for model complexity <img data-src=\"/img/ML/smalle.png\" alt=\"\"><br>model愈強，Ein愈小，和Eout誤差愈大 <img data-src=\"/img/ML/modelcomplexity.png\" alt=\"\">  </p>\n<p>number of data(N) should be 10000 d_vc in theory; 10 d_vc is enough in practice, because VC bound is loose <img data-src=\"/img/ML/nanddvc.png\" alt=\"\"> <img data-src=\"/img/ML/hoffedingloose.png\" alt=\"\"></p>\n<p>question: <img data-src=\"/img/ML/q2.png\" alt=\"\"><br>all of above(increase power of model)</p>\n<h2 id=\"Chap08-Noise-and-Error\"><a href=\"#Chap08-Noise-and-Error\" class=\"headerlink\" title=\"Chap08 Noise and Error\"></a>Chap08 Noise and Error</h2><ul>\n<li>Noise in y<ul>\n<li>Example: good customer mislabeled as bad</li>\n</ul>\n</li>\n<li>Noise in x<ul>\n<li>Example: incorrect feature calculation </li>\n</ul>\n</li>\n<li>Would get probabilisic output y ≠ h(x) by given P(y|x)</li>\n</ul>\n<p>Does VC bound works in noise? Yes, if i.i.d.(Independent and identically distributed) <img data-src=\"/img/ML/iid.png\" alt=\"\"><br>→ we can view as ‘ideal mini-target’ + noise<br>→ learning goal is to <strong>predict ideal mini-target(which is Y that has high P(Y|X) given X) on often seen inputs(X with high P(X))</strong> </p>\n<p>Eout use expectation instead of Σ , $err$ means pointwise error(only consider a point x) <img data-src=\"/img/ML/einout.png\" alt=\"\">  </p>\n<h3 id=\"Error-Measure\"><a href=\"#Error-Measure\" class=\"headerlink\" title=\"Error Measure\"></a>Error Measure</h3><p><img data-src=\"/img/ML/01andsquare.png\" alt=\"\"></p>\n<ul>\n<li>classification(0/1 error)<ul>\n<li>minimum flipping noise(最少錯誤的output) </li>\n<li>NP-hard to optimize</li>\n</ul>\n</li>\n<li>regression use squared error<ul>\n<li>minimum gaussian noise(output和正確答案的平方差最小)</li>\n</ul>\n</li>\n</ul>\n<p>Error is *<em>application/user dependent *</em> </p>\n<ul>\n<li>CIA fingerprint login error<ul>\n<li>not allow predict 0  to 1 <img data-src=\"/img/ML/unbalancedata.png\" alt=\"\">     </li>\n</ul>\n</li>\n<li>Supermarket member login error<ul>\n<li>not want to predict 1 to 0 </li>\n</ul>\n</li>\n<li>error weight is not the same!</li>\n</ul>\n<p>Example: pocket  </p>\n<ul>\n<li>modify Ein to $E^w_{in}$(with weight)</li>\n<li>weight愈高的錯誤愈容易被選來修正</li>\n</ul>\n<p>權重可以套用在許多機器學習的演算法</p>\n<h3 id=\"algorithm-choosing\"><a href=\"#algorithm-choosing\" class=\"headerlink\" title=\"algorithm choosing\"></a>algorithm choosing</h3><p>Algorithmic Error Measures $\\hat{err}$   </p>\n<ul>\n<li>True<ul>\n<li>error cannot be ignored or created</li>\n</ul>\n</li>\n<li>plausible(可用性)<ul>\n<li>0/1 error</li>\n<li>squared error</li>\n</ul>\n</li>\n<li>friendly(較容易的演算法)    <ul>\n<li>close form solution(有公式解，如Chap09的linear regression)</li>\n<li>convex objective function(可以持續更新的，如PLA)</li>\n</ul>\n</li>\n<li>$\\hat{err}$ is key part of many algorithms</li>\n</ul>\n<p><img data-src=\"/img/ML/err-flow.png\" alt=\"\"></p>\n<h2 id=\"參考資料\"><a href=\"#參考資料\" class=\"headerlink\" title=\"參考資料\"></a>參考資料</h2><p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2NvdXJzZS9udHVtbG9uZQ==\">Coursera機器學習基石<i class=\"fa fa-external-link-alt\"></i></span><br>C老師上課講解</p>\n",
            "tags": [
                "機器學習",
                "perceptron"
            ]
        }
    ]
}