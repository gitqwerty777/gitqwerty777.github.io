<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>QWERTY • Posts by &#34;推薦系統&#34; tag</title>
        <link>http://gitqwerty777.github.io</link>
        <description>Programming | Computer Science | Thought</description>
        <language>zh-TW</language>
        <pubDate>Fri, 21 Jan 2022 11:11:11 +0800</pubDate>
        <lastBuildDate>Fri, 21 Jan 2022 11:11:11 +0800</lastBuildDate>
        <category>C#</category>
        <category>CodingStyle</category>
        <category>Emacs</category>
        <category>編輯器</category>
        <category>CFR</category>
        <category>電腦對局理論</category>
        <category>指令</category>
        <category>機器學習</category>
        <category>perceptron</category>
        <category>readme</category>
        <category>文件</category>
        <category>github</category>
        <category>artificial intelligence</category>
        <category>search</category>
        <category>First-Order Logic</category>
        <category>大數</category>
        <category>程式</category>
        <category>C++</category>
        <category>Hexo</category>
        <category>網誌</category>
        <category>Markdown</category>
        <category>CleanCode</category>
        <category>重構</category>
        <category>TDD</category>
        <category>設計模式</category>
        <category>CMake</category>
        <category>Makefile</category>
        <category>Linux</category>
        <category>Todo</category>
        <category>註解</category>
        <category>經濟學</category>
        <category>策略</category>
        <category>競爭</category>
        <category>博弈論</category>
        <category>計算機結構</category>
        <category>人工智慧</category>
        <category>圍棋</category>
        <category>象棋</category>
        <category>蒙地卡羅</category>
        <category>Alpha-Beta搜尋</category>
        <category>強化學習</category>
        <category>計算機網路</category>
        <category>boost</category>
        <category>函式庫</category>
        <category>編譯</category>
        <category>gcc</category>
        <category>g++</category>
        <category>clang</category>
        <category>最佳化</category>
        <category>推薦系統</category>
        <category>FM</category>
        <category>FFM</category>
        <category>SVM</category>
        <category>Embedding</category>
        <category>自然語言處理</category>
        <category>外國用語</category>
        <category>萌典</category>
        <category>opencc</category>
        <category>PTT</category>
        <category>vuejs</category>
        <category>linux</category>
        <category>c</category>
        <category>compile</category>
        <category>gdb</category>
        <category>c語言</category>
        <category>cpp</category>
        <category>除錯</category>
        <category>git</category>
        <category>VMWare</category>
        <category>虛擬機</category>
        <category>IFTTT</category>
        <category>自動化</category>
        <category>備份</category>
        <category>webhook</category>
        <category>簡報</category>
        <category>軟體</category>
        <category>PowerPoint</category>
        <category>Latex</category>
        <category>JavaScript</category>
        <category>CSS</category>
        <category>Unity</category>
        <category>fcitx</category>
        <category>嘸蝦米</category>
        <category>輸入法</category>
        <category>硬碟</category>
        <category>記憶體</category>
        <category>效能</category>
        <category>錯誤</category>
        <category>makefile</category>
        <category>備忘錄</category>
        <category>存檔</category>
        <category>統計</category>
        <category>byobu</category>
        <category>screen</category>
        <category>tmux</category>
        <category>reactjs</category>
        <category>javascript</category>
        <category>WideAndDeep</category>
        <category>Google</category>
        <category>觀察者</category>
        <category>訂閱</category>
        <category>委託</category>
        <category>正規表示式(RegExp)</category>
        <category>上下文無關文法(CFG)</category>
        <category>hexo</category>
        <category>blog</category>
        <category>theme</category>
        <category>feature</category>
        <category>revealJS</category>
        <category>markdown</category>
        <category>rss</category>
        <category>facebook</category>
        <category>youtube</category>
        <category>ptt</category>
        <category>bilibili</category>
        <category>pixiv</category>
        <category>crawler</category>
        <category>SEO</category>
        <category>google</category>
        <category>html</category>
        <category>amazon</category>
        <category>webhost</category>
        <category>ssl</category>
        <category>漢字</category>
        <category>中文</category>
        <category>異體字</category>
        <category>unicode</category>
        <category>unity</category>
        <category>演算法</category>
        <category>隨機排序</category>
        <category>洗牌</category>
        <category>Fisher-Yates</category>
        <category>證明</category>
        <category>python</category>
        <item>
            <guid isPermalink="true">http://gitqwerty777.github.io/factorization-machines/</guid>
            <title>Factorization Machines(FM) 和 Field-Aware Factorization Machine(FFM)：推薦系統中的瑞士軍刀</title>
            <link>http://gitqwerty777.github.io/factorization-machines/</link>
            <category>推薦系統</category>
            <category>FM</category>
            <category>FFM</category>
            <category>SVM</category>
            <category>Embedding</category>
            <pubDate>Fri, 21 Jan 2022 11:11:11 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;Factorization-Machines-FM&#34;&gt;&lt;a href=&#34;#Factorization-Machines-FM&#34; class=&#34;headerlink&#34; title=&#34;Factorization Machines(FM)&#34;&gt;&lt;/a&gt;Factorization Machines(FM)&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;SVM&lt;ul&gt;
&lt;li&gt;難以在稀疏資料中學習&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Factorization Models(如Matrix Factorization)&lt;ul&gt;
&lt;li&gt;擴展性低：需要特定的輸入格式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;FM：克服SVM和Factorization Models的缺點&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可在稀疏資料中學習&lt;/li&gt;
&lt;li&gt;輸入資料可擴展&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;訓練時間為線性複雜度&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;理論&#34;&gt;&lt;a href=&#34;#理論&#34; class=&#34;headerlink&#34; title=&#34;理論&#34;&gt;&lt;/a&gt;理論&lt;/h3&gt;&lt;p&gt;FM將權重 $w_{ij}$ 設為兩個長度為k的&lt;strong&gt;隱向量&lt;/strong&gt;$V_i, V_j$的&lt;strong&gt;內積&lt;/strong&gt;，表示為$\langle V_i, V_j \rangle$&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/recommend/fm-formula.png&#34; alt=&#34;2維的FM公式&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$w_0$​是bias&lt;/li&gt;
&lt;li&gt;$w_i​$是特徵$i$的一維權重&lt;/li&gt;
&lt;li&gt;$w_{i,j}$​是特徵$i$和特徵$j$的二次交叉權重&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;隱向量長度$k$為hyperparameter&lt;/li&gt;
&lt;li&gt;FM將權重矩陣分解為隱向量的內積，破壞了權重的獨立性，所以在稀疏資料中仍能學習&lt;ol&gt;
&lt;li&gt;已知一正定矩陣$W$，必存在$V$使$W=VV^t$&lt;/li&gt;
&lt;li&gt;權重矩陣$W$必為正定&lt;/li&gt;
&lt;li&gt;所以$W$必能分解成隱向量矩陣$V$乘自身的轉置&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;原本$W$的大小為$\frac{n^2}{2}$，改成隱向量$V$之後大小為$kn$，$k$通常不會設很大，明顯減少參數數量&lt;ul&gt;
&lt;li&gt;限制$k$的大小也能限制FM模型的表達力，泛化能力較好&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/recommend/FM-structure.png&#34; alt=&#34;結構&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;效率&#34;&gt;&lt;a href=&#34;#效率&#34; class=&#34;headerlink&#34; title=&#34;效率&#34;&gt;&lt;/a&gt;效率&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/recommend/fm-time-complexity.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;整理公式後，Inference的時間複雜度從$O(kn^2)$降到了$O(kn)$，$n$為特徵維度&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第2行公式推導：表示為整個矩陣扣掉對角項再除以2，因為$W$是對稱矩陣&lt;/li&gt;
&lt;li&gt;詳細推導可看&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly95dWxvbmd0c2FpLm1lZGl1bS5jb20vZmFjdG9yaXphdGlvbi1tYWNoaW5lLTYzMTYwYmMyYzA2Yg==&#34;&gt;這篇&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;實作上只須計算非0元素的乘積，時間複雜度再下降到$O(km)$，$m$為平均一筆輸入資料中，值非0的特徵數&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;更新&#34;&gt;&lt;a href=&#34;#更新&#34; class=&#34;headerlink&#34; title=&#34;更新&#34;&gt;&lt;/a&gt;更新&lt;/h3&gt;&lt;p&gt;使用gradient descent學習參數&lt;br&gt;&lt;img data-src=&#34;/img/recommend/fm-gradient.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;$\sum^n_{j=1}v_{j, f}x_j$可以事先計算，所以每次梯度更新的時間複雜度為$O(1)$&lt;/p&gt;
&lt;p&gt;因此FM的訓練時間複雜度也是$O(km)$&lt;/p&gt;
&lt;h3 id=&#34;高維度FM&#34;&gt;&lt;a href=&#34;#高維度FM&#34; class=&#34;headerlink&#34; title=&#34;高維度FM&#34;&gt;&lt;/a&gt;高維度FM&lt;/h3&gt;&lt;p&gt;&lt;img data-src=&#34;/img/recommend/fm-dway.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;經過公式簡化(和二維的方法相似)，也可以在線性時間內計算&lt;/p&gt;
&lt;h3 id=&#34;FM-和-Factorization-Model-SVM-比較&#34;&gt;&lt;a href=&#34;#FM-和-Factorization-Model-SVM-比較&#34; class=&#34;headerlink&#34; title=&#34;FM 和 Factorization Model, SVM 比較&#34;&gt;&lt;/a&gt;FM 和 Factorization Model, SVM 比較&lt;/h3&gt;&lt;p&gt;論文中證明了兩件事&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;各種Factorization Model為FM的特化&lt;/li&gt;
&lt;li&gt;FM可以解決SVM在稀疏資料中無法成功訓練的問題&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;詳細證明看不懂，略過&lt;/p&gt;
&lt;h3 id=&#34;結論&#34;&gt;&lt;a href=&#34;#結論&#34; class=&#34;headerlink&#34; title=&#34;結論&#34;&gt;&lt;/a&gt;結論&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;FM速度快、容易實作，於2012~14年為業界主流模型&lt;/li&gt;
&lt;li&gt;FM產生的隱向量可視為一種embedding&lt;ul&gt;
&lt;li&gt;所以拿user的隱向量找相似隱向量的item，就是一個簡易且快速的推薦方法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FM適合類型特徵(離散)而非數值特徵(連續)，因為&lt;ul&gt;
&lt;li&gt;類型特徵可有多個隱向量，而數值特徵只有一個&lt;/li&gt;
&lt;li&gt;數值特徵不應使用同一個隱向量，如10歲和40歲&lt;/li&gt;
&lt;li&gt;FM速度和非零特徵數有關，數值特徵類型化後不影響訓練速度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;Field-aware-factorization-machines-FFM&#34;&gt;&lt;a href=&#34;#Field-aware-factorization-machines-FFM&#34; class=&#34;headerlink&#34; title=&#34;Field-aware factorization machines(FFM)&#34;&gt;&lt;/a&gt;Field-aware factorization machines(FFM)&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;FM：一個特徵有&lt;strong&gt;一個&lt;/strong&gt;隱向量&lt;/li&gt;
&lt;li&gt;FFM：一個特徵有&lt;strong&gt;一組&lt;/strong&gt;隱向量&lt;ul&gt;
&lt;li&gt;每個隱向量對應不同的&lt;strong&gt;特徵域&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;特徵域通常為一群代表相同性質的特徵，如one-hot特徵&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/recommend/ffm-formula.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;範例&#34;&gt;&lt;a href=&#34;#範例&#34; class=&#34;headerlink&#34; title=&#34;範例&#34;&gt;&lt;/a&gt;範例&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;出版商特徵域(P): ESPN, Vogue, and NBC&lt;/li&gt;
&lt;li&gt;廣告商特徵域(A): Nike, Gucci, and Adidas&lt;/li&gt;
&lt;li&gt;消費者性別特徵域(G): Male, Female&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在(ESPN, Nike) 和 (ESPN, Male) 中，ESPN的隱向量是不同的($V_{ESPN, A}$和 $V_{ESPN, G}$)&lt;/p&gt;
&lt;p&gt;FM的隱向量：$$V_{ESPN}V_{Nike}, V_{ESPN}V_{Male}, V_{Nike}V_{Male}$$&lt;br&gt;FFM的隱向量：$$V_{ESPN, A}V_{Nike, P}, V_{ESPN, G}V_{Male,P}, V_{Nike, G}V_{Male,A}$$&lt;/p&gt;
&lt;h3 id=&#34;結論-1&#34;&gt;&lt;a href=&#34;#結論-1&#34; class=&#34;headerlink&#34; title=&#34;結論&#34;&gt;&lt;/a&gt;結論&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;訓練時間複雜度為$O(kn^2)$&lt;/li&gt;
&lt;li&gt;因為FFM的隱向量限制在一個特徵域，FFM的$k$可以比FM的$k$小&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;公式比較&#34;&gt;&lt;a href=&#34;#公式比較&#34; class=&#34;headerlink&#34; title=&#34;公式比較&#34;&gt;&lt;/a&gt;公式比較&lt;/h2&gt;&lt;p&gt;只比較二次交叉項&lt;/p&gt;
&lt;p&gt;$$FM(v, x) = … + \sum^n_{j_1=1}{\sum^n_{j_2=j_1+1}{\langle v_{j_1}, v_{j_2}\rangle x_{j_1}x_{j_2}}}$$&lt;br&gt;$$FFM(v, x) = … + \sum^n_{j_1=1}{\sum^n_{j_2=j_1+1}{\langle v_{j_1, f_2}, v_{j_2, f_1}\rangle x_{j_1}x_{j_2}}}$$&lt;/p&gt;
&lt;h2 id=&#34;方法比較&#34;&gt;&lt;a href=&#34;#方法比較&#34; class=&#34;headerlink&#34; title=&#34;方法比較&#34;&gt;&lt;/a&gt;方法比較&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;FM：在LR(Logistic Regression)的基礎上，加入特徵交叉 &lt;/li&gt;
&lt;li&gt;FFM：在FM的基礎上，加入特徵域交叉&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;總結&#34;&gt;&lt;a href=&#34;#總結&#34; class=&#34;headerlink&#34; title=&#34;總結&#34;&gt;&lt;/a&gt;總結&lt;/h2&gt;&lt;p&gt;就算Deep Learning盛行，FM也是一個很好的Baseline Model&lt;/p&gt;
&lt;h2 id=&#34;Reference&#34;&gt;&lt;a href=&#34;#Reference&#34; class=&#34;headerlink&#34; title=&#34;Reference&#34;&gt;&lt;/a&gt;Reference&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly93d3cuY3NpZS5udHUuZWR1LnR3L35iOTcwNTMvcGFwZXIvUmVuZGxlMjAxMEZNLnBkZg==&#34;&gt;Rendle, Steffen. “Factorization machines.” 2010 IEEE International conference on data mining. IEEE, 2010&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE3MDEuMDQwOTk=&#34;&gt;Juan, Yuchin, et al. “Field-aware factorization machines for CTR prediction.” Proceedings of the 10th ACM conference on recommender systems. 2016&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNDMxNzQxMDg=&#34;&gt;FM：推薦算法中的瑞士軍刀&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly93bmdhdy5naXRodWIuaW8vZmllbGQtYXdhcmUtZmFjdG9yaXphdGlvbi1tYWNoaW5lcy13aXRoLXhsZWFybi8=&#34;&gt;Field-aware Factorization Machines with xLearn&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cDovL3dlYi5jcy51Y2xhLmVkdS9+Y2hvaHNpZWgvdGVhY2hpbmcvQ1MyNjBfV2ludGVyMjAxOS9sZWN0dXJlMTMucGRm&#34;&gt;http://web.cs.ucla.edu/~chohsieh/teaching/CS260_Winter2019/lecture13.pdf&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84OTYzOTMwNg==&#34;&gt;推薦系統系列（一）：FM理論與實踐&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly95dWxvbmd0c2FpLm1lZGl1bS5jb20vZmFjdG9yaXphdGlvbi1tYWNoaW5lLTYzMTYwYmMyYzA2Yg==&#34;&gt;初探Factorization Machine&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzMyODkyNTE0Mw==&#34;&gt;推薦系統算法FM、FFM使用時，連續性特徵，是直接作為輸入，還是經過離散化後one-hot處理呢？&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80NTMyMzk2NzU=&#34;&gt;FM模型連續特徵離散化&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://gitqwerty777.github.io/recommender-wide-and-deep/</guid>
            <title>Wide And Deep 論文簡介：快思慢想的神經網路版</title>
            <link>http://gitqwerty777.github.io/recommender-wide-and-deep/</link>
            <category>推薦系統</category>
            <category>WideAndDeep</category>
            <category>Google</category>
            <pubDate>Mon, 20 Dec 2021 15:18:44 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;簡介&#34;&gt;&lt;a href=&#34;#簡介&#34; class=&#34;headerlink&#34; title=&#34;簡介&#34;&gt;&lt;/a&gt;簡介&lt;/h2&gt;&lt;p&gt;Wide And Deep 模型由簡單的Wide模型和複雜的Deep模型組成&lt;/p&gt;
&lt;a id=&#34;more&#34;&gt;&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Wide&lt;ul&gt;
&lt;li&gt;Memorization(記憶)&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generalized linear model&lt;/strong&gt;(e.g., Linear Regression Model)&lt;/li&gt;
&lt;li&gt;適合學習稀疏、簡單的規則&lt;ul&gt;
&lt;li&gt;看了 A 電影的使用者經常喜歡看電影 B，這種「因為 A 所以 B」式的規則&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;從歷史資料學習規則(exploit)&lt;/li&gt;
&lt;li&gt;讓模型記住大量的直接且重要的規則，這正是單層的線性模型所擅長的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deep&lt;ul&gt;
&lt;li&gt;Generalization(泛化)&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Embedding-based models&lt;/strong&gt;(e.g., Deep Neural Network)&lt;/li&gt;
&lt;li&gt;適合學習通用、深層的規則&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;學習新的特徵組合(explore)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;合併 Wide and Deep(Jointly Training) &lt;img data-src=&#34;/img/recommend/wide-and-deep.png&#34; alt=&#34;&#34;&gt;&lt;ul&gt;
&lt;li&gt;既能快速處理和記憶大量歷史行為特徵，又具有強大的表達能力&lt;/li&gt;
&lt;li&gt;和 Deep-only 比: 準確率高&lt;/li&gt;
&lt;li&gt;和 Wide-only 比: 更好的泛化規則&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;當user-item matrix非常稀疏時，例如有獨特愛好的users以及很小眾的items，NN很難為users和items學習到有效的embedding。導致over-generalize，並推薦不怎麼相關的物品。此時Memorization就展示了優勢，它可以「記住」這些特殊的特徵組合&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;實作&#34;&gt;&lt;a href=&#34;#實作&#34; class=&#34;headerlink&#34; title=&#34;實作&#34;&gt;&lt;/a&gt;實作&lt;/h2&gt;&lt;h3 id=&#34;Wide&#34;&gt;&lt;a href=&#34;#Wide&#34; class=&#34;headerlink&#34; title=&#34;Wide&#34;&gt;&lt;/a&gt;Wide&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;$y = w^Tx+b$&lt;/li&gt;
&lt;li&gt;Cross product transformation&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/recommend/wide-and-deep-cross-product.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optimizer: Follow-the-regularized-leader (FTRL) + L1 regularization&lt;ul&gt;
&lt;li&gt;FTRL with L1非常注重模型的稀疏性。採用L1 FTRL是想讓Wide部分變得更加稀疏&lt;ul&gt;
&lt;li&gt;但是兩個id類特徵向量進行組合，在維度爆炸的同時，會讓原本已經非常稀疏的multihot特徵向量，變得更加稀疏。正因如此，wide部分的權重數量其實是海量的。為了不把數量如此之巨的權重都搬到線上進行model serving，採用FTRL過濾掉哪些稀疏特徵無疑是非常好的工程經驗&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Wide的輸入特徵較少&lt;ul&gt;
&lt;li&gt;只有已安裝app和瀏覽過的app&lt;/li&gt;
&lt;li&gt;希望能充份發揮Wide記憶能力強的優勢&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Deep&#34;&gt;&lt;a href=&#34;#Deep&#34; class=&#34;headerlink&#34; title=&#34;Deep&#34;&gt;&lt;/a&gt;Deep&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;特徵(節錄)&lt;ul&gt;
&lt;li&gt;用戶特徵&lt;ul&gt;
&lt;li&gt;年齡、國家、語言&lt;/li&gt;
&lt;li&gt;行為特徵&lt;ul&gt;
&lt;li&gt;已安裝App個數&lt;/li&gt;
&lt;li&gt;已安裝的App&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;情境特徵&lt;ul&gt;
&lt;li&gt;使用裝置&lt;/li&gt;
&lt;li&gt;目前時間(星期，小時)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;App特徵&lt;ul&gt;
&lt;li&gt;發佈時間&lt;/li&gt;
&lt;li&gt;下載數&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;候選App&lt;/li&gt;
&lt;li&gt;部份特徵有做embedding(Wide完全沒有)&lt;ul&gt;
&lt;li&gt;32 dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optimizer: AdaGrad&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;Jointly-Training&#34;&gt;&lt;a href=&#34;#Jointly-Training&#34; class=&#34;headerlink&#34; title=&#34;Jointly Training&#34;&gt;&lt;/a&gt;Jointly Training&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;同時更新Wide和Deep的權重&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/recommend/wide-and-deep-joint-train.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;結構圖&lt;ul&gt;
&lt;li&gt;&lt;img data-src=&#34;/img/recommend/wide-and-deep-features.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;結果&#34;&gt;&lt;a href=&#34;#結果&#34; class=&#34;headerlink&#34; title=&#34;結果&#34;&gt;&lt;/a&gt;結果&lt;/h2&gt;&lt;p&gt;實際用在 Google Play Store App 推薦&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&#34;/img/recommend/wide-and-deep-exp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep雖然離線結果較差，但實際結果仍比Wide好&lt;ul&gt;
&lt;li&gt;深層模型有學習到使用者的隱含喜好，而非直接記憶規則&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;心得&#34;&gt;&lt;a href=&#34;#心得&#34; class=&#34;headerlink&#34; title=&#34;心得&#34;&gt;&lt;/a&gt;心得&lt;/h2&gt;&lt;p&gt;這就是&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly93d3cubWFuYWdlcnRvZGF5LmNvbS50dy9hcnRpY2xlcy92aWV3LzUwOTA1Pw==&#34;&gt;快思慢想&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;的神經網路版&lt;/p&gt;
&lt;p&gt;Wide處理簡單的規則且省力，Deep處理複雜的規則但費力&lt;/p&gt;
&lt;p&gt;和純粹的deep learning相比，適合需要記憶大量簡易規則的情境。如App推薦中，有安裝A就推薦B&lt;/p&gt;
&lt;p&gt;Wide and Deep是一個架構，Wide模型和Deep模型可以為任意實作，所以衍生出許多變形，如DeepFM, Deep and Cross等&lt;/p&gt;
&lt;!--
deep的效率跟不上，可以固定住deep，對wide進行online learning來增強記憶性。
非常贊 跟我們的討論結果基本一致，deep部分做batch update保證准確性和充足表達能力，wide部分做online learning保證實效性。

用戶-物品互動太少 → over-generalize
wide部分的引入是為瞭解決 niche items的問題，對於很長尾的物品，dense features是沒法學到什麼東西的

However,deep neural networks with embeddings can over-generalize
and recommend less relevant items when the user-item inter-
actions are sparse and high-rank.
當user-item matrix非常稀疏時，例如有和獨特愛好的users以及很小眾的items，NN很難為users和items學習到有效的embedding。這種情況下，大部分user-item應該是沒有關聯的，但dense embedding 的方法還是可以得到對所有 user-item pair 的非零預測，因此導致 over-generalize並推薦不怎麼相關的物品。此時Memorization就展示了優勢，它可以“記住”這些特殊的特徵組合。
https://en.wikipedia.org/wiki/Rank_(linear_algebra)
--&gt;

&lt;h2 id=&#34;Reference&#34;&gt;&lt;a href=&#34;#Reference&#34; class=&#34;headerlink&#34; title=&#34;Reference&#34;&gt;&lt;/a&gt;Reference&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE2MDYuMDc3OTI=&#34;&gt;Cheng, Heng-Tze, et al. “Wide &amp;amp; deep learning for recommender systems.” Proceedings of the 1st workshop on deep learning for recommender systems. 2016.&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9tZWRpdW0uY29tL2RhdGEtc2NpZW50aXN0cy1wbGF5Z3JvdW5kL3dpZGUtZGVlcCVFNiVBOCVBMSVFNSU5RSU4Qi0lRTYlOEUlQTglRTglOTYlQTYlRTclQjMlQkIlRTclQjUlQjEtJUU1JThFJTlGJUU3JTkwJTg2LThiYWRhY2Y3NzdmMw==&#34;&gt;https://medium.com/data-scientists-playground/wide-deep%E6%A8%A1%E5%9E%8B-%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1-%E5%8E%9F%E7%90%86-8badacf777f3&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xNDI5NTg4MzQ=&#34;&gt;https://zhuanlan.zhihu.com/p/142958834&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81MzM2MTUxOQ==&#34;&gt;https://zhuanlan.zhihu.com/p/53361519&lt;i class=&#34;fa fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
 ]]></description>
        </item>
    </channel>
</rss>
