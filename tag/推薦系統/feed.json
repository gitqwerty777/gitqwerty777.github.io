{
    "version": "https://jsonfeed.org/version/1",
    "title": "QWERTY • All posts by \"推薦系統\" tag",
    "description": "Programming | Computer Science | Thought",
    "home_page_url": "http://gitqwerty777.github.io",
    "items": [
        {
            "id": "http://gitqwerty777.github.io/factorization-machines/",
            "url": "http://gitqwerty777.github.io/factorization-machines/",
            "title": "Factorization Machines(FM) 和 Field-Aware Factorization Machine(FFM)：推薦系統中的瑞士軍刀",
            "date_published": "2022-01-21T03:11:11.000Z",
            "content_html": "<h2 id=\"Factorization-Machines-FM\"><a href=\"#Factorization-Machines-FM\" class=\"headerlink\" title=\"Factorization Machines(FM)\"></a>Factorization Machines(FM)</h2><ul>\n<li>SVM<ul>\n<li>難以在稀疏資料中學習</li>\n</ul>\n</li>\n<li>Factorization Models(如Matrix Factorization)<ul>\n<li>擴展性低：需要特定的輸入格式</li>\n</ul>\n</li>\n</ul>\n<p>FM：克服SVM和Factorization Models的缺點</p>\n<ul>\n<li>可在稀疏資料中學習</li>\n<li>輸入資料可擴展</li>\n<li><strong>訓練時間為線性複雜度</strong></li>\n</ul>\n<h3 id=\"理論\"><a href=\"#理論\" class=\"headerlink\" title=\"理論\"></a>理論</h3><p>FM將權重 $w_{ij}$ 設為兩個長度為k的<strong>隱向量</strong>$V_i, V_j$的<strong>內積</strong>，表示為$\\langle V_i, V_j \\rangle$</p>\n<p><img data-src=\"/img/recommend/fm-formula.png\" alt=\"2維的FM公式\"></p>\n<ol>\n<li>$w_0$​是bias</li>\n<li>$w_i​$是特徵$i$的一維權重</li>\n<li>$w_{i,j}$​是特徵$i$和特徵$j$的二次交叉權重</li>\n</ol>\n<ul>\n<li>隱向量長度$k$為hyperparameter</li>\n<li>FM將權重矩陣分解為隱向量的內積，破壞了權重的獨立性，所以在稀疏資料中仍能學習<ol>\n<li>已知一正定矩陣$W$，必存在$V$使$W=VV^t$</li>\n<li>權重矩陣$W$必為正定</li>\n<li>所以$W$必能分解成隱向量矩陣$V$乘自身的轉置</li>\n</ol>\n</li>\n<li>原本$W$的大小為$\\frac{n^2}{2}$，改成隱向量$V$之後大小為$kn$，$k$通常不會設很大，明顯減少參數數量<ul>\n<li>限制$k$的大小也能限制FM模型的表達力，泛化能力較好</li>\n</ul>\n</li>\n</ul>\n<p><img data-src=\"/img/recommend/FM-structure.png\" alt=\"結構\"></p>\n<h3 id=\"效率\"><a href=\"#效率\" class=\"headerlink\" title=\"效率\"></a>效率</h3><p><img data-src=\"/img/recommend/fm-time-complexity.png\" alt=\"\"></p>\n<p>整理公式後，Inference的時間複雜度從$O(kn^2)$降到了$O(kn)$，$n$為特徵維度</p>\n<ul>\n<li>第2行公式推導：表示為整個矩陣扣掉對角項再除以2，因為$W$是對稱矩陣</li>\n<li>詳細推導可看<span class=\"exturl\" data-url=\"aHR0cHM6Ly95dWxvbmd0c2FpLm1lZGl1bS5jb20vZmFjdG9yaXphdGlvbi1tYWNoaW5lLTYzMTYwYmMyYzA2Yg==\">這篇<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li>實作上只須計算非0元素的乘積，時間複雜度再下降到$O(km)$，$m$為平均一筆輸入資料中，值非0的特徵數</li>\n</ul>\n<h3 id=\"更新\"><a href=\"#更新\" class=\"headerlink\" title=\"更新\"></a>更新</h3><p>使用gradient descent學習參數<br><img data-src=\"/img/recommend/fm-gradient.png\" alt=\"\"></p>\n<p>$\\sum^n_{j=1}v_{j, f}x_j$可以事先計算，所以每次梯度更新的時間複雜度為$O(1)$</p>\n<p>因此FM的訓練時間複雜度也是$O(km)$</p>\n<h3 id=\"高維度FM\"><a href=\"#高維度FM\" class=\"headerlink\" title=\"高維度FM\"></a>高維度FM</h3><p><img data-src=\"/img/recommend/fm-dway.png\" alt=\"\"></p>\n<p>經過公式簡化(和二維的方法相似)，也可以在線性時間內計算</p>\n<h3 id=\"FM-和-Factorization-Model-SVM-比較\"><a href=\"#FM-和-Factorization-Model-SVM-比較\" class=\"headerlink\" title=\"FM 和 Factorization Model, SVM 比較\"></a>FM 和 Factorization Model, SVM 比較</h3><p>論文中證明了兩件事</p>\n<ol>\n<li>各種Factorization Model為FM的特化</li>\n<li>FM可以解決SVM在稀疏資料中無法成功訓練的問題</li>\n</ol>\n<p>詳細證明看不懂，略過</p>\n<h3 id=\"結論\"><a href=\"#結論\" class=\"headerlink\" title=\"結論\"></a>結論</h3><ul>\n<li>FM速度快、容易實作，於2012~14年為業界主流模型</li>\n<li>FM產生的隱向量可視為一種embedding<ul>\n<li>所以拿user的隱向量找相似隱向量的item，就是一個簡易且快速的推薦方法</li>\n</ul>\n</li>\n<li>FM適合類型特徵(離散)而非數值特徵(連續)，因為<ul>\n<li>類型特徵可有多個隱向量，而數值特徵只有一個</li>\n<li>數值特徵不應使用同一個隱向量，如10歲和40歲</li>\n<li>FM速度和非零特徵數有關，數值特徵類型化後不影響訓練速度</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Field-aware-factorization-machines-FFM\"><a href=\"#Field-aware-factorization-machines-FFM\" class=\"headerlink\" title=\"Field-aware factorization machines(FFM)\"></a>Field-aware factorization machines(FFM)</h2><ul>\n<li>FM：一個特徵有<strong>一個</strong>隱向量</li>\n<li>FFM：一個特徵有<strong>一組</strong>隱向量<ul>\n<li>每個隱向量對應不同的<strong>特徵域</strong></li>\n<li>特徵域通常為一群代表相同性質的特徵，如one-hot特徵</li>\n</ul>\n</li>\n</ul>\n<p><img data-src=\"/img/recommend/ffm-formula.png\" alt=\"\"></p>\n<h3 id=\"範例\"><a href=\"#範例\" class=\"headerlink\" title=\"範例\"></a>範例</h3><ul>\n<li>出版商特徵域(P): ESPN, Vogue, and NBC</li>\n<li>廣告商特徵域(A): Nike, Gucci, and Adidas</li>\n<li>消費者性別特徵域(G): Male, Female</li>\n</ul>\n<p>在(ESPN, Nike) 和 (ESPN, Male) 中，ESPN的隱向量是不同的($V_{ESPN, A}$和 $V_{ESPN, G}$)</p>\n<p>FM的隱向量：$$V_{ESPN}V_{Nike}, V_{ESPN}V_{Male}, V_{Nike}V_{Male}$$<br>FFM的隱向量：$$V_{ESPN, A}V_{Nike, P}, V_{ESPN, G}V_{Male,P}, V_{Nike, G}V_{Male,A}$$</p>\n<h3 id=\"結論-1\"><a href=\"#結論-1\" class=\"headerlink\" title=\"結論\"></a>結論</h3><ul>\n<li>訓練時間複雜度為$O(kn^2)$</li>\n<li>因為FFM的隱向量限制在一個特徵域，FFM的$k$可以比FM的$k$小</li>\n</ul>\n<h2 id=\"公式比較\"><a href=\"#公式比較\" class=\"headerlink\" title=\"公式比較\"></a>公式比較</h2><p>只比較二次交叉項</p>\n<p>$$FM(v, x) = … + \\sum^n_{j_1=1}{\\sum^n_{j_2=j_1+1}{\\langle v_{j_1}, v_{j_2}\\rangle x_{j_1}x_{j_2}}}$$<br>$$FFM(v, x) = … + \\sum^n_{j_1=1}{\\sum^n_{j_2=j_1+1}{\\langle v_{j_1, f_2}, v_{j_2, f_1}\\rangle x_{j_1}x_{j_2}}}$$</p>\n<h2 id=\"方法比較\"><a href=\"#方法比較\" class=\"headerlink\" title=\"方法比較\"></a>方法比較</h2><ul>\n<li>FM：在LR(Logistic Regression)的基礎上，加入特徵交叉 </li>\n<li>FFM：在FM的基礎上，加入特徵域交叉</li>\n</ul>\n<h2 id=\"總結\"><a href=\"#總結\" class=\"headerlink\" title=\"總結\"></a>總結</h2><p>就算Deep Learning盛行，FM也是一個很好的Baseline Model</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY3NpZS5udHUuZWR1LnR3L35iOTcwNTMvcGFwZXIvUmVuZGxlMjAxMEZNLnBkZg==\">Rendle, Steffen. “Factorization machines.” 2010 IEEE International conference on data mining. IEEE, 2010<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE3MDEuMDQwOTk=\">Juan, Yuchin, et al. “Field-aware factorization machines for CTR prediction.” Proceedings of the 10th ACM conference on recommender systems. 2016<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNDMxNzQxMDg=\">FM：推薦算法中的瑞士軍刀<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93bmdhdy5naXRodWIuaW8vZmllbGQtYXdhcmUtZmFjdG9yaXphdGlvbi1tYWNoaW5lcy13aXRoLXhsZWFybi8=\">Field-aware Factorization Machines with xLearn<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cDovL3dlYi5jcy51Y2xhLmVkdS9+Y2hvaHNpZWgvdGVhY2hpbmcvQ1MyNjBfV2ludGVyMjAxOS9sZWN0dXJlMTMucGRm\">http://web.cs.ucla.edu/~chohsieh/teaching/CS260_Winter2019/lecture13.pdf<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84OTYzOTMwNg==\">推薦系統系列（一）：FM理論與實踐<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly95dWxvbmd0c2FpLm1lZGl1bS5jb20vZmFjdG9yaXphdGlvbi1tYWNoaW5lLTYzMTYwYmMyYzA2Yg==\">初探Factorization Machine<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzMyODkyNTE0Mw==\">推薦系統算法FM、FFM使用時，連續性特徵，是直接作為輸入，還是經過離散化後one-hot處理呢？<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80NTMyMzk2NzU=\">FM模型連續特徵離散化<i class=\"fa fa-external-link-alt\"></i></span></li>\n</ul>\n",
            "tags": [
                "推薦系統",
                "FM",
                "FFM",
                "SVM",
                "Embedding"
            ]
        },
        {
            "id": "http://gitqwerty777.github.io/recommender-wide-and-deep/",
            "url": "http://gitqwerty777.github.io/recommender-wide-and-deep/",
            "title": "Wide And Deep 論文簡介：快思慢想的神經網路版",
            "date_published": "2021-12-20T07:18:44.000Z",
            "content_html": "<h2 id=\"簡介\"><a href=\"#簡介\" class=\"headerlink\" title=\"簡介\"></a>簡介</h2><p>Wide And Deep 模型由簡單的Wide模型和複雜的Deep模型組成</p>\n<a id=\"more\"></a>\n\n<ul>\n<li>Wide<ul>\n<li>Memorization(記憶)<ul>\n<li><strong>Generalized linear model</strong>(e.g., Linear Regression Model)</li>\n<li>適合學習稀疏、簡單的規則<ul>\n<li>看了 A 電影的使用者經常喜歡看電影 B，這種「因為 A 所以 B」式的規則</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>從歷史資料學習規則(exploit)</li>\n<li>讓模型記住大量的直接且重要的規則，這正是單層的線性模型所擅長的</li>\n</ul>\n</li>\n<li>Deep<ul>\n<li>Generalization(泛化)<ul>\n<li><strong>Embedding-based models</strong>(e.g., Deep Neural Network)</li>\n<li>適合學習通用、深層的規則</li>\n</ul>\n</li>\n<li>學習新的特徵組合(explore)</li>\n</ul>\n</li>\n<li>合併 Wide and Deep(Jointly Training) <img data-src=\"/img/recommend/wide-and-deep.png\" alt=\"\"><ul>\n<li>既能快速處理和記憶大量歷史行為特徵，又具有強大的表達能力</li>\n<li>和 Deep-only 比: 準確率高</li>\n<li>和 Wide-only 比: 更好的泛化規則</li>\n</ul>\n</li>\n<li>當user-item matrix非常稀疏時，例如有獨特愛好的users以及很小眾的items，NN很難為users和items學習到有效的embedding。導致over-generalize，並推薦不怎麼相關的物品。此時Memorization就展示了優勢，它可以「記住」這些特殊的特徵組合</li>\n</ul>\n<h2 id=\"實作\"><a href=\"#實作\" class=\"headerlink\" title=\"實作\"></a>實作</h2><h3 id=\"Wide\"><a href=\"#Wide\" class=\"headerlink\" title=\"Wide\"></a>Wide</h3><ul>\n<li>$y = w^Tx+b$</li>\n<li>Cross product transformation<ul>\n<li><img data-src=\"/img/recommend/wide-and-deep-cross-product.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>Optimizer: Follow-the-regularized-leader (FTRL) + L1 regularization<ul>\n<li>FTRL with L1非常注重模型的稀疏性。採用L1 FTRL是想讓Wide部分變得更加稀疏<ul>\n<li>但是兩個id類特徵向量進行組合，在維度爆炸的同時，會讓原本已經非常稀疏的multihot特徵向量，變得更加稀疏。正因如此，wide部分的權重數量其實是海量的。為了不把數量如此之巨的權重都搬到線上進行model serving，採用FTRL過濾掉哪些稀疏特徵無疑是非常好的工程經驗</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Wide的輸入特徵較少<ul>\n<li>只有已安裝app和瀏覽過的app</li>\n<li>希望能充份發揮Wide記憶能力強的優勢</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Deep\"><a href=\"#Deep\" class=\"headerlink\" title=\"Deep\"></a>Deep</h3><ul>\n<li>特徵(節錄)<ul>\n<li>用戶特徵<ul>\n<li>年齡、國家、語言</li>\n<li>行為特徵<ul>\n<li>已安裝App個數</li>\n<li>已安裝的App</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>情境特徵<ul>\n<li>使用裝置</li>\n<li>目前時間(星期，小時)</li>\n</ul>\n</li>\n<li>App特徵<ul>\n<li>發佈時間</li>\n<li>下載數</li>\n</ul>\n</li>\n<li>候選App</li>\n<li>部份特徵有做embedding(Wide完全沒有)<ul>\n<li>32 dimension</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Optimizer: AdaGrad</li>\n</ul>\n<h3 id=\"Jointly-Training\"><a href=\"#Jointly-Training\" class=\"headerlink\" title=\"Jointly Training\"></a>Jointly Training</h3><ul>\n<li>同時更新Wide和Deep的權重<ul>\n<li><img data-src=\"/img/recommend/wide-and-deep-joint-train.png\" alt=\"\"></li>\n</ul>\n</li>\n<li>結構圖<ul>\n<li><img data-src=\"/img/recommend/wide-and-deep-features.png\" alt=\"\"></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"結果\"><a href=\"#結果\" class=\"headerlink\" title=\"結果\"></a>結果</h2><p>實際用在 Google Play Store App 推薦</p>\n<p><img data-src=\"/img/recommend/wide-and-deep-exp.png\" alt=\"\"></p>\n<ul>\n<li>Deep雖然離線結果較差，但實際結果仍比Wide好<ul>\n<li>深層模型有學習到使用者的隱含喜好，而非直接記憶規則</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"心得\"><a href=\"#心得\" class=\"headerlink\" title=\"心得\"></a>心得</h2><p>這就是<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cubWFuYWdlcnRvZGF5LmNvbS50dy9hcnRpY2xlcy92aWV3LzUwOTA1Pw==\">快思慢想<i class=\"fa fa-external-link-alt\"></i></span>的神經網路版</p>\n<p>Wide處理簡單的規則且省力，Deep處理複雜的規則但費力</p>\n<p>和純粹的deep learning相比，適合需要記憶大量簡易規則的情境。如App推薦中，有安裝A就推薦B</p>\n<p>Wide and Deep是一個架構，Wide模型和Deep模型可以為任意實作，所以衍生出許多變形，如DeepFM, Deep and Cross等</p>\n<!--\ndeep的效率跟不上，可以固定住deep，對wide進行online learning來增強記憶性。\n非常贊 跟我們的討論結果基本一致，deep部分做batch update保證准確性和充足表達能力，wide部分做online learning保證實效性。\n\n用戶-物品互動太少 → over-generalize\nwide部分的引入是為瞭解決 niche items的問題，對於很長尾的物品，dense features是沒法學到什麼東西的\n\nHowever,deep neural networks with embeddings can over-generalize\nand recommend less relevant items when the user-item inter-\nactions are sparse and high-rank.\n當user-item matrix非常稀疏時，例如有和獨特愛好的users以及很小眾的items，NN很難為users和items學習到有效的embedding。這種情況下，大部分user-item應該是沒有關聯的，但dense embedding 的方法還是可以得到對所有 user-item pair 的非零預測，因此導致 over-generalize並推薦不怎麼相關的物品。此時Memorization就展示了優勢，它可以“記住”這些特殊的特徵組合。\nhttps://en.wikipedia.org/wiki/Rank_(linear_algebra)\n-->\n\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE2MDYuMDc3OTI=\">Cheng, Heng-Tze, et al. “Wide &amp; deep learning for recommender systems.” Proceedings of the 1st workshop on deep learning for recommender systems. 2016.<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tZWRpdW0uY29tL2RhdGEtc2NpZW50aXN0cy1wbGF5Z3JvdW5kL3dpZGUtZGVlcCVFNiVBOCVBMSVFNSU5RSU4Qi0lRTYlOEUlQTglRTglOTYlQTYlRTclQjMlQkIlRTclQjUlQjEtJUU1JThFJTlGJUU3JTkwJTg2LThiYWRhY2Y3NzdmMw==\">https://medium.com/data-scientists-playground/wide-deep%E6%A8%A1%E5%9E%8B-%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1-%E5%8E%9F%E7%90%86-8badacf777f3<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xNDI5NTg4MzQ=\">https://zhuanlan.zhihu.com/p/142958834<i class=\"fa fa-external-link-alt\"></i></span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81MzM2MTUxOQ==\">https://zhuanlan.zhihu.com/p/53361519<i class=\"fa fa-external-link-alt\"></i></span></li>\n</ul>\n",
            "tags": [
                "推薦系統",
                "WideAndDeep",
                "Google"
            ]
        }
    ]
}