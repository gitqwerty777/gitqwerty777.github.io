<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/img/favicon.webp">
  <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon.webp">
  <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon.webp">
  <link rel="mask-icon" href="/img/favicon.webp" color="#222">
  <meta name="google-site-verification" content="45plYlJRhxb-g8Tl8seizYgih_JUsmcJRH6oJHplkj0">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+TC:300,300italic,400,400italic,700,700italic%7CCormorant+Garamond:300,300italic,400,400italic,700,700italic%7CNoto+Serif+TC:300,300italic,400,400italic,700,700italic%7CFira+Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/green/pace-theme-barber-shop.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"gitqwerty777.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":true,"version":"8.12.1","exturl":true,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"show_result":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInDown","sidebar":"fadeInRight"}},"prism":false,"i18n":{"placeholder":"搜尋...","empty":"我們無法找到任何有關 ${query} 的搜索結果","hits_time":"${hits} 找到 ${time} 個結果","hits":"找到 ${hits} 個結果"},"path":"/search.json","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false}}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/config.min.js"></script>

    <meta name="description" content="Chap08 Syntax and GrammarsGrammar      represent certain knowledges of what we know about a language General criteria linguistic naturalness mathematical power computational effectiveness    Context-f">
<meta property="og:type" content="article">
<meta property="og:title" content="自然語言處理(下)">
<meta property="og:url" content="http://gitqwerty777.github.io/natural-language-processing2/index.html">
<meta property="og:site_name" content="QWERTY">
<meta property="og:description" content="Chap08 Syntax and GrammarsGrammar      represent certain knowledges of what we know about a language General criteria linguistic naturalness mathematical power computational effectiveness    Context-f">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/np-parse.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/modelclass.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/index-example.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/index-grammar.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/head-np.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/dependency-parse.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/top-down.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/cnf-transform.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/CKY-table2.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/CKY-table3.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/CKY-ex.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/CKY-algo.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/dot-a.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/dot-b.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/chart-struct.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/chart-initialize.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/chartchart-init.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/chart-fund.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/chart-bottom.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/after-assump.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/pm-ex.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/pm-ex2.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/prg-sink.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/prg-graph.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/prg-bf.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/prg-bf2.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/inside-induction.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/inside-grid.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/outside-graph.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/inout.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/parse-probability.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/outside-forward.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/inoutagain.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/pi.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/pi2.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/pi3.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/pi4.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/pi5.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/pi6.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/parse.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/depend.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/depend-ex.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/data-driven.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/parse-algo.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/local-feature-example.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/local-train-with-parse.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/arc-eager-example.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/IE.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/syntax-semantic.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/FOL-better.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/complicate-NP.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/complicate-NP-induction.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/ambiguity-of-same-POS.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/store.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/retrieve.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/set-based-model.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/uncategories.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/categories.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/wordnet-hierarchy.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/thematic-roles.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/bayesian-decision.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/bayesian-decision2.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/bayesian-classifier.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/yarowsky-algo.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/yarowsky-algo2.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/yarowsky-algo3.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/yarowsky-result.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/subcategorization.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/brent-null-hypothesis.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/likelihood-ratio-vn.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/attach-example.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/selection-strength.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/selectional-association.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/selectional-example.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/similarity-measure.webp">
<meta property="og:image" content="http://www.wannabehacks.co.uk/images/Inverted_pyramid_in_comprehensive_form.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/discourse-relation.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/chinese-coherence-relation.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/reference-resolution.webp">
<meta property="og:image" content="http://gitqwerty777.github.io/img/NLP/givenness-hierarchy.webp">
<meta property="article:published_time" content="2015-05-01T04:37:47.000Z">
<meta property="article:modified_time" content="2015-05-01T04:37:47.000Z">
<meta property="article:author" content="qwerty">
<meta property="article:tag" content="機器學習">
<meta property="article:tag" content="自然語言處理">
<meta property="article:tag" content="統計">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://gitqwerty777.github.io/img/NLP/np-parse.webp">


<link rel="canonical" href="http://gitqwerty777.github.io/natural-language-processing2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-TW","comments":true,"permalink":"http://gitqwerty777.github.io/natural-language-processing2/","path":"natural-language-processing2/","title":"自然語言處理(下)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>自然語言處理(下) | QWERTY</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-51310670-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-51310670-1","only_pageview":false}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/third-party/analytics/google-analytics.min.js"></script>






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="QWERTY" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">QWERTY</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Hello World!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首頁</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>文章<span class="badge">60</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>關於</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fas fa-rss-square fa-fw"></i>RSS</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>標籤<span class="badge">142</span></a></li><li class="menu-item menu-item-ptt標籤雲"><span class="exturl" data-url="aHR0cHM6Ly9xd2VydHk3NzcubWUvcHR0LXRhZy1jbG91ZC8="><i class="fas fa-hashtag fa-fw"></i>PTT標籤雲</span></li><li class="menu-item menu-item-支語警察"><a href="/foreign-terms-police" rel="section"><i class="fas fa-language fa-fw"></i>支語警察</a></li><li class="menu-item menu-item-英文聊天機器人"><span class="exturl" data-url="aHR0cHM6Ly9jaGF0Ym90LnF3ZXJ0eTc3Ny5tZQ=="><i class="fas fa-comment-dots fa-fw"></i>英文聊天機器人</span></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜尋
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜尋..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Chap08-Syntax-and-Grammars"><span class="nav-number">1.</span> <span class="nav-text">Chap08 Syntax and Grammars</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Treebanks-and-headfinding"><span class="nav-number">1.1.</span> <span class="nav-text">Treebanks and headfinding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dependency-Grammars"><span class="nav-number">1.2.</span> <span class="nav-text">Dependency Grammars</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chap09-Syntactic-Parsing"><span class="nav-number">2.</span> <span class="nav-text">Chap09 Syntactic Parsing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CKY-bottom-up"><span class="nav-number">2.1.</span> <span class="nav-text">CKY(bottom-up)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Earley"><span class="nav-number">2.2.</span> <span class="nav-text">Earley</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Full-Syntactic-Parsing"><span class="nav-number">2.3.</span> <span class="nav-text">Full Syntactic Parsing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chap10-Statistical-Parsing"><span class="nav-number">3.</span> <span class="nav-text">Chap10 Statistical Parsing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chap11-Dependency-Parsing"><span class="nav-number">4.</span> <span class="nav-text">Chap11 Dependency Parsing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Graph-based-dependency-parsing-models"><span class="nav-number">4.1.</span> <span class="nav-text">Graph-based dependency parsing models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transition-based-model"><span class="nav-number">4.2.</span> <span class="nav-text">Transition-based model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#parsing-processes"><span class="nav-number">4.2.1.</span> <span class="nav-text">parsing processes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Decoding-algorithms"><span class="nav-number">4.2.2.</span> <span class="nav-text">Decoding algorithms</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Score-Models"><span class="nav-number">4.2.3.</span> <span class="nav-text">Score Models</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chap12-Semantic-Representation-and-Computational-Semantics"><span class="nav-number">5.</span> <span class="nav-text">Chap12 Semantic Representation and Computational Semantics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Information-Extraction"><span class="nav-number">5.1.</span> <span class="nav-text">Information Extraction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Compositional-Semantics"><span class="nav-number">5.2.</span> <span class="nav-text">Compositional Semantics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#FOL"><span class="nav-number">5.2.1.</span> <span class="nav-text">FOL</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Set-Based-Models"><span class="nav-number">5.3.</span> <span class="nav-text">Set-Based Models</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chap13-Lexical-Semantics"><span class="nav-number">6.</span> <span class="nav-text">Chap13 Lexical Semantics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#WSD"><span class="nav-number">6.1.</span> <span class="nav-text">WSD</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Naive-Bayes"><span class="nav-number">6.1.1.</span> <span class="nav-text">Naive Bayes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dictionary-Based-Disambiguation"><span class="nav-number">6.1.2.</span> <span class="nav-text">Dictionary-Based Disambiguation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Unsupervised-Disambiguation"><span class="nav-number">6.1.3.</span> <span class="nav-text">Unsupervised Disambiguation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lexical-Acquisition"><span class="nav-number">6.2.</span> <span class="nav-text">Lexical Acquisition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Verb-Subcategorization"><span class="nav-number">6.2.1.</span> <span class="nav-text">Verb Subcategorization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chap14-Computational-Discourse"><span class="nav-number">7.</span> <span class="nav-text">Chap14 Computational Discourse</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reference-Resolution"><span class="nav-number">7.1.</span> <span class="nav-text">Reference Resolution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%83%E8%80%83%E8%B3%87%E6%96%99"><span class="nav-number">8.</span> <span class="nav-text">參考資料</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="qwerty"
      src="/img/qwerty.webp">
  <p class="site-author-name" itemprop="name">qwerty</p>
  <div class="site-description" itemprop="description">Programming | Computer Science | Thought</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">60</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分類</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">142</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cDovL2dpdGh1Yi5jb20vZ2l0cXdlcnR5Nzc3" title="GitHub → http:&#x2F;&#x2F;github.com&#x2F;gitqwerty777"><i class="fab fa-github fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmdvb2hjbDc3N0BnbWFpbC5jb20=" title="E-Mail → mailto:goohcl777@gmail.com"><i class="fa fa-envelope fa-fw"></i></span>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpoX1RX"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/big/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <span class="exturl" data-url="aHR0cHM6Ly9xd2VydHk3NzcubWU=" title="https:&#x2F;&#x2F;qwerty777.me">My Main Page</span>
        </li>
        <li class="links-of-blogroll-item">
          <span class="exturl" data-url="aHR0cDovL3F3ZXJ0eTc3Ny5tZS9saWZlLw==" title="http:&#x2F;&#x2F;qwerty777.me&#x2F;life&#x2F;">My Second Blog -- Wysiwyg</span>
        </li>
    </ul>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="回到頂端">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dpdHF3ZXJ0eTc3Nw==" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="http://gitqwerty777.github.io/natural-language-processing2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/qwerty.webp">
      <meta itemprop="name" content="qwerty">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QWERTY">
      <meta itemprop="description" content="Programming | Computer Science | Thought">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="自然語言處理(下) | QWERTY">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          自然語言處理(下)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2015-05-01 12:37:47" itemprop="dateCreated datePublished" datetime="2015-05-01T12:37:47+08:00">2015-05-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分類於</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">筆記</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="文章字數">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">文章字數：</span>
      <span>32k</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="Chap08-Syntax-and-Grammars"><a href="#Chap08-Syntax-and-Grammars" class="headerlink" title="Chap08 Syntax and Grammars"></a>Chap08 Syntax and Grammars</h2><p>Grammar    </p>
<ul>
<li>represent certain knowledges of what we know about a language</li>
<li>General criteria<ul>
<li>linguistic naturalness</li>
<li>mathematical power</li>
<li>computational effectiveness</li>
</ul>
</li>
</ul>
<p>Context-free grammars(CFG)</p>
<ul>
<li>Alias<ul>
<li>Phrase structure grammars</li>
<li>Backus-Naur form</li>
</ul>
</li>
<li>More powerful than finite state machine</li>
<li>Rules <ul>
<li>Terminals <ul>
<li>words</li>
</ul>
</li>
<li>Non-terminals <ul>
<li>Noun phrase, Verb phrase …</li>
</ul>
</li>
<li>Generate strings in the language</li>
<li>Reject strings not in the language  </li>
<li>LHS → RHS<ul>
<li>LHS: Non-terminals </li>
<li>RHS: Non-terminals or Terminals</li>
</ul>
</li>
<li>Context Free<ul>
<li>probability of a subtree does not depend on words not dominated by the subtree(subtree出現的機率和上下文無關)</li>
</ul>
</li>
</ul>
</li>
</ul>
<span id="more"></span>

<p>Evaluation  </p>
<ol>
<li>Does it undergenerate?<ul>
<li>rules cannot completely explain language</li>
<li>not a problem if the goal is to produce a language</li>
</ul>
</li>
<li>Does it overgenerate?<ul>
<li>rules overly explain the language</li>
<li>not a problem if the goal is to recognize or understand well-formed(correct) language</li>
</ul>
</li>
<li>Does it assign appropriate structures to the strings that it generates?</li>
</ol>
<p>Parsing  </p>
<ul>
<li>take a string and a grammar</li>
<li>assigning trees that covers all and only words in input strings</li>
<li>return parse tree for that string</li>
</ul>
<p>English Grammar Fragment  </p>
<ul>
<li>Sentences</li>
<li>Noun Phrases<ul>
<li>Ex. NP → det Nominal</li>
<li><strong>head: central criticial noun in NP</strong><ul>
<li>important in statistical parsing</li>
<li>after det(冠詞), before pp(介系詞片語) <img data-src="/img/NLP/np-parse.webp"></li>
</ul>
</li>
<li>Agreement<ul>
<li>a part of overgenerate</li>
<li>This flight(○)</li>
<li>This flights(×)</li>
</ul>
</li>
</ul>
</li>
<li>Verb Phrases<ul>
<li>head verb with arguments</li>
<li>Subcategorization(分類)<ul>
<li>categorize according to VP rules</li>
<li>a part of overgenerate</li>
<li>Prefer<ul>
<li>I prefer [to leave earlier]TO-VP</li>
</ul>
</li>
<li>Told<ul>
<li>I was told [United has a flight]S</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Overgenerate Solution<br>    - transform into multiple rules<br>        - NP → Single_Det Single_Nominal<br>        - NP → 複數_Det 複數_Nominal<br>        - Will generate a lot of rules!<br>    - out of CFG framework<br>        - Chomsky hierarchy of languages <img data-src="/img/NLP/modelclass.webp"></p>
<p><span class="exturl" data-url="aHR0cDovL3poLndpa2lwZWRpYS5vcmcvd2lraS8lRTklOTklODQlRTYlQTAlODclRTglQUYlQUQlRTglQTglODA=">Indexed Grammar<i class="fa fa-external-link-alt"></i></span>  </p>
<ul>
<li>Indexed grammars and languages problem <img data-src="/img/NLP/index-example.webp"> </li>
<li>recognized by nested stack automata <img data-src="/img/NLP/index-grammar.webp"></li>
</ul>
<h3 id="Treebanks-and-headfinding"><a href="#Treebanks-and-headfinding" class="headerlink" title="Treebanks and headfinding"></a>Treebanks and headfinding</h3><p>critical to the development of statistical parsers</p>
<p>Treebanks  </p>
<ul>
<li>corpora with parse trees<ul>
<li>created by automatic parser and human annotators</li>
</ul>
</li>
<li>Ex. Penn Treebank</li>
<li>Grammar<ul>
<li>Treebanks implicitly define a grammar<ul>
<li>Simply make all subtrees fit the rules</li>
</ul>
</li>
<li>parse tree tend to be very flat to avoid recursion<ul>
<li>Penn Treebank has ~4500 different rules for VPs</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Head Finding  </p>
<ul>
<li>use tree traversal rules specific to each non-terminal in the grammar</li>
<li>先向右再向左 <img data-src="/img/NLP/head-np.webp"><!--重要--></li>
</ul>
<h3 id="Dependency-Grammars"><a href="#Dependency-Grammars" class="headerlink" title="Dependency Grammars"></a>Dependency Grammars</h3><ul>
<li>every possible parse is a tree <img data-src="/img/NLP/dependency-parse.webp"><ul>
<li>every node is a word </li>
<li>every link is dependency relations between words</li>
</ul>
</li>
<li>Advantage<ul>
<li>Deals well with long-distance word order languages <ul>
<li>structure is flexible</li>
</ul>
</li>
<li>Parsing is much faster than CFG</li>
<li>Tree can be used by later applications</li>
</ul>
</li>
<li>Approaches<!--重要--><ul>
<li>Optimization-based approaches <ul>
<li>search for the tree that matches some criteria the best</li>
<li>spanning tree algorithms</li>
</ul>
</li>
<li>Shift-reduce approaches<ul>
<li>greedily take actions based on the current word and state</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Summary  </p>
<ul>
<li>Constituency(顧客, words that behave as a single unit) is a key phenomena easily captured with CFG rules<ul>
<li>But agreement and subcategorization make problems</li>
</ul>
</li>
</ul>
<h2 id="Chap09-Syntactic-Parsing"><a href="#Chap09-Syntactic-Parsing" class="headerlink" title="Chap09 Syntactic Parsing"></a>Chap09 Syntactic Parsing</h2><ul>
<li>Top-Down Search <img data-src="/img/NLP/top-down.webp"><ul>
<li>Search trees among possible answers  <!--- But suggests trees that are not consistent with words--></li>
</ul>
</li>
<li>Bottom-Up Parsing<ul>
<li>Only forms trees that can fit the words <!-- global tree may not form answer(S) --></li>
</ul>
</li>
<li>Mixed parsing strategy<ul>
<li>looks like Binomial Search</li>
<li>The number of rules tried at each deicision of the analysis (branching factor)<ul>
<li>top-down parsing: categories of LHS(Left Hand Side) word</li>
<li>bottom-up parsing: categories of left most RHS(Right Hand Side) word<ul>
<li>倒推：從最左邊可以倒推的字開始</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>backtracking methods are doomed because of two inter-related problems  </p>
<ul>
<li>(1)Structural and lexical ambiguity<ul>
<li>PP(介系詞片語) attachment<ul>
<li>PP can attach to [sentences, verb phrases, noun phrases, and adjectival phrases]</li>
</ul>
</li>
<li>coordination<ul>
<li>P and Q or R <ul>
<li>P and (Q or R)</li>
<li>(P and Q) or R</li>
</ul>
</li>
</ul>
</li>
<li>noun-noun compounding<ul>
<li>town widget hammer<ul>
<li>((town widget) hammer)</li>
<li>(town (widget hammer))</li>
</ul>
</li>
</ul>
</li>
<li>Solution<ul>
<li>how to determine the intended structure?</li>
<li>how to store the partial structures?</li>
</ul>
</li>
</ul>
</li>
<li>(2)Shared subproblems<ul>
<li>naïve backtracking will lead to duplicated work(不一定會對，所以會一直backtrack…)</li>
</ul>
</li>
</ul>
<p>Dynamic Programming  </p>
<ul>
<li>Avoid doing repeated work</li>
<li>Solve in polynomial time</li>
<li>approaches<ul>
<li>CKY</li>
<li>Earley</li>
</ul>
</li>
</ul>
<h3 id="CKY-bottom-up"><a href="#CKY-bottom-up" class="headerlink" title="CKY(bottom-up)"></a>CKY(bottom-up)</h3><ul>
<li>transform rules into Chomsky-Normal Form <img data-src="/img/NLP/cnf-transform.webp"></li>
<li>build a table <ul>
<li>A spanning from i to j in the input is in [i,j]</li>
<li>A → BC &#x3D;&#x3D; [i,j] → [i,k] [k,j]</li>
<li>entire string &#x3D; [0, n] <ul>
<li>expected to be S</li>
</ul>
</li>
</ul>
</li>
<li>iterate all possible k <img data-src="/img/NLP/CKY-table2.webp"><ul>
<li>[i,j] &#x3D; [i,i+1]x[i+1, j], [i,i+2]x[i+2,j] ……</li>
</ul>
</li>
<li>fill the table a column at a time, from left to right, bottom to top <img data-src="/img/NLP/CKY-table3.webp"><ul>
<li>Ex. [1,3] &#x3D; [1,2]Det + [2,3] Nomimal, Noun &#x3D; NP</li>
<li>Ex. <img data-src="/img/NLP/CKY-ex.webp"></li>
</ul>
</li>
<li>Algorithm <img data-src="/img/NLP/CKY-algo.webp"></li>
<li>Performance<ul>
<li>a lot of elements unrelated to the answer</li>
<li>can use online search to fill table (from left to right)</li>
</ul>
</li>
</ul>
<h3 id="Earley"><a href="#Earley" class="headerlink" title="Earley"></a>Earley</h3><ul>
<li>parser that exploits chart as data structure</li>
<li>node &#x3D; vertex</li>
<li>arc &#x3D; edge<ul>
<li>active edge: (a) and (b)</li>
<li>inactive edge: (c)</li>
</ul>
</li>
</ul>
<p>decorated grammar  </p>
<ul>
<li>(a) “•” in the first <img data-src="/img/NLP/dot-a.webp"><ul>
<li>• NP VP</li>
<li>A hypothesis has been made, but has not been verified yet</li>
</ul>
</li>
<li>(b) “•” in the middle <img data-src="/img/NLP/dot-b.webp"><ul>
<li>NP • VP</li>
<li>A hypothesis has been partially confirmed</li>
</ul>
</li>
<li>(c) “•” in the last<ul>
<li>NP VP •</li>
<li>A hypothesis has been wholly confirmed</li>
</ul>
</li>
<li>representation of edge <img data-src="/img/NLP/chart-struct.webp"></li>
<li>initialization <img data-src="/img/NLP/chart-initialize.webp"><ul>
<li>for each rule A → W, if A is a category that can span a chart (typically S), add &lt;0, 0, A → •W&gt; <img data-src="/img/NLP/chartchart-init.webp"><ul>
<li>A implies •W from position 0 to 0</li>
</ul>
</li>
</ul>
</li>
<li>Housekeeping<ul>
<li>prevent duplicate rules</li>
</ul>
</li>
</ul>
<p>Fundamental rule  </p>
<ul>
<li>If the chart contains &lt;i, j, A → W1 •B W2&gt; and &lt;j, k, B → W3 •&gt;, then add edge &lt;i, k, A → W1 B •W2&gt; to the chart <img data-src="/img/NLP/chart-fund.webp"></li>
<li>Notes<ol>
<li>New edge may be either active or inactive</li>
<li>does not remove the active edge that has succeeded</li>
</ol>
</li>
</ul>
<p>Bottom-up rule  </p>
<ul>
<li>if adding edge &lt;i, j, C → W1 •&gt; to the chart, then for every rule that has the form B → C W2, add &lt;i, i, B → • C W2&gt; <img data-src="/img/NLP/chart-bottom.webp"></li>
</ul>
<p>Top-down rule   </p>
<ul>
<li>If adding edge &lt;i, j, C → W1 •B W2&gt; to the chart, then for each rule B → W, add &lt; j, j, B →•W&gt;</li>
</ul>
<h3 id="Full-Syntactic-Parsing"><a href="#Full-Syntactic-Parsing" class="headerlink" title="Full Syntactic Parsing"></a>Full Syntactic Parsing</h3><ul>
<li>necessary for deep semantic analysis of texts</li>
<li>not practical for many applications (given typical resources)<ul>
<li>O(n^3) for straight parsing</li>
<li>O(n^5) for probabilistic versions</li>
<li>Too slow for real time applications (search engines)</li>
</ul>
</li>
<li>Two Alternatives<ul>
<li>Dependency parsing<ul>
<li>Change the underlying grammar formalism</li>
<li>can get a lot done with just binary relations among the words</li>
<li>詳見Chap08 dependency grammar</li>
</ul>
</li>
<li>Partial parsing<ul>
<li>Approximate phrase-structure parsing with finite-state and statistical approaches</li>
</ul>
</li>
<li>Both of these approaches give up something (syntactic, structure) in return for more robust and efficient parsing</li>
</ul>
</li>
</ul>
<p>Partial parsing</p>
<ul>
<li>For many applications you don’t really need full parse</li>
<li>For example, if you’re interested in locating all the people, places and organizations  <ul>
<li>base-NP chunking <ul>
<li>[NP The morning flight] from [NP Denvar] has arrived</li>
</ul>
</li>
</ul>
</li>
<li>Two approaches<ul>
<li>Rule-based (hierarchical) transduction(轉導) <!--???--><ul>
<li>Restrict recursive rules (make the rules flat)<ul>
<li>like NP → NP VP</li>
</ul>
</li>
<li>Group the rules so that RHS of the rules can refer to non-terminals introduced in earlier transducers, but not later ones</li>
<li>Combine the rules in a group in the same way we did with the rules for spelling changes</li>
<li>Combine the groups into a cascade<ul>
<li>can be used to find the sequence of flat chunks you’re interested in</li>
<li>or approximate hierarchical trees you get from full parsing with a CFG</li>
</ul>
</li>
<li>Typical Architecture ![](&#x2F;img&#x2F;NLP&#x2F;Cascaded Transducers.webp)<ul>
<li>Phase 1: Part of speech tags</li>
<li>Phase 2: Base syntactic phrases</li>
<li>Phase 3: Larger verb and noun groups</li>
<li>Phase 4: Sentential level rules</li>
</ul>
</li>
</ul>
</li>
<li>Statistical sequence labeling<ul>
<li>HMMs</li>
<li>MEMMs</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Chap10-Statistical-Parsing"><a href="#Chap10-Statistical-Parsing" class="headerlink" title="Chap10 Statistical Parsing"></a>Chap10 Statistical Parsing</h2><p>Motivation  </p>
<ul>
<li>N-gram models and HMM Tagging only allowed us to process sentences linearly</li>
<li><strong>Probabilistic Context Free Grammars</strong>(PCFG)<ul>
<li>alias: Stochastic context-free grammar(SCFG)</li>
<li>simplest and most natural probabilistic model for tree structures</li>
<li>closely related to those for HMMs</li>
<li>為每一個CFG的規則標示其發生的可能性</li>
</ul>
</li>
</ul>
<p>Idea  </p>
<ul>
<li>reduce “right” parse to “most probable parse”<ul>
<li>Argmax P(Parse|Sentence)</li>
</ul>
</li>
</ul>
<p>A PCFG consists of  </p>
<ul>
<li>set of terminals, {wk}</li>
<li>set of nonterminals, {Ni}</li>
<li>start symbol N1</li>
<li>set of rules<ul>
<li>{Ni –&gt; ξj}(ξj is a sequence of terminals and nonterminals)</li>
</ul>
</li>
<li>probabilities of rules<ul>
<li>total probability of imply Ni to other sequence ξj is 1 </li>
<li>∀i Σj P(Ni → ξj) &#x3D; 1</li>
</ul>
</li>
<li>Probability of sentence according to grammar G <ul>
<li>P($w_{1m}$) &#x3D; sum of P($w_{1m}$, t) for every possible tree t</li>
</ul>
</li>
<li>Nj dominates the words wa … wb<ul>
<li>Nj → wa … wb</li>
</ul>
</li>
</ul>
<p>Assumptions of the Model  </p>
<ul>
<li>Place Invariance<ul>
<li>probability of a subtree does not depend on its position in the string</li>
<li>similar to time invariance in HMMs</li>
</ul>
</li>
<li>Ancestor Free<ul>
<li>probability of a subtree does not depend on nodes in the derivation outside the subtree(subtree的機率只和subtree內的node有關)</li>
<li>can simplify probability calculation <img data-src="/img/NLP/after-assump.webp"></li>
</ul>
</li>
</ul>
<p>Questions of PCFGs(similar to three questions of HMM)    </p>
<ul>
<li>Assign probabilities to parse trees<ul>
<li>What is the probability of a sentence $w_{1m}$ according to a grammar G<ul>
<li>P(w1m|G)</li>
</ul>
</li>
</ul>
</li>
<li>Parsing with probabilities(Decoding)<ul>
<li>What is the most likely parse for a sentence<ul>
<li>argmax_t P(t|w1m,G) </li>
<li>How to efficiently find the best (or N best) trees</li>
</ul>
</li>
</ul>
</li>
<li>Training the model (Learning) <ul>
<li>How to set rule probabilities(parameter of grammar model) that maximize the probability of a sentence<ul>
<li>argmax_G P(w1m|G)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Simple Probability Model  </p>
<ul>
<li>probability of a tree is the product of the probabilities of rules in derivation</li>
<li>Rule Probabilities<ul>
<li>S → NP </li>
<li>P(NP | S)</li>
</ul>
</li>
<li>Training the Model<ul>
<li>estimate probability from data</li>
<li>P(α → β | α) &#x3D; Count(α→β) &#x2F; Count(α) &#x3D; Count(α→β) &#x2F; Σγ Count(α→γ)</li>
</ul>
</li>
<li>Parsing (Decoding)<ul>
<li>trees with highest probability in the model</li>
</ul>
</li>
<li>Example: Book the dinner flight<ul>
<li><img data-src="/img/NLP/pm-ex.webp"></li>
<li><img data-src="/img/NLP/pm-ex2.webp"></li>
<li>too slow!</li>
</ul>
</li>
</ul>
<p>Dynamic Programming again  </p>
<ul>
<li>use CKY and Earley to <strong>parse</strong></li>
<li>Viterbi and HMMs to <strong>get the best parse</strong></li>
<li>Parameters of a PCFG in Chomsky Normal Form<ul>
<li>P(Nj→NrNs | G) , $n^3$ matrix of parameters</li>
<li>P(Nj→wk | G), $nV$ parameters</li>
<li>n is the number of nonterminals </li>
<li>V is the number of terminals</li>
</ul>
</li>
<li>Σr,s P(Nj→NrNs) + ΣkP(Nj→wk) &#x3D; 1<ul>
<li>所有由Nj導出的rule，機率總和必為1</li>
</ul>
</li>
</ul>
<p>Probabilistic Regular Grammars (PRG)    </p>
<ul>
<li>start state N1 </li>
<li>rules<ul>
<li>Ni → wjNk</li>
<li>Ni → wj</li>
</ul>
</li>
<li>PRG is a HMM with [start state] and [finish(sink) state] <img data-src="/img/NLP/prg-sink.webp"></li>
</ul>
<p>Inside and Outside probability <img data-src="/img/NLP/prg-graph.webp"> <img data-src="/img/NLP/prg-bf.webp"> <img data-src="/img/NLP/prg-bf2.webp">  </p>
<ul>
<li>Forward(Outside) probability<ul>
<li>$ α_i(t) &#x3D; P(w_{1(t-1)}, X_t &#x3D; i)$</li>
<li>everything above a certain node(include the node)</li>
</ul>
</li>
<li>Backward(Inside) probability<ul>
<li>$ β_i(t, T) &#x3D; P(w_{tT} | X_t &#x3D; i)$</li>
<li>everything below a certain node</li>
<li>total probability of generating words $w_t \cdots w_T$, given the root nonterminal $N^i$ and a grammar G</li>
</ul>
</li>
</ul>
<p>Inside Algorithm (bottom-up)      </p>
<ul>
<li>$P(w_{1m} | G) &#x3D; P(N_1 → w_{1m} | G) &#x3D; P(w_{1m} | N^1_{1m}, G) &#x3D; B_1(1,m)$<ul>
<li>$B_1(1,m)$ is Inside probability<ul>
<li>P(w1~wm are below N1(start symbol))</li>
</ul>
</li>
</ul>
</li>
<li>base rule<ul>
<li>$ B_j(k, k) &#x3D; P(w_k | N^j_{kk}, G) &#x3D; P(N^j → w_k | G)$</li>
</ul>
</li>
<li>$ B_j(p, q) &#x3D; P(w_{pq} | N^j_{pq}, G) &#x3D; $ <img data-src="/img/NLP/inside-induction.webp"><ul>
<li>try every possible rules to split Nj, product of **rule probabilty and segments’ inside probabilities **</li>
</ul>
</li>
<li>use grid to solve again<ul>
<li><img data-src="/img/NLP/inside-grid.webp"><ul>
<li>X軸代表起始座標，Y軸代表長度<ul>
<li>(2,3) → flies like ants</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Outside Algorithm (top-down)     </p>
<ul>
<li>$ P(w_{1m} | G) &#x3D; Σ_j α_j(k, k)P(N^j → w_k$ <img data-src="/img/NLP/outside-graph.webp"> <!--為何是sum...--><ul>
<li>outside probability of wk x (inside) probability of wk  of every Nj</li>
</ul>
</li>
<li>basecase <ul>
<li>$ α_1(1, m) &#x3D; 1, α_j(1,m) &#x3D; $</li>
<li>P(N1) &#x3D; 1, P(Nj outside w1 to wm) &#x3D; 0</li>
</ul>
</li>
<li>自己的outside probability 等於 <ul>
<li>爸爸的outside probability 乘以 爸爸的inside probability 除以 自己的inside probability<ul>
<li>inside x outside 是固定值？</li>
</ul>
</li>
<li>爸爸的inside probabiliity 除以 自己的inside probability 就是其兄弟的inside probability</li>
<li>使用此公式計算 <img data-src="/img/NLP/inout.webp"></li>
</ul>
</li>
<li>$ α_j(p, q)β_j(p, q) &#x3D; P(w_{1m}, N^j_{pq} | G) $<ul>
<li>某個點的inside 乘 outside &#x3D; 在某grammar中，出現此句子，且包含此點的機率 </li>
<li>所有點的總和：在某grammar下，某parse tree(包含所有node)的機率 <img data-src="/img/NLP/parse-probability.webp"></li>
</ul>
</li>
<li>Outside example: 這些數字理論上算起來會一樣… <img data-src="/img/NLP/outside-forward.webp"></li>
</ul>
<p>Finding the Most Likely Parse for a Sentence     </p>
<ul>
<li>δi(p,q)&#x3D; the highest inside probability parse of a subtree $N_{pq}^i$</li>
<li>Initialization <ul>
<li>δi(p,p)&#x3D; P(Ni → wp)</li>
</ul>
</li>
<li>Induction and Store backtrace<ul>
<li>δi(p,q)&#x3D; $argmax(j,k,r)P(Ni→NjNk)δj(p,r)δk(r+1,q)$</li>
<li>找所有可能的切法</li>
</ul>
</li>
<li>Termination<ul>
<li>answer &#x3D; δ1(1,m)</li>
</ul>
</li>
</ul>
<p>Training a PCFG</p>
<ul>
<li>find the optimal probabilities among grammar rules</li>
<li>use EM Training Algorithm to seek the grammar that maximizes the likelihood of the training data<ul>
<li>Inside-Outside Algorithm</li>
</ul>
</li>
<li><img data-src="/img/NLP/inoutagain.webp"></li>
<li>將產生句子的機率視為π，為Nj產生pq的機率 <img data-src="/img/NLP/pi.webp"></li>
<li>Nj被使用的機率 <img data-src="/img/NLP/pi2.webp"></li>
<li>Nj被使用，且Nj→NrNs的機率 <img data-src="/img/NLP/pi3.webp"></li>
<li>Nj→NrNs這條rule被使用的機率&#x3D;前兩式相除 <img data-src="/img/NLP/pi4.webp"></li>
<li>Nj→wk <img data-src="/img/NLP/pi5.webp"><ul>
<li>僅分子差異 <img data-src="/img/NLP/pi6.webp"></li>
</ul>
</li>
</ul>
<p>Problems with the Inside-Outside Algorithm    </p>
<ul>
<li>Extremely Slow<ul>
<li>For each sentence, each iteration of training is $O(m^3n^3)$</li>
</ul>
</li>
<li>Local Maxima</li>
<li>Satisfactory learning requires many more nonterminals than are theoretically needed to describe the language</li>
<li>There is no guarantee that the learned nonterminals will be linguistically motivated</li>
</ul>
<h2 id="Chap11-Dependency-Parsing"><a href="#Chap11-Dependency-Parsing" class="headerlink" title="Chap11 Dependency Parsing"></a>Chap11 Dependency Parsing</h2><p><span class="exturl" data-url="aHR0cDovL3N0cC5saW5nZmlsLnV1LnNlL35uaXZyZS9kb2NzL0FDTHNsaWRlcy5wZGY=">COLING-ACL 2006, Dependency Parsing, by Joachim Nivre and Sandra Kuebler<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL25hYWNsaGx0MjAxMC5pc2kuZWR1L3R1dG9yaWFscy90Ny1zbGlkZXMucGRm">NAACL 2010, Recent Advances in Dependency Parsing, by Qin Iris. Wang and YueZhang<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3NpdGUvemhlbmdodWFubHAvcHVibGljYXRpb25zL0lKQ05MUDIwMTMtdHV0b3JpYWwtRFAucGRmP2F0dHJlZGlyZWN0cz0wJmQ9MQ==">IJCNLP 2013, Dependency Parsing: Past, Present, and Future, by Zhenghua Li, Wenliang Chen, Min Zhang<i class="fa fa-external-link-alt"></i></span></p>
<p>Dependency Structure vs. Constituency Structure <img data-src="/img/NLP/parse.webp"><br>Parsing is one way to deal with the ambiguity problem in<br>natural language<br>dependency syntax is syntactic relations (dependencies) </p>
<p>Constraint: between word pairs  <img data-src="/img/NLP/depend.webp"><br>    Projective: No crossing links(a word and its dependents form a contiguous substring of the sentence)<br>    An arc (wi , r ,wj ) ∈ A is projective iff wi →∗ wk for all:<br>    i &lt; k &lt; j when i &lt; j<br>    j &lt; k &lt; i when j &lt; i<br>    射出去的那一方也可以射到兩個字中間的任何一字<br><img data-src="/img/NLP/depend-ex.webp"></p>
<p>Non-projective Dependency Trees  </p>
<ul>
<li>Long-distance dependencies  </li>
<li>With crossing links</li>
<li>Not so frequent in English<ul>
<li>All the dependency trees from Penn Treebank are projective</li>
</ul>
</li>
<li>Common in other languages with free word order<ul>
<li>Prague(23%) and Czech, German and Dutch</li>
</ul>
</li>
</ul>
<p>Data Driven Dependency Parsing  </p>
<ul>
<li>Data-driven parsing<ul>
<li>No grammar &#x2F; rules needed</li>
<li>Parsing decisions are made based on learned models</li>
<li>deal with ambiguities well</li>
<li><img data-src="/img/NLP/data-driven.webp"></li>
</ul>
</li>
<li>Three approaches<ul>
<li>Graph-based models</li>
<li>Transition-based models(good in practice)<ul>
<li>Define a transition system for <strong>mapping a sentence to its dependency tree</strong></li>
<li>Predefine some transition actions</li>
<li>Learning: predicting the next state transition, by transition history</li>
<li>Parsing: construct the optimal transition sequence</li>
<li>Greedy search &#x2F; beam search</li>
<li>Features are defined over a richer parsing history</li>
</ul>
</li>
<li>Hybrid models</li>
</ul>
</li>
</ul>
<p>Comparison   </p>
<ul>
<li>Graph-based models<ul>
<li>Find the optimal tree from all the possible ones</li>
<li>Global, exhaustive</li>
</ul>
</li>
<li>Transition-based models<ul>
<li>Predefine some actions (shift and reduce)</li>
<li>use stack to hold partially built parses</li>
<li><strong>Find the optimal action sequence</strong></li>
<li>Local, Greedy or beam search</li>
</ul>
</li>
<li>The two models produce different types of errors</li>
</ul>
<p>Hybrid Models  </p>
<ul>
<li>Three integration methods<ul>
<li>Ensemble approach: parsing time integration (Sagae &amp; Lavie 2006)</li>
<li>Feature-based integration (Nivre &amp; Mcdonald 2008)</li>
<li>Single model combination (Zhang &amp; Clark 2008)</li>
</ul>
</li>
<li>Gain benefits from both models</li>
</ul>
<p><img data-src="/img/NLP/parse-algo.webp"></p>
<h3 id="Graph-based-dependency-parsing-models"><a href="#Graph-based-dependency-parsing-models" class="headerlink" title="Graph-based dependency parsing models"></a>Graph-based dependency parsing models</h3><ul>
<li>Search for a tree with the highest score</li>
<li>Define search space<ul>
<li>Exhaustive search</li>
<li>Features are defined over a limited parsing history</li>
</ul>
</li>
<li>The score is linear combination of features <ul>
<li>What features we can use? (later)</li>
<li>What learning approaches can lead us to find the best tree with the highest score (later)</li>
</ul>
</li>
<li>Applicable to both probabilistic and nonprobabilistic models</li>
</ul>
<p>Features  </p>
<ul>
<li>dynamic features<ul>
<li>Take into account the link labels of the surrounding word-pairs when predicting the label of current pair</li>
<li>Commonly used in sequential labeling</li>
<li>A word’s children are generated first(先生child, 再找parent), before it modifies another word</li>
</ul>
</li>
</ul>
<p>Learning Approaches   </p>
<ul>
<li>Local learning approaches<ul>
<li>Learn a local link classifier given of features defined on training data</li>
<li>example <img data-src="/img/NLP/local-feature-example.webp"><ul>
<li>3-class classification: No link, left link or right link</li>
<li>Efficient O(n) local training</li>
</ul>
</li>
<li>local training and parsing <img data-src="/img/NLP/local-train-with-parse.webp"></li>
<li>Learn the weights of features<ul>
<li>Maximum entropy models (Ratnaparkhi 99, Charniak 00)</li>
<li>Support vector machines (Yamada &amp; Matsumoto 03)</li>
<li>Use a richer feature set!</li>
</ul>
</li>
</ul>
</li>
<li>Global learning approaches</li>
<li>Unsupervised&#x2F;Semi-supervised learning approaches<ul>
<li>Use both annotated training data and un-annotated raw text</li>
</ul>
</li>
</ul>
<h3 id="Transition-based-model"><a href="#Transition-based-model" class="headerlink" title="Transition-based model"></a>Transition-based model</h3><ul>
<li>Stack holds partially built parses</li>
<li>Queue holds unprocessed words</li>
<li>Actions<ul>
<li>use input words to build output parse</li>
</ul>
</li>
</ul>
<h4 id="parsing-processes"><a href="#parsing-processes" class="headerlink" title="parsing processes"></a>parsing processes</h4><p>Arc-eager parser  </p>
<ul>
<li>4 tranition actions<ul>
<li>SHIFT: push stack</li>
<li>REDUCE: pop stack</li>
<li>ARC-LEFT: pop stack and add link</li>
<li>ARC-RIGHT: push stack and add link</li>
</ul>
</li>
<li><img data-src="/img/NLP/arc-eager-example.webp"></li>
<li>Time complexity: linear<ul>
<li>every word will be pushed once and popped once(except root)</li>
</ul>
</li>
<li>parse<ul>
<li>by actions: arcleft → arclect subject, noun, …</li>
</ul>
</li>
</ul>
<p>Arc-standard parser  </p>
<ul>
<li>3 actions<ul>
<li>SHIFT: push</li>
<li>LEFT: pop leftmost stack element and add</li>
<li>RIGHT: pop rightmost stack element and add</li>
</ul>
</li>
<li>Also linear time</li>
</ul>
<p>Non-projectivity  </p>
<ul>
<li>neither of parser can solve it<ul>
<li>online reorder<ul>
<li>add extra action: swap</li>
<li>not linear: $N^2$, but expect to belinear</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Decoding-algorithms"><a href="#Decoding-algorithms" class="headerlink" title="Decoding algorithms"></a>Decoding algorithms</h4><p>search action sequence to build the parse<br>scoring action given context<br>Candidate item &lt;S, G, Q&gt;</p>
<ul>
<li>greedy local search<ul>
<li>initialize: Q &#x3D; input</li>
<li>goal: S&#x3D;[root], G&#x3D;tree, Q&#x3D;[]</li>
</ul>
</li>
<li>problem: one error leads to incorrect parse<ul>
<li>Beam search: keep N highest partial states<ul>
<li>use total score of all actions to rank a parse</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Score-Models"><a href="#Score-Models" class="headerlink" title="Score Models"></a>Score Models</h4><ul>
<li>linear model</li>
<li>SVM</li>
</ul>
<h2 id="Chap12-Semantic-Representation-and-Computational-Semantics"><a href="#Chap12-Semantic-Representation-and-Computational-Semantics" class="headerlink" title="Chap12 Semantic Representation and Computational Semantics"></a>Chap12 Semantic Representation and Computational Semantics</h2><p>Semantic aren’t primarily descriptions of inputs</p>
<p>Semantic Processing  </p>
<ul>
<li>reason about the truth</li>
<li>answer questions based on content<ul>
<li>Touchstone application is often question answering</li>
</ul>
</li>
<li>inference to determine the truth that isn’t actually know</li>
</ul>
<p>Method    </p>
<ul>
<li>principled, theoretically motivated approach<ul>
<li>Computational&#x2F;Compositional Semantics</li>
</ul>
</li>
<li>limited, practical approaches that have some hope of being useful<ul>
<li>Information extraction</li>
</ul>
</li>
</ul>
<h3 id="Information-Extraction"><a href="#Information-Extraction" class="headerlink" title="Information Extraction"></a>Information Extraction</h3><p>Information Extraction &#x3D; segmentation + classification +  association + clustering <img data-src="/img/NLP/IE.webp"></p>
<ul>
<li>superficial analysis <ul>
<li>pulls out only the entities, relations and roles related to consuming application</li>
</ul>
</li>
<li>Similar to chunking</li>
</ul>
<h3 id="Compositional-Semantics"><a href="#Compositional-Semantics" class="headerlink" title="Compositional Semantics"></a>Compositional Semantics</h3><ul>
<li>Use First-Order Logic(FOL) representation that accounts for all the entities, roles and relations present in a sentence</li>
<li>Similar to our approach to full parsing</li>
<li>Compositional: The meaning of a whole is derived from the meanings of the parts(syntatic) <img data-src="/img/NLP/syntax-semantic.webp"></li>
<li>Syntax-Driven Semantic Analysis<ul>
<li>The composition of meaning representations is guided by the <strong>syntactic</strong> components and relations provided by the  grammars</li>
</ul>
</li>
</ul>
<h4 id="FOL"><a href="#FOL" class="headerlink" title="FOL"></a>FOL</h4><ul>
<li>allow to answer yes&#x2F;no questions</li>
<li>allow variable</li>
<li>allow inference</li>
</ul>
<p>Events, actions and relationships can be captured with representations that consist of predicates with arguments  </p>
<ul>
<li>Predicates<ul>
<li>Primarily Verbs, VPs, Sentences</li>
<li>Verbs introduce&#x2F;refer to events and processes</li>
</ul>
</li>
<li>Arguments <ul>
<li>Primarily Nouns, Nominals, NPs, PPs</li>
<li>Nouns introduce the things that play roles in those events</li>
</ul>
</li>
<li>Example: Mary gave a list to John <ul>
<li>Giving(Mary, John, List)</li>
<li>Gave: Predicate</li>
<li>Mary, John, List: Argument</li>
<li>better representation <img data-src="/img/NLP/FOL-better.webp"></li>
</ul>
</li>
<li>Lambda Forms<ul>
<li>Allow variables to be bound</li>
<li>λxP(x)(Sally) &#x3D; P(Sally)</li>
</ul>
</li>
</ul>
<p>Ambiguation  </p>
<ul>
<li>mismatch between syntax and semantics<ul>
<li>displaced arguments</li>
<li>complex NPs with quantifiers<ul>
<li>A menu</li>
<li>Every restaurant <img data-src="/img/NLP/complicate-NP.webp"></li>
<li>Not every waiter</li>
<li>Most restaurants</li>
<li><img data-src="/img/NLP/complicate-NP-induction.webp"></li>
</ul>
</li>
<li>still preserving strict compositionality</li>
</ul>
</li>
<li>Two (syntax) rules to revise<ul>
<li>The S rule<ul>
<li>S → NP VP, NP.Sem(VP.Sem)</li>
<li>NP and VP swapped, because S is NP</li>
</ul>
</li>
<li>Simple NP’s like proper nouns<ul>
<li>λx.Franco(x)</li>
</ul>
</li>
</ul>
</li>
<li>Store and Retrieve  <ul>
<li><img data-src="/img/NLP/ambiguity-of-same-POS.webp"></li>
<li>Retrieving the quantifiers one at a time and placing them in front</li>
<li>The order determines the meaning <img data-src="/img/NLP/store.webp"></li>
<li>retrieve <img data-src="/img/NLP/retrieve.webp"></li>
</ul>
</li>
</ul>
<h3 id="Set-Based-Models"><a href="#Set-Based-Models" class="headerlink" title="Set-Based Models"></a>Set-Based Models</h3><ul>
<li>domain: the set of elements</li>
<li>entity: elements of domain</li>
<li>Properties of the elements: sets of elements from the domain</li>
<li>Relations: sets of tuples of elements from the domain</li>
<li>FOL<ul>
<li>FOL Terms → elements of the domain<ul>
<li>Med -&gt; “f”</li>
</ul>
</li>
<li>FOL atomic formula → sets, or sets of tuples<ul>
<li>Noisy(Med) is true if “f is in the set of elements that corresponds to the noisy relation</li>
<li>Near(Med, Rio) is true if “the tuple &lt;f,g&gt; is in the set of tuples that corresponds to “Near” in the interpretation</li>
</ul>
</li>
</ul>
</li>
<li>Example: Everyone likes a noisy restaurant <img data-src="/img/NLP/set-based-model.webp"><ul>
<li>There is a particular restaurant out there; it’s a noisy place; everybody likes it 有一家吵雜的餐廳大家都喜歡</li>
<li>Everybody has at least one noisy restaurant that they like 大家都喜歡一家吵雜的餐廳</li>
<li>Everybody likes noisy restaurants (i.e., there is no noisy restaurant out there that is disliked by anyone) 大家都喜歡吵雜的餐廳</li>
<li>Using predicates to create <strong>categories</strong> of concepts <ul>
<li>people and restaurants</li>
<li>basis for OWL (Web Ontology Language)網絡本體語言</li>
</ul>
</li>
<li>before <img data-src="/img/NLP/uncategories.webp"></li>
<li>after <img data-src="/img/NLP/categories.webp"></li>
</ul>
</li>
</ul>
<h2 id="Chap13-Lexical-Semantics"><a href="#Chap13-Lexical-Semantics" class="headerlink" title="Chap13 Lexical Semantics"></a>Chap13 Lexical Semantics</h2><p>we didn’t do word meaning in compositional semantics</p>
<p>WordNet  </p>
<ul>
<li>meaning and relationship about words<ul>
<li>hypernym(上位詞)<ul>
<li>breakfast → meal</li>
</ul>
</li>
<li>hierarchies <img data-src="/img/NLP/wordnet-hierarchy.webp"></li>
</ul>
</li>
</ul>
<p>In our semantics examples, we used various FOL predicates to capture various aspects of events, including the notion of roles<br>Havers, takers, givers, servers, etc.</p>
<p>Thematic roles(語義關係) <img data-src="/img/NLP/thematic-roles.webp"></p>
<ul>
<li>semantic generalizations over the specific roles that occur with specific verbs<ul>
<li>provide a shallow level of semantic analysis</li>
<li>tied to syntactic analysis</li>
</ul>
</li>
<li>i.e. Takers, givers, eaters, makers, doers, killers<ul>
<li>They’re all the agents of the actions</li>
</ul>
</li>
<li>AGENTS are often subjects</li>
<li>In a VP-&gt;V NP rule, the NP is often a THEME</li>
</ul>
<p>2 major English resources using thematic data</p>
<ul>
<li>PropBank<ul>
<li>Layered on the Penn TreeBank</li>
<li>Small number (25ish) labels</li>
</ul>
</li>
<li>FrameNet<ul>
<li>Based on frame semantics</li>
<li>Large number of frame-specific labels</li>
</ul>
</li>
</ul>
<p>Example  </p>
<ul>
<li>[McAdams and crew] covered [the floors] with [checked linoleum].格子花紋油毯<ul>
<li>Arg0 (agent: the causer of the smearing)</li>
<li>Arg1 (theme: “thing covered”)</li>
<li>Arg2 (covering: “stuff being smeared”)</li>
</ul>
</li>
<li>including agent and theme, remaining args are verb specific</li>
</ul>
<p>Logical Statements  </p>
<ul>
<li>Example: EAT – Eating(e) ^Agent(e,x)^ Theme(e,y)^Food(y)<ul>
<li>(adding in all the right quantifiers and lambdas)</li>
</ul>
</li>
<li>Use WordNet to encode the selection restrictions</li>
<li>Unfortunately, language is creative<ul>
<li>… ate glass on an empty stomach accompanied only by water and tea</li>
<li>you <strong>can’t eat gold</strong> for lunch if you’re hungry</li>
</ul>
</li>
</ul>
<p>can we discover a verb’s restrictions by using a corpus and WordNet?    </p>
<ol>
<li>Parse sentences and find heads</li>
<li>Label the thematic roles</li>
<li>Collect statistics on the co-occurrence of particular headwords with particular thematic roles</li>
<li>Use the WordNet hypernym structure to <strong>find the most meaningful level to use as a restriction</strong></li>
</ol>
<h3 id="WSD"><a href="#WSD" class="headerlink" title="WSD"></a>WSD</h3><p>Word sense disambiguation  </p>
<ul>
<li>select right sense for a word </li>
<li>Semantic selection restrictions can be used to disambiguate<ul>
<li>Ambiguous arguments to unambiguous predicates</li>
<li>Ambiguous predicates with unambiguous arguments</li>
</ul>
</li>
<li>Ambiguous arguments<ul>
<li>Prepare a dish(菜餚)</li>
<li>Wash a dish(盤子)</li>
</ul>
</li>
<li>Ambiguous predicates<ul>
<li>Serve (任職&#x2F;服務) Denver</li>
<li>Serve (供應) breakfast</li>
</ul>
</li>
</ul>
<p>Methodology   </p>
<ul>
<li>Supervised Disambiguation<ul>
<li>based on a labeled training set</li>
</ul>
</li>
<li>Dictionary-Based Disambiguation<ul>
<li>based on lexical resource like dictionaries</li>
</ul>
</li>
<li>Unsupervised Disambiguation<ul>
<li>label training data is expensive </li>
<li>based on unlabeled corpora</li>
</ul>
</li>
<li>Upper(human) and Lower(simple model) Bounds</li>
<li>Pseudoword<ul>
<li>Generate artificial evaluation data for comparison and improvement of text processing algorithms</li>
</ul>
</li>
</ul>
<p>Supervised ML Approaches  </p>
<ul>
<li>What’s a tag?<ul>
<li>In WordNet, “bass” in a text has 8 possible tags or labels (bass1 through bass8)</li>
</ul>
</li>
<li>require very simple representation for training data<ul>
<li>Vectors of sets of feature&#x2F;value pairs</li>
<li>need to extract training data by characterization of text surrounding the target</li>
</ul>
</li>
<li>If you decide to use features that require more analysis (say parse trees) then the ML part may be doing less work (relatively) if these features are truly informative</li>
<li>Classification<ul>
<li>Naïve Bayes (the right thing to try first)</li>
<li>Decision lists</li>
<li>Decision trees</li>
<li>MaxEnt</li>
<li>Support vector machines</li>
<li>Nearest neighbor methods…</li>
<li>choice of technique depends on features that have been used</li>
</ul>
</li>
<li>Bootstrapping<ul>
<li>Use when don’t have enough data to train a system…</li>
<li>集中有放回的均勻抽樣</li>
</ul>
</li>
</ul>
<h4 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h4><ul>
<li>Argmax P(sense|feature vector) <img data-src="/img/NLP/bayesian-decision.webp"> </li>
<li>find maximum probabilty of words given possible sk <img data-src="/img/NLP/bayesian-decision2.webp"></li>
<li><img data-src="/img/NLP/bayesian-classifier.webp"></li>
<li>assumption<ul>
<li>bag of words model<ul>
<li>structure and order of words is ignored</li>
<li>each pair of words in the bag is independent</li>
</ul>
</li>
</ul>
</li>
<li>73% correct</li>
</ul>
<h4 id="Dictionary-Based-Disambiguation"><a href="#Dictionary-Based-Disambiguation" class="headerlink" title="Dictionary-Based Disambiguation"></a>Dictionary-Based Disambiguation</h4><ol>
<li>Disambiguation based on sense definitions</li>
<li>Thesaurus-Based Disambiguation</li>
<li>Disambiguation based on translations in a second-language corpus</li>
</ol>
<p>sense definition</p>
<ul>
<li>find keywords in definition of a word<ul>
<li>cone<ul>
<li>… pollen-bearing scales or bracts in <strong>trees</strong></li>
<li>shape for holding <strong>ice cream</strong></li>
</ul>
</li>
<li>50%~70% accuracies</li>
<li>Alternatives<ul>
<li>Several iterations to determine correct sense</li>
<li>Combine the dictionary-based and thesaurus-based disambiguation</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU3JUI0JUEyJUU1JUJDJTk1JUU1JTg1JUI4">Thesaurus-Based(索引典)<i class="fa fa-external-link-alt"></i></span> Disambiguation    </p>
<ul>
<li>Category can determine which word senses are used</li>
<li>Each word is assigned one or more subject codes which correspond to its different meanings<ul>
<li>select the most often subject code</li>
<li>考慮w的context，有多少words的senses與w相同</li>
</ul>
</li>
<li>Walker’s Algorithm<ul>
<li>50% accuracy for “interest, point, power, state, and terms”</li>
</ul>
</li>
<li>Problems<ul>
<li>general topic categorization, e.g., mouse in computer</li>
<li>coverage, e.g., Navratilova</li>
</ul>
</li>
<li>Yarowsky’s Algorithm <img data-src="/img/NLP/yarowsky-algo.webp"> <img data-src="/img/NLP/yarowsky-algo2.webp"> <img data-src="/img/NLP/yarowsky-algo3.webp"><ul>
<li><ol>
<li>categorize sentences</li>
</ol>
</li>
<li><ol start="2">
<li>categorize words</li>
</ol>
</li>
<li><ol start="3">
<li>disambiguate by decision rule for Naïve Bayes</li>
</ol>
</li>
<li>result <img data-src="/img/NLP/yarowsky-result.webp"></li>
</ul>
</li>
</ul>
<p>Disambiguation based on translations in a second-language corpus  </p>
<ul>
<li>the word “interest” has two translations in German<ul>
<li>“Beteiligung” (legal share–50% a interest in the company)</li>
<li>“Interesse” (attention, concern–her interest in Mathematics)</li>
</ul>
</li>
<li>Example: … showed interest …<ul>
<li>Look up English-German dictionary, show → zeigen</li>
<li>Compute R(Interesse, zeigen) and R(Beteiligung, zeigen)</li>
<li>R(Interesse, zeigen) &gt; R(Beteiligung, zeigen)</li>
</ul>
</li>
</ul>
<h4 id="Unsupervised-Disambiguation"><a href="#Unsupervised-Disambiguation" class="headerlink" title="Unsupervised Disambiguation"></a>Unsupervised Disambiguation</h4><p>P(vj|sk) are estimated using the EM algorithm  </p>
<ol>
<li>Random initialization of P(vj|sk)(word)</li>
<li>For each context ci of w, compute P(ci|sk)(sentence)</li>
<li>Use P(ci|sk) as training data</li>
<li>Reestimate P(vj|sk)(word)</li>
</ol>
<p>Surface Representations(features)   </p>
<ul>
<li>Collocational<ul>
<li>words that appear in specific positions to the right and left of the target word</li>
<li>limited to the words themselves as well as part of speech</li>
<li>Example: guitar and bassplayer stand<ul>
<li>[guitar, NN, and, CJC, player, NN, stand, VVB]</li>
<li>In other words, a vector consisting of [position n word, position n part-of-speech…]</li>
</ul>
</li>
</ul>
</li>
<li>Co-occurrence<ul>
<li>words that occur regardless of position</li>
<li>Typically limited to frequency counts</li>
<li>Assume we’ve settled on a possible vocabulary of 12 words that includes guitarand playerbut not andand stand</li>
<li>Example: guitar and bassplayer stand<ul>
<li>Assume a 12-word sentence includes guitar and player but not “and” and stand</li>
<li>[0,0,0,1,0,0,0,0,0,1,0,0]</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Applications  </p>
<ul>
<li>tagging<ul>
<li>translation</li>
<li>information retrieval</li>
</ul>
</li>
</ul>
<p>different label  </p>
<ul>
<li>Generic thematic roles (aka case roles)<ul>
<li>Agent, instrument, source, goal, etc.</li>
</ul>
</li>
<li>Propbank labels<ul>
<li>Common set of labels ARG0-ARG4, ARGM</li>
<li>specific to verb semantics</li>
</ul>
</li>
<li>FrameNet frame elements<ul>
<li>Conceptual and frame-specific</li>
</ul>
</li>
<li>Example: [Ochocinco] bought [Burke] [a diamond ring]<ul>
<li>generic: Agent, Goal, Theme</li>
<li>propbank: ARG0, ARG2, ARG1</li>
<li>framenet: Customer, Recipe, Goods</li>
</ul>
</li>
</ul>
<p>Semantic Role Labeling  </p>
<ul>
<li>automatically identify and label thematic roles<ul>
<li>For each verb in a sentence<ul>
<li>For each constituent<ul>
<li>Decide if it is an argument to that verb</li>
<li>if it is an argument, determine what kind</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>feature<ul>
<li>from parse and lexical item</li>
<li>“path”</li>
</ul>
</li>
</ul>
<h3 id="Lexical-Acquisition"><a href="#Lexical-Acquisition" class="headerlink" title="Lexical Acquisition"></a>Lexical Acquisition</h3><ul>
<li>Verb Subcategorization<ul>
<li>the syntactic means by which verbs express their arguments</li>
</ul>
</li>
<li>Attachment Ambiguity<ul>
<li>The children ate the cake with their hands</li>
<li>The children ate the cake with blue icing</li>
</ul>
</li>
<li>SelectionalPreferences<ul>
<li>The semantic categorization of a verb’s arguments</li>
</ul>
</li>
<li>Semantic Similarity (refer to IR course)<ul>
<li>Semantic similarity between words</li>
</ul>
</li>
</ul>
<h4 id="Verb-Subcategorization"><a href="#Verb-Subcategorization" class="headerlink" title="Verb Subcategorization"></a>Verb Subcategorization</h4><p>a particular set of syntactic categories that a verb can appear with is called a <strong>subcategorization frame</strong> <img data-src="/img/NLP/subcategorization.webp"></p>
<p>Brent’s subcategorization frame learner  </p>
<ol>
<li>Cues: Define a regular pattern of words and syntactic categories<ol>
<li>ε: error rate of assigning frame f to verb v based on cue cj</li>
</ol>
</li>
<li>Hypothesis Testing: Define null hypothesis H0: “the frame is not appropriate for the verb” <ol>
<li>Reject this hypothesis if the cue cj indicates with high probability that our H0 is wrong</li>
</ol>
</li>
</ol>
<p>Example<br>Cues  </p>
<ul>
<li><p>regular pattern for subcategorization frame “NP NP”</p>
<ul>
<li>(OBJ | SUBJ_OBJ | CAP) (PUNC |CC)<br>Null hypothesis testing</li>
</ul>
</li>
<li><p>Verb vi occurs a total of n times in the corpus and there are m &lt; n occurrences with a cue for frame fj</p>
</li>
<li><p>Reject the null hypothesis H0 that vi does not accept fj with the following probability of error <img data-src="/img/NLP/brent-null-hypothesis.webp"></p>
</li>
<li><p>Brent’s system does well at precision, but not well at recall</p>
</li>
<li><p>Manning’s system</p>
<ul>
<li>solve this problem by using a tagger and running the cue detection on the output of the tagger</li>
<li>learn a lot of subcategorization frames, even those it is low-reliability</li>
<li>still low performance </li>
<li>improve : use prior knowledge</li>
</ul>
</li>
</ul>
<p>PCFG prefers to parse common construction  </p>
<ul>
<li>P(A|prep, verb, np1, np2, w) ~&#x3D; P(A|prep, verb, np1, np2)<ul>
<li>Do not count the word outside of frame</li>
<li>w: words outside of “verb np1(prep np2)”</li>
<li>A: random variable representing attachment decision</li>
<li>V(A): verb or np1</li>
<li>Counter example<ul>
<li>Fred saw a movie with Arnold Schwarzenegger</li>
</ul>
</li>
</ul>
</li>
<li>P(A|prep, verb, np1, np2, noun1, noun2) ~&#x3D; P(A|prep, verb, noun1, noun2)<ul>
<li>noun1 &#x3D; head of np1, noun2 &#x3D; head of np2</li>
<li>total parameters: $10^{13}$ &#x3D; #(prep) x #(verb) x #(noun) x #(noun)</li>
</ul>
</li>
<li>P(A&#x3D; noun | prep, verb, noun1) vs. P(A&#x3D; verb | prep, verb, noun1)<ul>
<li>compare probability to be verb and probability to be noun</li>
</ul>
</li>
</ul>
<p>Technique: Alternative to reduce parameters   </p>
<ul>
<li>Condition probabilities on fewer things</li>
<li>Condition probabilities on more general things</li>
</ul>
<p>The model asks the following questions  </p>
<ul>
<li>VAp: Is there a PP headed by p and following the verb v which attaches to v(VAp&#x3D;1) or not (VAp&#x3D;0)?</li>
<li>NAp: Is there a PP headed by p and following the noun n which attaches to n (NAp&#x3D;1) or not (NAp&#x3D;0)?</li>
<li>(1) Determine the attachment of a PP that is immediately following an object noun, i.e. compute the probability of NAp&#x3D;1</li>
<li>In order for the first PP headed by the preposition p to attach to the verb, both VAp&#x3D;1 and NAp&#x3D;0<ul>
<li>calculate likelihood ratio between V and N <img data-src="/img/NLP/likelihood-ratio-vn.webp"></li>
<li>maximum estimation<ul>
<li>P(VA &#x3D; 1 | v) &#x3D; C(v, p) &#x2F; C(v)</li>
<li>P(NA &#x3D; 1 | n) &#x3D; C(n, p) &#x2F; C(n)</li>
</ul>
</li>
</ul>
</li>
<li>Estimation of PP attachment counts<ul>
<li>Sure Noun Attach<ul>
<li>If a noun is followed by a PP but no preceding verb, increment C(prep attached to noun)</li>
</ul>
</li>
<li>Sure Verb Attach<ul>
<li>if a passive verb is followed by a PP other than a “by” phrase, increment C(prep attached to verb) </li>
<li>if a PP follows both a noun phrase and a verb but the noun phrase is a pronoun, increment C(prep attached to verb)</li>
</ul>
</li>
<li>Ambiguous Attach<ul>
<li>if a PP follows both a noun and a verb, see if the probabilities based on the attachment decided by previous way</li>
<li>otherwise increment both attachment counters by 0.5</li>
</ul>
</li>
<li><img data-src="/img/NLP/attach-example.webp"></li>
<li>Sparse data is a major cause of the difference between the human and program performance(attachment indeterminacy不確定性)</li>
</ul>
</li>
</ul>
<p>Using Semantic Information  </p>
<ul>
<li>condition on semantic tags of verb &amp; noun<ul>
<li>Sue bought a plant with Jane(human)</li>
<li>Sue bought a plant with yellow leaves(object)</li>
</ul>
</li>
</ul>
<p>Assumption<br>The noun phrase serves as the subject of the relative clause</p>
<ul>
<li>collect “ subject-verb” and “verb-object” pairs.(training part)  </li>
<li>compute t-score (testing part) <ul>
<li>t-score &gt; 0.10 (significant)</li>
</ul>
</li>
</ul>
<p>P (relative clause attaches to x | main verb of clause &#x3D;v) &gt; P (relative clause attaches to y | main verb of clause&#x3D;v)<br>↔ P (x&#x3D; subject&#x2F;object | v) &gt; P (y&#x3D; subject&#x2F; object|v)</p>
<p>Selectional Preferences  </p>
<ul>
<li>Most verbs prefer particular type of arguments<ul>
<li>eat → object (food item)</li>
<li>think → subject (people)</li>
<li>bark → subject (dog)</li>
</ul>
</li>
<li>Aspects of meaning of a word can be inferred<ul>
<li>Susan had never eaten a fresh <strong>durian</strong> before (food item)</li>
</ul>
</li>
<li>Selectional preferences can be used to rank different parses of a sentence</li>
<li>Selectional preference strength<ul>
<li>how strongly the verb constrains its direct object</li>
<li><img data-src="/img/NLP/selection-strength.webp"></li>
<li>KL divergence between the prior distribution of direct objects of general verb and the distribution of direct objects of specific verb</li>
<li>2 assumptions<ul>
<li>only the head noun of the object is considered</li>
<li>rather than dealing with individual nouns, we look at classes of nouns</li>
</ul>
</li>
</ul>
</li>
<li>Selectional association<ul>
<li>Selectional Association between a verb and a class is this class’s contribution to S(v) &#x2F; the overall preference strength S(v) <img data-src="/img/NLP/selectional-association.webp"></li>
<li>There is also a rule for assigning association strengths to nouns instead of noun classes<ul>
<li>If noun belongs to several classes, then its choose the highest association strength among all classes</li>
</ul>
</li>
<li>estimating the probability that a direct object in noun class c occurs given a verb v<ul>
<li>A(interrupt, chair) &#x3D; max(A(interrupt, people), A(interrupt, furniture)) &#x3D; A(interrupt, people)</li>
</ul>
</li>
</ul>
</li>
<li>Example <img data-src="/img/NLP/selectional-example.webp"><ul>
<li>eat prefers fooditem <ul>
<li>A(eat, food)&#x3D;1.08 → very specific</li>
</ul>
</li>
<li>seehas a uniform distribution<ul>
<li>A(see, people)&#x3D;A(see, furniture)&#x3D;A(see, food)&#x3D;A(see, action)&#x3D;0 → no selectional preference</li>
</ul>
</li>
<li>find disprefers action item<ul>
<li>A(find, action)&#x3D;-0.13 → less specific</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Semantic Similarity  </p>
<ul>
<li>assessing semantic similarity between a new word and other already known words</li>
<li>Vector Space vs Probabilistic</li>
<li>Vector Space<ul>
<li>Words can be expressed in different spaces: document space, word spaceand modifier space</li>
<li>Similarity measures for binary vectors: matching coefficient, Dice coefficient, Jaccard(or Tanimoto) coefficient, Overlap coefficientand cosine</li>
<li>Similarity measures for the real-valued vector space: cosine, Euclidean Distance, normalized correlation coefficient<ul>
<li>cosine assumes a Euclidean space which is not well-motivated when dealing with word counts</li>
</ul>
</li>
<li><img data-src="/img/NLP/similarity-measure.webp"></li>
</ul>
</li>
<li>Probabilistic Measures<ul>
<li>viewing word counts by representing them as probability distributions</li>
<li>compare two probability distributions using<ul>
<li>KL Divergence</li>
<li>Information Radius(Irad)</li>
<li>L1Norm</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Chap14-Computational-Discourse"><a href="#Chap14-Computational-Discourse" class="headerlink" title="Chap14 Computational Discourse"></a>Chap14 Computational Discourse</h2><table>
<thead>
<tr>
<th>Level</th>
<th>Well-formedness constraints</th>
<th>Types of ambiguity</th>
</tr>
</thead>
<tbody><tr>
<td>Lexical</td>
<td>Rules of inflection and derivation</td>
<td></td>
</tr>
<tr>
<td>structural, morpheme boundaries, morpheme identity</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Syntactic</td>
<td>Grammar rules</td>
<td>structural, POS</td>
</tr>
<tr>
<td>Semantic</td>
<td>Selection restrictions</td>
<td>word sense, quantifier scope</td>
</tr>
<tr>
<td><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU4JUFGJUFEJUU3JTk0JUE4JUU1JUFEJUE2">Pragmatic<i class="fa fa-external-link-alt"></i></span></td>
<td>conversation principles</td>
<td>pragmatic function</td>
</tr>
</tbody></table>
<p>Computational Discourse  </p>
<ul>
<li>Discourse(語篇)<ul>
<li>A group of sentences with the same coherence relation</li>
</ul>
</li>
<li>Coherence relation<ul>
<li>the 2nd sentence offers the reader an explaination or cause for the 1st sentence</li>
</ul>
</li>
<li>Entity-based Coherence<ul>
<li>relationships with the entities, introducing them and following them in a focused way</li>
<li>Discourse Segmentation<ul>
<li>Divide a document into a linear sequence of multiparagraph passages</li>
<li>Academic article<ul>
<li>Abstract</li>
<li>Introduction</li>
<li>Methodology</li>
<li>Results</li>
<li>Conclusion</li>
</ul>
</li>
<li><img data-src="http://www.wannabehacks.co.uk/images/Inverted_pyramid_in_comprehensive_form.webp" alt="Inverted Pyramid"></li>
<li>Applications<ul>
<li>News</li>
<li>Summarize different segments of a document</li>
<li>Extract information from inside a single discourse segment</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>TextTiling (Hearst,1997)  </p>
<ul>
<li>Tokenization<ul>
<li>Each space-delimited word in the input is converted to lower-case</li>
<li>Words in a stop list of function words are thrown out</li>
<li>The remaining words are morphologically stemmed</li>
<li>The stemmed words are grouped into pseudo-sentencesof length w &#x3D; 20</li>
</ul>
</li>
<li>Lexical score determination<ul>
<li>compute a lexical cohesion(結合) score between pseudo-sentences<ul>
<li>score: average similarity of words in the pseudo-sentences before gap to pseudo-sentences after the gap(??)</li>
</ul>
</li>
</ul>
</li>
<li>Boundary identification	<ul>
<li>Compute a depth score for each gap</li>
<li>Boundaries are assigned at any valley which is deeper than a cutoff</li>
</ul>
</li>
</ul>
<p>Coherence Relations  </p>
<ul>
<li>Result<ul>
<li>The Tin Woodman was caught in the rain. His joints rusted</li>
</ul>
</li>
<li>Explanation<ul>
<li>John hid Bill’s car keys. He was drunk</li>
</ul>
</li>
<li>Parallel<ul>
<li>The Scarecrow wanted some brains</li>
<li>The Tin Woodman wanted a heart</li>
</ul>
</li>
<li>Elaboration(詳細論述)<ul>
<li>Dorothy was from Kansas</li>
<li>She lived in the midst of the great Kansas prairies</li>
</ul>
</li>
<li>Occasion(起因)<ul>
<li>Dorothy picked up the oil-can</li>
<li>She oiled the Tin Woodman’s joints</li>
</ul>
</li>
</ul>
<p>Coherence Relation Assignment  </p>
<ul>
<li>Discourse parsing</li>
<li>Open problems</li>
</ul>
<p>Cue-Phrase-Based Algorithm  </p>
<ul>
<li><p>Using cue phrases</p>
<ul>
<li>Segment the text into discourse segments</li>
<li>Classify the relationship between each consecutive discourse</li>
</ul>
</li>
<li><p>Cue phrase</p>
<ul>
<li>connectives, which are often conjunctions or adverbs <ul>
<li>because, although, but, for example, yet, with, and</li>
</ul>
</li>
</ul>
</li>
<li><p>discourse uses vs. sentential uses</p>
<ul>
<li><strong>With</strong> its distant orbit, Mars exhibits frigid weather conditions. (因為長距離的運行軌道，火星天氣酷寒)</li>
<li>We can see Mars <strong>with</strong> an ordinary telescope</li>
</ul>
</li>
<li><p><img data-src="/img/NLP/discourse-relation.webp"></p>
</li>
<li><p>Temporal Relation  </p>
<ul>
<li>ordered in time (Asynchronous)<ul>
<li>before, after …</li>
</ul>
</li>
<li>overlapped (Synchronous)<ul>
<li>at the same time</li>
</ul>
</li>
</ul>
</li>
<li><p>Contingency Relation</p>
<ul>
<li>因果關係，附帶條件</li>
</ul>
</li>
<li><p>Comparison Relation</p>
<ul>
<li>difference between two arguments</li>
</ul>
</li>
<li><p>Expansion Relation</p>
<ul>
<li>expands the information for one argument in the other one or continues the narrative flow</li>
</ul>
</li>
<li><p>Implicit Relation</p>
<ul>
<li>Discourse marker is absent</li>
<li>颱風來襲，學校停止上課</li>
</ul>
</li>
<li><p>Chinese Relation Words <img data-src="/img/NLP/chinese-coherence-relation.webp"> </p>
<ul>
<li>Ambiguous Discourse Markers <ul>
<li>而：而且, 然而, 因而</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Reference-Resolution"><a href="#Reference-Resolution" class="headerlink" title="Reference Resolution"></a>Reference Resolution</h3><p><img data-src="/img/NLP/reference-resolution.webp">  </p>
<ul>
<li>Evoke<ul>
<li>When a referent is first mentioned in a discourse, we say that a representation for it is <strong>evoked into</strong> the model</li>
</ul>
</li>
<li>Access<ul>
<li>Upon subsequent mention, this representation is <strong>accessed from</strong> the model</li>
</ul>
</li>
</ul>
<p>Five Types of Referring Expressions  </p>
<ul>
<li>Indefinite Noun Phrases(不定名詞)<ul>
<li>marked with the determiner a, some, this …</li>
<li>Create a new internal symbol and add to the current world model<ul>
<li>Mayumi has bought a new automobile</li>
<li>automobile(g123)</li>
<li>new(g123)</li>
<li>owns(mayumi, g123)</li>
</ul>
</li>
<li>non-specific sense to describe an object<ul>
<li>Mayumi wantsto buy a new XJE</li>
</ul>
</li>
<li>whole classes of objects<ul>
<li>A new automobiletypically requires repair twice in the first 12 months</li>
</ul>
</li>
<li>collect one or more properties<ul>
<li>The Macho GTE XL is a new automobile</li>
</ul>
</li>
<li>Question and commands<ul>
<li>Is her automobile in a parking placenear the exit?</li>
<li>Put her automobile into a parking placenear the exit!</li>
</ul>
</li>
</ul>
</li>
<li>Definite Noun Phrases(定名詞)<ul>
<li>simple referential and generic uses(the same as indefinite)</li>
<li>indicate an individual by description that they satisfy<ul>
<li>The manufacturer <strong>of this automobile</strong> should be indicted</li>
</ul>
</li>
</ul>
</li>
<li>Pronouns(代名詞)<ul>
<li>reference backs to entities that have been introduced by previous nounphrases in a discourse</li>
<li>non-referential noun phrase<ul>
<li>non-exist object</li>
</ul>
</li>
<li>logical variable<ul>
<li>No male driveradmits that heis incompetent</li>
</ul>
</li>
<li>something that is available from the context of utterance, but has not been explicitly mentioned before<ul>
<li>Here they come, late again!</li>
<li>Can’t easily know who are “they”</li>
</ul>
</li>
<li>Anaphora<ul>
<li>Number Agreements<ul>
<li>John has a Ford Falcon. It is red</li>
<li>John has three Ford Falcons. They are red</li>
</ul>
</li>
<li>Person Agreement(人稱)</li>
<li>Gender Agreement</li>
<li>Selection Restrictions<ul>
<li>verb and its arguments</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Demonstratives (指示詞)<ul>
<li>this, that</li>
</ul>
</li>
<li>Names<ul>
<li>Full name &gt; long definite description &gt; short definite description &gt; last name&gt; first name &gt; distal demonstrative &gt; proximate demonstrative &gt; NP &gt; stressed pronoun &gt; unstressed pronoun</li>
</ul>
</li>
</ul>
<p>Information Status  </p>
<ul>
<li>Referential forms used to provide new or old information</li>
<li>givenness hierarchy <img data-src="/img/NLP/givenness-hierarchy.webp"></li>
<li>Definite-indefinite is a clue to given-new status<ul>
<li>The sales managere(given) employed a foreign distributor(new)</li>
</ul>
</li>
<li>If there are ambiguous noun phrases in a sentence, then it extracts the presuppositions to provide extra constraints</li>
<li>When some new information is added to knowledge base, check if it is consistent with what we already know</li>
</ul>
<p>Active model of understanding  </p>
<ul>
<li>Given a text, build up predictions or expectations about new information and actively compare these with successive input to resolve ambiguities</li>
<li>Construct a proof of the information provided in a sentence from the existing world knowledge and plausible inference rules illustrated</li>
<li>the inference are not sensitive to the order<ul>
<li>if the proposition that the disc is heavy is inferred, then it is not changed after the discourse has finished</li>
<li>Solution: describe the propositions in temporal order</li>
</ul>
</li>
<li>Script: encapsulate a sequence of actions that belong together into a script<pre class="language-none"><code class="language-none">automobile_buying:
&lt;&#123;customer(C), automobile(A), dealer(D), garage(G)&#125;,
	&lt;
		goes(C, G),
		test_drives(C, A),
		orders(C, A, D),
		delivers(D, A, C),
		drives(C, A)
	&gt;
&gt;</code></pre></li>
</ul>
<h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ul>
<li>HHChen 課堂講義</li>
</ul>

    </div>

    
    
    
      


    <footer class="post-footer">
          <div class="followme">
  <span>歡迎訂閱RSS</span>

  <div class="social-list">

      <div class="social-item">
        <a target="_blank" class="social-link" href="/atom.xml">
          <span class="icon">
            <i class="fa fa-rss"></i>
          </span>

          <span class="label">RSS</span>
        </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/" rel="tag"><i class="fa fa-tag"></i> 機器學習</a>
              <a href="/tags/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86/" rel="tag"><i class="fa fa-tag"></i> 自然語言處理</a>
              <a href="/tags/%E7%B5%B1%E8%A8%88/" rel="tag"><i class="fa fa-tag"></i> 統計</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/natural-language-processing/" rel="prev" title="自然語言處理(上)">
                  <i class="fa fa-chevron-left"></i> 自然語言處理(上)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/Emacs-tips/" rel="next" title="Emacs常用指令表">
                  Emacs常用指令表 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2014 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fab fa-free-code-camp"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">qwerty</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>總字數：</span>
    <span title="總字數">455k</span>
  </span>
</div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9waXNjZXMv">NexT.Pisces</span> 強力驅動
  </div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-61c5cf9cb3476405" async="async"></script>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/comments.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/motion.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/next-boot.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/pjax.min.js"></script>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.0/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/third-party/search/local-search.min.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"forest","dark":"forest"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.1.1/mermaid.min.js","integrity":"sha256-8L3O8tirFUa8Va4NSTAyIbHJeLd6OnlcxgupV9F77e0="}}</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/third-party/tags/mermaid.min.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/third-party/pace.min.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.1/es5/tex-mml-chtml.js","integrity":"sha256-hlC2uSQYTmPsrzGZTEQEg9PZ1a/+SV6VBCTclohf2og="}}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/third-party/math/mathjax.min.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"disqusforqwerty","count":false,"i18n":{"disqus":"disqus"}}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-next/8.12.1/third-party/comments/disqus.min.js"></script>

</body>
</html>
